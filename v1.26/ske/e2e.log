I0817 02:59:57.039957      18 e2e.go:126] Starting e2e run "ced1975d-86f1-47d4-89a0-25054b775345" on Ginkgo node 1
Aug 17 02:59:57.062: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692241196 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 17 02:59:57.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 02:59:57.214: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 17 02:59:57.239: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 17 02:59:57.275: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 17 02:59:57.275: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Aug 17 02:59:57.275: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'calico-node-windows' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-proxy' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node-win' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'dcgm-exporter' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-windows' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'windows-exporter' (0 seconds elapsed)
Aug 17 02:59:57.289: INFO: e2e test version: v1.26.7
Aug 17 02:59:57.292: INFO: kube-apiserver version: v1.26.7
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 17 02:59:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 02:59:57.297: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.084 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 17 02:59:57.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 02:59:57.214: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 17 02:59:57.239: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 17 02:59:57.275: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 17 02:59:57.275: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Aug 17 02:59:57.275: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'calico-node-windows' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-proxy' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node-win' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'dcgm-exporter' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-windows' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'windows-exporter' (0 seconds elapsed)
    Aug 17 02:59:57.289: INFO: e2e test version: v1.26.7
    Aug 17 02:59:57.292: INFO: kube-apiserver version: v1.26.7
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 17 02:59:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 02:59:57.297: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 02:59:57.322
Aug 17 02:59:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 02:59:57.323
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 02:59:57.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 02:59:57.342
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-d3f0c441-280f-4ed5-a4cf-713020019540 08/17/23 02:59:57.346
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 02:59:57.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9213" for this suite. 08/17/23 02:59:57.356
------------------------------
• [0.041 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 02:59:57.322
    Aug 17 02:59:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 02:59:57.323
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 02:59:57.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 02:59:57.342
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-d3f0c441-280f-4ed5-a4cf-713020019540 08/17/23 02:59:57.346
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 02:59:57.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9213" for this suite. 08/17/23 02:59:57.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 02:59:57.364
Aug 17 02:59:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 02:59:57.365
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 02:59:57.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 02:59:57.382
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 02:59:57.386
Aug 17 02:59:57.395: INFO: Waiting up to 5m0s for pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba" in namespace "emptydir-62" to be "Succeeded or Failed"
Aug 17 02:59:57.400: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488667ms
Aug 17 02:59:59.405: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009727991s
Aug 17 03:00:01.407: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.011918024s
Aug 17 03:00:03.406: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010828776s
STEP: Saw pod success 08/17/23 03:00:03.406
Aug 17 03:00:03.406: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba" satisfied condition "Succeeded or Failed"
Aug 17 03:00:03.417: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba container test-container: <nil>
STEP: delete the pod 08/17/23 03:00:03.433
Aug 17 03:00:03.447: INFO: Waiting for pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba to disappear
Aug 17 03:00:03.453: INFO: Pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-62" for this suite. 08/17/23 03:00:03.465
------------------------------
• [SLOW TEST] [6.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 02:59:57.364
    Aug 17 02:59:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 02:59:57.365
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 02:59:57.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 02:59:57.382
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 02:59:57.386
    Aug 17 02:59:57.395: INFO: Waiting up to 5m0s for pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba" in namespace "emptydir-62" to be "Succeeded or Failed"
    Aug 17 02:59:57.400: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488667ms
    Aug 17 02:59:59.405: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009727991s
    Aug 17 03:00:01.407: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.011918024s
    Aug 17 03:00:03.406: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010828776s
    STEP: Saw pod success 08/17/23 03:00:03.406
    Aug 17 03:00:03.406: INFO: Pod "pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba" satisfied condition "Succeeded or Failed"
    Aug 17 03:00:03.417: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba container test-container: <nil>
    STEP: delete the pod 08/17/23 03:00:03.433
    Aug 17 03:00:03.447: INFO: Waiting for pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba to disappear
    Aug 17 03:00:03.453: INFO: Pod pod-dbbf96bc-5a28-4148-af73-6a5d03f8d5ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-62" for this suite. 08/17/23 03:00:03.465
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:03.476
Aug 17 03:00:03.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 03:00:03.478
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:03.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:03.512
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/17/23 03:00:03.519
STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 03:00:03.529
STEP: delete the deployment 08/17/23 03:00:03.648
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/17/23 03:00:03.661
STEP: Gathering metrics 08/17/23 03:00:04.193
W0817 03:00:04.207805      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 03:00:04.207: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:04.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6470" for this suite. 08/17/23 03:00:04.215
------------------------------
• [0.749 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:03.476
    Aug 17 03:00:03.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 03:00:03.478
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:03.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:03.512
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/17/23 03:00:03.519
    STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 03:00:03.529
    STEP: delete the deployment 08/17/23 03:00:03.648
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/17/23 03:00:03.661
    STEP: Gathering metrics 08/17/23 03:00:04.193
    W0817 03:00:04.207805      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 03:00:04.207: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:04.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6470" for this suite. 08/17/23 03:00:04.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:04.226
Aug 17 03:00:04.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 03:00:04.227
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:04.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:04.249
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 17 03:00:04.272: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 03:00:09.282: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 03:00:09.282
STEP: Scaling up "test-rs" replicaset  08/17/23 03:00:09.283
Aug 17 03:00:09.303: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/17/23 03:00:09.303
W0817 03:00:09.312271      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 03:00:09.315: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 03:00:09.324: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 03:00:09.343: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 03:00:09.355: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
Aug 17 03:00:10.850: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 2, AvailableReplicas 2
Aug 17 03:00:11.685: INFO: observed Replicaset test-rs in namespace replicaset-3794 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3794" for this suite. 08/17/23 03:00:11.694
------------------------------
• [SLOW TEST] [7.478 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:04.226
    Aug 17 03:00:04.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 03:00:04.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:04.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:04.249
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 17 03:00:04.272: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 03:00:09.282: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 03:00:09.282
    STEP: Scaling up "test-rs" replicaset  08/17/23 03:00:09.283
    Aug 17 03:00:09.303: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/17/23 03:00:09.303
    W0817 03:00:09.312271      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 03:00:09.315: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 03:00:09.324: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 03:00:09.343: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 03:00:09.355: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 1, AvailableReplicas 1
    Aug 17 03:00:10.850: INFO: observed ReplicaSet test-rs in namespace replicaset-3794 with ReadyReplicas 2, AvailableReplicas 2
    Aug 17 03:00:11.685: INFO: observed Replicaset test-rs in namespace replicaset-3794 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3794" for this suite. 08/17/23 03:00:11.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:11.704
Aug 17 03:00:11.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context 08/17/23 03:00:11.705
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:11.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:11.728
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 03:00:11.734
Aug 17 03:00:11.745: INFO: Waiting up to 5m0s for pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c" in namespace "security-context-3496" to be "Succeeded or Failed"
Aug 17 03:00:11.750: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.843122ms
Aug 17 03:00:13.755: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010780775s
Aug 17 03:00:15.757: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01199608s
STEP: Saw pod success 08/17/23 03:00:15.757
Aug 17 03:00:15.757: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c" satisfied condition "Succeeded or Failed"
Aug 17 03:00:15.763: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c container test-container: <nil>
STEP: delete the pod 08/17/23 03:00:15.774
Aug 17 03:00:15.786: INFO: Waiting for pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c to disappear
Aug 17 03:00:15.790: INFO: Pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:15.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3496" for this suite. 08/17/23 03:00:15.801
------------------------------
• [4.106 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:11.704
    Aug 17 03:00:11.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context 08/17/23 03:00:11.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:11.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:11.728
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 03:00:11.734
    Aug 17 03:00:11.745: INFO: Waiting up to 5m0s for pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c" in namespace "security-context-3496" to be "Succeeded or Failed"
    Aug 17 03:00:11.750: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.843122ms
    Aug 17 03:00:13.755: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010780775s
    Aug 17 03:00:15.757: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01199608s
    STEP: Saw pod success 08/17/23 03:00:15.757
    Aug 17 03:00:15.757: INFO: Pod "security-context-89315a75-a07c-4bf9-adaa-539e54d7037c" satisfied condition "Succeeded or Failed"
    Aug 17 03:00:15.763: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c container test-container: <nil>
    STEP: delete the pod 08/17/23 03:00:15.774
    Aug 17 03:00:15.786: INFO: Waiting for pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c to disappear
    Aug 17 03:00:15.790: INFO: Pod security-context-89315a75-a07c-4bf9-adaa-539e54d7037c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:15.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3496" for this suite. 08/17/23 03:00:15.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:15.812
Aug 17 03:00:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:00:15.813
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:15.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:15.835
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/17/23 03:00:15.842
STEP: Creating a ResourceQuota 08/17/23 03:00:20.848
STEP: Ensuring resource quota status is calculated 08/17/23 03:00:20.855
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:22.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2041" for this suite. 08/17/23 03:00:22.871
------------------------------
• [SLOW TEST] [7.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:15.812
    Aug 17 03:00:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:00:15.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:15.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:15.835
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/17/23 03:00:15.842
    STEP: Creating a ResourceQuota 08/17/23 03:00:20.848
    STEP: Ensuring resource quota status is calculated 08/17/23 03:00:20.855
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:22.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2041" for this suite. 08/17/23 03:00:22.871
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:22.881
Aug 17 03:00:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename endpointslicemirroring 08/17/23 03:00:22.882
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:22.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:22.902
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/17/23 03:00:22.925
Aug 17 03:00:22.937: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/17/23 03:00:24.943
Aug 17 03:00:24.958: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/17/23 03:00:26.964
Aug 17 03:00:26.977: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:28.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2318" for this suite. 08/17/23 03:00:28.991
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:22.881
    Aug 17 03:00:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename endpointslicemirroring 08/17/23 03:00:22.882
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:22.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:22.902
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/17/23 03:00:22.925
    Aug 17 03:00:22.937: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/17/23 03:00:24.943
    Aug 17 03:00:24.958: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/17/23 03:00:26.964
    Aug 17 03:00:26.977: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:28.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2318" for this suite. 08/17/23 03:00:28.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:29.002
Aug 17 03:00:29.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:00:29.003
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:29.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:29.024
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4112/configmap-test-19d5ba44-3e7e-44e6-85e4-0e9c35a80262 08/17/23 03:00:29.03
STEP: Creating a pod to test consume configMaps 08/17/23 03:00:29.036
Aug 17 03:00:29.046: INFO: Waiting up to 5m0s for pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821" in namespace "configmap-4112" to be "Succeeded or Failed"
Aug 17 03:00:29.052: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222909ms
Aug 17 03:00:31.057: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011131616s
Aug 17 03:00:33.058: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011670153s
STEP: Saw pod success 08/17/23 03:00:33.058
Aug 17 03:00:33.058: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821" satisfied condition "Succeeded or Failed"
Aug 17 03:00:33.063: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 container env-test: <nil>
STEP: delete the pod 08/17/23 03:00:33.073
Aug 17 03:00:33.085: INFO: Waiting for pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 to disappear
Aug 17 03:00:33.089: INFO: Pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:00:33.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4112" for this suite. 08/17/23 03:00:33.097
------------------------------
• [4.103 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:29.002
    Aug 17 03:00:29.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:00:29.003
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:29.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:29.024
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4112/configmap-test-19d5ba44-3e7e-44e6-85e4-0e9c35a80262 08/17/23 03:00:29.03
    STEP: Creating a pod to test consume configMaps 08/17/23 03:00:29.036
    Aug 17 03:00:29.046: INFO: Waiting up to 5m0s for pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821" in namespace "configmap-4112" to be "Succeeded or Failed"
    Aug 17 03:00:29.052: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222909ms
    Aug 17 03:00:31.057: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011131616s
    Aug 17 03:00:33.058: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011670153s
    STEP: Saw pod success 08/17/23 03:00:33.058
    Aug 17 03:00:33.058: INFO: Pod "pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821" satisfied condition "Succeeded or Failed"
    Aug 17 03:00:33.063: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 container env-test: <nil>
    STEP: delete the pod 08/17/23 03:00:33.073
    Aug 17 03:00:33.085: INFO: Waiting for pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 to disappear
    Aug 17 03:00:33.089: INFO: Pod pod-configmaps-35248ddb-dae2-4806-a5c4-d3a27f7b2821 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:00:33.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4112" for this suite. 08/17/23 03:00:33.097
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:00:33.105
Aug 17 03:00:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename taint-single-pod 08/17/23 03:00:33.106
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:33.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:33.126
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 17 03:00:33.131: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:01:33.186: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 17 03:01:33.191: INFO: Starting informer...
STEP: Starting pod... 08/17/23 03:01:33.191
Aug 17 03:01:33.412: INFO: Pod is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
STEP: Trying to apply a taint on the Node 08/17/23 03:01:33.412
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:01:33.429
STEP: Waiting short time to make sure Pod is queued for deletion 08/17/23 03:01:33.434
Aug 17 03:01:33.434: INFO: Pod wasn't evicted. Proceeding
Aug 17 03:01:33.434: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:01:33.45
STEP: Waiting some time to make sure that toleration time passed. 08/17/23 03:01:33.456
Aug 17 03:02:48.457: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:02:48.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-2137" for this suite. 08/17/23 03:02:48.464
------------------------------
• [SLOW TEST] [135.366 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:00:33.105
    Aug 17 03:00:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename taint-single-pod 08/17/23 03:00:33.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:00:33.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:00:33.126
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 17 03:00:33.131: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:01:33.186: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 17 03:01:33.191: INFO: Starting informer...
    STEP: Starting pod... 08/17/23 03:01:33.191
    Aug 17 03:01:33.412: INFO: Pod is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
    STEP: Trying to apply a taint on the Node 08/17/23 03:01:33.412
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:01:33.429
    STEP: Waiting short time to make sure Pod is queued for deletion 08/17/23 03:01:33.434
    Aug 17 03:01:33.434: INFO: Pod wasn't evicted. Proceeding
    Aug 17 03:01:33.434: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:01:33.45
    STEP: Waiting some time to make sure that toleration time passed. 08/17/23 03:01:33.456
    Aug 17 03:02:48.457: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:02:48.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-2137" for this suite. 08/17/23 03:02:48.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:02:48.499
Aug 17 03:02:48.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:02:48.5
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:02:48.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:02:48.521
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:02:48.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4153" for this suite. 08/17/23 03:02:48.581
------------------------------
• [0.092 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:02:48.499
    Aug 17 03:02:48.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:02:48.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:02:48.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:02:48.521
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:02:48.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4153" for this suite. 08/17/23 03:02:48.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:02:48.589
Aug 17 03:02:48.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:02:48.59
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:02:48.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:02:48.609
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 03:02:48.627: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:03:48.683: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:03:48.688
Aug 17 03:03:48.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 03:03:48.689
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:03:48.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:03:48.707
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 17 03:03:48.728: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 17 03:03:48.733: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 17 03:03:48.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:03:48.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9425" for this suite. 08/17/23 03:03:48.813
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-808" for this suite. 08/17/23 03:03:48.82
------------------------------
• [SLOW TEST] [60.237 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:02:48.589
    Aug 17 03:02:48.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:02:48.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:02:48.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:02:48.609
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 03:02:48.627: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:03:48.683: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:03:48.688
    Aug 17 03:03:48.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 03:03:48.689
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:03:48.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:03:48.707
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 17 03:03:48.728: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 17 03:03:48.733: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:03:48.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:03:48.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9425" for this suite. 08/17/23 03:03:48.813
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-808" for this suite. 08/17/23 03:03:48.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:03:48.829
Aug 17 03:03:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 03:03:48.829
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:03:48.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:03:48.847
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/17/23 03:03:48.856
STEP: Patching the Job 08/17/23 03:03:48.862
STEP: Watching for Job to be patched 08/17/23 03:03:48.876
Aug 17 03:03:48.879: INFO: Event ADDED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 17 03:03:48.879: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 17 03:03:48.879: INFO: Event MODIFIED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/17/23 03:03:48.879
STEP: Watching for Job to be updated 08/17/23 03:03:48.889
Aug 17 03:03:48.892: INFO: Event MODIFIED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:03:48.892: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/17/23 03:03:48.892
Aug 17 03:03:48.896: INFO: Job: e2e-77cdc as labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc]
STEP: Waiting for job to complete 08/17/23 03:03:48.896
STEP: Delete a job collection with a labelselector 08/17/23 03:04:00.901
STEP: Watching for Job to be deleted 08/17/23 03:04:00.909
Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.914: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.914: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 17 03:04:00.914: INFO: Event DELETED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/17/23 03:04:00.914
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:00.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4040" for this suite. 08/17/23 03:04:00.924
------------------------------
• [SLOW TEST] [12.102 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:03:48.829
    Aug 17 03:03:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 03:03:48.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:03:48.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:03:48.847
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/17/23 03:03:48.856
    STEP: Patching the Job 08/17/23 03:03:48.862
    STEP: Watching for Job to be patched 08/17/23 03:03:48.876
    Aug 17 03:03:48.879: INFO: Event ADDED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 17 03:03:48.879: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 17 03:03:48.879: INFO: Event MODIFIED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/17/23 03:03:48.879
    STEP: Watching for Job to be updated 08/17/23 03:03:48.889
    Aug 17 03:03:48.892: INFO: Event MODIFIED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:03:48.892: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/17/23 03:03:48.892
    Aug 17 03:03:48.896: INFO: Job: e2e-77cdc as labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc]
    STEP: Waiting for job to complete 08/17/23 03:03:48.896
    STEP: Delete a job collection with a labelselector 08/17/23 03:04:00.901
    STEP: Watching for Job to be deleted 08/17/23 03:04:00.909
    Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.912: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.913: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.914: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.914: INFO: Event MODIFIED observed for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 17 03:04:00.914: INFO: Event DELETED found for Job e2e-77cdc in namespace job-4040 with labels: map[e2e-77cdc:patched e2e-job-label:e2e-77cdc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/17/23 03:04:00.914
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:00.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4040" for this suite. 08/17/23 03:04:00.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:00.932
Aug 17 03:04:00.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:04:00.933
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:00.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:00.956
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-f6adf2c7-aa7d-417e-9b1e-4111701eaa3d 08/17/23 03:04:00.961
STEP: Creating a pod to test consume configMaps 08/17/23 03:04:00.968
Aug 17 03:04:00.981: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a" in namespace "projected-491" to be "Succeeded or Failed"
Aug 17 03:04:00.987: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64837ms
Aug 17 03:04:02.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011441156s
Aug 17 03:04:04.994: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012813944s
Aug 17 03:04:06.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01143814s
STEP: Saw pod success 08/17/23 03:04:06.993
Aug 17 03:04:06.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a" satisfied condition "Succeeded or Failed"
Aug 17 03:04:06.998: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:04:07.048
Aug 17 03:04:07.061: INFO: Waiting for pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a to disappear
Aug 17 03:04:07.065: INFO: Pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:07.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-491" for this suite. 08/17/23 03:04:07.073
------------------------------
• [SLOW TEST] [6.149 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:00.932
    Aug 17 03:04:00.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:04:00.933
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:00.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:00.956
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-f6adf2c7-aa7d-417e-9b1e-4111701eaa3d 08/17/23 03:04:00.961
    STEP: Creating a pod to test consume configMaps 08/17/23 03:04:00.968
    Aug 17 03:04:00.981: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a" in namespace "projected-491" to be "Succeeded or Failed"
    Aug 17 03:04:00.987: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64837ms
    Aug 17 03:04:02.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011441156s
    Aug 17 03:04:04.994: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012813944s
    Aug 17 03:04:06.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01143814s
    STEP: Saw pod success 08/17/23 03:04:06.993
    Aug 17 03:04:06.993: INFO: Pod "pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a" satisfied condition "Succeeded or Failed"
    Aug 17 03:04:06.998: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:04:07.048
    Aug 17 03:04:07.061: INFO: Waiting for pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a to disappear
    Aug 17 03:04:07.065: INFO: Pod pod-projected-configmaps-93f50323-4c9e-4e36-bce3-396eec34f39a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:07.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-491" for this suite. 08/17/23 03:04:07.073
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:07.084
Aug 17 03:04:07.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:04:07.085
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:07.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:07.105
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-9e42a121-6640-49fe-b38d-9bbf56eb19e5 08/17/23 03:04:07.11
STEP: Creating a pod to test consume configMaps 08/17/23 03:04:07.116
Aug 17 03:04:07.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e" in namespace "projected-8414" to be "Succeeded or Failed"
Aug 17 03:04:07.131: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959058ms
Aug 17 03:04:09.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011556882s
Aug 17 03:04:11.139: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0141928s
Aug 17 03:04:13.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011590306s
STEP: Saw pod success 08/17/23 03:04:13.137
Aug 17 03:04:13.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e" satisfied condition "Succeeded or Failed"
Aug 17 03:04:13.142: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/17/23 03:04:13.152
Aug 17 03:04:13.167: INFO: Waiting for pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e to disappear
Aug 17 03:04:13.171: INFO: Pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:13.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8414" for this suite. 08/17/23 03:04:13.182
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:07.084
    Aug 17 03:04:07.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:04:07.085
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:07.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:07.105
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-9e42a121-6640-49fe-b38d-9bbf56eb19e5 08/17/23 03:04:07.11
    STEP: Creating a pod to test consume configMaps 08/17/23 03:04:07.116
    Aug 17 03:04:07.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e" in namespace "projected-8414" to be "Succeeded or Failed"
    Aug 17 03:04:07.131: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959058ms
    Aug 17 03:04:09.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011556882s
    Aug 17 03:04:11.139: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0141928s
    Aug 17 03:04:13.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011590306s
    STEP: Saw pod success 08/17/23 03:04:13.137
    Aug 17 03:04:13.137: INFO: Pod "pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e" satisfied condition "Succeeded or Failed"
    Aug 17 03:04:13.142: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:04:13.152
    Aug 17 03:04:13.167: INFO: Waiting for pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e to disappear
    Aug 17 03:04:13.171: INFO: Pod pod-projected-configmaps-4347626b-f73f-4036-bcba-8a81e510ab1e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:13.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8414" for this suite. 08/17/23 03:04:13.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:13.192
Aug 17 03:04:13.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:04:13.193
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:13.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:13.214
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/17/23 03:04:13.22
STEP: Getting a ResourceQuota 08/17/23 03:04:13.226
STEP: Updating a ResourceQuota 08/17/23 03:04:13.232
STEP: Verifying a ResourceQuota was modified 08/17/23 03:04:13.239
STEP: Deleting a ResourceQuota 08/17/23 03:04:13.243
STEP: Verifying the deleted ResourceQuota 08/17/23 03:04:13.251
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:13.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7920" for this suite. 08/17/23 03:04:13.263
------------------------------
• [0.078 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:13.192
    Aug 17 03:04:13.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:04:13.193
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:13.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:13.214
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/17/23 03:04:13.22
    STEP: Getting a ResourceQuota 08/17/23 03:04:13.226
    STEP: Updating a ResourceQuota 08/17/23 03:04:13.232
    STEP: Verifying a ResourceQuota was modified 08/17/23 03:04:13.239
    STEP: Deleting a ResourceQuota 08/17/23 03:04:13.243
    STEP: Verifying the deleted ResourceQuota 08/17/23 03:04:13.251
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:13.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7920" for this suite. 08/17/23 03:04:13.263
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:13.271
Aug 17 03:04:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:04:13.272
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:13.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:13.291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:04:13.313
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:04:13.763
STEP: Deploying the webhook pod 08/17/23 03:04:13.772
STEP: Wait for the deployment to be ready 08/17/23 03:04:13.786
Aug 17 03:04:13.795: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:04:15.813
STEP: Verifying the service has paired with the endpoint 08/17/23 03:04:15.828
Aug 17 03:04:16.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 17 03:04:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7940-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 03:04:17.352
STEP: Creating a custom resource while v1 is storage version 08/17/23 03:04:17.456
STEP: Patching Custom Resource Definition to set v2 as storage 08/17/23 03:04:19.645
STEP: Patching the custom resource while v2 is storage version 08/17/23 03:04:19.662
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:20.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1999" for this suite. 08/17/23 03:04:20.308
STEP: Destroying namespace "webhook-1999-markers" for this suite. 08/17/23 03:04:20.321
------------------------------
• [SLOW TEST] [7.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:13.271
    Aug 17 03:04:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:04:13.272
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:13.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:13.291
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:04:13.313
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:04:13.763
    STEP: Deploying the webhook pod 08/17/23 03:04:13.772
    STEP: Wait for the deployment to be ready 08/17/23 03:04:13.786
    Aug 17 03:04:13.795: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:04:15.813
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:04:15.828
    Aug 17 03:04:16.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 17 03:04:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7940-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 03:04:17.352
    STEP: Creating a custom resource while v1 is storage version 08/17/23 03:04:17.456
    STEP: Patching Custom Resource Definition to set v2 as storage 08/17/23 03:04:19.645
    STEP: Patching the custom resource while v2 is storage version 08/17/23 03:04:19.662
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:20.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1999" for this suite. 08/17/23 03:04:20.308
    STEP: Destroying namespace "webhook-1999-markers" for this suite. 08/17/23 03:04:20.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:20.339
Aug 17 03:04:20.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 03:04:20.34
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:20.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:20.374
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/17/23 03:04:20.381
STEP: Ensuring active pods == parallelism 08/17/23 03:04:20.389
STEP: delete a job 08/17/23 03:04:22.395
STEP: deleting Job.batch foo in namespace job-5390, will wait for the garbage collector to delete the pods 08/17/23 03:04:22.396
Aug 17 03:04:22.458: INFO: Deleting Job.batch foo took: 7.010139ms
Aug 17 03:04:22.559: INFO: Terminating Job.batch foo pods took: 100.832004ms
STEP: Ensuring job was deleted 08/17/23 03:04:55.36
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5390" for this suite. 08/17/23 03:04:55.374
------------------------------
• [SLOW TEST] [35.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:20.339
    Aug 17 03:04:20.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 03:04:20.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:20.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:20.374
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/17/23 03:04:20.381
    STEP: Ensuring active pods == parallelism 08/17/23 03:04:20.389
    STEP: delete a job 08/17/23 03:04:22.395
    STEP: deleting Job.batch foo in namespace job-5390, will wait for the garbage collector to delete the pods 08/17/23 03:04:22.396
    Aug 17 03:04:22.458: INFO: Deleting Job.batch foo took: 7.010139ms
    Aug 17 03:04:22.559: INFO: Terminating Job.batch foo pods took: 100.832004ms
    STEP: Ensuring job was deleted 08/17/23 03:04:55.36
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5390" for this suite. 08/17/23 03:04:55.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:55.388
Aug 17 03:04:55.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:04:55.388
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:55.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:55.418
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/17/23 03:04:55.423
Aug 17 03:04:55.433: INFO: Waiting up to 5m0s for pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f" in namespace "downward-api-395" to be "Succeeded or Failed"
Aug 17 03:04:55.437: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692593ms
Aug 17 03:04:57.443: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389923s
Aug 17 03:04:59.442: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008745121s
STEP: Saw pod success 08/17/23 03:04:59.442
Aug 17 03:04:59.442: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f" satisfied condition "Succeeded or Failed"
Aug 17 03:04:59.446: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f container dapi-container: <nil>
STEP: delete the pod 08/17/23 03:04:59.498
Aug 17 03:04:59.508: INFO: Waiting for pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f to disappear
Aug 17 03:04:59.512: INFO: Pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 03:04:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-395" for this suite. 08/17/23 03:04:59.52
------------------------------
• [4.141 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:55.388
    Aug 17 03:04:55.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:04:55.388
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:55.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:55.418
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/17/23 03:04:55.423
    Aug 17 03:04:55.433: INFO: Waiting up to 5m0s for pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f" in namespace "downward-api-395" to be "Succeeded or Failed"
    Aug 17 03:04:55.437: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692593ms
    Aug 17 03:04:57.443: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389923s
    Aug 17 03:04:59.442: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008745121s
    STEP: Saw pod success 08/17/23 03:04:59.442
    Aug 17 03:04:59.442: INFO: Pod "downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f" satisfied condition "Succeeded or Failed"
    Aug 17 03:04:59.446: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f container dapi-container: <nil>
    STEP: delete the pod 08/17/23 03:04:59.498
    Aug 17 03:04:59.508: INFO: Waiting for pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f to disappear
    Aug 17 03:04:59.512: INFO: Pod downward-api-20cc6d14-d9e6-4ecb-a0f4-a9b1b1d1eb8f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:04:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-395" for this suite. 08/17/23 03:04:59.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:04:59.529
Aug 17 03:04:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:04:59.529
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:59.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:59.549
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:04:59.554
Aug 17 03:04:59.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c" in namespace "downward-api-6016" to be "Succeeded or Failed"
Aug 17 03:04:59.569: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74767ms
Aug 17 03:05:01.575: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010969084s
Aug 17 03:05:03.575: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011035621s
STEP: Saw pod success 08/17/23 03:05:03.575
Aug 17 03:05:03.576: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c" satisfied condition "Succeeded or Failed"
Aug 17 03:05:03.580: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c container client-container: <nil>
STEP: delete the pod 08/17/23 03:05:03.59
Aug 17 03:05:03.603: INFO: Waiting for pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c to disappear
Aug 17 03:05:03.607: INFO: Pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:03.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6016" for this suite. 08/17/23 03:05:03.614
------------------------------
• [4.093 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:04:59.529
    Aug 17 03:04:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:04:59.529
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:04:59.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:04:59.549
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:04:59.554
    Aug 17 03:04:59.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c" in namespace "downward-api-6016" to be "Succeeded or Failed"
    Aug 17 03:04:59.569: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74767ms
    Aug 17 03:05:01.575: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010969084s
    Aug 17 03:05:03.575: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011035621s
    STEP: Saw pod success 08/17/23 03:05:03.575
    Aug 17 03:05:03.576: INFO: Pod "downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c" satisfied condition "Succeeded or Failed"
    Aug 17 03:05:03.580: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c container client-container: <nil>
    STEP: delete the pod 08/17/23 03:05:03.59
    Aug 17 03:05:03.603: INFO: Waiting for pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c to disappear
    Aug 17 03:05:03.607: INFO: Pod downwardapi-volume-760a9c94-9e85-4170-9ce1-8c136eb7f92c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:03.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6016" for this suite. 08/17/23 03:05:03.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:03.623
Aug 17 03:05:03.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 03:05:03.624
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:03.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:03.643
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/17/23 03:05:03.649
Aug 17 03:05:03.658: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1968" to be "running and ready"
Aug 17 03:05:03.665: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.312313ms
Aug 17 03:05:03.665: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:05.671: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013279904s
Aug 17 03:05:05.671: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 17 03:05:05.671: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/17/23 03:05:05.676
STEP: Then the orphan pod is adopted 08/17/23 03:05:05.682
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:06.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1968" for this suite. 08/17/23 03:05:06.699
------------------------------
• [3.084 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:03.623
    Aug 17 03:05:03.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 03:05:03.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:03.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:03.643
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/17/23 03:05:03.649
    Aug 17 03:05:03.658: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1968" to be "running and ready"
    Aug 17 03:05:03.665: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.312313ms
    Aug 17 03:05:03.665: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:05.671: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013279904s
    Aug 17 03:05:05.671: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 17 03:05:05.671: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/17/23 03:05:05.676
    STEP: Then the orphan pod is adopted 08/17/23 03:05:05.682
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:06.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1968" for this suite. 08/17/23 03:05:06.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:06.708
Aug 17 03:05:06.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:05:06.709
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:06.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:06.731
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-331122a7-bcf9-4014-a000-8642978bce45 08/17/23 03:05:06.736
STEP: Creating a pod to test consume configMaps 08/17/23 03:05:06.744
Aug 17 03:05:06.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929" in namespace "projected-6374" to be "Succeeded or Failed"
Aug 17 03:05:06.764: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Pending", Reason="", readiness=false. Elapsed: 10.480585ms
Aug 17 03:05:08.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015730067s
Aug 17 03:05:10.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016354793s
STEP: Saw pod success 08/17/23 03:05:10.77
Aug 17 03:05:10.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929" satisfied condition "Succeeded or Failed"
Aug 17 03:05:10.775: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:05:10.825
Aug 17 03:05:10.839: INFO: Waiting for pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 to disappear
Aug 17 03:05:10.843: INFO: Pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:10.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6374" for this suite. 08/17/23 03:05:10.851
------------------------------
• [4.151 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:06.708
    Aug 17 03:05:06.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:05:06.709
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:06.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:06.731
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-331122a7-bcf9-4014-a000-8642978bce45 08/17/23 03:05:06.736
    STEP: Creating a pod to test consume configMaps 08/17/23 03:05:06.744
    Aug 17 03:05:06.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929" in namespace "projected-6374" to be "Succeeded or Failed"
    Aug 17 03:05:06.764: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Pending", Reason="", readiness=false. Elapsed: 10.480585ms
    Aug 17 03:05:08.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015730067s
    Aug 17 03:05:10.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016354793s
    STEP: Saw pod success 08/17/23 03:05:10.77
    Aug 17 03:05:10.770: INFO: Pod "pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929" satisfied condition "Succeeded or Failed"
    Aug 17 03:05:10.775: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:05:10.825
    Aug 17 03:05:10.839: INFO: Waiting for pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 to disappear
    Aug 17 03:05:10.843: INFO: Pod pod-projected-configmaps-b25c717e-1707-49fc-8d68-d85afab84929 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:10.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6374" for this suite. 08/17/23 03:05:10.851
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:10.861
Aug 17 03:05:10.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:05:10.862
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:10.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:10.881
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 03:05:10.886
Aug 17 03:05:10.898: INFO: Waiting up to 5m0s for pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec" in namespace "emptydir-1497" to be "Succeeded or Failed"
Aug 17 03:05:10.903: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.431569ms
Aug 17 03:05:12.908: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010545632s
Aug 17 03:05:14.909: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011543325s
STEP: Saw pod success 08/17/23 03:05:14.909
Aug 17 03:05:14.909: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec" satisfied condition "Succeeded or Failed"
Aug 17 03:05:14.914: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec container test-container: <nil>
STEP: delete the pod 08/17/23 03:05:14.923
Aug 17 03:05:14.934: INFO: Waiting for pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec to disappear
Aug 17 03:05:14.938: INFO: Pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:14.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1497" for this suite. 08/17/23 03:05:14.946
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:10.861
    Aug 17 03:05:10.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:05:10.862
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:10.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:10.881
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 03:05:10.886
    Aug 17 03:05:10.898: INFO: Waiting up to 5m0s for pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec" in namespace "emptydir-1497" to be "Succeeded or Failed"
    Aug 17 03:05:10.903: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.431569ms
    Aug 17 03:05:12.908: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010545632s
    Aug 17 03:05:14.909: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011543325s
    STEP: Saw pod success 08/17/23 03:05:14.909
    Aug 17 03:05:14.909: INFO: Pod "pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec" satisfied condition "Succeeded or Failed"
    Aug 17 03:05:14.914: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec container test-container: <nil>
    STEP: delete the pod 08/17/23 03:05:14.923
    Aug 17 03:05:14.934: INFO: Waiting for pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec to disappear
    Aug 17 03:05:14.938: INFO: Pod pod-ef8638eb-c72f-4696-b2e7-2bd7633aafec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:14.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1497" for this suite. 08/17/23 03:05:14.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:14.957
Aug 17 03:05:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:05:14.958
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:14.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:14.976
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:05:14.987
Aug 17 03:05:14.997: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4064" to be "running and ready"
Aug 17 03:05:15.002: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.873988ms
Aug 17 03:05:15.002: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:17.007: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009623777s
Aug 17 03:05:17.007: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 03:05:17.007: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/17/23 03:05:17.011
Aug 17 03:05:17.017: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4064" to be "running and ready"
Aug 17 03:05:17.021: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010046ms
Aug 17 03:05:17.021: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:19.028: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011465495s
Aug 17 03:05:19.029: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 17 03:05:19.029: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/17/23 03:05:19.034
Aug 17 03:05:19.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 03:05:19.051: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 03:05:21.053: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 03:05:21.058: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 17 03:05:23.053: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 17 03:05:23.058: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/17/23 03:05:23.058
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:23.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4064" for this suite. 08/17/23 03:05:23.115
------------------------------
• [SLOW TEST] [8.166 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:14.957
    Aug 17 03:05:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:05:14.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:14.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:14.976
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:05:14.987
    Aug 17 03:05:14.997: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4064" to be "running and ready"
    Aug 17 03:05:15.002: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.873988ms
    Aug 17 03:05:15.002: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:17.007: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009623777s
    Aug 17 03:05:17.007: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 03:05:17.007: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/17/23 03:05:17.011
    Aug 17 03:05:17.017: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4064" to be "running and ready"
    Aug 17 03:05:17.021: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010046ms
    Aug 17 03:05:17.021: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:19.028: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011465495s
    Aug 17 03:05:19.029: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 17 03:05:19.029: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/17/23 03:05:19.034
    Aug 17 03:05:19.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 17 03:05:19.051: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 17 03:05:21.053: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 17 03:05:21.058: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 17 03:05:23.053: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 17 03:05:23.058: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/17/23 03:05:23.058
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:23.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4064" for this suite. 08/17/23 03:05:23.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:23.124
Aug 17 03:05:23.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename hostport 08/17/23 03:05:23.125
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:23.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:23.144
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/17/23 03:05:23.155
Aug 17 03:05:23.166: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5884" to be "running and ready"
Aug 17 03:05:23.171: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741748ms
Aug 17 03:05:23.171: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:25.176: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009769993s
Aug 17 03:05:25.176: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:27.176: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.010432439s
Aug 17 03:05:27.176: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 03:05:27.176: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.11.4 on the node which pod1 resides and expect scheduled 08/17/23 03:05:27.176
Aug 17 03:05:27.183: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5884" to be "running and ready"
Aug 17 03:05:27.188: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206ms
Aug 17 03:05:27.189: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:29.194: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010667815s
Aug 17 03:05:29.194: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 03:05:29.194: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.11.4 but use UDP protocol on the node which pod2 resides 08/17/23 03:05:29.194
Aug 17 03:05:29.201: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5884" to be "running and ready"
Aug 17 03:05:29.205: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95045ms
Aug 17 03:05:29.205: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:31.211: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009614098s
Aug 17 03:05:31.211: INFO: The phase of Pod pod3 is Running (Ready = false)
Aug 17 03:05:33.211: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009690875s
Aug 17 03:05:33.211: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 17 03:05:33.211: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 17 03:05:33.217: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5884" to be "running and ready"
Aug 17 03:05:33.222: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279671ms
Aug 17 03:05:33.222: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:35.227: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319965s
Aug 17 03:05:35.227: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 17 03:05:35.227: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/17/23 03:05:35.231
Aug 17 03:05:35.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.11.4 http://127.0.0.1:54323/hostname] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:05:35.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:05:35.232: INFO: ExecWithOptions: Clientset creation
Aug 17 03:05:35.232: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.11.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.4, port: 54323 08/17/23 03:05:35.572
Aug 17 03:05:35.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.11.4:54323/hostname] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:05:35.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:05:35.572: INFO: ExecWithOptions: Clientset creation
Aug 17 03:05:35.572: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.11.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.4, port: 54323 UDP 08/17/23 03:05:36.007
Aug 17 03:05:36.008: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.11.4 54323] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:05:36.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:05:36.008: INFO: ExecWithOptions: Clientset creation
Aug 17 03:05:36.008: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.11.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:41.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5884" for this suite. 08/17/23 03:05:41.409
------------------------------
• [SLOW TEST] [18.293 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:23.124
    Aug 17 03:05:23.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename hostport 08/17/23 03:05:23.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:23.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:23.144
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/17/23 03:05:23.155
    Aug 17 03:05:23.166: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5884" to be "running and ready"
    Aug 17 03:05:23.171: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741748ms
    Aug 17 03:05:23.171: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:25.176: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009769993s
    Aug 17 03:05:25.176: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:27.176: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.010432439s
    Aug 17 03:05:27.176: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 03:05:27.176: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.11.4 on the node which pod1 resides and expect scheduled 08/17/23 03:05:27.176
    Aug 17 03:05:27.183: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5884" to be "running and ready"
    Aug 17 03:05:27.188: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206ms
    Aug 17 03:05:27.189: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:29.194: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010667815s
    Aug 17 03:05:29.194: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 03:05:29.194: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.11.4 but use UDP protocol on the node which pod2 resides 08/17/23 03:05:29.194
    Aug 17 03:05:29.201: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5884" to be "running and ready"
    Aug 17 03:05:29.205: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95045ms
    Aug 17 03:05:29.205: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:31.211: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009614098s
    Aug 17 03:05:31.211: INFO: The phase of Pod pod3 is Running (Ready = false)
    Aug 17 03:05:33.211: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009690875s
    Aug 17 03:05:33.211: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 17 03:05:33.211: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 17 03:05:33.217: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5884" to be "running and ready"
    Aug 17 03:05:33.222: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279671ms
    Aug 17 03:05:33.222: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:35.227: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319965s
    Aug 17 03:05:35.227: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 17 03:05:35.227: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/17/23 03:05:35.231
    Aug 17 03:05:35.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.11.4 http://127.0.0.1:54323/hostname] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:05:35.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:05:35.232: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:05:35.232: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.11.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.4, port: 54323 08/17/23 03:05:35.572
    Aug 17 03:05:35.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.11.4:54323/hostname] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:05:35.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:05:35.572: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:05:35.572: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.11.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.4, port: 54323 UDP 08/17/23 03:05:36.007
    Aug 17 03:05:36.008: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.11.4 54323] Namespace:hostport-5884 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:05:36.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:05:36.008: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:05:36.008: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-5884/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.11.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:41.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5884" for this suite. 08/17/23 03:05:41.409
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:41.418
Aug 17 03:05:41.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:05:41.418
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:41.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:41.436
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:41.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8012" for this suite. 08/17/23 03:05:41.456
------------------------------
• [0.046 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:41.418
    Aug 17 03:05:41.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:05:41.418
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:41.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:41.436
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:41.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8012" for this suite. 08/17/23 03:05:41.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:41.466
Aug 17 03:05:41.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:05:41.467
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:41.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:41.485
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-8c81c324-ffd9-44b2-b079-5fb11f7eb954 08/17/23 03:05:41.49
STEP: Creating a pod to test consume secrets 08/17/23 03:05:41.496
Aug 17 03:05:41.505: INFO: Waiting up to 5m0s for pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb" in namespace "secrets-6297" to be "Succeeded or Failed"
Aug 17 03:05:41.509: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625167ms
Aug 17 03:05:43.516: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010097097s
Aug 17 03:05:45.516: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Running", Reason="", readiness=false. Elapsed: 4.010901878s
Aug 17 03:05:47.515: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009511172s
STEP: Saw pod success 08/17/23 03:05:47.515
Aug 17 03:05:47.515: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb" satisfied condition "Succeeded or Failed"
Aug 17 03:05:47.520: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:05:47.57
Aug 17 03:05:47.583: INFO: Waiting for pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb to disappear
Aug 17 03:05:47.587: INFO: Pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:05:47.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6297" for this suite. 08/17/23 03:05:47.593
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:41.466
    Aug 17 03:05:41.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:05:41.467
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:41.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:41.485
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-8c81c324-ffd9-44b2-b079-5fb11f7eb954 08/17/23 03:05:41.49
    STEP: Creating a pod to test consume secrets 08/17/23 03:05:41.496
    Aug 17 03:05:41.505: INFO: Waiting up to 5m0s for pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb" in namespace "secrets-6297" to be "Succeeded or Failed"
    Aug 17 03:05:41.509: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625167ms
    Aug 17 03:05:43.516: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010097097s
    Aug 17 03:05:45.516: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Running", Reason="", readiness=false. Elapsed: 4.010901878s
    Aug 17 03:05:47.515: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009511172s
    STEP: Saw pod success 08/17/23 03:05:47.515
    Aug 17 03:05:47.515: INFO: Pod "pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb" satisfied condition "Succeeded or Failed"
    Aug 17 03:05:47.520: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:05:47.57
    Aug 17 03:05:47.583: INFO: Waiting for pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb to disappear
    Aug 17 03:05:47.587: INFO: Pod pod-secrets-8cc40de2-3d1f-4241-ab50-7d1c02598efb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:05:47.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6297" for this suite. 08/17/23 03:05:47.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:05:47.602
Aug 17 03:05:47.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:05:47.603
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:47.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:47.621
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5072 08/17/23 03:05:47.631
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[] 08/17/23 03:05:47.646
Aug 17 03:05:47.661: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5072 08/17/23 03:05:47.661
Aug 17 03:05:47.673: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5072" to be "running and ready"
Aug 17 03:05:47.680: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.620146ms
Aug 17 03:05:47.680: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:49.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309559s
Aug 17 03:05:49.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:51.686: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013036287s
Aug 17 03:05:51.686: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 03:05:51.686: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod1:[80]] 08/17/23 03:05:51.69
Aug 17 03:05:51.705: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/17/23 03:05:51.705
Aug 17 03:05:51.705: INFO: Creating new exec pod
Aug 17 03:05:51.711: INFO: Waiting up to 5m0s for pod "execpodbcpzs" in namespace "services-5072" to be "running"
Aug 17 03:05:51.721: INFO: Pod "execpodbcpzs": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022053ms
Aug 17 03:05:53.726: INFO: Pod "execpodbcpzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.015556126s
Aug 17 03:05:53.726: INFO: Pod "execpodbcpzs" satisfied condition "running"
Aug 17 03:05:54.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 03:05:55.227: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 03:05:55.227: INFO: stdout: ""
Aug 17 03:05:55.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
Aug 17 03:05:55.754: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
Aug 17 03:05:55.754: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5072 08/17/23 03:05:55.754
Aug 17 03:05:55.761: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5072" to be "running and ready"
Aug 17 03:05:55.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295391ms
Aug 17 03:05:55.765: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:05:57.772: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294484s
Aug 17 03:05:57.772: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 03:05:57.772: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod1:[80] pod2:[80]] 08/17/23 03:05:57.777
Aug 17 03:05:57.801: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/17/23 03:05:57.801
Aug 17 03:05:58.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 03:05:59.275: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 03:05:59.275: INFO: stdout: ""
Aug 17 03:05:59.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
Aug 17 03:05:59.746: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
Aug 17 03:05:59.746: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5072 08/17/23 03:05:59.746
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod2:[80]] 08/17/23 03:05:59.777
Aug 17 03:05:59.802: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/17/23 03:05:59.802
Aug 17 03:06:00.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 17 03:06:01.207: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 17 03:06:01.207: INFO: stdout: ""
Aug 17 03:06:01.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
Aug 17 03:06:01.583: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
Aug 17 03:06:01.583: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5072 08/17/23 03:06:01.583
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[] 08/17/23 03:06:01.597
Aug 17 03:06:01.611: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:01.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5072" for this suite. 08/17/23 03:06:01.637
------------------------------
• [SLOW TEST] [14.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:05:47.602
    Aug 17 03:05:47.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:05:47.603
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:05:47.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:05:47.621
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5072 08/17/23 03:05:47.631
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[] 08/17/23 03:05:47.646
    Aug 17 03:05:47.661: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5072 08/17/23 03:05:47.661
    Aug 17 03:05:47.673: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5072" to be "running and ready"
    Aug 17 03:05:47.680: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.620146ms
    Aug 17 03:05:47.680: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:49.686: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309559s
    Aug 17 03:05:49.686: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:51.686: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013036287s
    Aug 17 03:05:51.686: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 03:05:51.686: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod1:[80]] 08/17/23 03:05:51.69
    Aug 17 03:05:51.705: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/17/23 03:05:51.705
    Aug 17 03:05:51.705: INFO: Creating new exec pod
    Aug 17 03:05:51.711: INFO: Waiting up to 5m0s for pod "execpodbcpzs" in namespace "services-5072" to be "running"
    Aug 17 03:05:51.721: INFO: Pod "execpodbcpzs": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022053ms
    Aug 17 03:05:53.726: INFO: Pod "execpodbcpzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.015556126s
    Aug 17 03:05:53.726: INFO: Pod "execpodbcpzs" satisfied condition "running"
    Aug 17 03:05:54.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 03:05:55.227: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 03:05:55.227: INFO: stdout: ""
    Aug 17 03:05:55.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
    Aug 17 03:05:55.754: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
    Aug 17 03:05:55.754: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5072 08/17/23 03:05:55.754
    Aug 17 03:05:55.761: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5072" to be "running and ready"
    Aug 17 03:05:55.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295391ms
    Aug 17 03:05:55.765: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:05:57.772: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294484s
    Aug 17 03:05:57.772: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 03:05:57.772: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod1:[80] pod2:[80]] 08/17/23 03:05:57.777
    Aug 17 03:05:57.801: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/17/23 03:05:57.801
    Aug 17 03:05:58.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 03:05:59.275: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 03:05:59.275: INFO: stdout: ""
    Aug 17 03:05:59.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
    Aug 17 03:05:59.746: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
    Aug 17 03:05:59.746: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5072 08/17/23 03:05:59.746
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[pod2:[80]] 08/17/23 03:05:59.777
    Aug 17 03:05:59.802: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/17/23 03:05:59.802
    Aug 17 03:06:00.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 17 03:06:01.207: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 17 03:06:01.207: INFO: stdout: ""
    Aug 17 03:06:01.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5072 exec execpodbcpzs -- /bin/sh -x -c nc -v -z -w 2 172.20.22.146 80'
    Aug 17 03:06:01.583: INFO: stderr: "+ nc -v -z -w 2 172.20.22.146 80\nConnection to 172.20.22.146 80 port [tcp/http] succeeded!\n"
    Aug 17 03:06:01.583: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5072 08/17/23 03:06:01.583
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5072 to expose endpoints map[] 08/17/23 03:06:01.597
    Aug 17 03:06:01.611: INFO: successfully validated that service endpoint-test2 in namespace services-5072 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:01.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5072" for this suite. 08/17/23 03:06:01.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:01.646
Aug 17 03:06:01.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 03:06:01.648
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:01.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:01.67
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/17/23 03:06:01.68
STEP: Verify that the required pods have come up. 08/17/23 03:06:01.686
Aug 17 03:06:01.691: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 03:06:06.696: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 03:06:06.696
STEP: Getting /status 08/17/23 03:06:06.697
Aug 17 03:06:06.703: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/17/23 03:06:06.703
Aug 17 03:06:06.714: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/17/23 03:06:06.716
Aug 17 03:06:06.719: INFO: Observed &ReplicaSet event: ADDED
Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.720: INFO: Found replicaset test-rs in namespace replicaset-3784 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 03:06:06.720: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/17/23 03:06:06.72
Aug 17 03:06:06.720: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 03:06:06.729: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/17/23 03:06:06.729
Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: ADDED
Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.732: INFO: Observed replicaset test-rs in namespace replicaset-3784 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 03:06:06.732: INFO: Observed &ReplicaSet event: MODIFIED
Aug 17 03:06:06.732: INFO: Found replicaset test-rs in namespace replicaset-3784 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 17 03:06:06.732: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:06.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3784" for this suite. 08/17/23 03:06:06.738
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:01.646
    Aug 17 03:06:01.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 03:06:01.648
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:01.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:01.67
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/17/23 03:06:01.68
    STEP: Verify that the required pods have come up. 08/17/23 03:06:01.686
    Aug 17 03:06:01.691: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 03:06:06.696: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 03:06:06.696
    STEP: Getting /status 08/17/23 03:06:06.697
    Aug 17 03:06:06.703: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/17/23 03:06:06.703
    Aug 17 03:06:06.714: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/17/23 03:06:06.716
    Aug 17 03:06:06.719: INFO: Observed &ReplicaSet event: ADDED
    Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.720: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.720: INFO: Found replicaset test-rs in namespace replicaset-3784 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 03:06:06.720: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/17/23 03:06:06.72
    Aug 17 03:06:06.720: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 03:06:06.729: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/17/23 03:06:06.729
    Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: ADDED
    Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.731: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.732: INFO: Observed replicaset test-rs in namespace replicaset-3784 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 03:06:06.732: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 17 03:06:06.732: INFO: Found replicaset test-rs in namespace replicaset-3784 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 17 03:06:06.732: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:06.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3784" for this suite. 08/17/23 03:06:06.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:06.745
Aug 17 03:06:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:06:06.746
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:06.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:06.763
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/17/23 03:06:06.767
Aug 17 03:06:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:06:08.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:14.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8630" for this suite. 08/17/23 03:06:14.236
------------------------------
• [SLOW TEST] [7.499 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:06.745
    Aug 17 03:06:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:06:06.746
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:06.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:06.763
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/17/23 03:06:06.767
    Aug 17 03:06:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:06:08.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:14.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8630" for this suite. 08/17/23 03:06:14.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:14.247
Aug 17 03:06:14.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 03:06:14.248
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:14.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:14.271
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 17 03:06:14.285: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 17 03:06:19.291: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 03:06:19.291
Aug 17 03:06:19.291: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/17/23 03:06:19.306
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 03:06:21.330: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9988  b5e88eba-4534-433e-9fb3-96b548a68b48 214881 1 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 03:06:19 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-08-17 03:06:20 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 03:06:21.335: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9988  67c51101-129b-4522-8bf2-7cda70c0c1ff 214871 1 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b5e88eba-4534-433e-9fb3-96b548a68b48 0xc00397afa7 0xc00397afa8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5e88eba-4534-433e-9fb3-96b548a68b48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00397b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 03:06:21.339: INFO: Pod "test-cleanup-deployment-7698ff6f6b-v7n8l" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-v7n8l test-cleanup-deployment-7698ff6f6b- deployment-9988  e2f6ba26-dfd4-43e2-ab5b-f28b86af0ce1 214870 0 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:2855eb82f5ff64bcece44dc3495792f2e472f95200fdf221f5ea4555b0bab21f cni.projectcalico.org/podIP:172.21.15.71/32 cni.projectcalico.org/podIPs:172.21.15.71/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 67c51101-129b-4522-8bf2-7cda70c0c1ff 0xc00397b3f7 0xc00397b3f8}] [] [{kube-controller-manager Update v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67c51101-129b-4522-8bf2-7cda70c0c1ff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q47n6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q47n6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.71,StartTime:2023-08-17 03:06:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:06:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://89a49cf573b647157f1dd76799f36183d2c9a97d0d64cc207e2780b67fc9931b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:21.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9988" for this suite. 08/17/23 03:06:21.347
------------------------------
• [SLOW TEST] [7.107 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:14.247
    Aug 17 03:06:14.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 03:06:14.248
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:14.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:14.271
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 17 03:06:14.285: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 17 03:06:19.291: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 03:06:19.291
    Aug 17 03:06:19.291: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/17/23 03:06:19.306
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 03:06:21.330: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9988  b5e88eba-4534-433e-9fb3-96b548a68b48 214881 1 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 03:06:19 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-08-17 03:06:20 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 03:06:21.335: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9988  67c51101-129b-4522-8bf2-7cda70c0c1ff 214871 1 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b5e88eba-4534-433e-9fb3-96b548a68b48 0xc00397afa7 0xc00397afa8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5e88eba-4534-433e-9fb3-96b548a68b48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00397b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 03:06:21.339: INFO: Pod "test-cleanup-deployment-7698ff6f6b-v7n8l" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-v7n8l test-cleanup-deployment-7698ff6f6b- deployment-9988  e2f6ba26-dfd4-43e2-ab5b-f28b86af0ce1 214870 0 2023-08-17 03:06:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:2855eb82f5ff64bcece44dc3495792f2e472f95200fdf221f5ea4555b0bab21f cni.projectcalico.org/podIP:172.21.15.71/32 cni.projectcalico.org/podIPs:172.21.15.71/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 67c51101-129b-4522-8bf2-7cda70c0c1ff 0xc00397b3f7 0xc00397b3f8}] [] [{kube-controller-manager Update v1 2023-08-17 03:06:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67c51101-129b-4522-8bf2-7cda70c0c1ff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q47n6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q47n6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:06:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.71,StartTime:2023-08-17 03:06:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:06:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://89a49cf573b647157f1dd76799f36183d2c9a97d0d64cc207e2780b67fc9931b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:21.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9988" for this suite. 08/17/23 03:06:21.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:21.356
Aug 17 03:06:21.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:06:21.356
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:21.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:21.372
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/17/23 03:06:21.376
STEP: Ensuring ResourceQuota status is calculated 08/17/23 03:06:21.382
STEP: Creating a ResourceQuota with not terminating scope 08/17/23 03:06:23.387
STEP: Ensuring ResourceQuota status is calculated 08/17/23 03:06:23.392
STEP: Creating a long running pod 08/17/23 03:06:25.399
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/17/23 03:06:25.413
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/17/23 03:06:27.418
STEP: Deleting the pod 08/17/23 03:06:29.425
STEP: Ensuring resource quota status released the pod usage 08/17/23 03:06:29.436
STEP: Creating a terminating pod 08/17/23 03:06:31.442
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/17/23 03:06:31.455
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/17/23 03:06:33.461
STEP: Deleting the pod 08/17/23 03:06:35.467
STEP: Ensuring resource quota status released the pod usage 08/17/23 03:06:35.478
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:37.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6953" for this suite. 08/17/23 03:06:37.49
------------------------------
• [SLOW TEST] [16.142 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:21.356
    Aug 17 03:06:21.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:06:21.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:21.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:21.372
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/17/23 03:06:21.376
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 03:06:21.382
    STEP: Creating a ResourceQuota with not terminating scope 08/17/23 03:06:23.387
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 03:06:23.392
    STEP: Creating a long running pod 08/17/23 03:06:25.399
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/17/23 03:06:25.413
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/17/23 03:06:27.418
    STEP: Deleting the pod 08/17/23 03:06:29.425
    STEP: Ensuring resource quota status released the pod usage 08/17/23 03:06:29.436
    STEP: Creating a terminating pod 08/17/23 03:06:31.442
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/17/23 03:06:31.455
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/17/23 03:06:33.461
    STEP: Deleting the pod 08/17/23 03:06:35.467
    STEP: Ensuring resource quota status released the pod usage 08/17/23 03:06:35.478
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:37.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6953" for this suite. 08/17/23 03:06:37.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:37.5
Aug 17 03:06:37.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename watch 08/17/23 03:06:37.501
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:37.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:37.52
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/17/23 03:06:37.524
STEP: creating a new configmap 08/17/23 03:06:37.525
STEP: modifying the configmap once 08/17/23 03:06:37.531
STEP: closing the watch once it receives two notifications 08/17/23 03:06:37.54
Aug 17 03:06:37.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214995 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:06:37.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214996 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/17/23 03:06:37.54
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/17/23 03:06:37.55
STEP: deleting the configmap 08/17/23 03:06:37.552
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/17/23 03:06:37.559
Aug 17 03:06:37.559: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214997 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:06:37.559: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214998 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5017" for this suite. 08/17/23 03:06:37.566
------------------------------
• [0.073 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:37.5
    Aug 17 03:06:37.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename watch 08/17/23 03:06:37.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:37.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:37.52
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/17/23 03:06:37.524
    STEP: creating a new configmap 08/17/23 03:06:37.525
    STEP: modifying the configmap once 08/17/23 03:06:37.531
    STEP: closing the watch once it receives two notifications 08/17/23 03:06:37.54
    Aug 17 03:06:37.540: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214995 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:06:37.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214996 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/17/23 03:06:37.54
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/17/23 03:06:37.55
    STEP: deleting the configmap 08/17/23 03:06:37.552
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/17/23 03:06:37.559
    Aug 17 03:06:37.559: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214997 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:06:37.559: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5017  5dca932f-454d-4672-80bc-2c285f1b1937 214998 0 2023-08-17 03:06:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-17 03:06:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5017" for this suite. 08/17/23 03:06:37.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:37.575
Aug 17 03:06:37.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename containers 08/17/23 03:06:37.576
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:37.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:37.593
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/17/23 03:06:37.598
Aug 17 03:06:37.607: INFO: Waiting up to 5m0s for pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327" in namespace "containers-2439" to be "Succeeded or Failed"
Aug 17 03:06:37.611: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579281ms
Aug 17 03:06:39.622: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015619775s
Aug 17 03:06:41.618: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011330273s
STEP: Saw pod success 08/17/23 03:06:41.618
Aug 17 03:06:41.618: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327" satisfied condition "Succeeded or Failed"
Aug 17 03:06:41.623: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:06:41.636
Aug 17 03:06:41.646: INFO: Waiting for pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 to disappear
Aug 17 03:06:41.650: INFO: Pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:06:41.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2439" for this suite. 08/17/23 03:06:41.657
------------------------------
• [4.089 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:37.575
    Aug 17 03:06:37.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename containers 08/17/23 03:06:37.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:37.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:37.593
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/17/23 03:06:37.598
    Aug 17 03:06:37.607: INFO: Waiting up to 5m0s for pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327" in namespace "containers-2439" to be "Succeeded or Failed"
    Aug 17 03:06:37.611: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579281ms
    Aug 17 03:06:39.622: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015619775s
    Aug 17 03:06:41.618: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011330273s
    STEP: Saw pod success 08/17/23 03:06:41.618
    Aug 17 03:06:41.618: INFO: Pod "client-containers-288dfbab-060c-4b43-a360-dc38c92ca327" satisfied condition "Succeeded or Failed"
    Aug 17 03:06:41.623: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:06:41.636
    Aug 17 03:06:41.646: INFO: Waiting for pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 to disappear
    Aug 17 03:06:41.650: INFO: Pod client-containers-288dfbab-060c-4b43-a360-dc38c92ca327 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:06:41.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2439" for this suite. 08/17/23 03:06:41.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:06:41.665
Aug 17 03:06:41.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 03:06:41.666
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:41.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:41.682
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 in namespace container-probe-8913 08/17/23 03:06:41.686
Aug 17 03:06:41.695: INFO: Waiting up to 5m0s for pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069" in namespace "container-probe-8913" to be "not pending"
Aug 17 03:06:41.700: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069": Phase="Pending", Reason="", readiness=false. Elapsed: 5.238701ms
Aug 17 03:06:43.707: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069": Phase="Running", Reason="", readiness=true. Elapsed: 2.012128827s
Aug 17 03:06:43.707: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069" satisfied condition "not pending"
Aug 17 03:06:43.707: INFO: Started pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 in namespace container-probe-8913
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:06:43.707
Aug 17 03:06:43.711: INFO: Initial restart count of pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 is 0
STEP: deleting the pod 08/17/23 03:10:44.48
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 03:10:44.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8913" for this suite. 08/17/23 03:10:44.513
------------------------------
• [SLOW TEST] [242.856 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:06:41.665
    Aug 17 03:06:41.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 03:06:41.666
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:06:41.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:06:41.682
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 in namespace container-probe-8913 08/17/23 03:06:41.686
    Aug 17 03:06:41.695: INFO: Waiting up to 5m0s for pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069" in namespace "container-probe-8913" to be "not pending"
    Aug 17 03:06:41.700: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069": Phase="Pending", Reason="", readiness=false. Elapsed: 5.238701ms
    Aug 17 03:06:43.707: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069": Phase="Running", Reason="", readiness=true. Elapsed: 2.012128827s
    Aug 17 03:06:43.707: INFO: Pod "liveness-a1d42e10-b7a2-4780-a49c-e197e084f069" satisfied condition "not pending"
    Aug 17 03:06:43.707: INFO: Started pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 in namespace container-probe-8913
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:06:43.707
    Aug 17 03:06:43.711: INFO: Initial restart count of pod liveness-a1d42e10-b7a2-4780-a49c-e197e084f069 is 0
    STEP: deleting the pod 08/17/23 03:10:44.48
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:10:44.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8913" for this suite. 08/17/23 03:10:44.513
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:10:44.522
Aug 17 03:10:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:10:44.523
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:44.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:44.544
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-61254558-123d-4f8d-9ba7-d1220c559642 08/17/23 03:10:44.55
STEP: Creating a pod to test consume secrets 08/17/23 03:10:44.555
Aug 17 03:10:44.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51" in namespace "projected-5643" to be "Succeeded or Failed"
Aug 17 03:10:44.569: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.565852ms
Aug 17 03:10:46.575: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010971517s
Aug 17 03:10:48.574: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00913732s
STEP: Saw pod success 08/17/23 03:10:48.574
Aug 17 03:10:48.574: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51" satisfied condition "Succeeded or Failed"
Aug 17 03:10:48.578: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:10:48.63
Aug 17 03:10:48.640: INFO: Waiting for pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 to disappear
Aug 17 03:10:48.644: INFO: Pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 03:10:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5643" for this suite. 08/17/23 03:10:48.65
------------------------------
• [4.135 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:10:44.522
    Aug 17 03:10:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:10:44.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:44.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:44.544
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-61254558-123d-4f8d-9ba7-d1220c559642 08/17/23 03:10:44.55
    STEP: Creating a pod to test consume secrets 08/17/23 03:10:44.555
    Aug 17 03:10:44.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51" in namespace "projected-5643" to be "Succeeded or Failed"
    Aug 17 03:10:44.569: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.565852ms
    Aug 17 03:10:46.575: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010971517s
    Aug 17 03:10:48.574: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00913732s
    STEP: Saw pod success 08/17/23 03:10:48.574
    Aug 17 03:10:48.574: INFO: Pod "pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51" satisfied condition "Succeeded or Failed"
    Aug 17 03:10:48.578: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:10:48.63
    Aug 17 03:10:48.640: INFO: Waiting for pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 to disappear
    Aug 17 03:10:48.644: INFO: Pod pod-projected-secrets-5092afb4-ebfb-4eae-a89c-fa8845692c51 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:10:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5643" for this suite. 08/17/23 03:10:48.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:10:48.659
Aug 17 03:10:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 03:10:48.66
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:48.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:48.678
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 03:10:48.705
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:10:48.713
Aug 17 03:10:48.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:10:48.723: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:10:49.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:10:49.734: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:10:50.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:10:50.736: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/17/23 03:10:50.74
Aug 17 03:10:50.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:10:50.760: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:10:51.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:10:51.774: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:10:52.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:10:52.773: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:10:53.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:10:53.772: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:10:54.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:10:54.773: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:10:55.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:10:55.773: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:10:55.777
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6863, will wait for the garbage collector to delete the pods 08/17/23 03:10:55.777
Aug 17 03:10:55.838: INFO: Deleting DaemonSet.extensions daemon-set took: 6.875952ms
Aug 17 03:10:55.939: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848861ms
Aug 17 03:10:58.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:10:58.144: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 03:10:58.148: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"215967"},"items":null}

Aug 17 03:10:58.152: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"215967"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:10:58.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6863" for this suite. 08/17/23 03:10:58.172
------------------------------
• [SLOW TEST] [9.520 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:10:48.659
    Aug 17 03:10:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 03:10:48.66
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:48.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:48.678
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 03:10:48.705
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:10:48.713
    Aug 17 03:10:48.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:10:48.723: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:10:49.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:10:49.734: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:10:50.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:10:50.736: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/17/23 03:10:50.74
    Aug 17 03:10:50.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:10:50.760: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:10:51.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:10:51.774: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:10:52.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:10:52.773: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:10:53.772: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:10:53.772: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:10:54.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:10:54.773: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:10:55.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:10:55.773: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:10:55.777
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6863, will wait for the garbage collector to delete the pods 08/17/23 03:10:55.777
    Aug 17 03:10:55.838: INFO: Deleting DaemonSet.extensions daemon-set took: 6.875952ms
    Aug 17 03:10:55.939: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.848861ms
    Aug 17 03:10:58.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:10:58.144: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 03:10:58.148: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"215967"},"items":null}

    Aug 17 03:10:58.152: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"215967"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:10:58.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6863" for this suite. 08/17/23 03:10:58.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:10:58.188
Aug 17 03:10:58.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption 08/17/23 03:10:58.189
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:58.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:58.212
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/17/23 03:10:58.216
STEP: Waiting for the pdb to be processed 08/17/23 03:10:58.222
STEP: First trying to evict a pod which shouldn't be evictable 08/17/23 03:11:00.239
STEP: Waiting for all pods to be running 08/17/23 03:11:00.239
Aug 17 03:11:00.243: INFO: pods: 0 < 3
STEP: locating a running pod 08/17/23 03:11:02.248
STEP: Updating the pdb to allow a pod to be evicted 08/17/23 03:11:02.26
STEP: Waiting for the pdb to be processed 08/17/23 03:11:02.269
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 03:11:04.279
STEP: Waiting for all pods to be running 08/17/23 03:11:04.279
STEP: Waiting for the pdb to observed all healthy pods 08/17/23 03:11:04.283
STEP: Patching the pdb to disallow a pod to be evicted 08/17/23 03:11:04.308
STEP: Waiting for the pdb to be processed 08/17/23 03:11:04.321
STEP: Waiting for all pods to be running 08/17/23 03:11:06.332
STEP: locating a running pod 08/17/23 03:11:06.337
STEP: Deleting the pdb to allow a pod to be evicted 08/17/23 03:11:06.351
STEP: Waiting for the pdb to be deleted 08/17/23 03:11:06.358
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 03:11:06.362
STEP: Waiting for all pods to be running 08/17/23 03:11:06.362
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:06.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9883" for this suite. 08/17/23 03:11:06.399
------------------------------
• [SLOW TEST] [8.218 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:10:58.188
    Aug 17 03:10:58.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption 08/17/23 03:10:58.189
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:10:58.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:10:58.212
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/17/23 03:10:58.216
    STEP: Waiting for the pdb to be processed 08/17/23 03:10:58.222
    STEP: First trying to evict a pod which shouldn't be evictable 08/17/23 03:11:00.239
    STEP: Waiting for all pods to be running 08/17/23 03:11:00.239
    Aug 17 03:11:00.243: INFO: pods: 0 < 3
    STEP: locating a running pod 08/17/23 03:11:02.248
    STEP: Updating the pdb to allow a pod to be evicted 08/17/23 03:11:02.26
    STEP: Waiting for the pdb to be processed 08/17/23 03:11:02.269
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 03:11:04.279
    STEP: Waiting for all pods to be running 08/17/23 03:11:04.279
    STEP: Waiting for the pdb to observed all healthy pods 08/17/23 03:11:04.283
    STEP: Patching the pdb to disallow a pod to be evicted 08/17/23 03:11:04.308
    STEP: Waiting for the pdb to be processed 08/17/23 03:11:04.321
    STEP: Waiting for all pods to be running 08/17/23 03:11:06.332
    STEP: locating a running pod 08/17/23 03:11:06.337
    STEP: Deleting the pdb to allow a pod to be evicted 08/17/23 03:11:06.351
    STEP: Waiting for the pdb to be deleted 08/17/23 03:11:06.358
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/17/23 03:11:06.362
    STEP: Waiting for all pods to be running 08/17/23 03:11:06.362
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:06.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9883" for this suite. 08/17/23 03:11:06.399
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:06.407
Aug 17 03:11:06.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename discovery 08/17/23 03:11:06.407
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:06.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:06.429
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/17/23 03:11:06.435
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 17 03:11:06.683: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 17 03:11:06.690: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 17 03:11:06.690: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 17 03:11:06.690: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 17 03:11:06.690: INFO: Checking APIGroup: apps
Aug 17 03:11:06.692: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 17 03:11:06.692: INFO: Versions found [{apps/v1 v1}]
Aug 17 03:11:06.692: INFO: apps/v1 matches apps/v1
Aug 17 03:11:06.692: INFO: Checking APIGroup: events.k8s.io
Aug 17 03:11:06.697: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 17 03:11:06.697: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 17 03:11:06.697: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 17 03:11:06.697: INFO: Checking APIGroup: authentication.k8s.io
Aug 17 03:11:06.701: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 17 03:11:06.701: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 17 03:11:06.701: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 17 03:11:06.701: INFO: Checking APIGroup: authorization.k8s.io
Aug 17 03:11:06.704: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 17 03:11:06.704: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 17 03:11:06.704: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 17 03:11:06.704: INFO: Checking APIGroup: autoscaling
Aug 17 03:11:06.706: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 17 03:11:06.706: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 17 03:11:06.706: INFO: autoscaling/v2 matches autoscaling/v2
Aug 17 03:11:06.706: INFO: Checking APIGroup: batch
Aug 17 03:11:06.708: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 17 03:11:06.708: INFO: Versions found [{batch/v1 v1}]
Aug 17 03:11:06.708: INFO: batch/v1 matches batch/v1
Aug 17 03:11:06.708: INFO: Checking APIGroup: certificates.k8s.io
Aug 17 03:11:06.709: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 17 03:11:06.709: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 17 03:11:06.709: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 17 03:11:06.709: INFO: Checking APIGroup: networking.k8s.io
Aug 17 03:11:06.711: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 17 03:11:06.711: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 17 03:11:06.711: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 17 03:11:06.711: INFO: Checking APIGroup: policy
Aug 17 03:11:06.713: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 17 03:11:06.713: INFO: Versions found [{policy/v1 v1}]
Aug 17 03:11:06.713: INFO: policy/v1 matches policy/v1
Aug 17 03:11:06.713: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 17 03:11:06.717: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 17 03:11:06.717: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 17 03:11:06.717: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 17 03:11:06.717: INFO: Checking APIGroup: storage.k8s.io
Aug 17 03:11:06.724: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 17 03:11:06.724: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 17 03:11:06.724: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 17 03:11:06.724: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 17 03:11:06.728: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 17 03:11:06.728: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 17 03:11:06.728: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 17 03:11:06.728: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 17 03:11:06.732: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 17 03:11:06.732: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 17 03:11:06.732: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 17 03:11:06.732: INFO: Checking APIGroup: scheduling.k8s.io
Aug 17 03:11:06.734: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 17 03:11:06.734: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 17 03:11:06.734: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 17 03:11:06.734: INFO: Checking APIGroup: coordination.k8s.io
Aug 17 03:11:06.735: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 17 03:11:06.736: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 17 03:11:06.736: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 17 03:11:06.736: INFO: Checking APIGroup: node.k8s.io
Aug 17 03:11:06.737: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 17 03:11:06.737: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 17 03:11:06.737: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 17 03:11:06.737: INFO: Checking APIGroup: discovery.k8s.io
Aug 17 03:11:06.739: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 17 03:11:06.739: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 17 03:11:06.739: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 17 03:11:06.739: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 17 03:11:06.741: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 17 03:11:06.741: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 17 03:11:06.741: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 17 03:11:06.741: INFO: Checking APIGroup: crd.projectcalico.org
Aug 17 03:11:06.742: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 17 03:11:06.742: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 17 03:11:06.742: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 17 03:11:06.742: INFO: Checking APIGroup: metrics.k8s.io
Aug 17 03:11:06.744: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 17 03:11:06.744: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 17 03:11:06.744: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:06.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-126" for this suite. 08/17/23 03:11:06.752
------------------------------
• [0.353 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:06.407
    Aug 17 03:11:06.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename discovery 08/17/23 03:11:06.407
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:06.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:06.429
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/17/23 03:11:06.435
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 17 03:11:06.683: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 17 03:11:06.690: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 17 03:11:06.690: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 17 03:11:06.690: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 17 03:11:06.690: INFO: Checking APIGroup: apps
    Aug 17 03:11:06.692: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 17 03:11:06.692: INFO: Versions found [{apps/v1 v1}]
    Aug 17 03:11:06.692: INFO: apps/v1 matches apps/v1
    Aug 17 03:11:06.692: INFO: Checking APIGroup: events.k8s.io
    Aug 17 03:11:06.697: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 17 03:11:06.697: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 17 03:11:06.697: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 17 03:11:06.697: INFO: Checking APIGroup: authentication.k8s.io
    Aug 17 03:11:06.701: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 17 03:11:06.701: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 17 03:11:06.701: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 17 03:11:06.701: INFO: Checking APIGroup: authorization.k8s.io
    Aug 17 03:11:06.704: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 17 03:11:06.704: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 17 03:11:06.704: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 17 03:11:06.704: INFO: Checking APIGroup: autoscaling
    Aug 17 03:11:06.706: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 17 03:11:06.706: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 17 03:11:06.706: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 17 03:11:06.706: INFO: Checking APIGroup: batch
    Aug 17 03:11:06.708: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 17 03:11:06.708: INFO: Versions found [{batch/v1 v1}]
    Aug 17 03:11:06.708: INFO: batch/v1 matches batch/v1
    Aug 17 03:11:06.708: INFO: Checking APIGroup: certificates.k8s.io
    Aug 17 03:11:06.709: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 17 03:11:06.709: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 17 03:11:06.709: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 17 03:11:06.709: INFO: Checking APIGroup: networking.k8s.io
    Aug 17 03:11:06.711: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 17 03:11:06.711: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 17 03:11:06.711: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 17 03:11:06.711: INFO: Checking APIGroup: policy
    Aug 17 03:11:06.713: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 17 03:11:06.713: INFO: Versions found [{policy/v1 v1}]
    Aug 17 03:11:06.713: INFO: policy/v1 matches policy/v1
    Aug 17 03:11:06.713: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 17 03:11:06.717: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 17 03:11:06.717: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 17 03:11:06.717: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 17 03:11:06.717: INFO: Checking APIGroup: storage.k8s.io
    Aug 17 03:11:06.724: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 17 03:11:06.724: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 17 03:11:06.724: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 17 03:11:06.724: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 17 03:11:06.728: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 17 03:11:06.728: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 17 03:11:06.728: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 17 03:11:06.728: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 17 03:11:06.732: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 17 03:11:06.732: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 17 03:11:06.732: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 17 03:11:06.732: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 17 03:11:06.734: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 17 03:11:06.734: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 17 03:11:06.734: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 17 03:11:06.734: INFO: Checking APIGroup: coordination.k8s.io
    Aug 17 03:11:06.735: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 17 03:11:06.736: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 17 03:11:06.736: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 17 03:11:06.736: INFO: Checking APIGroup: node.k8s.io
    Aug 17 03:11:06.737: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 17 03:11:06.737: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 17 03:11:06.737: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 17 03:11:06.737: INFO: Checking APIGroup: discovery.k8s.io
    Aug 17 03:11:06.739: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 17 03:11:06.739: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 17 03:11:06.739: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 17 03:11:06.739: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 17 03:11:06.741: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 17 03:11:06.741: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 17 03:11:06.741: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 17 03:11:06.741: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 17 03:11:06.742: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 17 03:11:06.742: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 17 03:11:06.742: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 17 03:11:06.742: INFO: Checking APIGroup: metrics.k8s.io
    Aug 17 03:11:06.744: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 17 03:11:06.744: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 17 03:11:06.744: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:06.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-126" for this suite. 08/17/23 03:11:06.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:06.76
Aug 17 03:11:06.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:11:06.761
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:06.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:06.781
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-a88ff70c-2473-422a-97ed-fe3a3adff572 08/17/23 03:11:06.785
STEP: Creating a pod to test consume configMaps 08/17/23 03:11:06.79
Aug 17 03:11:06.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864" in namespace "configmap-6275" to be "Succeeded or Failed"
Aug 17 03:11:06.812: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Pending", Reason="", readiness=false. Elapsed: 12.891482ms
Aug 17 03:11:08.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018785896s
Aug 17 03:11:10.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018355718s
STEP: Saw pod success 08/17/23 03:11:10.818
Aug 17 03:11:10.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864" satisfied condition "Succeeded or Failed"
Aug 17 03:11:10.823: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:11:10.833
Aug 17 03:11:10.845: INFO: Waiting for pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 to disappear
Aug 17 03:11:10.848: INFO: Pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6275" for this suite. 08/17/23 03:11:10.855
------------------------------
• [4.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:06.76
    Aug 17 03:11:06.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:11:06.761
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:06.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:06.781
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-a88ff70c-2473-422a-97ed-fe3a3adff572 08/17/23 03:11:06.785
    STEP: Creating a pod to test consume configMaps 08/17/23 03:11:06.79
    Aug 17 03:11:06.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864" in namespace "configmap-6275" to be "Succeeded or Failed"
    Aug 17 03:11:06.812: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Pending", Reason="", readiness=false. Elapsed: 12.891482ms
    Aug 17 03:11:08.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018785896s
    Aug 17 03:11:10.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018355718s
    STEP: Saw pod success 08/17/23 03:11:10.818
    Aug 17 03:11:10.818: INFO: Pod "pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864" satisfied condition "Succeeded or Failed"
    Aug 17 03:11:10.823: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:11:10.833
    Aug 17 03:11:10.845: INFO: Waiting for pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 to disappear
    Aug 17 03:11:10.848: INFO: Pod pod-configmaps-8257bad1-a44d-4525-be42-55014b9b3864 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6275" for this suite. 08/17/23 03:11:10.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:10.867
Aug 17 03:11:10.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:11:10.868
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:10.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:10.886
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/17/23 03:11:10.891
Aug 17 03:11:10.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 create -f -'
Aug 17 03:11:11.373: INFO: stderr: ""
Aug 17 03:11:11.373: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/17/23 03:11:11.373
Aug 17 03:11:11.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 diff -f -'
Aug 17 03:11:11.625: INFO: rc: 1
Aug 17 03:11:11.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 delete -f -'
Aug 17 03:11:11.720: INFO: stderr: ""
Aug 17 03:11:11.720: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:11.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5752" for this suite. 08/17/23 03:11:11.729
------------------------------
• [0.869 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:10.867
    Aug 17 03:11:10.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:11:10.868
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:10.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:10.886
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/17/23 03:11:10.891
    Aug 17 03:11:10.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 create -f -'
    Aug 17 03:11:11.373: INFO: stderr: ""
    Aug 17 03:11:11.373: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/17/23 03:11:11.373
    Aug 17 03:11:11.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 diff -f -'
    Aug 17 03:11:11.625: INFO: rc: 1
    Aug 17 03:11:11.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5752 delete -f -'
    Aug 17 03:11:11.720: INFO: stderr: ""
    Aug 17 03:11:11.720: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:11.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5752" for this suite. 08/17/23 03:11:11.729
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:11.736
Aug 17 03:11:11.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:11:11.737
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:11.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:11.756
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6195.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6195.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/17/23 03:11:11.761
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6195.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6195.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/17/23 03:11:11.762
STEP: creating a pod to probe /etc/hosts 08/17/23 03:11:11.762
STEP: submitting the pod to kubernetes 08/17/23 03:11:11.762
Aug 17 03:11:11.771: INFO: Waiting up to 15m0s for pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d" in namespace "dns-6195" to be "running"
Aug 17 03:11:11.776: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756124ms
Aug 17 03:11:13.781: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01021909s
Aug 17 03:11:15.782: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011045529s
Aug 17 03:11:15.782: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:11:15.782
STEP: looking for the results for each expected name from probers 08/17/23 03:11:15.787
Aug 17 03:11:15.931: INFO: DNS probes using dns-6195/dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d succeeded

STEP: deleting the pod 08/17/23 03:11:15.931
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:15.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6195" for this suite. 08/17/23 03:11:15.957
------------------------------
• [4.227 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:11.736
    Aug 17 03:11:11.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:11:11.737
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:11.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:11.756
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6195.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6195.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/17/23 03:11:11.761
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6195.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6195.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/17/23 03:11:11.762
    STEP: creating a pod to probe /etc/hosts 08/17/23 03:11:11.762
    STEP: submitting the pod to kubernetes 08/17/23 03:11:11.762
    Aug 17 03:11:11.771: INFO: Waiting up to 15m0s for pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d" in namespace "dns-6195" to be "running"
    Aug 17 03:11:11.776: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756124ms
    Aug 17 03:11:13.781: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01021909s
    Aug 17 03:11:15.782: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011045529s
    Aug 17 03:11:15.782: INFO: Pod "dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:11:15.782
    STEP: looking for the results for each expected name from probers 08/17/23 03:11:15.787
    Aug 17 03:11:15.931: INFO: DNS probes using dns-6195/dns-test-a6f8d744-8014-47e4-90c4-1d8ca59f1b9d succeeded

    STEP: deleting the pod 08/17/23 03:11:15.931
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:15.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6195" for this suite. 08/17/23 03:11:15.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:15.965
Aug 17 03:11:15.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:11:15.966
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:15.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:15.982
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/17/23 03:11:15.988
Aug 17 03:11:15.988: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 17 03:11:15.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:16.211: INFO: stderr: ""
Aug 17 03:11:16.212: INFO: stdout: "service/agnhost-replica created\n"
Aug 17 03:11:16.212: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 17 03:11:16.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:16.442: INFO: stderr: ""
Aug 17 03:11:16.442: INFO: stdout: "service/agnhost-primary created\n"
Aug 17 03:11:16.442: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 17 03:11:16.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:16.635: INFO: stderr: ""
Aug 17 03:11:16.635: INFO: stdout: "service/frontend created\n"
Aug 17 03:11:16.635: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 17 03:11:16.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:16.855: INFO: stderr: ""
Aug 17 03:11:16.855: INFO: stdout: "deployment.apps/frontend created\n"
Aug 17 03:11:16.856: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 17 03:11:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:17.070: INFO: stderr: ""
Aug 17 03:11:17.070: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 17 03:11:17.070: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 17 03:11:17.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
Aug 17 03:11:17.337: INFO: stderr: ""
Aug 17 03:11:17.337: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/17/23 03:11:17.337
Aug 17 03:11:17.337: INFO: Waiting for all frontend pods to be Running.
Aug 17 03:11:22.390: INFO: Waiting for frontend to serve content.
Aug 17 03:11:22.484: INFO: Trying to add a new entry to the guestbook.
Aug 17 03:11:22.613: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/17/23 03:11:22.659
Aug 17 03:11:22.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:22.739: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:22.739: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 03:11:22.739
Aug 17 03:11:22.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:22.834: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:22.834: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 03:11:22.835
Aug 17 03:11:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:22.912: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:22.912: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 03:11:22.912
Aug 17 03:11:22.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:22.982: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:22.982: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 03:11:22.982
Aug 17 03:11:22.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:23.118: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:23.118: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/17/23 03:11:23.118
Aug 17 03:11:23.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
Aug 17 03:11:23.281: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:11:23.281: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:23.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-166" for this suite. 08/17/23 03:11:23.288
------------------------------
• [SLOW TEST] [7.338 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:15.965
    Aug 17 03:11:15.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:11:15.966
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:15.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:15.982
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/17/23 03:11:15.988
    Aug 17 03:11:15.988: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 17 03:11:15.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:16.211: INFO: stderr: ""
    Aug 17 03:11:16.212: INFO: stdout: "service/agnhost-replica created\n"
    Aug 17 03:11:16.212: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 17 03:11:16.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:16.442: INFO: stderr: ""
    Aug 17 03:11:16.442: INFO: stdout: "service/agnhost-primary created\n"
    Aug 17 03:11:16.442: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 17 03:11:16.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:16.635: INFO: stderr: ""
    Aug 17 03:11:16.635: INFO: stdout: "service/frontend created\n"
    Aug 17 03:11:16.635: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 17 03:11:16.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:16.855: INFO: stderr: ""
    Aug 17 03:11:16.855: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 17 03:11:16.856: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 17 03:11:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:17.070: INFO: stderr: ""
    Aug 17 03:11:17.070: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 17 03:11:17.070: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 17 03:11:17.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 create -f -'
    Aug 17 03:11:17.337: INFO: stderr: ""
    Aug 17 03:11:17.337: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/17/23 03:11:17.337
    Aug 17 03:11:17.337: INFO: Waiting for all frontend pods to be Running.
    Aug 17 03:11:22.390: INFO: Waiting for frontend to serve content.
    Aug 17 03:11:22.484: INFO: Trying to add a new entry to the guestbook.
    Aug 17 03:11:22.613: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/17/23 03:11:22.659
    Aug 17 03:11:22.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:22.739: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:22.739: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 03:11:22.739
    Aug 17 03:11:22.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:22.834: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:22.834: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 03:11:22.835
    Aug 17 03:11:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:22.912: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:22.912: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 03:11:22.912
    Aug 17 03:11:22.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:22.982: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:22.982: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 03:11:22.982
    Aug 17 03:11:22.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:23.118: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:23.118: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/17/23 03:11:23.118
    Aug 17 03:11:23.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-166 delete --grace-period=0 --force -f -'
    Aug 17 03:11:23.281: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:11:23.281: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:23.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-166" for this suite. 08/17/23 03:11:23.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:23.304
Aug 17 03:11:23.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/17/23 03:11:23.305
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:23.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:23.323
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/17/23 03:11:23.328
STEP: Creating hostNetwork=false pod 08/17/23 03:11:23.328
Aug 17 03:11:23.337: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4653" to be "running and ready"
Aug 17 03:11:23.341: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.937509ms
Aug 17 03:11:23.342: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:11:25.347: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00909178s
Aug 17 03:11:25.347: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:11:27.347: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487721s
Aug 17 03:11:27.347: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 17 03:11:27.347: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/17/23 03:11:27.351
Aug 17 03:11:27.357: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4653" to be "running and ready"
Aug 17 03:11:27.362: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666151ms
Aug 17 03:11:27.362: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:11:29.368: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010937964s
Aug 17 03:11:29.368: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 17 03:11:29.368: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/17/23 03:11:29.372
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/17/23 03:11:29.373
Aug 17 03:11:29.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:29.373: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:29.373: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 03:11:29.812: INFO: Exec stderr: ""
Aug 17 03:11:29.812: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:29.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:29.813: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:29.813: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 03:11:30.280: INFO: Exec stderr: ""
Aug 17 03:11:30.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:30.280: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:30.280: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 03:11:30.591: INFO: Exec stderr: ""
Aug 17 03:11:30.591: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:30.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:30.591: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:30.592: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 03:11:31.065: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/17/23 03:11:31.065
Aug 17 03:11:31.066: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:31.066: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:31.066: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 17 03:11:31.380: INFO: Exec stderr: ""
Aug 17 03:11:31.380: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:31.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:31.381: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:31.381: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 17 03:11:31.768: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/17/23 03:11:31.769
Aug 17 03:11:31.769: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:31.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:31.769: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:31.769: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 03:11:32.143: INFO: Exec stderr: ""
Aug 17 03:11:32.143: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:32.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:32.143: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:32.144: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 17 03:11:32.492: INFO: Exec stderr: ""
Aug 17 03:11:32.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:32.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:32.493: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:32.493: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 03:11:32.948: INFO: Exec stderr: ""
Aug 17 03:11:32.948: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:32.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:32.948: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:32.948: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 17 03:11:33.243: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:33.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4653" for this suite. 08/17/23 03:11:33.251
------------------------------
• [SLOW TEST] [9.956 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:23.304
    Aug 17 03:11:23.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/17/23 03:11:23.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:23.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:23.323
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/17/23 03:11:23.328
    STEP: Creating hostNetwork=false pod 08/17/23 03:11:23.328
    Aug 17 03:11:23.337: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4653" to be "running and ready"
    Aug 17 03:11:23.341: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.937509ms
    Aug 17 03:11:23.342: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:11:25.347: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00909178s
    Aug 17 03:11:25.347: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:11:27.347: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487721s
    Aug 17 03:11:27.347: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 17 03:11:27.347: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/17/23 03:11:27.351
    Aug 17 03:11:27.357: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4653" to be "running and ready"
    Aug 17 03:11:27.362: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666151ms
    Aug 17 03:11:27.362: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:11:29.368: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010937964s
    Aug 17 03:11:29.368: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 17 03:11:29.368: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/17/23 03:11:29.372
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/17/23 03:11:29.373
    Aug 17 03:11:29.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:29.373: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:29.373: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 03:11:29.812: INFO: Exec stderr: ""
    Aug 17 03:11:29.812: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:29.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:29.813: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:29.813: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 03:11:30.280: INFO: Exec stderr: ""
    Aug 17 03:11:30.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:30.280: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:30.280: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 03:11:30.591: INFO: Exec stderr: ""
    Aug 17 03:11:30.591: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:30.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:30.591: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:30.592: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 03:11:31.065: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/17/23 03:11:31.065
    Aug 17 03:11:31.066: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:31.066: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:31.066: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 17 03:11:31.380: INFO: Exec stderr: ""
    Aug 17 03:11:31.380: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:31.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:31.381: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:31.381: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 17 03:11:31.768: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/17/23 03:11:31.769
    Aug 17 03:11:31.769: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:31.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:31.769: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:31.769: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 03:11:32.143: INFO: Exec stderr: ""
    Aug 17 03:11:32.143: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:32.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:32.143: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:32.144: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 17 03:11:32.492: INFO: Exec stderr: ""
    Aug 17 03:11:32.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:32.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:32.493: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:32.493: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 03:11:32.948: INFO: Exec stderr: ""
    Aug 17 03:11:32.948: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4653 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:32.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:32.948: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:32.948: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4653/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 17 03:11:33.243: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:33.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4653" for this suite. 08/17/23 03:11:33.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:33.262
Aug 17 03:11:33.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:11:33.263
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:33.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:33.281
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6916 08/17/23 03:11:33.286
STEP: creating a selector 08/17/23 03:11:33.286
STEP: Creating the service pods in kubernetes 08/17/23 03:11:33.286
Aug 17 03:11:33.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 03:11:33.311: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6916" to be "running and ready"
Aug 17 03:11:33.315: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588836ms
Aug 17 03:11:33.315: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:11:35.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009814888s
Aug 17 03:11:35.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:37.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009164445s
Aug 17 03:11:37.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:39.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008645615s
Aug 17 03:11:39.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:41.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012092065s
Aug 17 03:11:41.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:43.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008725253s
Aug 17 03:11:43.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:45.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010402925s
Aug 17 03:11:45.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:47.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010746146s
Aug 17 03:11:47.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:49.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009339439s
Aug 17 03:11:49.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:51.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009461935s
Aug 17 03:11:51.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:53.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009095838s
Aug 17 03:11:53.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:11:55.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011841031s
Aug 17 03:11:55.323: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 03:11:55.323: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 03:11:55.329: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6916" to be "running and ready"
Aug 17 03:11:55.333: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.410522ms
Aug 17 03:11:55.333: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 03:11:55.333: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 03:11:55.338
Aug 17 03:11:55.345: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6916" to be "running"
Aug 17 03:11:55.349: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.469248ms
Aug 17 03:11:57.355: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010592831s
Aug 17 03:11:57.355: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 03:11:57.360: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 17 03:11:57.360: INFO: Breadth first check of 172.21.86.185 on host 192.168.11.3...
Aug 17 03:11:57.364: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.85:9080/dial?request=hostname&protocol=udp&host=172.21.86.185&port=8081&tries=1'] Namespace:pod-network-test-6916 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:57.365: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:57.365: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-6916/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.86.185%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 03:11:57.811: INFO: Waiting for responses: map[]
Aug 17 03:11:57.811: INFO: reached 172.21.86.185 after 0/1 tries
Aug 17 03:11:57.811: INFO: Breadth first check of 172.21.15.123 on host 192.168.11.4...
Aug 17 03:11:57.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.85:9080/dial?request=hostname&protocol=udp&host=172.21.15.123&port=8081&tries=1'] Namespace:pod-network-test-6916 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:11:57.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:11:57.817: INFO: ExecWithOptions: Clientset creation
Aug 17 03:11:57.818: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-6916/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.15.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 03:11:58.208: INFO: Waiting for responses: map[]
Aug 17 03:11:58.208: INFO: reached 172.21.15.123 after 0/1 tries
Aug 17 03:11:58.208: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 03:11:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6916" for this suite. 08/17/23 03:11:58.217
------------------------------
• [SLOW TEST] [24.965 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:33.262
    Aug 17 03:11:33.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:11:33.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:33.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:33.281
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6916 08/17/23 03:11:33.286
    STEP: creating a selector 08/17/23 03:11:33.286
    STEP: Creating the service pods in kubernetes 08/17/23 03:11:33.286
    Aug 17 03:11:33.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 03:11:33.311: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6916" to be "running and ready"
    Aug 17 03:11:33.315: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588836ms
    Aug 17 03:11:33.315: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:11:35.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009814888s
    Aug 17 03:11:35.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:37.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009164445s
    Aug 17 03:11:37.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:39.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008645615s
    Aug 17 03:11:39.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:41.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012092065s
    Aug 17 03:11:41.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:43.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008725253s
    Aug 17 03:11:43.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:45.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010402925s
    Aug 17 03:11:45.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:47.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010746146s
    Aug 17 03:11:47.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:49.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009339439s
    Aug 17 03:11:49.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:51.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009461935s
    Aug 17 03:11:51.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:53.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009095838s
    Aug 17 03:11:53.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:11:55.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011841031s
    Aug 17 03:11:55.323: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 03:11:55.323: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 03:11:55.329: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6916" to be "running and ready"
    Aug 17 03:11:55.333: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.410522ms
    Aug 17 03:11:55.333: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 03:11:55.333: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 03:11:55.338
    Aug 17 03:11:55.345: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6916" to be "running"
    Aug 17 03:11:55.349: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.469248ms
    Aug 17 03:11:57.355: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010592831s
    Aug 17 03:11:57.355: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 03:11:57.360: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 17 03:11:57.360: INFO: Breadth first check of 172.21.86.185 on host 192.168.11.3...
    Aug 17 03:11:57.364: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.85:9080/dial?request=hostname&protocol=udp&host=172.21.86.185&port=8081&tries=1'] Namespace:pod-network-test-6916 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:57.365: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:57.365: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-6916/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.86.185%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 03:11:57.811: INFO: Waiting for responses: map[]
    Aug 17 03:11:57.811: INFO: reached 172.21.86.185 after 0/1 tries
    Aug 17 03:11:57.811: INFO: Breadth first check of 172.21.15.123 on host 192.168.11.4...
    Aug 17 03:11:57.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.85:9080/dial?request=hostname&protocol=udp&host=172.21.15.123&port=8081&tries=1'] Namespace:pod-network-test-6916 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:11:57.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:11:57.817: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:11:57.818: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-6916/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.15.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 03:11:58.208: INFO: Waiting for responses: map[]
    Aug 17 03:11:58.208: INFO: reached 172.21.15.123 after 0/1 tries
    Aug 17 03:11:58.208: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:11:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6916" for this suite. 08/17/23 03:11:58.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:11:58.228
Aug 17 03:11:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:11:58.229
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:58.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:58.249
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-3434 08/17/23 03:11:58.255
STEP: creating service affinity-nodeport-transition in namespace services-3434 08/17/23 03:11:58.255
STEP: creating replication controller affinity-nodeport-transition in namespace services-3434 08/17/23 03:11:58.271
I0817 03:11:58.278578      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3434, replica count: 3
I0817 03:12:01.329673      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:12:01.352: INFO: Creating new exec pod
Aug 17 03:12:01.361: INFO: Waiting up to 5m0s for pod "execpod-affinityngldg" in namespace "services-3434" to be "running"
Aug 17 03:12:01.370: INFO: Pod "execpod-affinityngldg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.554321ms
Aug 17 03:12:03.376: INFO: Pod "execpod-affinityngldg": Phase="Running", Reason="", readiness=true. Elapsed: 2.01487211s
Aug 17 03:12:03.376: INFO: Pod "execpod-affinityngldg" satisfied condition "running"
Aug 17 03:12:04.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 17 03:12:04.920: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 17 03:12:04.920: INFO: stdout: ""
Aug 17 03:12:04.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 172.20.108.152 80'
Aug 17 03:12:05.378: INFO: stderr: "+ nc -v -z -w 2 172.20.108.152 80\nConnection to 172.20.108.152 80 port [tcp/http] succeeded!\n"
Aug 17 03:12:05.378: INFO: stdout: ""
Aug 17 03:12:05.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 31779'
Aug 17 03:12:05.752: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 31779\nConnection to 192.168.11.3 31779 port [tcp/*] succeeded!\n"
Aug 17 03:12:05.752: INFO: stdout: ""
Aug 17 03:12:05.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 31779'
Aug 17 03:12:06.289: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 31779\nConnection to 192.168.11.4 31779 port [tcp/*] succeeded!\n"
Aug 17 03:12:06.289: INFO: stdout: ""
Aug 17 03:12:06.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:31779/ ; done'
Aug 17 03:12:06.876: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n"
Aug 17 03:12:06.876: INFO: stdout: "\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7"
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:31779/ ; done'
Aug 17 03:12:07.486: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n"
Aug 17 03:12:07.486: INFO: stdout: "\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7"
Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
Aug 17 03:12:07.487: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3434, will wait for the garbage collector to delete the pods 08/17/23 03:12:07.509
Aug 17 03:12:07.574: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.336509ms
Aug 17 03:12:07.675: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.775239ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:12:09.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3434" for this suite. 08/17/23 03:12:09.503
------------------------------
• [SLOW TEST] [11.284 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:11:58.228
    Aug 17 03:11:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:11:58.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:11:58.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:11:58.249
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-3434 08/17/23 03:11:58.255
    STEP: creating service affinity-nodeport-transition in namespace services-3434 08/17/23 03:11:58.255
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3434 08/17/23 03:11:58.271
    I0817 03:11:58.278578      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3434, replica count: 3
    I0817 03:12:01.329673      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:12:01.352: INFO: Creating new exec pod
    Aug 17 03:12:01.361: INFO: Waiting up to 5m0s for pod "execpod-affinityngldg" in namespace "services-3434" to be "running"
    Aug 17 03:12:01.370: INFO: Pod "execpod-affinityngldg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.554321ms
    Aug 17 03:12:03.376: INFO: Pod "execpod-affinityngldg": Phase="Running", Reason="", readiness=true. Elapsed: 2.01487211s
    Aug 17 03:12:03.376: INFO: Pod "execpod-affinityngldg" satisfied condition "running"
    Aug 17 03:12:04.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 17 03:12:04.920: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 17 03:12:04.920: INFO: stdout: ""
    Aug 17 03:12:04.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 172.20.108.152 80'
    Aug 17 03:12:05.378: INFO: stderr: "+ nc -v -z -w 2 172.20.108.152 80\nConnection to 172.20.108.152 80 port [tcp/http] succeeded!\n"
    Aug 17 03:12:05.378: INFO: stdout: ""
    Aug 17 03:12:05.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 31779'
    Aug 17 03:12:05.752: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 31779\nConnection to 192.168.11.3 31779 port [tcp/*] succeeded!\n"
    Aug 17 03:12:05.752: INFO: stdout: ""
    Aug 17 03:12:05.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 31779'
    Aug 17 03:12:06.289: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 31779\nConnection to 192.168.11.4 31779 port [tcp/*] succeeded!\n"
    Aug 17 03:12:06.289: INFO: stdout: ""
    Aug 17 03:12:06.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:31779/ ; done'
    Aug 17 03:12:06.876: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n"
    Aug 17 03:12:06.876: INFO: stdout: "\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-p6l5c\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-qqwtz\naffinity-nodeport-transition-rjkz7"
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-p6l5c
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-qqwtz
    Aug 17 03:12:06.876: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3434 exec execpod-affinityngldg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:31779/ ; done'
    Aug 17 03:12:07.486: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:31779/\n"
    Aug 17 03:12:07.486: INFO: stdout: "\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7\naffinity-nodeport-transition-rjkz7"
    Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.486: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Received response from host: affinity-nodeport-transition-rjkz7
    Aug 17 03:12:07.487: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3434, will wait for the garbage collector to delete the pods 08/17/23 03:12:07.509
    Aug 17 03:12:07.574: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.336509ms
    Aug 17 03:12:07.675: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.775239ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:12:09.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3434" for this suite. 08/17/23 03:12:09.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:12:09.515
Aug 17 03:12:09.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:12:09.516
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:09.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:09.535
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:12:09.548
Aug 17 03:12:09.559: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7784" to be "running and ready"
Aug 17 03:12:09.564: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202957ms
Aug 17 03:12:09.564: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:12:11.570: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010994136s
Aug 17 03:12:11.571: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 03:12:11.571: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/17/23 03:12:11.576
Aug 17 03:12:11.585: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7784" to be "running and ready"
Aug 17 03:12:11.591: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.650553ms
Aug 17 03:12:11.591: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:12:13.597: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012086071s
Aug 17 03:12:13.597: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 17 03:12:13.597: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/17/23 03:12:13.602
Aug 17 03:12:13.611: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 03:12:13.616: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 03:12:15.617: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 03:12:15.623: INFO: Pod pod-with-prestop-http-hook still exists
Aug 17 03:12:17.617: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 17 03:12:17.623: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/17/23 03:12:17.623
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 03:12:17.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7784" for this suite. 08/17/23 03:12:17.643
------------------------------
• [SLOW TEST] [8.135 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:12:09.515
    Aug 17 03:12:09.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:12:09.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:09.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:09.535
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:12:09.548
    Aug 17 03:12:09.559: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7784" to be "running and ready"
    Aug 17 03:12:09.564: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202957ms
    Aug 17 03:12:09.564: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:12:11.570: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010994136s
    Aug 17 03:12:11.571: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 03:12:11.571: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/17/23 03:12:11.576
    Aug 17 03:12:11.585: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7784" to be "running and ready"
    Aug 17 03:12:11.591: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.650553ms
    Aug 17 03:12:11.591: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:12:13.597: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012086071s
    Aug 17 03:12:13.597: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 17 03:12:13.597: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/17/23 03:12:13.602
    Aug 17 03:12:13.611: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 03:12:13.616: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 17 03:12:15.617: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 03:12:15.623: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 17 03:12:17.617: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 17 03:12:17.623: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/17/23 03:12:17.623
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:12:17.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7784" for this suite. 08/17/23 03:12:17.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:12:17.652
Aug 17 03:12:17.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:12:17.654
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:17.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:17.672
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/17/23 03:12:17.677
Aug 17 03:12:17.685: INFO: Waiting up to 5m0s for pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a" in namespace "downward-api-6609" to be "Succeeded or Failed"
Aug 17 03:12:17.690: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649506ms
Aug 17 03:12:19.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011589825s
Aug 17 03:12:21.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011789683s
STEP: Saw pod success 08/17/23 03:12:21.697
Aug 17 03:12:21.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a" satisfied condition "Succeeded or Failed"
Aug 17 03:12:21.702: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a container dapi-container: <nil>
STEP: delete the pod 08/17/23 03:12:21.758
Aug 17 03:12:21.771: INFO: Waiting for pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a to disappear
Aug 17 03:12:21.775: INFO: Pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 03:12:21.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6609" for this suite. 08/17/23 03:12:21.783
------------------------------
• [4.140 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:12:17.652
    Aug 17 03:12:17.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:12:17.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:17.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:17.672
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/17/23 03:12:17.677
    Aug 17 03:12:17.685: INFO: Waiting up to 5m0s for pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a" in namespace "downward-api-6609" to be "Succeeded or Failed"
    Aug 17 03:12:17.690: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649506ms
    Aug 17 03:12:19.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011589825s
    Aug 17 03:12:21.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011789683s
    STEP: Saw pod success 08/17/23 03:12:21.697
    Aug 17 03:12:21.697: INFO: Pod "downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a" satisfied condition "Succeeded or Failed"
    Aug 17 03:12:21.702: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a container dapi-container: <nil>
    STEP: delete the pod 08/17/23 03:12:21.758
    Aug 17 03:12:21.771: INFO: Waiting for pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a to disappear
    Aug 17 03:12:21.775: INFO: Pod downward-api-91fc2c33-8ac0-4e47-b68e-7e6a681f1f0a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:12:21.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6609" for this suite. 08/17/23 03:12:21.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:12:21.793
Aug 17 03:12:21.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 03:12:21.794
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:21.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:21.813
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-743d9d95-dd85-41ef-894d-a841435348ec in namespace container-probe-1804 08/17/23 03:12:21.818
Aug 17 03:12:21.828: INFO: Waiting up to 5m0s for pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec" in namespace "container-probe-1804" to be "not pending"
Aug 17 03:12:21.833: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.749297ms
Aug 17 03:12:23.840: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011339703s
Aug 17 03:12:23.840: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec" satisfied condition "not pending"
Aug 17 03:12:23.840: INFO: Started pod liveness-743d9d95-dd85-41ef-894d-a841435348ec in namespace container-probe-1804
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:12:23.84
Aug 17 03:12:23.845: INFO: Initial restart count of pod liveness-743d9d95-dd85-41ef-894d-a841435348ec is 0
Aug 17 03:12:43.908: INFO: Restart count of pod container-probe-1804/liveness-743d9d95-dd85-41ef-894d-a841435348ec is now 1 (20.063317858s elapsed)
STEP: deleting the pod 08/17/23 03:12:43.908
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 03:12:43.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1804" for this suite. 08/17/23 03:12:43.934
------------------------------
• [SLOW TEST] [22.153 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:12:21.793
    Aug 17 03:12:21.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 03:12:21.794
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:21.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:21.813
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-743d9d95-dd85-41ef-894d-a841435348ec in namespace container-probe-1804 08/17/23 03:12:21.818
    Aug 17 03:12:21.828: INFO: Waiting up to 5m0s for pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec" in namespace "container-probe-1804" to be "not pending"
    Aug 17 03:12:21.833: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.749297ms
    Aug 17 03:12:23.840: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011339703s
    Aug 17 03:12:23.840: INFO: Pod "liveness-743d9d95-dd85-41ef-894d-a841435348ec" satisfied condition "not pending"
    Aug 17 03:12:23.840: INFO: Started pod liveness-743d9d95-dd85-41ef-894d-a841435348ec in namespace container-probe-1804
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:12:23.84
    Aug 17 03:12:23.845: INFO: Initial restart count of pod liveness-743d9d95-dd85-41ef-894d-a841435348ec is 0
    Aug 17 03:12:43.908: INFO: Restart count of pod container-probe-1804/liveness-743d9d95-dd85-41ef-894d-a841435348ec is now 1 (20.063317858s elapsed)
    STEP: deleting the pod 08/17/23 03:12:43.908
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:12:43.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1804" for this suite. 08/17/23 03:12:43.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:12:43.949
Aug 17 03:12:43.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 03:12:43.95
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:43.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:43.969
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/17/23 03:12:43.973
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:43.991
STEP: Creating a pod in the namespace 08/17/23 03:12:43.995
STEP: Waiting for the pod to have running status 08/17/23 03:12:44.003
Aug 17 03:12:44.003: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1431" to be "running"
Aug 17 03:12:44.007: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191542ms
Aug 17 03:12:46.013: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010036257s
Aug 17 03:12:46.013: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/17/23 03:12:46.013
STEP: Waiting for the namespace to be removed. 08/17/23 03:12:46.022
STEP: Recreating the namespace 08/17/23 03:12:57.027
STEP: Verifying there are no pods in the namespace 08/17/23 03:12:57.042
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:12:57.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3495" for this suite. 08/17/23 03:12:57.053
STEP: Destroying namespace "nsdeletetest-1431" for this suite. 08/17/23 03:12:57.061
Aug 17 03:12:57.065: INFO: Namespace nsdeletetest-1431 was already deleted
STEP: Destroying namespace "nsdeletetest-7484" for this suite. 08/17/23 03:12:57.065
------------------------------
• [SLOW TEST] [13.123 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:12:43.949
    Aug 17 03:12:43.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 03:12:43.95
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:43.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:43.969
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/17/23 03:12:43.973
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:43.991
    STEP: Creating a pod in the namespace 08/17/23 03:12:43.995
    STEP: Waiting for the pod to have running status 08/17/23 03:12:44.003
    Aug 17 03:12:44.003: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1431" to be "running"
    Aug 17 03:12:44.007: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191542ms
    Aug 17 03:12:46.013: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010036257s
    Aug 17 03:12:46.013: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/17/23 03:12:46.013
    STEP: Waiting for the namespace to be removed. 08/17/23 03:12:46.022
    STEP: Recreating the namespace 08/17/23 03:12:57.027
    STEP: Verifying there are no pods in the namespace 08/17/23 03:12:57.042
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:12:57.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3495" for this suite. 08/17/23 03:12:57.053
    STEP: Destroying namespace "nsdeletetest-1431" for this suite. 08/17/23 03:12:57.061
    Aug 17 03:12:57.065: INFO: Namespace nsdeletetest-1431 was already deleted
    STEP: Destroying namespace "nsdeletetest-7484" for this suite. 08/17/23 03:12:57.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:12:57.072
Aug 17 03:12:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:12:57.073
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:57.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:57.089
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-95c6b0bf-f16d-42a0-aa8a-d82f28b7a95c 08/17/23 03:12:57.093
STEP: Creating a pod to test consume configMaps 08/17/23 03:12:57.098
Aug 17 03:12:57.108: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105" in namespace "projected-5231" to be "Succeeded or Failed"
Aug 17 03:12:57.112: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21263ms
Aug 17 03:12:59.120: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012299082s
Aug 17 03:13:01.118: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009800723s
STEP: Saw pod success 08/17/23 03:13:01.118
Aug 17 03:13:01.118: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105" satisfied condition "Succeeded or Failed"
Aug 17 03:13:01.122: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:13:01.208
Aug 17 03:13:01.222: INFO: Waiting for pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 to disappear
Aug 17 03:13:01.228: INFO: Pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5231" for this suite. 08/17/23 03:13:01.237
------------------------------
• [4.173 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:12:57.072
    Aug 17 03:12:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:12:57.073
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:12:57.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:12:57.089
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-95c6b0bf-f16d-42a0-aa8a-d82f28b7a95c 08/17/23 03:12:57.093
    STEP: Creating a pod to test consume configMaps 08/17/23 03:12:57.098
    Aug 17 03:12:57.108: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105" in namespace "projected-5231" to be "Succeeded or Failed"
    Aug 17 03:12:57.112: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21263ms
    Aug 17 03:12:59.120: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012299082s
    Aug 17 03:13:01.118: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009800723s
    STEP: Saw pod success 08/17/23 03:13:01.118
    Aug 17 03:13:01.118: INFO: Pod "pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105" satisfied condition "Succeeded or Failed"
    Aug 17 03:13:01.122: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:13:01.208
    Aug 17 03:13:01.222: INFO: Waiting for pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 to disappear
    Aug 17 03:13:01.228: INFO: Pod pod-projected-configmaps-655b7bec-230e-41b2-ad6c-a05c7bf4b105 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5231" for this suite. 08/17/23 03:13:01.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:01.248
Aug 17 03:13:01.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:13:01.248
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:01.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:01.272
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/17/23 03:13:01.29
Aug 17 03:13:01.290: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373" in namespace "kubelet-test-2115" to be "completed"
Aug 17 03:13:01.296: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.570432ms
Aug 17 03:13:03.302: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01216412s
Aug 17 03:13:05.301: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011059914s
Aug 17 03:13:05.301: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:05.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2115" for this suite. 08/17/23 03:13:05.358
------------------------------
• [4.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:01.248
    Aug 17 03:13:01.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:13:01.248
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:01.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:01.272
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/17/23 03:13:01.29
    Aug 17 03:13:01.290: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373" in namespace "kubelet-test-2115" to be "completed"
    Aug 17 03:13:01.296: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Pending", Reason="", readiness=false. Elapsed: 5.570432ms
    Aug 17 03:13:03.302: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01216412s
    Aug 17 03:13:05.301: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011059914s
    Aug 17 03:13:05.301: INFO: Pod "agnhost-host-aliasesa2446857-cd8e-4967-a12c-18a5f16d5373" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:05.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2115" for this suite. 08/17/23 03:13:05.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:05.367
Aug 17 03:13:05.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:13:05.368
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:05.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:05.386
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4477 08/17/23 03:13:05.39
STEP: changing the ExternalName service to type=NodePort 08/17/23 03:13:05.396
STEP: creating replication controller externalname-service in namespace services-4477 08/17/23 03:13:05.413
I0817 03:13:05.418074      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4477, replica count: 2
I0817 03:13:08.469691      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:13:08.469: INFO: Creating new exec pod
Aug 17 03:13:08.476: INFO: Waiting up to 5m0s for pod "execpodvx6rh" in namespace "services-4477" to be "running"
Aug 17 03:13:08.481: INFO: Pod "execpodvx6rh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771107ms
Aug 17 03:13:10.486: INFO: Pod "execpodvx6rh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009813081s
Aug 17 03:13:12.487: INFO: Pod "execpodvx6rh": Phase="Running", Reason="", readiness=true. Elapsed: 4.010810063s
Aug 17 03:13:12.487: INFO: Pod "execpodvx6rh" satisfied condition "running"
Aug 17 03:13:13.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 17 03:13:14.010: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 03:13:14.010: INFO: stdout: ""
Aug 17 03:13:14.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 172.20.8.16 80'
Aug 17 03:13:14.527: INFO: stderr: "+ nc -v -z -w 2 172.20.8.16 80\nConnection to 172.20.8.16 80 port [tcp/http] succeeded!\n"
Aug 17 03:13:14.527: INFO: stdout: ""
Aug 17 03:13:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 32524'
Aug 17 03:13:15.046: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 32524\nConnection to 192.168.11.3 32524 port [tcp/*] succeeded!\n"
Aug 17 03:13:15.046: INFO: stdout: ""
Aug 17 03:13:15.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 32524'
Aug 17 03:13:15.531: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 32524\nConnection to 192.168.11.4 32524 port [tcp/*] succeeded!\n"
Aug 17 03:13:15.531: INFO: stdout: ""
Aug 17 03:13:15.531: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:15.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4477" for this suite. 08/17/23 03:13:15.556
------------------------------
• [SLOW TEST] [10.196 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:05.367
    Aug 17 03:13:05.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:13:05.368
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:05.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:05.386
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4477 08/17/23 03:13:05.39
    STEP: changing the ExternalName service to type=NodePort 08/17/23 03:13:05.396
    STEP: creating replication controller externalname-service in namespace services-4477 08/17/23 03:13:05.413
    I0817 03:13:05.418074      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4477, replica count: 2
    I0817 03:13:08.469691      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:13:08.469: INFO: Creating new exec pod
    Aug 17 03:13:08.476: INFO: Waiting up to 5m0s for pod "execpodvx6rh" in namespace "services-4477" to be "running"
    Aug 17 03:13:08.481: INFO: Pod "execpodvx6rh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771107ms
    Aug 17 03:13:10.486: INFO: Pod "execpodvx6rh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009813081s
    Aug 17 03:13:12.487: INFO: Pod "execpodvx6rh": Phase="Running", Reason="", readiness=true. Elapsed: 4.010810063s
    Aug 17 03:13:12.487: INFO: Pod "execpodvx6rh" satisfied condition "running"
    Aug 17 03:13:13.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 17 03:13:14.010: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 17 03:13:14.010: INFO: stdout: ""
    Aug 17 03:13:14.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 172.20.8.16 80'
    Aug 17 03:13:14.527: INFO: stderr: "+ nc -v -z -w 2 172.20.8.16 80\nConnection to 172.20.8.16 80 port [tcp/http] succeeded!\n"
    Aug 17 03:13:14.527: INFO: stdout: ""
    Aug 17 03:13:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 32524'
    Aug 17 03:13:15.046: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 32524\nConnection to 192.168.11.3 32524 port [tcp/*] succeeded!\n"
    Aug 17 03:13:15.046: INFO: stdout: ""
    Aug 17 03:13:15.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-4477 exec execpodvx6rh -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 32524'
    Aug 17 03:13:15.531: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 32524\nConnection to 192.168.11.4 32524 port [tcp/*] succeeded!\n"
    Aug 17 03:13:15.531: INFO: stdout: ""
    Aug 17 03:13:15.531: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:15.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4477" for this suite. 08/17/23 03:13:15.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:15.564
Aug 17 03:13:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:13:15.565
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:15.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:15.581
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/17/23 03:13:15.585
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local;sleep 1; done
 08/17/23 03:13:15.59
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local;sleep 1; done
 08/17/23 03:13:15.591
STEP: creating a pod to probe DNS 08/17/23 03:13:15.591
STEP: submitting the pod to kubernetes 08/17/23 03:13:15.591
Aug 17 03:13:15.599: INFO: Waiting up to 15m0s for pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19" in namespace "dns-9444" to be "running"
Aug 17 03:13:15.607: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279831ms
Aug 17 03:13:17.613: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19": Phase="Running", Reason="", readiness=true. Elapsed: 2.013891084s
Aug 17 03:13:17.613: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:13:17.613
STEP: looking for the results for each expected name from probers 08/17/23 03:13:17.618
Aug 17 03:13:17.708: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.752: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.759: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.765: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.772: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.784: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.791: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.797: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
Aug 17 03:13:17.797: INFO: Lookups using dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local]

Aug 17 03:13:22.886: INFO: DNS probes using dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19 succeeded

STEP: deleting the pod 08/17/23 03:13:22.886
STEP: deleting the test headless service 08/17/23 03:13:22.9
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:22.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9444" for this suite. 08/17/23 03:13:22.928
------------------------------
• [SLOW TEST] [7.372 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:15.564
    Aug 17 03:13:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:13:15.565
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:15.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:15.581
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/17/23 03:13:15.585
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local;sleep 1; done
     08/17/23 03:13:15.59
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9444.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local;sleep 1; done
     08/17/23 03:13:15.591
    STEP: creating a pod to probe DNS 08/17/23 03:13:15.591
    STEP: submitting the pod to kubernetes 08/17/23 03:13:15.591
    Aug 17 03:13:15.599: INFO: Waiting up to 15m0s for pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19" in namespace "dns-9444" to be "running"
    Aug 17 03:13:15.607: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279831ms
    Aug 17 03:13:17.613: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19": Phase="Running", Reason="", readiness=true. Elapsed: 2.013891084s
    Aug 17 03:13:17.613: INFO: Pod "dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:13:17.613
    STEP: looking for the results for each expected name from probers 08/17/23 03:13:17.618
    Aug 17 03:13:17.708: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.752: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.759: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.765: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.772: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.784: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.791: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.797: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local from pod dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19: the server could not find the requested resource (get pods dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19)
    Aug 17 03:13:17.797: INFO: Lookups using dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9444.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9444.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9444.svc.cluster.local jessie_udp@dns-test-service-2.dns-9444.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9444.svc.cluster.local]

    Aug 17 03:13:22.886: INFO: DNS probes using dns-9444/dns-test-0c64744c-18f4-43ac-89c0-aeb609877d19 succeeded

    STEP: deleting the pod 08/17/23 03:13:22.886
    STEP: deleting the test headless service 08/17/23 03:13:22.9
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:22.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9444" for this suite. 08/17/23 03:13:22.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:22.944
Aug 17 03:13:22.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:13:22.944
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:22.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:22.964
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-555f4a05-6147-4b40-abaf-7d390b6a0555 08/17/23 03:13:22.975
STEP: Creating the pod 08/17/23 03:13:22.981
Aug 17 03:13:22.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9" in namespace "configmap-5554" to be "running and ready"
Aug 17 03:13:22.995: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703661ms
Aug 17 03:13:22.995: INFO: The phase of Pod pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:13:25.002: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011315717s
Aug 17 03:13:25.002: INFO: The phase of Pod pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9 is Running (Ready = true)
Aug 17 03:13:25.002: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-555f4a05-6147-4b40-abaf-7d390b6a0555 08/17/23 03:13:25.015
STEP: waiting to observe update in volume 08/17/23 03:13:25.022
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5554" for this suite. 08/17/23 03:13:27.118
------------------------------
• [4.182 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:22.944
    Aug 17 03:13:22.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:13:22.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:22.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:22.964
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-555f4a05-6147-4b40-abaf-7d390b6a0555 08/17/23 03:13:22.975
    STEP: Creating the pod 08/17/23 03:13:22.981
    Aug 17 03:13:22.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9" in namespace "configmap-5554" to be "running and ready"
    Aug 17 03:13:22.995: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703661ms
    Aug 17 03:13:22.995: INFO: The phase of Pod pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:13:25.002: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011315717s
    Aug 17 03:13:25.002: INFO: The phase of Pod pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9 is Running (Ready = true)
    Aug 17 03:13:25.002: INFO: Pod "pod-configmaps-e70f8e3e-c568-4d30-b859-6b547193d1c9" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-555f4a05-6147-4b40-abaf-7d390b6a0555 08/17/23 03:13:25.015
    STEP: waiting to observe update in volume 08/17/23 03:13:25.022
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5554" for this suite. 08/17/23 03:13:27.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:27.126
Aug 17 03:13:27.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 03:13:27.128
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:27.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:27.144
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5550 08/17/23 03:13:27.148
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5550 08/17/23 03:13:27.157
Aug 17 03:13:27.167: INFO: Found 0 stateful pods, waiting for 1
Aug 17 03:13:37.173: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/17/23 03:13:37.181
STEP: Getting /status 08/17/23 03:13:37.188
Aug 17 03:13:37.192: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/17/23 03:13:37.192
Aug 17 03:13:37.202: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/17/23 03:13:37.202
Aug 17 03:13:37.204: INFO: Observed &StatefulSet event: ADDED
Aug 17 03:13:37.204: INFO: Found Statefulset ss in namespace statefulset-5550 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 03:13:37.205: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/17/23 03:13:37.205
Aug 17 03:13:37.205: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 03:13:37.211: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/17/23 03:13:37.211
Aug 17 03:13:37.214: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 03:13:37.214: INFO: Deleting all statefulset in ns statefulset-5550
Aug 17 03:13:37.218: INFO: Scaling statefulset ss to 0
Aug 17 03:13:47.240: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 03:13:47.244: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:47.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5550" for this suite. 08/17/23 03:13:47.267
------------------------------
• [SLOW TEST] [20.149 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:27.126
    Aug 17 03:13:27.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 03:13:27.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:27.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:27.144
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5550 08/17/23 03:13:27.148
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5550 08/17/23 03:13:27.157
    Aug 17 03:13:27.167: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 03:13:37.173: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/17/23 03:13:37.181
    STEP: Getting /status 08/17/23 03:13:37.188
    Aug 17 03:13:37.192: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/17/23 03:13:37.192
    Aug 17 03:13:37.202: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/17/23 03:13:37.202
    Aug 17 03:13:37.204: INFO: Observed &StatefulSet event: ADDED
    Aug 17 03:13:37.204: INFO: Found Statefulset ss in namespace statefulset-5550 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 03:13:37.205: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/17/23 03:13:37.205
    Aug 17 03:13:37.205: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 03:13:37.211: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/17/23 03:13:37.211
    Aug 17 03:13:37.214: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 03:13:37.214: INFO: Deleting all statefulset in ns statefulset-5550
    Aug 17 03:13:37.218: INFO: Scaling statefulset ss to 0
    Aug 17 03:13:47.240: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 03:13:47.244: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:47.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5550" for this suite. 08/17/23 03:13:47.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:47.276
Aug 17 03:13:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename ingress 08/17/23 03:13:47.277
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:47.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:47.297
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/17/23 03:13:47.301
STEP: getting /apis/networking.k8s.io 08/17/23 03:13:47.307
STEP: getting /apis/networking.k8s.iov1 08/17/23 03:13:47.309
STEP: creating 08/17/23 03:13:47.311
STEP: getting 08/17/23 03:13:47.329
STEP: listing 08/17/23 03:13:47.333
STEP: watching 08/17/23 03:13:47.338
Aug 17 03:13:47.338: INFO: starting watch
STEP: cluster-wide listing 08/17/23 03:13:47.341
STEP: cluster-wide watching 08/17/23 03:13:47.345
Aug 17 03:13:47.345: INFO: starting watch
STEP: patching 08/17/23 03:13:47.347
STEP: updating 08/17/23 03:13:47.353
Aug 17 03:13:47.362: INFO: waiting for watch events with expected annotations
Aug 17 03:13:47.363: INFO: saw patched and updated annotations
STEP: patching /status 08/17/23 03:13:47.363
STEP: updating /status 08/17/23 03:13:47.368
STEP: get /status 08/17/23 03:13:47.377
STEP: deleting 08/17/23 03:13:47.381
STEP: deleting a collection 08/17/23 03:13:47.395
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 17 03:13:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-9659" for this suite. 08/17/23 03:13:47.416
------------------------------
• [0.147 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:47.276
    Aug 17 03:13:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename ingress 08/17/23 03:13:47.277
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:47.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:47.297
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/17/23 03:13:47.301
    STEP: getting /apis/networking.k8s.io 08/17/23 03:13:47.307
    STEP: getting /apis/networking.k8s.iov1 08/17/23 03:13:47.309
    STEP: creating 08/17/23 03:13:47.311
    STEP: getting 08/17/23 03:13:47.329
    STEP: listing 08/17/23 03:13:47.333
    STEP: watching 08/17/23 03:13:47.338
    Aug 17 03:13:47.338: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 03:13:47.341
    STEP: cluster-wide watching 08/17/23 03:13:47.345
    Aug 17 03:13:47.345: INFO: starting watch
    STEP: patching 08/17/23 03:13:47.347
    STEP: updating 08/17/23 03:13:47.353
    Aug 17 03:13:47.362: INFO: waiting for watch events with expected annotations
    Aug 17 03:13:47.363: INFO: saw patched and updated annotations
    STEP: patching /status 08/17/23 03:13:47.363
    STEP: updating /status 08/17/23 03:13:47.368
    STEP: get /status 08/17/23 03:13:47.377
    STEP: deleting 08/17/23 03:13:47.381
    STEP: deleting a collection 08/17/23 03:13:47.395
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:13:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-9659" for this suite. 08/17/23 03:13:47.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:13:47.426
Aug 17 03:13:47.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename cronjob 08/17/23 03:13:47.426
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:47.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:47.443
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/17/23 03:13:47.447
STEP: Ensuring a job is scheduled 08/17/23 03:13:47.453
STEP: Ensuring exactly one is scheduled 08/17/23 03:14:01.459
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 03:14:01.464
STEP: Ensuring the job is replaced with a new one 08/17/23 03:14:01.47
STEP: Removing cronjob 08/17/23 03:15:01.476
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:01.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6528" for this suite. 08/17/23 03:15:01.492
------------------------------
• [SLOW TEST] [74.077 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:13:47.426
    Aug 17 03:13:47.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename cronjob 08/17/23 03:13:47.426
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:13:47.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:13:47.443
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/17/23 03:13:47.447
    STEP: Ensuring a job is scheduled 08/17/23 03:13:47.453
    STEP: Ensuring exactly one is scheduled 08/17/23 03:14:01.459
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 03:14:01.464
    STEP: Ensuring the job is replaced with a new one 08/17/23 03:14:01.47
    STEP: Removing cronjob 08/17/23 03:15:01.476
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:01.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6528" for this suite. 08/17/23 03:15:01.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:01.506
Aug 17 03:15:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:15:01.507
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:01.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:01.528
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/17/23 03:15:01.534
STEP: Creating a ResourceQuota 08/17/23 03:15:06.538
STEP: Ensuring resource quota status is calculated 08/17/23 03:15:06.544
STEP: Creating a Pod that fits quota 08/17/23 03:15:08.551
STEP: Ensuring ResourceQuota status captures the pod usage 08/17/23 03:15:08.566
STEP: Not allowing a pod to be created that exceeds remaining quota 08/17/23 03:15:10.572
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/17/23 03:15:10.576
STEP: Ensuring a pod cannot update its resource requirements 08/17/23 03:15:10.579
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/17/23 03:15:10.584
STEP: Deleting the pod 08/17/23 03:15:12.59
STEP: Ensuring resource quota status released the pod usage 08/17/23 03:15:12.6
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8659" for this suite. 08/17/23 03:15:14.615
------------------------------
• [SLOW TEST] [13.117 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:01.506
    Aug 17 03:15:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:15:01.507
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:01.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:01.528
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/17/23 03:15:01.534
    STEP: Creating a ResourceQuota 08/17/23 03:15:06.538
    STEP: Ensuring resource quota status is calculated 08/17/23 03:15:06.544
    STEP: Creating a Pod that fits quota 08/17/23 03:15:08.551
    STEP: Ensuring ResourceQuota status captures the pod usage 08/17/23 03:15:08.566
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/17/23 03:15:10.572
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/17/23 03:15:10.576
    STEP: Ensuring a pod cannot update its resource requirements 08/17/23 03:15:10.579
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/17/23 03:15:10.584
    STEP: Deleting the pod 08/17/23 03:15:12.59
    STEP: Ensuring resource quota status released the pod usage 08/17/23 03:15:12.6
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8659" for this suite. 08/17/23 03:15:14.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:14.623
Aug 17 03:15:14.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:15:14.624
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:14.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:14.641
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-5150338c-7503-40a9-b798-9852e9c32e27 08/17/23 03:15:14.645
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:14.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4901" for this suite. 08/17/23 03:15:14.654
------------------------------
• [0.040 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:14.623
    Aug 17 03:15:14.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:15:14.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:14.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:14.641
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-5150338c-7503-40a9-b798-9852e9c32e27 08/17/23 03:15:14.645
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:14.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4901" for this suite. 08/17/23 03:15:14.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:14.665
Aug 17 03:15:14.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:15:14.666
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:14.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:14.683
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:15:14.701
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:15:15.158
STEP: Deploying the webhook pod 08/17/23 03:15:15.167
STEP: Wait for the deployment to be ready 08/17/23 03:15:15.178
Aug 17 03:15:15.186: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:15:17.203
STEP: Verifying the service has paired with the endpoint 08/17/23 03:15:17.217
Aug 17 03:15:18.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 03:15:18.223
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 03:15:18.324
STEP: Creating a dummy validating-webhook-configuration object 08/17/23 03:15:18.43
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/17/23 03:15:18.485
STEP: Creating a dummy mutating-webhook-configuration object 08/17/23 03:15:18.494
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/17/23 03:15:18.548
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:18.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6414" for this suite. 08/17/23 03:15:18.608
STEP: Destroying namespace "webhook-6414-markers" for this suite. 08/17/23 03:15:18.616
------------------------------
• [3.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:14.665
    Aug 17 03:15:14.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:15:14.666
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:14.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:14.683
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:15:14.701
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:15:15.158
    STEP: Deploying the webhook pod 08/17/23 03:15:15.167
    STEP: Wait for the deployment to be ready 08/17/23 03:15:15.178
    Aug 17 03:15:15.186: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:15:17.203
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:15:17.217
    Aug 17 03:15:18.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 03:15:18.223
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/17/23 03:15:18.324
    STEP: Creating a dummy validating-webhook-configuration object 08/17/23 03:15:18.43
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/17/23 03:15:18.485
    STEP: Creating a dummy mutating-webhook-configuration object 08/17/23 03:15:18.494
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/17/23 03:15:18.548
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:18.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6414" for this suite. 08/17/23 03:15:18.608
    STEP: Destroying namespace "webhook-6414-markers" for this suite. 08/17/23 03:15:18.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:18.639
Aug 17 03:15:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-runtime 08/17/23 03:15:18.64
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:18.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:18.658
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/17/23 03:15:18.672
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/17/23 03:15:37.79
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/17/23 03:15:37.798
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/17/23 03:15:37.807
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/17/23 03:15:37.807
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/17/23 03:15:37.842
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/17/23 03:15:41.873
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/17/23 03:15:43.889
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/17/23 03:15:43.898
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/17/23 03:15:43.898
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/17/23 03:15:43.917
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/17/23 03:15:44.935
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/17/23 03:15:47.957
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/17/23 03:15:47.966
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/17/23 03:15:47.966
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:47.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1994" for this suite. 08/17/23 03:15:48.009
------------------------------
• [SLOW TEST] [29.378 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:18.639
    Aug 17 03:15:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-runtime 08/17/23 03:15:18.64
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:18.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:18.658
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/17/23 03:15:18.672
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/17/23 03:15:37.79
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/17/23 03:15:37.798
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/17/23 03:15:37.807
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/17/23 03:15:37.807
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/17/23 03:15:37.842
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/17/23 03:15:41.873
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/17/23 03:15:43.889
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/17/23 03:15:43.898
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/17/23 03:15:43.898
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/17/23 03:15:43.917
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/17/23 03:15:44.935
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/17/23 03:15:47.957
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/17/23 03:15:47.966
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/17/23 03:15:47.966
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:47.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1994" for this suite. 08/17/23 03:15:48.009
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:48.019
Aug 17 03:15:48.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 03:15:48.02
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:48.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:48.042
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/17/23 03:15:48.047
STEP: delete the rc 08/17/23 03:15:53.059
STEP: wait for all pods to be garbage collected 08/17/23 03:15:53.065
STEP: Gathering metrics 08/17/23 03:15:58.074
W0817 03:15:58.089421      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 03:15:58.089: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 03:15:58.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3053" for this suite. 08/17/23 03:15:58.096
------------------------------
• [SLOW TEST] [10.086 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:48.019
    Aug 17 03:15:48.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 03:15:48.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:48.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:48.042
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/17/23 03:15:48.047
    STEP: delete the rc 08/17/23 03:15:53.059
    STEP: wait for all pods to be garbage collected 08/17/23 03:15:53.065
    STEP: Gathering metrics 08/17/23 03:15:58.074
    W0817 03:15:58.089421      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 03:15:58.089: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:15:58.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3053" for this suite. 08/17/23 03:15:58.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:15:58.106
Aug 17 03:15:58.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 03:15:58.107
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:58.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:58.126
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/17/23 03:15:58.132
STEP: Ensuring active pods == parallelism 08/17/23 03:15:58.139
STEP: Orphaning one of the Job's Pods 08/17/23 03:16:00.147
Aug 17 03:16:00.669: INFO: Successfully updated pod "adopt-release-55tv9"
STEP: Checking that the Job readopts the Pod 08/17/23 03:16:00.669
Aug 17 03:16:00.669: INFO: Waiting up to 15m0s for pod "adopt-release-55tv9" in namespace "job-1358" to be "adopted"
Aug 17 03:16:00.673: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 4.563111ms
Aug 17 03:16:02.680: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01075907s
Aug 17 03:16:02.680: INFO: Pod "adopt-release-55tv9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/17/23 03:16:02.68
Aug 17 03:16:03.194: INFO: Successfully updated pod "adopt-release-55tv9"
STEP: Checking that the Job releases the Pod 08/17/23 03:16:03.197
Aug 17 03:16:03.198: INFO: Waiting up to 15m0s for pod "adopt-release-55tv9" in namespace "job-1358" to be "released"
Aug 17 03:16:03.203: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 5.459082ms
Aug 17 03:16:05.209: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011680475s
Aug 17 03:16:05.209: INFO: Pod "adopt-release-55tv9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 03:16:05.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1358" for this suite. 08/17/23 03:16:05.216
------------------------------
• [SLOW TEST] [7.119 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:15:58.106
    Aug 17 03:15:58.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 03:15:58.107
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:15:58.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:15:58.126
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/17/23 03:15:58.132
    STEP: Ensuring active pods == parallelism 08/17/23 03:15:58.139
    STEP: Orphaning one of the Job's Pods 08/17/23 03:16:00.147
    Aug 17 03:16:00.669: INFO: Successfully updated pod "adopt-release-55tv9"
    STEP: Checking that the Job readopts the Pod 08/17/23 03:16:00.669
    Aug 17 03:16:00.669: INFO: Waiting up to 15m0s for pod "adopt-release-55tv9" in namespace "job-1358" to be "adopted"
    Aug 17 03:16:00.673: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 4.563111ms
    Aug 17 03:16:02.680: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01075907s
    Aug 17 03:16:02.680: INFO: Pod "adopt-release-55tv9" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/17/23 03:16:02.68
    Aug 17 03:16:03.194: INFO: Successfully updated pod "adopt-release-55tv9"
    STEP: Checking that the Job releases the Pod 08/17/23 03:16:03.197
    Aug 17 03:16:03.198: INFO: Waiting up to 15m0s for pod "adopt-release-55tv9" in namespace "job-1358" to be "released"
    Aug 17 03:16:03.203: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 5.459082ms
    Aug 17 03:16:05.209: INFO: Pod "adopt-release-55tv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011680475s
    Aug 17 03:16:05.209: INFO: Pod "adopt-release-55tv9" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:16:05.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1358" for this suite. 08/17/23 03:16:05.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:16:05.226
Aug 17 03:16:05.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:16:05.227
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:16:05.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:16:05.254
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-837 08/17/23 03:16:05.263
STEP: creating a selector 08/17/23 03:16:05.263
STEP: Creating the service pods in kubernetes 08/17/23 03:16:05.263
Aug 17 03:16:05.263: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 03:16:05.289: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-837" to be "running and ready"
Aug 17 03:16:05.293: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01512ms
Aug 17 03:16:05.293: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:16:07.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010965571s
Aug 17 03:16:07.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:09.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010234855s
Aug 17 03:16:09.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:11.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009656319s
Aug 17 03:16:11.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:13.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009677148s
Aug 17 03:16:13.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:15.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009355835s
Aug 17 03:16:15.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:17.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011533748s
Aug 17 03:16:17.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:19.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010159853s
Aug 17 03:16:19.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:21.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009678492s
Aug 17 03:16:21.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:23.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009865361s
Aug 17 03:16:23.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:25.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00927749s
Aug 17 03:16:25.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:16:27.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010339695s
Aug 17 03:16:27.299: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 03:16:27.299: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 03:16:27.304: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-837" to be "running and ready"
Aug 17 03:16:27.309: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.925299ms
Aug 17 03:16:27.309: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 03:16:27.309: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 03:16:27.313
Aug 17 03:16:27.327: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-837" to be "running"
Aug 17 03:16:27.334: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714342ms
Aug 17 03:16:29.340: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553328s
Aug 17 03:16:29.340: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 03:16:29.343: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-837" to be "running"
Aug 17 03:16:29.348: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.401122ms
Aug 17 03:16:29.348: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 17 03:16:29.352: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 17 03:16:29.352: INFO: Going to poll 172.21.86.169 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 17 03:16:29.355: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.86.169 8081 | grep -v '^\s*$'] Namespace:pod-network-test-837 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:16:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:16:29.356: INFO: ExecWithOptions: Clientset creation
Aug 17 03:16:29.356: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-837/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.86.169+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 03:16:30.799: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 17 03:16:30.799: INFO: Going to poll 172.21.15.103 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 17 03:16:30.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.15.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-837 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:16:30.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:16:30.804: INFO: ExecWithOptions: Clientset creation
Aug 17 03:16:30.804: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-837/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.15.103+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 03:16:32.253: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 03:16:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-837" for this suite. 08/17/23 03:16:32.26
------------------------------
• [SLOW TEST] [27.042 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:16:05.226
    Aug 17 03:16:05.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:16:05.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:16:05.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:16:05.254
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-837 08/17/23 03:16:05.263
    STEP: creating a selector 08/17/23 03:16:05.263
    STEP: Creating the service pods in kubernetes 08/17/23 03:16:05.263
    Aug 17 03:16:05.263: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 03:16:05.289: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-837" to be "running and ready"
    Aug 17 03:16:05.293: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01512ms
    Aug 17 03:16:05.293: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:16:07.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010965571s
    Aug 17 03:16:07.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:09.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010234855s
    Aug 17 03:16:09.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:11.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009656319s
    Aug 17 03:16:11.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:13.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009677148s
    Aug 17 03:16:13.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:15.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009355835s
    Aug 17 03:16:15.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:17.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011533748s
    Aug 17 03:16:17.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:19.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010159853s
    Aug 17 03:16:19.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:21.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009678492s
    Aug 17 03:16:21.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:23.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009865361s
    Aug 17 03:16:23.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:25.298: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00927749s
    Aug 17 03:16:25.298: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:16:27.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010339695s
    Aug 17 03:16:27.299: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 03:16:27.299: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 03:16:27.304: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-837" to be "running and ready"
    Aug 17 03:16:27.309: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.925299ms
    Aug 17 03:16:27.309: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 03:16:27.309: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 03:16:27.313
    Aug 17 03:16:27.327: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-837" to be "running"
    Aug 17 03:16:27.334: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714342ms
    Aug 17 03:16:29.340: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553328s
    Aug 17 03:16:29.340: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 03:16:29.343: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-837" to be "running"
    Aug 17 03:16:29.348: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.401122ms
    Aug 17 03:16:29.348: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 17 03:16:29.352: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 17 03:16:29.352: INFO: Going to poll 172.21.86.169 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 17 03:16:29.355: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.86.169 8081 | grep -v '^\s*$'] Namespace:pod-network-test-837 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:16:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:16:29.356: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:16:29.356: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-837/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.86.169+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 03:16:30.799: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 17 03:16:30.799: INFO: Going to poll 172.21.15.103 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 17 03:16:30.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.15.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-837 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:16:30.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:16:30.804: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:16:30.804: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-837/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.15.103+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 03:16:32.253: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:16:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-837" for this suite. 08/17/23 03:16:32.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:16:32.27
Aug 17 03:16:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename cronjob 08/17/23 03:16:32.271
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:16:32.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:16:32.288
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/17/23 03:16:32.292
STEP: Ensuring more than one job is running at a time 08/17/23 03:16:32.299
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/17/23 03:18:00.305
STEP: Removing cronjob 08/17/23 03:18:00.31
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 03:18:00.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1658" for this suite. 08/17/23 03:18:00.326
------------------------------
• [SLOW TEST] [88.064 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:16:32.27
    Aug 17 03:16:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename cronjob 08/17/23 03:16:32.271
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:16:32.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:16:32.288
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/17/23 03:16:32.292
    STEP: Ensuring more than one job is running at a time 08/17/23 03:16:32.299
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/17/23 03:18:00.305
    STEP: Removing cronjob 08/17/23 03:18:00.31
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:18:00.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1658" for this suite. 08/17/23 03:18:00.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:18:00.335
Aug 17 03:18:00.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:18:00.336
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:18:00.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:18:00.359
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/17/23 03:18:00.38
STEP: waiting for available Endpoint 08/17/23 03:18:00.387
STEP: listing all Endpoints 08/17/23 03:18:00.389
STEP: updating the Endpoint 08/17/23 03:18:00.394
STEP: fetching the Endpoint 08/17/23 03:18:00.404
STEP: patching the Endpoint 08/17/23 03:18:00.409
STEP: fetching the Endpoint 08/17/23 03:18:00.419
STEP: deleting the Endpoint by Collection 08/17/23 03:18:00.424
STEP: waiting for Endpoint deletion 08/17/23 03:18:00.435
STEP: fetching the Endpoint 08/17/23 03:18:00.437
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:18:00.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5558" for this suite. 08/17/23 03:18:00.448
------------------------------
• [0.123 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:18:00.335
    Aug 17 03:18:00.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:18:00.336
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:18:00.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:18:00.359
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/17/23 03:18:00.38
    STEP: waiting for available Endpoint 08/17/23 03:18:00.387
    STEP: listing all Endpoints 08/17/23 03:18:00.389
    STEP: updating the Endpoint 08/17/23 03:18:00.394
    STEP: fetching the Endpoint 08/17/23 03:18:00.404
    STEP: patching the Endpoint 08/17/23 03:18:00.409
    STEP: fetching the Endpoint 08/17/23 03:18:00.419
    STEP: deleting the Endpoint by Collection 08/17/23 03:18:00.424
    STEP: waiting for Endpoint deletion 08/17/23 03:18:00.435
    STEP: fetching the Endpoint 08/17/23 03:18:00.437
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:18:00.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5558" for this suite. 08/17/23 03:18:00.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:18:00.462
Aug 17 03:18:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename taint-multiple-pods 08/17/23 03:18:00.462
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:18:00.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:18:00.483
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 17 03:18:00.487: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:19:00.544: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 17 03:19:00.549: INFO: Starting informer...
STEP: Starting pods... 08/17/23 03:19:00.549
Aug 17 03:19:00.771: INFO: Pod1 is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
Aug 17 03:19:00.984: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-876" to be "running"
Aug 17 03:19:00.988: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.412834ms
Aug 17 03:19:02.994: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010085159s
Aug 17 03:19:02.994: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 17 03:19:02.994: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-876" to be "running"
Aug 17 03:19:02.998: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.082943ms
Aug 17 03:19:02.998: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 17 03:19:02.998: INFO: Pod2 is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
STEP: Trying to apply a taint on the Node 08/17/23 03:19:02.998
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:19:03.014
STEP: Waiting for Pod1 and Pod2 to be deleted 08/17/23 03:19:03.019
Aug 17 03:19:09.373: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 17 03:19:29.403: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:19:29.419
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:29.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-876" for this suite. 08/17/23 03:19:29.43
------------------------------
• [SLOW TEST] [88.976 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:18:00.462
    Aug 17 03:18:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename taint-multiple-pods 08/17/23 03:18:00.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:18:00.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:18:00.483
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 17 03:18:00.487: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:19:00.544: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 17 03:19:00.549: INFO: Starting informer...
    STEP: Starting pods... 08/17/23 03:19:00.549
    Aug 17 03:19:00.771: INFO: Pod1 is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
    Aug 17 03:19:00.984: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-876" to be "running"
    Aug 17 03:19:00.988: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.412834ms
    Aug 17 03:19:02.994: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010085159s
    Aug 17 03:19:02.994: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 17 03:19:02.994: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-876" to be "running"
    Aug 17 03:19:02.998: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.082943ms
    Aug 17 03:19:02.998: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 17 03:19:02.998: INFO: Pod2 is running on ske-ubuntu-79fff84d86x69988-vjwlx. Tainting Node
    STEP: Trying to apply a taint on the Node 08/17/23 03:19:02.998
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:19:03.014
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/17/23 03:19:03.019
    Aug 17 03:19:09.373: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 17 03:19:29.403: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/17/23 03:19:29.419
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:29.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-876" for this suite. 08/17/23 03:19:29.43
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:29.438
Aug 17 03:19:29.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename events 08/17/23 03:19:29.439
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:29.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:29.456
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/17/23 03:19:29.459
STEP: listing all events in all namespaces 08/17/23 03:19:29.464
STEP: patching the test event 08/17/23 03:19:29.468
STEP: fetching the test event 08/17/23 03:19:29.475
STEP: updating the test event 08/17/23 03:19:29.478
STEP: getting the test event 08/17/23 03:19:29.487
STEP: deleting the test event 08/17/23 03:19:29.49
STEP: listing all events in all namespaces 08/17/23 03:19:29.496
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:29.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8799" for this suite. 08/17/23 03:19:29.507
------------------------------
• [0.076 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:29.438
    Aug 17 03:19:29.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename events 08/17/23 03:19:29.439
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:29.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:29.456
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/17/23 03:19:29.459
    STEP: listing all events in all namespaces 08/17/23 03:19:29.464
    STEP: patching the test event 08/17/23 03:19:29.468
    STEP: fetching the test event 08/17/23 03:19:29.475
    STEP: updating the test event 08/17/23 03:19:29.478
    STEP: getting the test event 08/17/23 03:19:29.487
    STEP: deleting the test event 08/17/23 03:19:29.49
    STEP: listing all events in all namespaces 08/17/23 03:19:29.496
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:29.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8799" for this suite. 08/17/23 03:19:29.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:29.515
Aug 17 03:19:29.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:19:29.516
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:29.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:29.532
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a9efbd76-cc77-4f9a-bf54-790ca210deb2 08/17/23 03:19:29.542
STEP: Creating the pod 08/17/23 03:19:29.547
Aug 17 03:19:29.556: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9" in namespace "projected-9594" to be "running and ready"
Aug 17 03:19:29.560: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894144ms
Aug 17 03:19:29.560: INFO: The phase of Pod pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:19:31.565: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00900772s
Aug 17 03:19:31.565: INFO: The phase of Pod pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9 is Running (Ready = true)
Aug 17 03:19:31.565: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-a9efbd76-cc77-4f9a-bf54-790ca210deb2 08/17/23 03:19:31.582
STEP: waiting to observe update in volume 08/17/23 03:19:31.589
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:33.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9594" for this suite. 08/17/23 03:19:33.686
------------------------------
• [4.179 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:29.515
    Aug 17 03:19:29.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:19:29.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:29.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:29.532
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-a9efbd76-cc77-4f9a-bf54-790ca210deb2 08/17/23 03:19:29.542
    STEP: Creating the pod 08/17/23 03:19:29.547
    Aug 17 03:19:29.556: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9" in namespace "projected-9594" to be "running and ready"
    Aug 17 03:19:29.560: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894144ms
    Aug 17 03:19:29.560: INFO: The phase of Pod pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:19:31.565: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00900772s
    Aug 17 03:19:31.565: INFO: The phase of Pod pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9 is Running (Ready = true)
    Aug 17 03:19:31.565: INFO: Pod "pod-projected-configmaps-533296d2-2f6c-48a5-8c45-b5860c5fc9b9" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-a9efbd76-cc77-4f9a-bf54-790ca210deb2 08/17/23 03:19:31.582
    STEP: waiting to observe update in volume 08/17/23 03:19:31.589
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:33.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9594" for this suite. 08/17/23 03:19:33.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:33.697
Aug 17 03:19:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 03:19:33.698
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:33.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:33.716
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 17 03:19:33.720: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/17/23 03:19:34.738
STEP: Checking rc "condition-test" has the desired failure condition set 08/17/23 03:19:34.743
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/17/23 03:19:35.752
Aug 17 03:19:35.761: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/17/23 03:19:35.761
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8221" for this suite. 08/17/23 03:19:36.778
------------------------------
• [3.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:33.697
    Aug 17 03:19:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 03:19:33.698
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:33.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:33.716
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 17 03:19:33.720: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/17/23 03:19:34.738
    STEP: Checking rc "condition-test" has the desired failure condition set 08/17/23 03:19:34.743
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/17/23 03:19:35.752
    Aug 17 03:19:35.761: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/17/23 03:19:35.761
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8221" for this suite. 08/17/23 03:19:36.778
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:36.787
Aug 17 03:19:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:19:36.787
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:36.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:36.803
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/17/23 03:19:36.807
Aug 17 03:19:36.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/17/23 03:19:42.852
Aug 17 03:19:42.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:19:44.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9003" for this suite. 08/17/23 03:19:50.276
------------------------------
• [SLOW TEST] [13.496 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:36.787
    Aug 17 03:19:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:19:36.787
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:36.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:36.803
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/17/23 03:19:36.807
    Aug 17 03:19:36.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/17/23 03:19:42.852
    Aug 17 03:19:42.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:19:44.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9003" for this suite. 08/17/23 03:19:50.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:50.283
Aug 17 03:19:50.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:19:50.284
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:50.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:50.299
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 17 03:19:50.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:53.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8551" for this suite. 08/17/23 03:19:53.443
------------------------------
• [3.168 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:50.283
    Aug 17 03:19:50.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:19:50.284
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:50.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:50.299
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 17 03:19:50.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:53.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8551" for this suite. 08/17/23 03:19:53.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:53.456
Aug 17 03:19:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 03:19:53.457
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:53.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:53.474
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/17/23 03:19:53.479
STEP: When the matched label of one of its pods change 08/17/23 03:19:53.484
Aug 17 03:19:53.488: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 17 03:19:58.494: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/17/23 03:19:58.517
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 03:19:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4923" for this suite. 08/17/23 03:19:58.533
------------------------------
• [SLOW TEST] [5.084 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:53.456
    Aug 17 03:19:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 03:19:53.457
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:53.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:53.474
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/17/23 03:19:53.479
    STEP: When the matched label of one of its pods change 08/17/23 03:19:53.484
    Aug 17 03:19:53.488: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 17 03:19:58.494: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/17/23 03:19:58.517
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:19:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4923" for this suite. 08/17/23 03:19:58.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:19:58.542
Aug 17 03:19:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:19:58.543
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:58.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:58.558
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:19:58.562
Aug 17 03:19:58.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb" in namespace "projected-7006" to be "Succeeded or Failed"
Aug 17 03:19:58.574: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654806ms
Aug 17 03:20:00.582: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012188668s
Aug 17 03:20:02.581: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012159016s
STEP: Saw pod success 08/17/23 03:20:02.582
Aug 17 03:20:02.582: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb" satisfied condition "Succeeded or Failed"
Aug 17 03:20:02.586: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb container client-container: <nil>
STEP: delete the pod 08/17/23 03:20:02.597
Aug 17 03:20:02.609: INFO: Waiting for pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb to disappear
Aug 17 03:20:02.613: INFO: Pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7006" for this suite. 08/17/23 03:20:02.621
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:19:58.542
    Aug 17 03:19:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:19:58.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:19:58.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:19:58.558
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:19:58.562
    Aug 17 03:19:58.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb" in namespace "projected-7006" to be "Succeeded or Failed"
    Aug 17 03:19:58.574: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654806ms
    Aug 17 03:20:00.582: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012188668s
    Aug 17 03:20:02.581: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012159016s
    STEP: Saw pod success 08/17/23 03:20:02.582
    Aug 17 03:20:02.582: INFO: Pod "downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb" satisfied condition "Succeeded or Failed"
    Aug 17 03:20:02.586: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb container client-container: <nil>
    STEP: delete the pod 08/17/23 03:20:02.597
    Aug 17 03:20:02.609: INFO: Waiting for pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb to disappear
    Aug 17 03:20:02.613: INFO: Pod downwardapi-volume-b6ffb4ea-8522-40ec-be69-8e6f865bd7bb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7006" for this suite. 08/17/23 03:20:02.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:02.635
Aug 17 03:20:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:20:02.636
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:02.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:02.656
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/17/23 03:20:02.661
Aug 17 03:20:02.671: INFO: Waiting up to 5m0s for pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a" in namespace "projected-6694" to be "running and ready"
Aug 17 03:20:02.675: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175257ms
Aug 17 03:20:02.676: INFO: The phase of Pod labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:20:04.682: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470615s
Aug 17 03:20:04.682: INFO: The phase of Pod labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a is Running (Ready = true)
Aug 17 03:20:04.682: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a" satisfied condition "running and ready"
Aug 17 03:20:05.214: INFO: Successfully updated pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6694" for this suite. 08/17/23 03:20:07.251
------------------------------
• [4.624 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:02.635
    Aug 17 03:20:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:20:02.636
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:02.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:02.656
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/17/23 03:20:02.661
    Aug 17 03:20:02.671: INFO: Waiting up to 5m0s for pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a" in namespace "projected-6694" to be "running and ready"
    Aug 17 03:20:02.675: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175257ms
    Aug 17 03:20:02.676: INFO: The phase of Pod labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:20:04.682: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470615s
    Aug 17 03:20:04.682: INFO: The phase of Pod labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a is Running (Ready = true)
    Aug 17 03:20:04.682: INFO: Pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a" satisfied condition "running and ready"
    Aug 17 03:20:05.214: INFO: Successfully updated pod "labelsupdate235622eb-c8db-40f7-ac4e-e493bdd4c30a"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6694" for this suite. 08/17/23 03:20:07.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:07.261
Aug 17 03:20:07.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:20:07.262
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:07.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:07.28
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:20:07.299
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:20:07.657
STEP: Deploying the webhook pod 08/17/23 03:20:07.665
STEP: Wait for the deployment to be ready 08/17/23 03:20:07.678
Aug 17 03:20:07.687: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:20:09.703
STEP: Verifying the service has paired with the endpoint 08/17/23 03:20:09.715
Aug 17 03:20:10.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 17 03:20:10.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1066-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 03:20:11.239
STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 03:20:11.345
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8701" for this suite. 08/17/23 03:20:14.103
STEP: Destroying namespace "webhook-8701-markers" for this suite. 08/17/23 03:20:14.115
------------------------------
• [SLOW TEST] [6.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:07.261
    Aug 17 03:20:07.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:20:07.262
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:07.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:07.28
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:20:07.299
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:20:07.657
    STEP: Deploying the webhook pod 08/17/23 03:20:07.665
    STEP: Wait for the deployment to be ready 08/17/23 03:20:07.678
    Aug 17 03:20:07.687: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:20:09.703
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:20:09.715
    Aug 17 03:20:10.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 17 03:20:10.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1066-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 03:20:11.239
    STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 03:20:11.345
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8701" for this suite. 08/17/23 03:20:14.103
    STEP: Destroying namespace "webhook-8701-markers" for this suite. 08/17/23 03:20:14.115
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:14.132
Aug 17 03:20:14.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:20:14.135
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:14.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:14.175
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:20:14.182
Aug 17 03:20:14.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7" in namespace "projected-1641" to be "Succeeded or Failed"
Aug 17 03:20:14.198: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084038ms
Aug 17 03:20:16.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011847556s
Aug 17 03:20:18.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01119986s
STEP: Saw pod success 08/17/23 03:20:18.205
Aug 17 03:20:18.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7" satisfied condition "Succeeded or Failed"
Aug 17 03:20:18.210: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 container client-container: <nil>
STEP: delete the pod 08/17/23 03:20:18.22
Aug 17 03:20:18.233: INFO: Waiting for pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 to disappear
Aug 17 03:20:18.237: INFO: Pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:18.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1641" for this suite. 08/17/23 03:20:18.245
------------------------------
• [4.121 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:14.132
    Aug 17 03:20:14.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:20:14.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:14.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:14.175
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:20:14.182
    Aug 17 03:20:14.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7" in namespace "projected-1641" to be "Succeeded or Failed"
    Aug 17 03:20:14.198: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084038ms
    Aug 17 03:20:16.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011847556s
    Aug 17 03:20:18.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01119986s
    STEP: Saw pod success 08/17/23 03:20:18.205
    Aug 17 03:20:18.205: INFO: Pod "downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7" satisfied condition "Succeeded or Failed"
    Aug 17 03:20:18.210: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:20:18.22
    Aug 17 03:20:18.233: INFO: Waiting for pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 to disappear
    Aug 17 03:20:18.237: INFO: Pod downwardapi-volume-cb258099-30dc-4eff-9df3-89c9ff4607b7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:18.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1641" for this suite. 08/17/23 03:20:18.245
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:18.255
Aug 17 03:20:18.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:20:18.256
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:18.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:18.275
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:20:18.279
Aug 17 03:20:18.289: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201" in namespace "projected-4931" to be "Succeeded or Failed"
Aug 17 03:20:18.294: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Pending", Reason="", readiness=false. Elapsed: 4.401431ms
Aug 17 03:20:20.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010696353s
Aug 17 03:20:22.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010104352s
STEP: Saw pod success 08/17/23 03:20:22.3
Aug 17 03:20:22.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201" satisfied condition "Succeeded or Failed"
Aug 17 03:20:22.304: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 container client-container: <nil>
STEP: delete the pod 08/17/23 03:20:22.314
Aug 17 03:20:22.326: INFO: Waiting for pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 to disappear
Aug 17 03:20:22.330: INFO: Pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4931" for this suite. 08/17/23 03:20:22.336
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:18.255
    Aug 17 03:20:18.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:20:18.256
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:18.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:18.275
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:20:18.279
    Aug 17 03:20:18.289: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201" in namespace "projected-4931" to be "Succeeded or Failed"
    Aug 17 03:20:18.294: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Pending", Reason="", readiness=false. Elapsed: 4.401431ms
    Aug 17 03:20:20.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010696353s
    Aug 17 03:20:22.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010104352s
    STEP: Saw pod success 08/17/23 03:20:22.3
    Aug 17 03:20:22.300: INFO: Pod "downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201" satisfied condition "Succeeded or Failed"
    Aug 17 03:20:22.304: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:20:22.314
    Aug 17 03:20:22.326: INFO: Waiting for pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 to disappear
    Aug 17 03:20:22.330: INFO: Pod downwardapi-volume-83be7bbf-fbb5-4f9d-9d3a-bc5d75454201 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4931" for this suite. 08/17/23 03:20:22.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:22.346
Aug 17 03:20:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:20:22.347
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:22.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:22.366
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 17 03:20:22.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 03:20:23.85
Aug 17 03:20:23.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 create -f -'
Aug 17 03:20:24.459: INFO: stderr: ""
Aug 17 03:20:24.459: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 03:20:24.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 delete e2e-test-crd-publish-openapi-6511-crds test-cr'
Aug 17 03:20:24.534: INFO: stderr: ""
Aug 17 03:20:24.534: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 17 03:20:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 apply -f -'
Aug 17 03:20:24.731: INFO: stderr: ""
Aug 17 03:20:24.731: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 17 03:20:24.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 delete e2e-test-crd-publish-openapi-6511-crds test-cr'
Aug 17 03:20:24.806: INFO: stderr: ""
Aug 17 03:20:24.806: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/17/23 03:20:24.806
Aug 17 03:20:24.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 explain e2e-test-crd-publish-openapi-6511-crds'
Aug 17 03:20:24.995: INFO: stderr: ""
Aug 17 03:20:24.995: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6511-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:20:26.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7614" for this suite. 08/17/23 03:20:26.522
------------------------------
• [4.187 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:22.346
    Aug 17 03:20:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:20:22.347
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:22.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:22.366
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 17 03:20:22.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 03:20:23.85
    Aug 17 03:20:23.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 create -f -'
    Aug 17 03:20:24.459: INFO: stderr: ""
    Aug 17 03:20:24.459: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 17 03:20:24.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 delete e2e-test-crd-publish-openapi-6511-crds test-cr'
    Aug 17 03:20:24.534: INFO: stderr: ""
    Aug 17 03:20:24.534: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 17 03:20:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 apply -f -'
    Aug 17 03:20:24.731: INFO: stderr: ""
    Aug 17 03:20:24.731: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 17 03:20:24.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 --namespace=crd-publish-openapi-7614 delete e2e-test-crd-publish-openapi-6511-crds test-cr'
    Aug 17 03:20:24.806: INFO: stderr: ""
    Aug 17 03:20:24.806: INFO: stdout: "e2e-test-crd-publish-openapi-6511-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/17/23 03:20:24.806
    Aug 17 03:20:24.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-7614 explain e2e-test-crd-publish-openapi-6511-crds'
    Aug 17 03:20:24.995: INFO: stderr: ""
    Aug 17 03:20:24.995: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6511-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:20:26.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7614" for this suite. 08/17/23 03:20:26.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:20:26.535
Aug 17 03:20:26.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename cronjob 08/17/23 03:20:26.535
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:26.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:26.558
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/17/23 03:20:26.563
STEP: Ensuring no jobs are scheduled 08/17/23 03:20:26.569
STEP: Ensuring no job exists by listing jobs explicitly 08/17/23 03:25:26.58
STEP: Removing cronjob 08/17/23 03:25:26.584
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 03:25:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8400" for this suite. 08/17/23 03:25:26.597
------------------------------
• [SLOW TEST] [300.070 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:20:26.535
    Aug 17 03:20:26.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename cronjob 08/17/23 03:20:26.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:20:26.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:20:26.558
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/17/23 03:20:26.563
    STEP: Ensuring no jobs are scheduled 08/17/23 03:20:26.569
    STEP: Ensuring no job exists by listing jobs explicitly 08/17/23 03:25:26.58
    STEP: Removing cronjob 08/17/23 03:25:26.584
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:25:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8400" for this suite. 08/17/23 03:25:26.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:25:26.606
Aug 17 03:25:26.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:25:26.607
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:25:26.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:25:26.624
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/17/23 03:25:26.628
STEP: Creating a ResourceQuota 08/17/23 03:25:31.633
STEP: Ensuring resource quota status is calculated 08/17/23 03:25:31.638
STEP: Creating a Service 08/17/23 03:25:33.643
STEP: Creating a NodePort Service 08/17/23 03:25:33.658
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/17/23 03:25:33.676
STEP: Ensuring resource quota status captures service creation 08/17/23 03:25:33.698
STEP: Deleting Services 08/17/23 03:25:35.703
STEP: Ensuring resource quota status released usage 08/17/23 03:25:35.732
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:25:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7126" for this suite. 08/17/23 03:25:37.745
------------------------------
• [SLOW TEST] [11.147 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:25:26.606
    Aug 17 03:25:26.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:25:26.607
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:25:26.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:25:26.624
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/17/23 03:25:26.628
    STEP: Creating a ResourceQuota 08/17/23 03:25:31.633
    STEP: Ensuring resource quota status is calculated 08/17/23 03:25:31.638
    STEP: Creating a Service 08/17/23 03:25:33.643
    STEP: Creating a NodePort Service 08/17/23 03:25:33.658
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/17/23 03:25:33.676
    STEP: Ensuring resource quota status captures service creation 08/17/23 03:25:33.698
    STEP: Deleting Services 08/17/23 03:25:35.703
    STEP: Ensuring resource quota status released usage 08/17/23 03:25:35.732
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:25:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7126" for this suite. 08/17/23 03:25:37.745
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:25:37.753
Aug 17 03:25:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename endpointslice 08/17/23 03:25:37.754
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:25:37.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:25:37.772
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/17/23 03:25:42.83
STEP: referencing matching pods with named port 08/17/23 03:25:47.84
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/17/23 03:25:52.851
STEP: recreating EndpointSlices after they've been deleted 08/17/23 03:25:57.861
Aug 17 03:25:57.883: INFO: EndpointSlice for Service endpointslice-7977/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:07.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7977" for this suite. 08/17/23 03:26:07.904
------------------------------
• [SLOW TEST] [30.158 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:25:37.753
    Aug 17 03:25:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename endpointslice 08/17/23 03:25:37.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:25:37.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:25:37.772
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/17/23 03:25:42.83
    STEP: referencing matching pods with named port 08/17/23 03:25:47.84
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/17/23 03:25:52.851
    STEP: recreating EndpointSlices after they've been deleted 08/17/23 03:25:57.861
    Aug 17 03:25:57.883: INFO: EndpointSlice for Service endpointslice-7977/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:07.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7977" for this suite. 08/17/23 03:26:07.904
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:07.913
Aug 17 03:26:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:26:07.914
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:07.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:07.933
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 03:26:07.938
Aug 17 03:26:07.948: INFO: Waiting up to 5m0s for pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb" in namespace "emptydir-7238" to be "Succeeded or Failed"
Aug 17 03:26:07.952: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300674ms
Aug 17 03:26:09.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.0109256s
Aug 17 03:26:11.958: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Running", Reason="", readiness=false. Elapsed: 4.009979884s
Aug 17 03:26:13.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010732006s
STEP: Saw pod success 08/17/23 03:26:13.959
Aug 17 03:26:13.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb" satisfied condition "Succeeded or Failed"
Aug 17 03:26:13.963: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb container test-container: <nil>
STEP: delete the pod 08/17/23 03:26:13.976
Aug 17 03:26:13.989: INFO: Waiting for pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb to disappear
Aug 17 03:26:13.993: INFO: Pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:13.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7238" for this suite. 08/17/23 03:26:14
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:07.913
    Aug 17 03:26:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:26:07.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:07.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:07.933
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/17/23 03:26:07.938
    Aug 17 03:26:07.948: INFO: Waiting up to 5m0s for pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb" in namespace "emptydir-7238" to be "Succeeded or Failed"
    Aug 17 03:26:07.952: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300674ms
    Aug 17 03:26:09.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.0109256s
    Aug 17 03:26:11.958: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Running", Reason="", readiness=false. Elapsed: 4.009979884s
    Aug 17 03:26:13.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010732006s
    STEP: Saw pod success 08/17/23 03:26:13.959
    Aug 17 03:26:13.959: INFO: Pod "pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb" satisfied condition "Succeeded or Failed"
    Aug 17 03:26:13.963: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb container test-container: <nil>
    STEP: delete the pod 08/17/23 03:26:13.976
    Aug 17 03:26:13.989: INFO: Waiting for pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb to disappear
    Aug 17 03:26:13.993: INFO: Pod pod-8125651c-9eb9-4e4d-90a9-a3af372c10cb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:13.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7238" for this suite. 08/17/23 03:26:14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:14.01
Aug 17 03:26:14.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:26:14.01
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:14.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:14.028
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-42d3357c-3958-42b5-a5a4-0dcbe1843a44 08/17/23 03:26:14.034
STEP: Creating a pod to test consume secrets 08/17/23 03:26:14.04
Aug 17 03:26:14.050: INFO: Waiting up to 5m0s for pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0" in namespace "secrets-4584" to be "Succeeded or Failed"
Aug 17 03:26:14.054: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16351ms
Aug 17 03:26:16.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010326702s
Aug 17 03:26:18.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010236766s
Aug 17 03:26:20.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009681048s
STEP: Saw pod success 08/17/23 03:26:20.06
Aug 17 03:26:20.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0" satisfied condition "Succeeded or Failed"
Aug 17 03:26:20.064: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:26:20.075
Aug 17 03:26:20.088: INFO: Waiting for pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 to disappear
Aug 17 03:26:20.092: INFO: Pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:20.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4584" for this suite. 08/17/23 03:26:20.099
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:14.01
    Aug 17 03:26:14.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:26:14.01
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:14.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:14.028
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-42d3357c-3958-42b5-a5a4-0dcbe1843a44 08/17/23 03:26:14.034
    STEP: Creating a pod to test consume secrets 08/17/23 03:26:14.04
    Aug 17 03:26:14.050: INFO: Waiting up to 5m0s for pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0" in namespace "secrets-4584" to be "Succeeded or Failed"
    Aug 17 03:26:14.054: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16351ms
    Aug 17 03:26:16.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010326702s
    Aug 17 03:26:18.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010236766s
    Aug 17 03:26:20.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009681048s
    STEP: Saw pod success 08/17/23 03:26:20.06
    Aug 17 03:26:20.060: INFO: Pod "pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0" satisfied condition "Succeeded or Failed"
    Aug 17 03:26:20.064: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:26:20.075
    Aug 17 03:26:20.088: INFO: Waiting for pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 to disappear
    Aug 17 03:26:20.092: INFO: Pod pod-secrets-86859b80-c8c5-4839-9b54-244431f067a0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:20.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4584" for this suite. 08/17/23 03:26:20.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:20.108
Aug 17 03:26:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:26:20.109
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:20.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:20.129
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1414 08/17/23 03:26:20.134
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[] 08/17/23 03:26:20.149
Aug 17 03:26:20.160: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1414 08/17/23 03:26:20.16
Aug 17 03:26:20.170: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1414" to be "running and ready"
Aug 17 03:26:20.175: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697901ms
Aug 17 03:26:20.175: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:26:22.181: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011027466s
Aug 17 03:26:22.181: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:26:24.180: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009887129s
Aug 17 03:26:24.180: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 17 03:26:24.180: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod1:[100]] 08/17/23 03:26:24.184
Aug 17 03:26:24.197: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1414 08/17/23 03:26:24.197
Aug 17 03:26:24.203: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1414" to be "running and ready"
Aug 17 03:26:24.208: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593925ms
Aug 17 03:26:24.208: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:26:26.213: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010163062s
Aug 17 03:26:26.213: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 17 03:26:26.213: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod1:[100] pod2:[101]] 08/17/23 03:26:26.217
Aug 17 03:26:26.238: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/17/23 03:26:26.238
Aug 17 03:26:26.238: INFO: Creating new exec pod
Aug 17 03:26:26.247: INFO: Waiting up to 5m0s for pod "execpod7x4rr" in namespace "services-1414" to be "running"
Aug 17 03:26:26.252: INFO: Pod "execpod7x4rr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022745ms
Aug 17 03:26:28.257: INFO: Pod "execpod7x4rr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010877261s
Aug 17 03:26:30.258: INFO: Pod "execpod7x4rr": Phase="Running", Reason="", readiness=true. Elapsed: 4.011384284s
Aug 17 03:26:30.258: INFO: Pod "execpod7x4rr" satisfied condition "running"
Aug 17 03:26:31.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 17 03:26:31.691: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 17 03:26:31.691: INFO: stdout: ""
Aug 17 03:26:31.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 172.20.123.169 80'
Aug 17 03:26:32.119: INFO: stderr: "+ nc -v -z -w 2 172.20.123.169 80\nConnection to 172.20.123.169 80 port [tcp/http] succeeded!\n"
Aug 17 03:26:32.119: INFO: stdout: ""
Aug 17 03:26:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 17 03:26:32.632: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 17 03:26:32.632: INFO: stdout: ""
Aug 17 03:26:32.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 172.20.123.169 81'
Aug 17 03:26:33.056: INFO: stderr: "+ nc -v -z -w 2 172.20.123.169 81\nConnection to 172.20.123.169 81 port [tcp/*] succeeded!\n"
Aug 17 03:26:33.056: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1414 08/17/23 03:26:33.056
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod2:[101]] 08/17/23 03:26:33.068
Aug 17 03:26:33.089: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1414 08/17/23 03:26:33.089
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[] 08/17/23 03:26:33.1
Aug 17 03:26:33.116: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:33.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1414" for this suite. 08/17/23 03:26:33.137
------------------------------
• [SLOW TEST] [13.037 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:20.108
    Aug 17 03:26:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:26:20.109
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:20.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:20.129
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1414 08/17/23 03:26:20.134
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[] 08/17/23 03:26:20.149
    Aug 17 03:26:20.160: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1414 08/17/23 03:26:20.16
    Aug 17 03:26:20.170: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1414" to be "running and ready"
    Aug 17 03:26:20.175: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697901ms
    Aug 17 03:26:20.175: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:26:22.181: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011027466s
    Aug 17 03:26:22.181: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:26:24.180: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009887129s
    Aug 17 03:26:24.180: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 17 03:26:24.180: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod1:[100]] 08/17/23 03:26:24.184
    Aug 17 03:26:24.197: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1414 08/17/23 03:26:24.197
    Aug 17 03:26:24.203: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1414" to be "running and ready"
    Aug 17 03:26:24.208: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593925ms
    Aug 17 03:26:24.208: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:26:26.213: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010163062s
    Aug 17 03:26:26.213: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 17 03:26:26.213: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod1:[100] pod2:[101]] 08/17/23 03:26:26.217
    Aug 17 03:26:26.238: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/17/23 03:26:26.238
    Aug 17 03:26:26.238: INFO: Creating new exec pod
    Aug 17 03:26:26.247: INFO: Waiting up to 5m0s for pod "execpod7x4rr" in namespace "services-1414" to be "running"
    Aug 17 03:26:26.252: INFO: Pod "execpod7x4rr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022745ms
    Aug 17 03:26:28.257: INFO: Pod "execpod7x4rr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010877261s
    Aug 17 03:26:30.258: INFO: Pod "execpod7x4rr": Phase="Running", Reason="", readiness=true. Elapsed: 4.011384284s
    Aug 17 03:26:30.258: INFO: Pod "execpod7x4rr" satisfied condition "running"
    Aug 17 03:26:31.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 17 03:26:31.691: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 17 03:26:31.691: INFO: stdout: ""
    Aug 17 03:26:31.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 172.20.123.169 80'
    Aug 17 03:26:32.119: INFO: stderr: "+ nc -v -z -w 2 172.20.123.169 80\nConnection to 172.20.123.169 80 port [tcp/http] succeeded!\n"
    Aug 17 03:26:32.119: INFO: stdout: ""
    Aug 17 03:26:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 17 03:26:32.632: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 17 03:26:32.632: INFO: stdout: ""
    Aug 17 03:26:32.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1414 exec execpod7x4rr -- /bin/sh -x -c nc -v -z -w 2 172.20.123.169 81'
    Aug 17 03:26:33.056: INFO: stderr: "+ nc -v -z -w 2 172.20.123.169 81\nConnection to 172.20.123.169 81 port [tcp/*] succeeded!\n"
    Aug 17 03:26:33.056: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1414 08/17/23 03:26:33.056
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[pod2:[101]] 08/17/23 03:26:33.068
    Aug 17 03:26:33.089: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1414 08/17/23 03:26:33.089
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1414 to expose endpoints map[] 08/17/23 03:26:33.1
    Aug 17 03:26:33.116: INFO: successfully validated that service multi-endpoint-test in namespace services-1414 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:33.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1414" for this suite. 08/17/23 03:26:33.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:33.146
Aug 17 03:26:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:26:33.147
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:33.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:33.165
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/17/23 03:26:33.174
STEP: watching for the Service to be added 08/17/23 03:26:33.184
Aug 17 03:26:33.186: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 17 03:26:33.186: INFO: Service test-service-7mk9d created
STEP: Getting /status 08/17/23 03:26:33.186
Aug 17 03:26:33.191: INFO: Service test-service-7mk9d has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/17/23 03:26:33.191
STEP: watching for the Service to be patched 08/17/23 03:26:33.198
Aug 17 03:26:33.202: INFO: observed Service test-service-7mk9d in namespace services-8042 with annotations: map[] & LoadBalancer: {[]}
Aug 17 03:26:33.202: INFO: Found Service test-service-7mk9d in namespace services-8042 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 17 03:26:33.202: INFO: Service test-service-7mk9d has service status patched
STEP: updating the ServiceStatus 08/17/23 03:26:33.202
Aug 17 03:26:33.213: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/17/23 03:26:33.213
Aug 17 03:26:33.216: INFO: Observed Service test-service-7mk9d in namespace services-8042 with annotations: map[] & Conditions: {[]}
Aug 17 03:26:33.216: INFO: Observed event: &Service{ObjectMeta:{test-service-7mk9d  services-8042  c72d2157-932c-4d14-a71f-2291d8fcc61f 221550 0 2023-08-17 03:26:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-17 03:26:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-17 03:26:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.20.118.96,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.20.118.96],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 17 03:26:33.216: INFO: Found Service test-service-7mk9d in namespace services-8042 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 03:26:33.216: INFO: Service test-service-7mk9d has service status updated
STEP: patching the service 08/17/23 03:26:33.216
STEP: watching for the Service to be patched 08/17/23 03:26:33.224
Aug 17 03:26:33.227: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
Aug 17 03:26:33.227: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
Aug 17 03:26:33.228: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
Aug 17 03:26:33.228: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service:patched test-service-static:true]
Aug 17 03:26:33.228: INFO: Service test-service-7mk9d patched
STEP: deleting the service 08/17/23 03:26:33.228
STEP: watching for the Service to be deleted 08/17/23 03:26:33.245
Aug 17 03:26:33.248: INFO: Observed event: ADDED
Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
Aug 17 03:26:33.248: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 17 03:26:33.248: INFO: Service test-service-7mk9d deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8042" for this suite. 08/17/23 03:26:33.257
------------------------------
• [0.119 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:33.146
    Aug 17 03:26:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:26:33.147
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:33.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:33.165
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/17/23 03:26:33.174
    STEP: watching for the Service to be added 08/17/23 03:26:33.184
    Aug 17 03:26:33.186: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 17 03:26:33.186: INFO: Service test-service-7mk9d created
    STEP: Getting /status 08/17/23 03:26:33.186
    Aug 17 03:26:33.191: INFO: Service test-service-7mk9d has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/17/23 03:26:33.191
    STEP: watching for the Service to be patched 08/17/23 03:26:33.198
    Aug 17 03:26:33.202: INFO: observed Service test-service-7mk9d in namespace services-8042 with annotations: map[] & LoadBalancer: {[]}
    Aug 17 03:26:33.202: INFO: Found Service test-service-7mk9d in namespace services-8042 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 17 03:26:33.202: INFO: Service test-service-7mk9d has service status patched
    STEP: updating the ServiceStatus 08/17/23 03:26:33.202
    Aug 17 03:26:33.213: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/17/23 03:26:33.213
    Aug 17 03:26:33.216: INFO: Observed Service test-service-7mk9d in namespace services-8042 with annotations: map[] & Conditions: {[]}
    Aug 17 03:26:33.216: INFO: Observed event: &Service{ObjectMeta:{test-service-7mk9d  services-8042  c72d2157-932c-4d14-a71f-2291d8fcc61f 221550 0 2023-08-17 03:26:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-17 03:26:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-17 03:26:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.20.118.96,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.20.118.96],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 17 03:26:33.216: INFO: Found Service test-service-7mk9d in namespace services-8042 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 03:26:33.216: INFO: Service test-service-7mk9d has service status updated
    STEP: patching the service 08/17/23 03:26:33.216
    STEP: watching for the Service to be patched 08/17/23 03:26:33.224
    Aug 17 03:26:33.227: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
    Aug 17 03:26:33.227: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
    Aug 17 03:26:33.228: INFO: observed Service test-service-7mk9d in namespace services-8042 with labels: map[test-service-static:true]
    Aug 17 03:26:33.228: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service:patched test-service-static:true]
    Aug 17 03:26:33.228: INFO: Service test-service-7mk9d patched
    STEP: deleting the service 08/17/23 03:26:33.228
    STEP: watching for the Service to be deleted 08/17/23 03:26:33.245
    Aug 17 03:26:33.248: INFO: Observed event: ADDED
    Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
    Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
    Aug 17 03:26:33.248: INFO: Observed event: MODIFIED
    Aug 17 03:26:33.248: INFO: Found Service test-service-7mk9d in namespace services-8042 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 17 03:26:33.248: INFO: Service test-service-7mk9d deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8042" for this suite. 08/17/23 03:26:33.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:33.266
Aug 17 03:26:33.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename proxy 08/17/23 03:26:33.267
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:33.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:33.289
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 17 03:26:33.293: INFO: Creating pod...
Aug 17 03:26:33.302: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8137" to be "running"
Aug 17 03:26:33.306: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924225ms
Aug 17 03:26:35.311: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009120104s
Aug 17 03:26:35.311: INFO: Pod "agnhost" satisfied condition "running"
Aug 17 03:26:35.311: INFO: Creating service...
Aug 17 03:26:35.323: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/DELETE
Aug 17 03:26:35.416: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 03:26:35.416: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/GET
Aug 17 03:26:35.444: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 17 03:26:35.444: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/HEAD
Aug 17 03:26:35.465: INFO: http.Client request:HEAD | StatusCode:200
Aug 17 03:26:35.465: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 17 03:26:35.509: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 03:26:35.509: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/PATCH
Aug 17 03:26:35.515: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 03:26:35.515: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/POST
Aug 17 03:26:35.521: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 03:26:35.521: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/PUT
Aug 17 03:26:35.527: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 03:26:35.527: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/DELETE
Aug 17 03:26:35.535: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 03:26:35.535: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/GET
Aug 17 03:26:35.543: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 17 03:26:35.543: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/HEAD
Aug 17 03:26:35.550: INFO: http.Client request:HEAD | StatusCode:200
Aug 17 03:26:35.550: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/OPTIONS
Aug 17 03:26:35.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 03:26:35.558: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/PATCH
Aug 17 03:26:35.565: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 03:26:35.565: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/POST
Aug 17 03:26:35.573: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 03:26:35.573: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/PUT
Aug 17 03:26:35.581: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:35.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8137" for this suite. 08/17/23 03:26:35.591
------------------------------
• [2.333 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:33.266
    Aug 17 03:26:33.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename proxy 08/17/23 03:26:33.267
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:33.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:33.289
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 17 03:26:33.293: INFO: Creating pod...
    Aug 17 03:26:33.302: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8137" to be "running"
    Aug 17 03:26:33.306: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924225ms
    Aug 17 03:26:35.311: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009120104s
    Aug 17 03:26:35.311: INFO: Pod "agnhost" satisfied condition "running"
    Aug 17 03:26:35.311: INFO: Creating service...
    Aug 17 03:26:35.323: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/DELETE
    Aug 17 03:26:35.416: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 03:26:35.416: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/GET
    Aug 17 03:26:35.444: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 17 03:26:35.444: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/HEAD
    Aug 17 03:26:35.465: INFO: http.Client request:HEAD | StatusCode:200
    Aug 17 03:26:35.465: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 17 03:26:35.509: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 03:26:35.509: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/PATCH
    Aug 17 03:26:35.515: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 03:26:35.515: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/POST
    Aug 17 03:26:35.521: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 03:26:35.521: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/pods/agnhost/proxy/some/path/with/PUT
    Aug 17 03:26:35.527: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 03:26:35.527: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/DELETE
    Aug 17 03:26:35.535: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 03:26:35.535: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/GET
    Aug 17 03:26:35.543: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 17 03:26:35.543: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/HEAD
    Aug 17 03:26:35.550: INFO: http.Client request:HEAD | StatusCode:200
    Aug 17 03:26:35.550: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/OPTIONS
    Aug 17 03:26:35.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 03:26:35.558: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/PATCH
    Aug 17 03:26:35.565: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 03:26:35.565: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/POST
    Aug 17 03:26:35.573: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 03:26:35.573: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-8137/services/test-service/proxy/some/path/with/PUT
    Aug 17 03:26:35.581: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:35.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8137" for this suite. 08/17/23 03:26:35.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:35.601
Aug 17 03:26:35.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:26:35.602
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:35.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:35.62
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:26:35.625
Aug 17 03:26:35.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 17 03:26:35.692: INFO: stderr: ""
Aug 17 03:26:35.692: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/17/23 03:26:35.692
Aug 17 03:26:35.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 17 03:26:36.272: INFO: stderr: ""
Aug 17 03:26:36.272: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:26:36.272
Aug 17 03:26:36.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 delete pods e2e-test-httpd-pod'
Aug 17 03:26:38.320: INFO: stderr: ""
Aug 17 03:26:38.320: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:38.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3059" for this suite. 08/17/23 03:26:38.329
------------------------------
• [2.734 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:35.601
    Aug 17 03:26:35.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:26:35.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:35.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:35.62
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:26:35.625
    Aug 17 03:26:35.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 17 03:26:35.692: INFO: stderr: ""
    Aug 17 03:26:35.692: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/17/23 03:26:35.692
    Aug 17 03:26:35.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 17 03:26:36.272: INFO: stderr: ""
    Aug 17 03:26:36.272: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:26:36.272
    Aug 17 03:26:36.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3059 delete pods e2e-test-httpd-pod'
    Aug 17 03:26:38.320: INFO: stderr: ""
    Aug 17 03:26:38.320: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:38.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3059" for this suite. 08/17/23 03:26:38.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:38.34
Aug 17 03:26:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:26:38.34
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:38.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:38.359
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/17/23 03:26:38.366
STEP: fetching the ConfigMap 08/17/23 03:26:38.375
STEP: patching the ConfigMap 08/17/23 03:26:38.381
STEP: listing all ConfigMaps in all namespaces with a label selector 08/17/23 03:26:38.392
STEP: deleting the ConfigMap by collection with a label selector 08/17/23 03:26:38.398
STEP: listing all ConfigMaps in test namespace 08/17/23 03:26:38.408
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:26:38.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1742" for this suite. 08/17/23 03:26:38.419
------------------------------
• [0.087 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:38.34
    Aug 17 03:26:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:26:38.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:38.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:38.359
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/17/23 03:26:38.366
    STEP: fetching the ConfigMap 08/17/23 03:26:38.375
    STEP: patching the ConfigMap 08/17/23 03:26:38.381
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/17/23 03:26:38.392
    STEP: deleting the ConfigMap by collection with a label selector 08/17/23 03:26:38.398
    STEP: listing all ConfigMaps in test namespace 08/17/23 03:26:38.408
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:26:38.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1742" for this suite. 08/17/23 03:26:38.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:26:38.427
Aug 17 03:26:38.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename subpath 08/17/23 03:26:38.428
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:38.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:38.449
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 03:26:38.454
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-d9cf 08/17/23 03:26:38.466
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:26:38.466
Aug 17 03:26:38.475: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d9cf" in namespace "subpath-8403" to be "Succeeded or Failed"
Aug 17 03:26:38.484: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353475ms
Aug 17 03:26:40.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015298968s
Aug 17 03:26:42.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014400005s
Aug 17 03:26:44.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015228971s
Aug 17 03:26:46.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 8.014525508s
Aug 17 03:26:48.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 10.015521841s
Aug 17 03:26:50.491: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 12.016887334s
Aug 17 03:26:52.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 14.014214394s
Aug 17 03:26:54.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 16.014788593s
Aug 17 03:26:56.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 18.014724012s
Aug 17 03:26:58.493: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 20.018443598s
Aug 17 03:27:00.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=false. Elapsed: 22.014836398s
Aug 17 03:27:02.491: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016013009s
STEP: Saw pod success 08/17/23 03:27:02.491
Aug 17 03:27:02.491: INFO: Pod "pod-subpath-test-configmap-d9cf" satisfied condition "Succeeded or Failed"
Aug 17 03:27:02.496: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-configmap-d9cf container test-container-subpath-configmap-d9cf: <nil>
STEP: delete the pod 08/17/23 03:27:02.508
Aug 17 03:27:02.521: INFO: Waiting for pod pod-subpath-test-configmap-d9cf to disappear
Aug 17 03:27:02.526: INFO: Pod pod-subpath-test-configmap-d9cf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d9cf 08/17/23 03:27:02.526
Aug 17 03:27:02.527: INFO: Deleting pod "pod-subpath-test-configmap-d9cf" in namespace "subpath-8403"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:02.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8403" for this suite. 08/17/23 03:27:02.54
------------------------------
• [SLOW TEST] [24.122 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:26:38.427
    Aug 17 03:26:38.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename subpath 08/17/23 03:26:38.428
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:26:38.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:26:38.449
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 03:26:38.454
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-d9cf 08/17/23 03:26:38.466
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:26:38.466
    Aug 17 03:26:38.475: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d9cf" in namespace "subpath-8403" to be "Succeeded or Failed"
    Aug 17 03:26:38.484: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353475ms
    Aug 17 03:26:40.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015298968s
    Aug 17 03:26:42.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.014400005s
    Aug 17 03:26:44.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015228971s
    Aug 17 03:26:46.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 8.014525508s
    Aug 17 03:26:48.490: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 10.015521841s
    Aug 17 03:26:50.491: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 12.016887334s
    Aug 17 03:26:52.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 14.014214394s
    Aug 17 03:26:54.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 16.014788593s
    Aug 17 03:26:56.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 18.014724012s
    Aug 17 03:26:58.493: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=true. Elapsed: 20.018443598s
    Aug 17 03:27:00.489: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Running", Reason="", readiness=false. Elapsed: 22.014836398s
    Aug 17 03:27:02.491: INFO: Pod "pod-subpath-test-configmap-d9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016013009s
    STEP: Saw pod success 08/17/23 03:27:02.491
    Aug 17 03:27:02.491: INFO: Pod "pod-subpath-test-configmap-d9cf" satisfied condition "Succeeded or Failed"
    Aug 17 03:27:02.496: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-configmap-d9cf container test-container-subpath-configmap-d9cf: <nil>
    STEP: delete the pod 08/17/23 03:27:02.508
    Aug 17 03:27:02.521: INFO: Waiting for pod pod-subpath-test-configmap-d9cf to disappear
    Aug 17 03:27:02.526: INFO: Pod pod-subpath-test-configmap-d9cf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-d9cf 08/17/23 03:27:02.526
    Aug 17 03:27:02.527: INFO: Deleting pod "pod-subpath-test-configmap-d9cf" in namespace "subpath-8403"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:02.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8403" for this suite. 08/17/23 03:27:02.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:02.55
Aug 17 03:27:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 03:27:02.551
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:02.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:02.574
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/17/23 03:27:02.58
Aug 17 03:27:02.591: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 03:27:07.596: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 03:27:07.596
STEP: getting scale subresource 08/17/23 03:27:07.596
STEP: updating a scale subresource 08/17/23 03:27:07.6
STEP: verifying the replicaset Spec.Replicas was modified 08/17/23 03:27:07.606
STEP: Patch a scale subresource 08/17/23 03:27:07.61
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:07.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5884" for this suite. 08/17/23 03:27:07.632
------------------------------
• [SLOW TEST] [5.090 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:02.55
    Aug 17 03:27:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 03:27:02.551
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:02.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:02.574
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/17/23 03:27:02.58
    Aug 17 03:27:02.591: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 03:27:07.596: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 03:27:07.596
    STEP: getting scale subresource 08/17/23 03:27:07.596
    STEP: updating a scale subresource 08/17/23 03:27:07.6
    STEP: verifying the replicaset Spec.Replicas was modified 08/17/23 03:27:07.606
    STEP: Patch a scale subresource 08/17/23 03:27:07.61
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:07.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5884" for this suite. 08/17/23 03:27:07.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:07.643
Aug 17 03:27:07.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename subpath 08/17/23 03:27:07.644
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:07.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:07.664
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 03:27:07.669
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-w58d 08/17/23 03:27:07.68
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:27:07.68
Aug 17 03:27:07.690: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w58d" in namespace "subpath-2363" to be "Succeeded or Failed"
Aug 17 03:27:07.695: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217855ms
Aug 17 03:27:09.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011732741s
Aug 17 03:27:11.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011518363s
Aug 17 03:27:13.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 6.011041023s
Aug 17 03:27:15.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 8.010442994s
Aug 17 03:27:17.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 10.010754321s
Aug 17 03:27:19.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 12.010673992s
Aug 17 03:27:21.702: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 14.011988599s
Aug 17 03:27:23.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 16.011788722s
Aug 17 03:27:25.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 18.010326799s
Aug 17 03:27:27.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 20.011125073s
Aug 17 03:27:29.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=false. Elapsed: 22.011615427s
Aug 17 03:27:31.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011501443s
STEP: Saw pod success 08/17/23 03:27:31.701
Aug 17 03:27:31.701: INFO: Pod "pod-subpath-test-secret-w58d" satisfied condition "Succeeded or Failed"
Aug 17 03:27:31.706: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-secret-w58d container test-container-subpath-secret-w58d: <nil>
STEP: delete the pod 08/17/23 03:27:31.716
Aug 17 03:27:31.730: INFO: Waiting for pod pod-subpath-test-secret-w58d to disappear
Aug 17 03:27:31.733: INFO: Pod pod-subpath-test-secret-w58d no longer exists
STEP: Deleting pod pod-subpath-test-secret-w58d 08/17/23 03:27:31.733
Aug 17 03:27:31.734: INFO: Deleting pod "pod-subpath-test-secret-w58d" in namespace "subpath-2363"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:31.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2363" for this suite. 08/17/23 03:27:31.743
------------------------------
• [SLOW TEST] [24.107 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:07.643
    Aug 17 03:27:07.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename subpath 08/17/23 03:27:07.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:07.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:07.664
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 03:27:07.669
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-w58d 08/17/23 03:27:07.68
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:27:07.68
    Aug 17 03:27:07.690: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w58d" in namespace "subpath-2363" to be "Succeeded or Failed"
    Aug 17 03:27:07.695: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217855ms
    Aug 17 03:27:09.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011732741s
    Aug 17 03:27:11.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011518363s
    Aug 17 03:27:13.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 6.011041023s
    Aug 17 03:27:15.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 8.010442994s
    Aug 17 03:27:17.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 10.010754321s
    Aug 17 03:27:19.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 12.010673992s
    Aug 17 03:27:21.702: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 14.011988599s
    Aug 17 03:27:23.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 16.011788722s
    Aug 17 03:27:25.700: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 18.010326799s
    Aug 17 03:27:27.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=true. Elapsed: 20.011125073s
    Aug 17 03:27:29.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Running", Reason="", readiness=false. Elapsed: 22.011615427s
    Aug 17 03:27:31.701: INFO: Pod "pod-subpath-test-secret-w58d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011501443s
    STEP: Saw pod success 08/17/23 03:27:31.701
    Aug 17 03:27:31.701: INFO: Pod "pod-subpath-test-secret-w58d" satisfied condition "Succeeded or Failed"
    Aug 17 03:27:31.706: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-secret-w58d container test-container-subpath-secret-w58d: <nil>
    STEP: delete the pod 08/17/23 03:27:31.716
    Aug 17 03:27:31.730: INFO: Waiting for pod pod-subpath-test-secret-w58d to disappear
    Aug 17 03:27:31.733: INFO: Pod pod-subpath-test-secret-w58d no longer exists
    STEP: Deleting pod pod-subpath-test-secret-w58d 08/17/23 03:27:31.733
    Aug 17 03:27:31.734: INFO: Deleting pod "pod-subpath-test-secret-w58d" in namespace "subpath-2363"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:31.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2363" for this suite. 08/17/23 03:27:31.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:31.753
Aug 17 03:27:31.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:27:31.754
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:31.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:31.772
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9045 08/17/23 03:27:31.777
STEP: creating a selector 08/17/23 03:27:31.777
STEP: Creating the service pods in kubernetes 08/17/23 03:27:31.777
Aug 17 03:27:31.777: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 03:27:31.800: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9045" to be "running and ready"
Aug 17 03:27:31.804: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319225ms
Aug 17 03:27:31.805: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:27:33.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008921302s
Aug 17 03:27:33.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:27:35.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00981554s
Aug 17 03:27:35.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:27:37.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009819669s
Aug 17 03:27:37.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:27:39.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009122013s
Aug 17 03:27:39.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:27:41.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010381858s
Aug 17 03:27:41.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:27:43.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009417757s
Aug 17 03:27:43.810: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 03:27:43.810: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 03:27:43.814: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9045" to be "running and ready"
Aug 17 03:27:43.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.112652ms
Aug 17 03:27:43.818: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 03:27:43.818: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 03:27:43.822
Aug 17 03:27:43.827: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9045" to be "running"
Aug 17 03:27:43.831: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67726ms
Aug 17 03:27:45.836: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00942491s
Aug 17 03:27:45.836: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 03:27:45.841: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 17 03:27:45.841: INFO: Breadth first check of 172.21.86.174 on host 192.168.11.3...
Aug 17 03:27:45.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.77:9080/dial?request=hostname&protocol=http&host=172.21.86.174&port=8083&tries=1'] Namespace:pod-network-test-9045 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:27:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:27:45.846: INFO: ExecWithOptions: Clientset creation
Aug 17 03:27:45.846: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-9045/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.86.174%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 03:27:46.295: INFO: Waiting for responses: map[]
Aug 17 03:27:46.295: INFO: reached 172.21.86.174 after 0/1 tries
Aug 17 03:27:46.295: INFO: Breadth first check of 172.21.15.117 on host 192.168.11.4...
Aug 17 03:27:46.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.77:9080/dial?request=hostname&protocol=http&host=172.21.15.117&port=8083&tries=1'] Namespace:pod-network-test-9045 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:27:46.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:27:46.301: INFO: ExecWithOptions: Clientset creation
Aug 17 03:27:46.301: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-9045/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.15.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 17 03:27:46.810: INFO: Waiting for responses: map[]
Aug 17 03:27:46.810: INFO: reached 172.21.15.117 after 0/1 tries
Aug 17 03:27:46.810: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:46.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9045" for this suite. 08/17/23 03:27:46.817
------------------------------
• [SLOW TEST] [15.070 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:31.753
    Aug 17 03:27:31.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:27:31.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:31.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:31.772
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9045 08/17/23 03:27:31.777
    STEP: creating a selector 08/17/23 03:27:31.777
    STEP: Creating the service pods in kubernetes 08/17/23 03:27:31.777
    Aug 17 03:27:31.777: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 03:27:31.800: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9045" to be "running and ready"
    Aug 17 03:27:31.804: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319225ms
    Aug 17 03:27:31.805: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:27:33.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008921302s
    Aug 17 03:27:33.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:27:35.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00981554s
    Aug 17 03:27:35.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:27:37.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009819669s
    Aug 17 03:27:37.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:27:39.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009122013s
    Aug 17 03:27:39.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:27:41.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010381858s
    Aug 17 03:27:41.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:27:43.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009417757s
    Aug 17 03:27:43.810: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 03:27:43.810: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 03:27:43.814: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9045" to be "running and ready"
    Aug 17 03:27:43.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.112652ms
    Aug 17 03:27:43.818: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 03:27:43.818: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 03:27:43.822
    Aug 17 03:27:43.827: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9045" to be "running"
    Aug 17 03:27:43.831: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67726ms
    Aug 17 03:27:45.836: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00942491s
    Aug 17 03:27:45.836: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 03:27:45.841: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 17 03:27:45.841: INFO: Breadth first check of 172.21.86.174 on host 192.168.11.3...
    Aug 17 03:27:45.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.77:9080/dial?request=hostname&protocol=http&host=172.21.86.174&port=8083&tries=1'] Namespace:pod-network-test-9045 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:27:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:27:45.846: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:27:45.846: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-9045/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.86.174%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 03:27:46.295: INFO: Waiting for responses: map[]
    Aug 17 03:27:46.295: INFO: reached 172.21.86.174 after 0/1 tries
    Aug 17 03:27:46.295: INFO: Breadth first check of 172.21.15.117 on host 192.168.11.4...
    Aug 17 03:27:46.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.15.77:9080/dial?request=hostname&protocol=http&host=172.21.15.117&port=8083&tries=1'] Namespace:pod-network-test-9045 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:27:46.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:27:46.301: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:27:46.301: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-9045/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.15.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.15.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 17 03:27:46.810: INFO: Waiting for responses: map[]
    Aug 17 03:27:46.810: INFO: reached 172.21.15.117 after 0/1 tries
    Aug 17 03:27:46.810: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:46.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9045" for this suite. 08/17/23 03:27:46.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:46.826
Aug 17 03:27:46.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 03:27:46.826
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:46.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:46.846
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 08/17/23 03:27:46.873
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:27:46.878
Aug 17 03:27:46.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:27:46.887: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:27:47.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:27:47.900: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:27:48.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:27:48.899: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/17/23 03:27:48.902
Aug 17 03:27:48.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:27:48.925: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:27:49.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:27:49.938: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:27:50.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:27:50.938: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/17/23 03:27:50.938
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:27:50.945
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2352, will wait for the garbage collector to delete the pods 08/17/23 03:27:50.945
Aug 17 03:27:51.006: INFO: Deleting DaemonSet.extensions daemon-set took: 6.458736ms
Aug 17 03:27:51.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.084456ms
Aug 17 03:27:53.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:27:53.511: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 03:27:53.515: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"222205"},"items":null}

Aug 17 03:27:53.519: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"222206"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:53.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2352" for this suite. 08/17/23 03:27:53.54
------------------------------
• [SLOW TEST] [6.721 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:46.826
    Aug 17 03:27:46.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 03:27:46.826
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:46.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:46.846
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 08/17/23 03:27:46.873
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:27:46.878
    Aug 17 03:27:46.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:27:46.887: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:27:47.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:27:47.900: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:27:48.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:27:48.899: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/17/23 03:27:48.902
    Aug 17 03:27:48.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:27:48.925: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:27:49.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:27:49.938: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:27:50.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:27:50.938: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/17/23 03:27:50.938
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:27:50.945
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2352, will wait for the garbage collector to delete the pods 08/17/23 03:27:50.945
    Aug 17 03:27:51.006: INFO: Deleting DaemonSet.extensions daemon-set took: 6.458736ms
    Aug 17 03:27:51.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.084456ms
    Aug 17 03:27:53.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:27:53.511: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 03:27:53.515: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"222205"},"items":null}

    Aug 17 03:27:53.519: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"222206"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:53.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2352" for this suite. 08/17/23 03:27:53.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:53.547
Aug 17 03:27:53.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename ingressclass 08/17/23 03:27:53.548
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:53.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:53.563
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/17/23 03:27:53.568
STEP: getting /apis/networking.k8s.io 08/17/23 03:27:53.571
STEP: getting /apis/networking.k8s.iov1 08/17/23 03:27:53.573
STEP: creating 08/17/23 03:27:53.575
STEP: getting 08/17/23 03:27:53.588
STEP: listing 08/17/23 03:27:53.592
STEP: watching 08/17/23 03:27:53.595
Aug 17 03:27:53.595: INFO: starting watch
STEP: patching 08/17/23 03:27:53.597
STEP: updating 08/17/23 03:27:53.601
Aug 17 03:27:53.606: INFO: waiting for watch events with expected annotations
Aug 17 03:27:53.606: INFO: saw patched and updated annotations
STEP: deleting 08/17/23 03:27:53.606
STEP: deleting a collection 08/17/23 03:27:53.617
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 17 03:27:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3021" for this suite. 08/17/23 03:27:53.638
------------------------------
• [0.098 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:53.547
    Aug 17 03:27:53.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename ingressclass 08/17/23 03:27:53.548
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:53.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:53.563
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/17/23 03:27:53.568
    STEP: getting /apis/networking.k8s.io 08/17/23 03:27:53.571
    STEP: getting /apis/networking.k8s.iov1 08/17/23 03:27:53.573
    STEP: creating 08/17/23 03:27:53.575
    STEP: getting 08/17/23 03:27:53.588
    STEP: listing 08/17/23 03:27:53.592
    STEP: watching 08/17/23 03:27:53.595
    Aug 17 03:27:53.595: INFO: starting watch
    STEP: patching 08/17/23 03:27:53.597
    STEP: updating 08/17/23 03:27:53.601
    Aug 17 03:27:53.606: INFO: waiting for watch events with expected annotations
    Aug 17 03:27:53.606: INFO: saw patched and updated annotations
    STEP: deleting 08/17/23 03:27:53.606
    STEP: deleting a collection 08/17/23 03:27:53.617
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:27:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3021" for this suite. 08/17/23 03:27:53.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:27:53.646
Aug 17 03:27:53.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-webhook 08/17/23 03:27:53.647
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:53.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:53.662
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/17/23 03:27:53.666
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 03:27:53.863
STEP: Deploying the custom resource conversion webhook pod 08/17/23 03:27:53.872
STEP: Wait for the deployment to be ready 08/17/23 03:27:53.883
Aug 17 03:27:53.892: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 17 03:27:55.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/17/23 03:27:57.911
STEP: Verifying the service has paired with the endpoint 08/17/23 03:27:57.922
Aug 17 03:27:58.923: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 17 03:27:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Creating a v1 custom resource 08/17/23 03:28:01.705
STEP: v2 custom resource should be converted 08/17/23 03:28:01.712
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:28:02.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9220" for this suite. 08/17/23 03:28:02.326
------------------------------
• [SLOW TEST] [8.696 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:27:53.646
    Aug 17 03:27:53.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-webhook 08/17/23 03:27:53.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:27:53.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:27:53.662
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/17/23 03:27:53.666
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 03:27:53.863
    STEP: Deploying the custom resource conversion webhook pod 08/17/23 03:27:53.872
    STEP: Wait for the deployment to be ready 08/17/23 03:27:53.883
    Aug 17 03:27:53.892: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Aug 17 03:27:55.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 27, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/17/23 03:27:57.911
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:27:57.922
    Aug 17 03:27:58.923: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 17 03:27:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Creating a v1 custom resource 08/17/23 03:28:01.705
    STEP: v2 custom resource should be converted 08/17/23 03:28:01.712
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:28:02.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9220" for this suite. 08/17/23 03:28:02.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:28:02.356
Aug 17 03:28:02.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:28:02.357
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:02.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:02.381
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:28:02.396
Aug 17 03:28:02.410: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce" in namespace "projected-6953" to be "Succeeded or Failed"
Aug 17 03:28:02.417: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370925ms
Aug 17 03:28:04.423: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013600854s
Aug 17 03:28:06.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014415978s
Aug 17 03:28:08.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014139448s
STEP: Saw pod success 08/17/23 03:28:08.424
Aug 17 03:28:08.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce" satisfied condition "Succeeded or Failed"
Aug 17 03:28:08.428: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce container client-container: <nil>
STEP: delete the pod 08/17/23 03:28:08.477
Aug 17 03:28:08.487: INFO: Waiting for pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce to disappear
Aug 17 03:28:08.491: INFO: Pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:28:08.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6953" for this suite. 08/17/23 03:28:08.498
------------------------------
• [SLOW TEST] [6.148 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:28:02.356
    Aug 17 03:28:02.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:28:02.357
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:02.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:02.381
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:28:02.396
    Aug 17 03:28:02.410: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce" in namespace "projected-6953" to be "Succeeded or Failed"
    Aug 17 03:28:02.417: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370925ms
    Aug 17 03:28:04.423: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013600854s
    Aug 17 03:28:06.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014415978s
    Aug 17 03:28:08.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014139448s
    STEP: Saw pod success 08/17/23 03:28:08.424
    Aug 17 03:28:08.424: INFO: Pod "downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce" satisfied condition "Succeeded or Failed"
    Aug 17 03:28:08.428: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce container client-container: <nil>
    STEP: delete the pod 08/17/23 03:28:08.477
    Aug 17 03:28:08.487: INFO: Waiting for pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce to disappear
    Aug 17 03:28:08.491: INFO: Pod downwardapi-volume-83e7cc3b-c0d9-43c8-9e8f-3a5c8055c2ce no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:28:08.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6953" for this suite. 08/17/23 03:28:08.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:28:08.507
Aug 17 03:28:08.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:28:08.508
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:08.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:08.526
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-a116d3ad-a4c2-4efc-89ed-316bba3d9ab3 08/17/23 03:28:08.531
STEP: Creating a pod to test consume configMaps 08/17/23 03:28:08.537
Aug 17 03:28:08.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e" in namespace "configmap-9744" to be "Succeeded or Failed"
Aug 17 03:28:08.550: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578432ms
Aug 17 03:28:10.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009846652s
Aug 17 03:28:12.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009764243s
STEP: Saw pod success 08/17/23 03:28:12.556
Aug 17 03:28:12.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e" satisfied condition "Succeeded or Failed"
Aug 17 03:28:12.560: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:28:12.612
Aug 17 03:28:12.623: INFO: Waiting for pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e to disappear
Aug 17 03:28:12.627: INFO: Pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:28:12.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9744" for this suite. 08/17/23 03:28:12.634
------------------------------
• [4.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:28:08.507
    Aug 17 03:28:08.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:28:08.508
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:08.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:08.526
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-a116d3ad-a4c2-4efc-89ed-316bba3d9ab3 08/17/23 03:28:08.531
    STEP: Creating a pod to test consume configMaps 08/17/23 03:28:08.537
    Aug 17 03:28:08.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e" in namespace "configmap-9744" to be "Succeeded or Failed"
    Aug 17 03:28:08.550: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578432ms
    Aug 17 03:28:10.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009846652s
    Aug 17 03:28:12.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009764243s
    STEP: Saw pod success 08/17/23 03:28:12.556
    Aug 17 03:28:12.556: INFO: Pod "pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e" satisfied condition "Succeeded or Failed"
    Aug 17 03:28:12.560: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:28:12.612
    Aug 17 03:28:12.623: INFO: Waiting for pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e to disappear
    Aug 17 03:28:12.627: INFO: Pod pod-configmaps-e5b57ac7-0240-49dc-9ebc-69e83988395e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:28:12.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9744" for this suite. 08/17/23 03:28:12.634
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:28:12.643
Aug 17 03:28:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename containers 08/17/23 03:28:12.644
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:12.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:12.664
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/17/23 03:28:12.67
Aug 17 03:28:12.679: INFO: Waiting up to 5m0s for pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068" in namespace "containers-6470" to be "Succeeded or Failed"
Aug 17 03:28:12.684: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Pending", Reason="", readiness=false. Elapsed: 5.276137ms
Aug 17 03:28:14.690: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010820798s
Aug 17 03:28:16.688: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009454132s
STEP: Saw pod success 08/17/23 03:28:16.688
Aug 17 03:28:16.688: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068" satisfied condition "Succeeded or Failed"
Aug 17 03:28:16.692: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:28:16.701
Aug 17 03:28:16.711: INFO: Waiting for pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 to disappear
Aug 17 03:28:16.715: INFO: Pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:28:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6470" for this suite. 08/17/23 03:28:16.723
------------------------------
• [4.087 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:28:12.643
    Aug 17 03:28:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename containers 08/17/23 03:28:12.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:12.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:12.664
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/17/23 03:28:12.67
    Aug 17 03:28:12.679: INFO: Waiting up to 5m0s for pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068" in namespace "containers-6470" to be "Succeeded or Failed"
    Aug 17 03:28:12.684: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Pending", Reason="", readiness=false. Elapsed: 5.276137ms
    Aug 17 03:28:14.690: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010820798s
    Aug 17 03:28:16.688: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009454132s
    STEP: Saw pod success 08/17/23 03:28:16.688
    Aug 17 03:28:16.688: INFO: Pod "client-containers-330d4488-cade-4df6-82e0-efe2bf197068" satisfied condition "Succeeded or Failed"
    Aug 17 03:28:16.692: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:28:16.701
    Aug 17 03:28:16.711: INFO: Waiting for pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 to disappear
    Aug 17 03:28:16.715: INFO: Pod client-containers-330d4488-cade-4df6-82e0-efe2bf197068 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:28:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6470" for this suite. 08/17/23 03:28:16.723
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:28:16.731
Aug 17 03:28:16.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename events 08/17/23 03:28:16.731
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:16.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:16.752
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/17/23 03:28:16.758
STEP: get a list of Events with a label in the current namespace 08/17/23 03:28:16.776
STEP: delete a list of events 08/17/23 03:28:16.78
Aug 17 03:28:16.780: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/17/23 03:28:16.799
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 17 03:28:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4171" for this suite. 08/17/23 03:28:16.811
------------------------------
• [0.086 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:28:16.731
    Aug 17 03:28:16.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename events 08/17/23 03:28:16.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:16.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:16.752
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/17/23 03:28:16.758
    STEP: get a list of Events with a label in the current namespace 08/17/23 03:28:16.776
    STEP: delete a list of events 08/17/23 03:28:16.78
    Aug 17 03:28:16.780: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/17/23 03:28:16.799
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:28:16.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4171" for this suite. 08/17/23 03:28:16.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:28:16.819
Aug 17 03:28:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-pred 08/17/23 03:28:16.82
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:16.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:16.837
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 03:28:16.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 03:28:16.853: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 03:28:16.859: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
Aug 17 03:28:16.871: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 03:28:16.872: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 03:28:16.872: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container coredns ready: true, restart count 0
Aug 17 03:28:16.872: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container coredns ready: true, restart count 0
Aug 17 03:28:16.872: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 03:28:16.872: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container metrics-server ready: true, restart count 0
Aug 17 03:28:16.872: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Aug 17 03:28:16.872: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.872: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 03:28:16.872: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.873: INFO: 	Container vpn-target ready: true, restart count 0
Aug 17 03:28:16.873: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:28:16.873: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:28:16.873: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 03:28:16.873: INFO: 
Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
Aug 17 03:28:16.882: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 03:28:16.882: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 03:28:16.882: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 03:28:16.882: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 03:28:16.882: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container e2e ready: true, restart count 0
Aug 17 03:28:16.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:28:16.882: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:28:16.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:28:16.882: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:28:16.882
Aug 17 03:28:16.890: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4828" to be "running"
Aug 17 03:28:16.894: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812876ms
Aug 17 03:28:18.901: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011109854s
Aug 17 03:28:18.902: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:28:18.906
STEP: Trying to apply a random label on the found node. 08/17/23 03:28:18.926
STEP: verifying the node has the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 95 08/17/23 03:28:18.965
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/17/23 03:28:18.973
Aug 17 03:28:18.979: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4828" to be "not pending"
Aug 17 03:28:18.983: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677559ms
Aug 17 03:28:20.988: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009119766s
Aug 17 03:28:20.988: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.11.4 on the node which pod4 resides and expect not scheduled 08/17/23 03:28:20.988
Aug 17 03:28:20.994: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4828" to be "not pending"
Aug 17 03:28:20.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882535ms
Aug 17 03:28:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01006575s
Aug 17 03:28:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009743301s
Aug 17 03:28:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009022359s
Aug 17 03:28:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008551894s
Aug 17 03:28:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008774272s
Aug 17 03:28:33.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008455112s
Aug 17 03:28:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010145025s
Aug 17 03:28:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008796258s
Aug 17 03:28:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010106578s
Aug 17 03:28:41.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008698887s
Aug 17 03:28:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009279953s
Aug 17 03:28:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009444251s
Aug 17 03:28:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009649928s
Aug 17 03:28:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009837285s
Aug 17 03:28:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.00941735s
Aug 17 03:28:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009013175s
Aug 17 03:28:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009515828s
Aug 17 03:28:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.0090079s
Aug 17 03:28:59.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009704924s
Aug 17 03:29:01.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008965619s
Aug 17 03:29:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009544322s
Aug 17 03:29:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009000501s
Aug 17 03:29:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010083201s
Aug 17 03:29:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010514212s
Aug 17 03:29:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009756799s
Aug 17 03:29:13.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009448055s
Aug 17 03:29:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.0099189s
Aug 17 03:29:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008992769s
Aug 17 03:29:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010023659s
Aug 17 03:29:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.0097106s
Aug 17 03:29:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009825533s
Aug 17 03:29:25.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009362842s
Aug 17 03:29:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.0086412s
Aug 17 03:29:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009841799s
Aug 17 03:29:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008661569s
Aug 17 03:29:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00949208s
Aug 17 03:29:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009351747s
Aug 17 03:29:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009048826s
Aug 17 03:29:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009596158s
Aug 17 03:29:41.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009156316s
Aug 17 03:29:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009686085s
Aug 17 03:29:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0088125s
Aug 17 03:29:47.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008953833s
Aug 17 03:29:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009894415s
Aug 17 03:29:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00968136s
Aug 17 03:29:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009462037s
Aug 17 03:29:55.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008639305s
Aug 17 03:29:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009270959s
Aug 17 03:29:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009445962s
Aug 17 03:30:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009836883s
Aug 17 03:30:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009621882s
Aug 17 03:30:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010325283s
Aug 17 03:30:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009115616s
Aug 17 03:30:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009776129s
Aug 17 03:30:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010025436s
Aug 17 03:30:13.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009382051s
Aug 17 03:30:15.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009415016s
Aug 17 03:30:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009454713s
Aug 17 03:30:19.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011334869s
Aug 17 03:30:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008705793s
Aug 17 03:30:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009691938s
Aug 17 03:30:25.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008653376s
Aug 17 03:30:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009238057s
Aug 17 03:30:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009768493s
Aug 17 03:30:31.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011015166s
Aug 17 03:30:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009131219s
Aug 17 03:30:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009895442s
Aug 17 03:30:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009088093s
Aug 17 03:30:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010759709s
Aug 17 03:30:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.010233872s
Aug 17 03:30:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009147096s
Aug 17 03:30:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009376197s
Aug 17 03:30:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009923552s
Aug 17 03:30:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009720335s
Aug 17 03:30:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010296946s
Aug 17 03:30:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009169051s
Aug 17 03:30:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009850002s
Aug 17 03:30:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009394454s
Aug 17 03:30:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008770486s
Aug 17 03:31:01.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009195344s
Aug 17 03:31:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00970953s
Aug 17 03:31:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009956188s
Aug 17 03:31:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010343535s
Aug 17 03:31:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011195572s
Aug 17 03:31:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009778376s
Aug 17 03:31:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010183257s
Aug 17 03:31:15.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009246945s
Aug 17 03:31:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009258981s
Aug 17 03:31:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009411617s
Aug 17 03:31:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009440985s
Aug 17 03:31:23.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009167918s
Aug 17 03:31:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009701364s
Aug 17 03:31:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009066468s
Aug 17 03:31:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009164251s
Aug 17 03:31:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008897989s
Aug 17 03:31:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009326969s
Aug 17 03:31:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009031194s
Aug 17 03:31:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010581439s
Aug 17 03:31:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010238856s
Aug 17 03:31:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009516393s
Aug 17 03:31:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009812295s
Aug 17 03:31:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009828581s
Aug 17 03:31:47.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008971575s
Aug 17 03:31:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009581403s
Aug 17 03:31:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009689439s
Aug 17 03:31:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009934214s
Aug 17 03:31:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010477846s
Aug 17 03:31:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009725204s
Aug 17 03:31:59.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01161336s
Aug 17 03:32:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01037334s
Aug 17 03:32:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010003315s
Aug 17 03:32:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00962201s
Aug 17 03:32:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010400423s
Aug 17 03:32:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010433748s
Aug 17 03:32:11.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010612388s
Aug 17 03:32:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009726373s
Aug 17 03:32:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010345549s
Aug 17 03:32:17.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010274156s
Aug 17 03:32:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01003692s
Aug 17 03:32:21.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012723798s
Aug 17 03:32:23.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.0090352s
Aug 17 03:32:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009888362s
Aug 17 03:32:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009992647s
Aug 17 03:32:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009135866s
Aug 17 03:32:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009446017s
Aug 17 03:32:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.010304673s
Aug 17 03:32:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009507781s
Aug 17 03:32:37.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009790605s
Aug 17 03:32:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009765846s
Aug 17 03:32:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010380896s
Aug 17 03:32:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009215358s
Aug 17 03:32:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009936304s
Aug 17 03:32:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00971437s
Aug 17 03:32:49.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014114987s
Aug 17 03:32:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.010334679s
Aug 17 03:32:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008623775s
Aug 17 03:32:55.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009008728s
Aug 17 03:32:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009523583s
Aug 17 03:32:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008803995s
Aug 17 03:33:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010388325s
Aug 17 03:33:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.010100751s
Aug 17 03:33:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009186584s
Aug 17 03:33:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.00946813s
Aug 17 03:33:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009515981s
Aug 17 03:33:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009552891s
Aug 17 03:33:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009942916s
Aug 17 03:33:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009775721s
Aug 17 03:33:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009345371s
Aug 17 03:33:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009559738s
Aug 17 03:33:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010151063s
Aug 17 03:33:21.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01391355s
STEP: removing the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 03:33:21.008
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 08/17/23 03:33:21.024
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:33:21.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4828" for this suite. 08/17/23 03:33:21.035
------------------------------
• [SLOW TEST] [304.224 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:28:16.819
    Aug 17 03:28:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-pred 08/17/23 03:28:16.82
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:28:16.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:28:16.837
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 03:28:16.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 03:28:16.853: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 03:28:16.859: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
    Aug 17 03:28:16.871: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Aug 17 03:28:16.872: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.872: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 03:28:16.872: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.873: INFO: 	Container vpn-target ready: true, restart count 0
    Aug 17 03:28:16.873: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:28:16.873: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:28:16.873: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 03:28:16.873: INFO: 
    Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
    Aug 17 03:28:16.882: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:28:16.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:28:16.882: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:28:16.882
    Aug 17 03:28:16.890: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4828" to be "running"
    Aug 17 03:28:16.894: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812876ms
    Aug 17 03:28:18.901: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011109854s
    Aug 17 03:28:18.902: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:28:18.906
    STEP: Trying to apply a random label on the found node. 08/17/23 03:28:18.926
    STEP: verifying the node has the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 95 08/17/23 03:28:18.965
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/17/23 03:28:18.973
    Aug 17 03:28:18.979: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4828" to be "not pending"
    Aug 17 03:28:18.983: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677559ms
    Aug 17 03:28:20.988: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009119766s
    Aug 17 03:28:20.988: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.11.4 on the node which pod4 resides and expect not scheduled 08/17/23 03:28:20.988
    Aug 17 03:28:20.994: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4828" to be "not pending"
    Aug 17 03:28:20.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882535ms
    Aug 17 03:28:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01006575s
    Aug 17 03:28:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009743301s
    Aug 17 03:28:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009022359s
    Aug 17 03:28:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008551894s
    Aug 17 03:28:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008774272s
    Aug 17 03:28:33.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008455112s
    Aug 17 03:28:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010145025s
    Aug 17 03:28:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008796258s
    Aug 17 03:28:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010106578s
    Aug 17 03:28:41.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008698887s
    Aug 17 03:28:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009279953s
    Aug 17 03:28:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009444251s
    Aug 17 03:28:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009649928s
    Aug 17 03:28:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009837285s
    Aug 17 03:28:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.00941735s
    Aug 17 03:28:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009013175s
    Aug 17 03:28:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009515828s
    Aug 17 03:28:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.0090079s
    Aug 17 03:28:59.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009704924s
    Aug 17 03:29:01.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008965619s
    Aug 17 03:29:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009544322s
    Aug 17 03:29:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009000501s
    Aug 17 03:29:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010083201s
    Aug 17 03:29:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010514212s
    Aug 17 03:29:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009756799s
    Aug 17 03:29:13.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009448055s
    Aug 17 03:29:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.0099189s
    Aug 17 03:29:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008992769s
    Aug 17 03:29:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010023659s
    Aug 17 03:29:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.0097106s
    Aug 17 03:29:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009825533s
    Aug 17 03:29:25.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009362842s
    Aug 17 03:29:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.0086412s
    Aug 17 03:29:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009841799s
    Aug 17 03:29:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008661569s
    Aug 17 03:29:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00949208s
    Aug 17 03:29:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009351747s
    Aug 17 03:29:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009048826s
    Aug 17 03:29:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009596158s
    Aug 17 03:29:41.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009156316s
    Aug 17 03:29:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009686085s
    Aug 17 03:29:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0088125s
    Aug 17 03:29:47.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008953833s
    Aug 17 03:29:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009894415s
    Aug 17 03:29:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00968136s
    Aug 17 03:29:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009462037s
    Aug 17 03:29:55.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008639305s
    Aug 17 03:29:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009270959s
    Aug 17 03:29:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009445962s
    Aug 17 03:30:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009836883s
    Aug 17 03:30:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009621882s
    Aug 17 03:30:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010325283s
    Aug 17 03:30:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009115616s
    Aug 17 03:30:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009776129s
    Aug 17 03:30:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010025436s
    Aug 17 03:30:13.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009382051s
    Aug 17 03:30:15.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009415016s
    Aug 17 03:30:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009454713s
    Aug 17 03:30:19.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011334869s
    Aug 17 03:30:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008705793s
    Aug 17 03:30:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009691938s
    Aug 17 03:30:25.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008653376s
    Aug 17 03:30:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009238057s
    Aug 17 03:30:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009768493s
    Aug 17 03:30:31.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011015166s
    Aug 17 03:30:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009131219s
    Aug 17 03:30:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009895442s
    Aug 17 03:30:37.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009088093s
    Aug 17 03:30:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010759709s
    Aug 17 03:30:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.010233872s
    Aug 17 03:30:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009147096s
    Aug 17 03:30:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009376197s
    Aug 17 03:30:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009923552s
    Aug 17 03:30:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009720335s
    Aug 17 03:30:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010296946s
    Aug 17 03:30:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009169051s
    Aug 17 03:30:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009850002s
    Aug 17 03:30:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009394454s
    Aug 17 03:30:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008770486s
    Aug 17 03:31:01.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009195344s
    Aug 17 03:31:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00970953s
    Aug 17 03:31:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009956188s
    Aug 17 03:31:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010343535s
    Aug 17 03:31:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011195572s
    Aug 17 03:31:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009778376s
    Aug 17 03:31:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010183257s
    Aug 17 03:31:15.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009246945s
    Aug 17 03:31:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009258981s
    Aug 17 03:31:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009411617s
    Aug 17 03:31:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009440985s
    Aug 17 03:31:23.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009167918s
    Aug 17 03:31:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009701364s
    Aug 17 03:31:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009066468s
    Aug 17 03:31:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009164251s
    Aug 17 03:31:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008897989s
    Aug 17 03:31:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009326969s
    Aug 17 03:31:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009031194s
    Aug 17 03:31:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010581439s
    Aug 17 03:31:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010238856s
    Aug 17 03:31:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009516393s
    Aug 17 03:31:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009812295s
    Aug 17 03:31:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009828581s
    Aug 17 03:31:47.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008971575s
    Aug 17 03:31:49.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009581403s
    Aug 17 03:31:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009689439s
    Aug 17 03:31:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009934214s
    Aug 17 03:31:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010477846s
    Aug 17 03:31:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009725204s
    Aug 17 03:31:59.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01161336s
    Aug 17 03:32:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01037334s
    Aug 17 03:32:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010003315s
    Aug 17 03:32:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00962201s
    Aug 17 03:32:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010400423s
    Aug 17 03:32:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010433748s
    Aug 17 03:32:11.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010612388s
    Aug 17 03:32:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009726373s
    Aug 17 03:32:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010345549s
    Aug 17 03:32:17.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010274156s
    Aug 17 03:32:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01003692s
    Aug 17 03:32:21.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012723798s
    Aug 17 03:32:23.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.0090352s
    Aug 17 03:32:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009888362s
    Aug 17 03:32:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009992647s
    Aug 17 03:32:29.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009135866s
    Aug 17 03:32:31.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009446017s
    Aug 17 03:32:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.010304673s
    Aug 17 03:32:35.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009507781s
    Aug 17 03:32:37.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009790605s
    Aug 17 03:32:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009765846s
    Aug 17 03:32:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010380896s
    Aug 17 03:32:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009215358s
    Aug 17 03:32:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009936304s
    Aug 17 03:32:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00971437s
    Aug 17 03:32:49.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014114987s
    Aug 17 03:32:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.010334679s
    Aug 17 03:32:53.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008623775s
    Aug 17 03:32:55.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009008728s
    Aug 17 03:32:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009523583s
    Aug 17 03:32:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.008803995s
    Aug 17 03:33:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010388325s
    Aug 17 03:33:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.010100751s
    Aug 17 03:33:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009186584s
    Aug 17 03:33:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.00946813s
    Aug 17 03:33:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009515981s
    Aug 17 03:33:11.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009552891s
    Aug 17 03:33:13.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009942916s
    Aug 17 03:33:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009775721s
    Aug 17 03:33:17.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009345371s
    Aug 17 03:33:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009559738s
    Aug 17 03:33:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010151063s
    Aug 17 03:33:21.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01391355s
    STEP: removing the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 03:33:21.008
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-8b6bac8c-3815-4394-a976-a0e99b260573 08/17/23 03:33:21.024
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:33:21.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4828" for this suite. 08/17/23 03:33:21.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:33:21.049
Aug 17 03:33:21.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context 08/17/23 03:33:21.05
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:21.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:21.069
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 03:33:21.073
Aug 17 03:33:21.081: INFO: Waiting up to 5m0s for pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe" in namespace "security-context-9470" to be "Succeeded or Failed"
Aug 17 03:33:21.085: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677198ms
Aug 17 03:33:23.090: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.008727196s
Aug 17 03:33:25.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Running", Reason="", readiness=false. Elapsed: 4.009735537s
Aug 17 03:33:27.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009527472s
STEP: Saw pod success 08/17/23 03:33:27.091
Aug 17 03:33:27.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe" satisfied condition "Succeeded or Failed"
Aug 17 03:33:27.095: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe container test-container: <nil>
STEP: delete the pod 08/17/23 03:33:27.108
Aug 17 03:33:27.121: INFO: Waiting for pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe to disappear
Aug 17 03:33:27.125: INFO: Pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 03:33:27.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9470" for this suite. 08/17/23 03:33:27.131
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:33:21.049
    Aug 17 03:33:21.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context 08/17/23 03:33:21.05
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:21.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:21.069
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/17/23 03:33:21.073
    Aug 17 03:33:21.081: INFO: Waiting up to 5m0s for pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe" in namespace "security-context-9470" to be "Succeeded or Failed"
    Aug 17 03:33:21.085: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.677198ms
    Aug 17 03:33:23.090: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.008727196s
    Aug 17 03:33:25.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Running", Reason="", readiness=false. Elapsed: 4.009735537s
    Aug 17 03:33:27.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009527472s
    STEP: Saw pod success 08/17/23 03:33:27.091
    Aug 17 03:33:27.091: INFO: Pod "security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe" satisfied condition "Succeeded or Failed"
    Aug 17 03:33:27.095: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe container test-container: <nil>
    STEP: delete the pod 08/17/23 03:33:27.108
    Aug 17 03:33:27.121: INFO: Waiting for pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe to disappear
    Aug 17 03:33:27.125: INFO: Pod security-context-c48f5479-15c7-4d09-b43a-8f6acd0e5cfe no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:33:27.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9470" for this suite. 08/17/23 03:33:27.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:33:27.139
Aug 17 03:33:27.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename init-container 08/17/23 03:33:27.14
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:27.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:27.157
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/17/23 03:33:27.162
Aug 17 03:33:27.163: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:33:31.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9727" for this suite. 08/17/23 03:33:31.235
------------------------------
• [4.103 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:33:27.139
    Aug 17 03:33:27.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename init-container 08/17/23 03:33:27.14
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:27.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:27.157
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/17/23 03:33:27.162
    Aug 17 03:33:27.163: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:33:31.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9727" for this suite. 08/17/23 03:33:31.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:33:31.244
Aug 17 03:33:31.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:33:31.245
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:31.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:31.265
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/17/23 03:33:31.27
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/17/23 03:33:31.27
STEP: creating a pod to probe DNS 08/17/23 03:33:31.27
STEP: submitting the pod to kubernetes 08/17/23 03:33:31.27
Aug 17 03:33:31.281: INFO: Waiting up to 15m0s for pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a" in namespace "dns-2920" to be "running"
Aug 17 03:33:31.285: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239648ms
Aug 17 03:33:33.291: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009671603s
Aug 17 03:33:33.291: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:33:33.291
STEP: looking for the results for each expected name from probers 08/17/23 03:33:33.296
Aug 17 03:33:33.444: INFO: DNS probes using dns-2920/dns-test-635959c7-d405-4a1f-b74a-34494623283a succeeded

STEP: deleting the pod 08/17/23 03:33:33.444
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:33:33.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2920" for this suite. 08/17/23 03:33:33.477
------------------------------
• [2.241 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:33:31.244
    Aug 17 03:33:31.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:33:31.245
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:31.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:31.265
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/17/23 03:33:31.27
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/17/23 03:33:31.27
    STEP: creating a pod to probe DNS 08/17/23 03:33:31.27
    STEP: submitting the pod to kubernetes 08/17/23 03:33:31.27
    Aug 17 03:33:31.281: INFO: Waiting up to 15m0s for pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a" in namespace "dns-2920" to be "running"
    Aug 17 03:33:31.285: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239648ms
    Aug 17 03:33:33.291: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009671603s
    Aug 17 03:33:33.291: INFO: Pod "dns-test-635959c7-d405-4a1f-b74a-34494623283a" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:33:33.291
    STEP: looking for the results for each expected name from probers 08/17/23 03:33:33.296
    Aug 17 03:33:33.444: INFO: DNS probes using dns-2920/dns-test-635959c7-d405-4a1f-b74a-34494623283a succeeded

    STEP: deleting the pod 08/17/23 03:33:33.444
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:33:33.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2920" for this suite. 08/17/23 03:33:33.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:33:33.492
Aug 17 03:33:33.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:33:33.493
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:33.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:33.511
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/17/23 03:33:33.516
STEP: Getting a ResourceQuota 08/17/23 03:33:33.522
STEP: Listing all ResourceQuotas with LabelSelector 08/17/23 03:33:33.525
STEP: Patching the ResourceQuota 08/17/23 03:33:33.529
STEP: Deleting a Collection of ResourceQuotas 08/17/23 03:33:33.535
STEP: Verifying the deleted ResourceQuota 08/17/23 03:33:33.544
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:33:33.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4509" for this suite. 08/17/23 03:33:33.555
------------------------------
• [0.070 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:33:33.492
    Aug 17 03:33:33.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:33:33.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:33.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:33.511
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/17/23 03:33:33.516
    STEP: Getting a ResourceQuota 08/17/23 03:33:33.522
    STEP: Listing all ResourceQuotas with LabelSelector 08/17/23 03:33:33.525
    STEP: Patching the ResourceQuota 08/17/23 03:33:33.529
    STEP: Deleting a Collection of ResourceQuotas 08/17/23 03:33:33.535
    STEP: Verifying the deleted ResourceQuota 08/17/23 03:33:33.544
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:33:33.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4509" for this suite. 08/17/23 03:33:33.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:33:33.565
Aug 17 03:33:33.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename init-container 08/17/23 03:33:33.565
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:33.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:33.584
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/17/23 03:33:33.589
Aug 17 03:33:33.590: INFO: PodSpec: initContainers in spec.initContainers
Aug 17 03:34:17.337: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed5cebf5-1195-49dd-95c7-c956246b218a", GenerateName:"", Namespace:"init-container-5872", SelfLink:"", UID:"962f86aa-a369-4a43-9d83-e69db60e92b8", ResourceVersion:"223813", Generation:0, CreationTimestamp:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"589987209"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"ec1a6a44e9173b48f504a6f8529ab1a2923554f53fff67ded29148b8ee006ad1", "cni.projectcalico.org/podIP":"172.21.15.85/32", "cni.projectcalico.org/podIPs":"172.21.15.85/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120eeb8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 33, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120ef90), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 34, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120eff0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jrwrc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ca5440), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0048678e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ske-ubuntu-79fff84d86x69988-vjwlx", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0000ee770), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004867960)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004867980)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004867988), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00486798c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000a1c5a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.11.4", PodIP:"172.21.15.85", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.15.85"}}, StartTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000ee850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000ee8c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://957b062649d19fb3d0677daa692413292c8b7e8c5bb58c90179868a43d09b7a5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ca54c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ca54a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004867a0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:34:17.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5872" for this suite. 08/17/23 03:34:17.345
------------------------------
• [SLOW TEST] [43.789 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:33:33.565
    Aug 17 03:33:33.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename init-container 08/17/23 03:33:33.565
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:33:33.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:33:33.584
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/17/23 03:33:33.589
    Aug 17 03:33:33.590: INFO: PodSpec: initContainers in spec.initContainers
    Aug 17 03:34:17.337: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed5cebf5-1195-49dd-95c7-c956246b218a", GenerateName:"", Namespace:"init-container-5872", SelfLink:"", UID:"962f86aa-a369-4a43-9d83-e69db60e92b8", ResourceVersion:"223813", Generation:0, CreationTimestamp:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"589987209"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"ec1a6a44e9173b48f504a6f8529ab1a2923554f53fff67ded29148b8ee006ad1", "cni.projectcalico.org/podIP":"172.21.15.85/32", "cni.projectcalico.org/podIPs":"172.21.15.85/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120eeb8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 33, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120ef90), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 17, 3, 34, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00120eff0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jrwrc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ca5440), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jrwrc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0048678e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ske-ubuntu-79fff84d86x69988-vjwlx", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0000ee770), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004867960)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004867980)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004867988), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00486798c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000a1c5a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.11.4", PodIP:"172.21.15.85", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.15.85"}}, StartTime:time.Date(2023, time.August, 17, 3, 33, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000ee850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000ee8c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://957b062649d19fb3d0677daa692413292c8b7e8c5bb58c90179868a43d09b7a5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ca54c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ca54a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004867a0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:34:17.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5872" for this suite. 08/17/23 03:34:17.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:34:17.355
Aug 17 03:34:17.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:34:17.356
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:34:17.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:34:17.376
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3864 08/17/23 03:34:17.381
STEP: creating a selector 08/17/23 03:34:17.381
STEP: Creating the service pods in kubernetes 08/17/23 03:34:17.381
Aug 17 03:34:17.381: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 17 03:34:17.410: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3864" to be "running and ready"
Aug 17 03:34:17.415: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959306ms
Aug 17 03:34:17.415: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:34:19.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011028555s
Aug 17 03:34:19.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:21.423: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012903002s
Aug 17 03:34:21.423: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:23.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011351218s
Aug 17 03:34:23.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:25.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011494066s
Aug 17 03:34:25.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:27.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010590971s
Aug 17 03:34:27.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:29.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010371903s
Aug 17 03:34:29.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:31.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010983039s
Aug 17 03:34:31.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:33.423: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012985286s
Aug 17 03:34:33.423: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:35.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012154439s
Aug 17 03:34:35.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:37.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009879754s
Aug 17 03:34:37.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 17 03:34:39.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010132718s
Aug 17 03:34:39.420: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 17 03:34:39.420: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 17 03:34:39.424: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3864" to be "running and ready"
Aug 17 03:34:39.429: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.662514ms
Aug 17 03:34:39.429: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 17 03:34:39.429: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/17/23 03:34:39.433
Aug 17 03:34:39.447: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3864" to be "running"
Aug 17 03:34:39.452: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810645ms
Aug 17 03:34:41.458: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011126487s
Aug 17 03:34:41.458: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 17 03:34:41.462: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3864" to be "running"
Aug 17 03:34:41.466: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.952832ms
Aug 17 03:34:41.466: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 17 03:34:41.471: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 17 03:34:41.471: INFO: Going to poll 172.21.86.190 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 17 03:34:41.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.86.190:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3864 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:34:41.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:34:41.475: INFO: ExecWithOptions: Clientset creation
Aug 17 03:34:41.475: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3864/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.86.190%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 03:34:41.924: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 17 03:34:41.924: INFO: Going to poll 172.21.15.81 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 17 03:34:41.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.15.81:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3864 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:34:41.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:34:41.929: INFO: ExecWithOptions: Clientset creation
Aug 17 03:34:41.929: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3864/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.15.81%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 03:34:42.396: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 17 03:34:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3864" for this suite. 08/17/23 03:34:42.404
------------------------------
• [SLOW TEST] [25.057 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:34:17.355
    Aug 17 03:34:17.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pod-network-test 08/17/23 03:34:17.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:34:17.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:34:17.376
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3864 08/17/23 03:34:17.381
    STEP: creating a selector 08/17/23 03:34:17.381
    STEP: Creating the service pods in kubernetes 08/17/23 03:34:17.381
    Aug 17 03:34:17.381: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 17 03:34:17.410: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3864" to be "running and ready"
    Aug 17 03:34:17.415: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959306ms
    Aug 17 03:34:17.415: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:34:19.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011028555s
    Aug 17 03:34:19.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:21.423: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012903002s
    Aug 17 03:34:21.423: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:23.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011351218s
    Aug 17 03:34:23.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:25.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011494066s
    Aug 17 03:34:25.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:27.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010590971s
    Aug 17 03:34:27.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:29.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010371903s
    Aug 17 03:34:29.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:31.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010983039s
    Aug 17 03:34:31.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:33.423: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012985286s
    Aug 17 03:34:33.423: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:35.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012154439s
    Aug 17 03:34:35.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:37.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009879754s
    Aug 17 03:34:37.420: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 17 03:34:39.420: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010132718s
    Aug 17 03:34:39.420: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 17 03:34:39.420: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 17 03:34:39.424: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3864" to be "running and ready"
    Aug 17 03:34:39.429: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.662514ms
    Aug 17 03:34:39.429: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 17 03:34:39.429: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/17/23 03:34:39.433
    Aug 17 03:34:39.447: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3864" to be "running"
    Aug 17 03:34:39.452: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810645ms
    Aug 17 03:34:41.458: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011126487s
    Aug 17 03:34:41.458: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 17 03:34:41.462: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3864" to be "running"
    Aug 17 03:34:41.466: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.952832ms
    Aug 17 03:34:41.466: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 17 03:34:41.471: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 17 03:34:41.471: INFO: Going to poll 172.21.86.190 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 17 03:34:41.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.86.190:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3864 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:34:41.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:34:41.475: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:34:41.475: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3864/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.86.190%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 03:34:41.924: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 17 03:34:41.924: INFO: Going to poll 172.21.15.81 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 17 03:34:41.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.15.81:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3864 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:34:41.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:34:41.929: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:34:41.929: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3864/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.15.81%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 03:34:42.396: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:34:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3864" for this suite. 08/17/23 03:34:42.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:34:42.415
Aug 17 03:34:42.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:34:42.415
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:34:42.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:34:42.434
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 03:34:42.455: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:35:42.526: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:35:42.531
Aug 17 03:35:42.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 03:35:42.532
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:35:42.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:35:42.551
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/17/23 03:35:42.556
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:35:42.556
Aug 17 03:35:42.568: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9870" to be "running"
Aug 17 03:35:42.572: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579344ms
Aug 17 03:35:44.578: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009301915s
Aug 17 03:35:44.578: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:35:44.582
Aug 17 03:35:44.594: INFO: found a healthy node: ske-ubuntu-79fff84d86x69988-vjwlx
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 17 03:35:50.679: INFO: pods created so far: [1 1 1]
Aug 17 03:35:50.679: INFO: length of pods created so far: 3
Aug 17 03:35:52.691: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 17 03:35:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:35:59.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9870" for this suite. 08/17/23 03:35:59.784
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7960" for this suite. 08/17/23 03:35:59.795
------------------------------
• [SLOW TEST] [77.389 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:34:42.415
    Aug 17 03:34:42.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:34:42.415
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:34:42.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:34:42.434
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 03:34:42.455: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:35:42.526: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:35:42.531
    Aug 17 03:35:42.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption-path 08/17/23 03:35:42.532
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:35:42.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:35:42.551
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/17/23 03:35:42.556
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:35:42.556
    Aug 17 03:35:42.568: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9870" to be "running"
    Aug 17 03:35:42.572: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579344ms
    Aug 17 03:35:44.578: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009301915s
    Aug 17 03:35:44.578: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:35:44.582
    Aug 17 03:35:44.594: INFO: found a healthy node: ske-ubuntu-79fff84d86x69988-vjwlx
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 17 03:35:50.679: INFO: pods created so far: [1 1 1]
    Aug 17 03:35:50.679: INFO: length of pods created so far: 3
    Aug 17 03:35:52.691: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:35:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:35:59.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9870" for this suite. 08/17/23 03:35:59.784
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7960" for this suite. 08/17/23 03:35:59.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:35:59.804
Aug 17 03:35:59.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:35:59.805
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:35:59.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:35:59.824
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-52df12fc-6bf5-49eb-8ac1-51f5baddd698 08/17/23 03:35:59.834
STEP: Creating configMap with name cm-test-opt-upd-23236508-ba40-45ac-a476-276ca9292de3 08/17/23 03:35:59.841
STEP: Creating the pod 08/17/23 03:35:59.846
Aug 17 03:35:59.859: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890" in namespace "projected-8654" to be "running and ready"
Aug 17 03:35:59.863: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576404ms
Aug 17 03:35:59.863: INFO: The phase of Pod pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:36:01.870: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890": Phase="Running", Reason="", readiness=true. Elapsed: 2.011281525s
Aug 17 03:36:01.870: INFO: The phase of Pod pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890 is Running (Ready = true)
Aug 17 03:36:01.870: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-52df12fc-6bf5-49eb-8ac1-51f5baddd698 08/17/23 03:36:02.015
STEP: Updating configmap cm-test-opt-upd-23236508-ba40-45ac-a476-276ca9292de3 08/17/23 03:36:02.024
STEP: Creating configMap with name cm-test-opt-create-bb68df7a-79f9-4d2d-a58b-bee2068cd1f9 08/17/23 03:36:02.031
STEP: waiting to observe update in volume 08/17/23 03:36:02.038
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:04.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8654" for this suite. 08/17/23 03:36:04.281
------------------------------
• [4.485 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:35:59.804
    Aug 17 03:35:59.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:35:59.805
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:35:59.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:35:59.824
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-52df12fc-6bf5-49eb-8ac1-51f5baddd698 08/17/23 03:35:59.834
    STEP: Creating configMap with name cm-test-opt-upd-23236508-ba40-45ac-a476-276ca9292de3 08/17/23 03:35:59.841
    STEP: Creating the pod 08/17/23 03:35:59.846
    Aug 17 03:35:59.859: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890" in namespace "projected-8654" to be "running and ready"
    Aug 17 03:35:59.863: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576404ms
    Aug 17 03:35:59.863: INFO: The phase of Pod pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:36:01.870: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890": Phase="Running", Reason="", readiness=true. Elapsed: 2.011281525s
    Aug 17 03:36:01.870: INFO: The phase of Pod pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890 is Running (Ready = true)
    Aug 17 03:36:01.870: INFO: Pod "pod-projected-configmaps-1f3c72f5-5add-4a4e-8fba-2714339d4890" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-52df12fc-6bf5-49eb-8ac1-51f5baddd698 08/17/23 03:36:02.015
    STEP: Updating configmap cm-test-opt-upd-23236508-ba40-45ac-a476-276ca9292de3 08/17/23 03:36:02.024
    STEP: Creating configMap with name cm-test-opt-create-bb68df7a-79f9-4d2d-a58b-bee2068cd1f9 08/17/23 03:36:02.031
    STEP: waiting to observe update in volume 08/17/23 03:36:02.038
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:04.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8654" for this suite. 08/17/23 03:36:04.281
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:04.29
Aug 17 03:36:04.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:36:04.291
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:04.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:04.312
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:36:04.342
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:36:04.645
STEP: Deploying the webhook pod 08/17/23 03:36:04.655
STEP: Wait for the deployment to be ready 08/17/23 03:36:04.671
Aug 17 03:36:04.680: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 03:36:06.695
STEP: Verifying the service has paired with the endpoint 08/17/23 03:36:06.707
Aug 17 03:36:07.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/17/23 03:36:07.712
STEP: Creating a custom resource definition that should be denied by the webhook 08/17/23 03:36:07.817
Aug 17 03:36:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:07.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4990" for this suite. 08/17/23 03:36:07.968
STEP: Destroying namespace "webhook-4990-markers" for this suite. 08/17/23 03:36:07.979
------------------------------
• [3.696 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:04.29
    Aug 17 03:36:04.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:36:04.291
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:04.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:04.312
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:36:04.342
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:36:04.645
    STEP: Deploying the webhook pod 08/17/23 03:36:04.655
    STEP: Wait for the deployment to be ready 08/17/23 03:36:04.671
    Aug 17 03:36:04.680: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 03:36:06.695
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:36:06.707
    Aug 17 03:36:07.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/17/23 03:36:07.712
    STEP: Creating a custom resource definition that should be denied by the webhook 08/17/23 03:36:07.817
    Aug 17 03:36:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:07.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4990" for this suite. 08/17/23 03:36:07.968
    STEP: Destroying namespace "webhook-4990-markers" for this suite. 08/17/23 03:36:07.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:07.989
Aug 17 03:36:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:36:07.989
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:08.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:08.011
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 17 03:36:08.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 03:36:09.62
Aug 17 03:36:09.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 create -f -'
Aug 17 03:36:10.313: INFO: stderr: ""
Aug 17 03:36:10.313: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 03:36:10.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 delete e2e-test-crd-publish-openapi-5141-crds test-cr'
Aug 17 03:36:10.388: INFO: stderr: ""
Aug 17 03:36:10.388: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 17 03:36:10.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 apply -f -'
Aug 17 03:36:10.640: INFO: stderr: ""
Aug 17 03:36:10.640: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 17 03:36:10.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 delete e2e-test-crd-publish-openapi-5141-crds test-cr'
Aug 17 03:36:10.742: INFO: stderr: ""
Aug 17 03:36:10.742: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/17/23 03:36:10.742
Aug 17 03:36:10.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 explain e2e-test-crd-publish-openapi-5141-crds'
Aug 17 03:36:10.983: INFO: stderr: ""
Aug 17 03:36:10.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5141-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:12.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8623" for this suite. 08/17/23 03:36:12.543
------------------------------
• [4.566 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:07.989
    Aug 17 03:36:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:36:07.989
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:08.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:08.011
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 17 03:36:08.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 03:36:09.62
    Aug 17 03:36:09.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 create -f -'
    Aug 17 03:36:10.313: INFO: stderr: ""
    Aug 17 03:36:10.313: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 17 03:36:10.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 delete e2e-test-crd-publish-openapi-5141-crds test-cr'
    Aug 17 03:36:10.388: INFO: stderr: ""
    Aug 17 03:36:10.388: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 17 03:36:10.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 apply -f -'
    Aug 17 03:36:10.640: INFO: stderr: ""
    Aug 17 03:36:10.640: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 17 03:36:10.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 --namespace=crd-publish-openapi-8623 delete e2e-test-crd-publish-openapi-5141-crds test-cr'
    Aug 17 03:36:10.742: INFO: stderr: ""
    Aug 17 03:36:10.742: INFO: stdout: "e2e-test-crd-publish-openapi-5141-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/17/23 03:36:10.742
    Aug 17 03:36:10.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-8623 explain e2e-test-crd-publish-openapi-5141-crds'
    Aug 17 03:36:10.983: INFO: stderr: ""
    Aug 17 03:36:10.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5141-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:12.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8623" for this suite. 08/17/23 03:36:12.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:12.555
Aug 17 03:36:12.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 03:36:12.556
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:12.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:12.578
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/17/23 03:36:12.584
STEP: Ensuring job reaches completions 08/17/23 03:36:12.591
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:26.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4231" for this suite. 08/17/23 03:36:26.604
------------------------------
• [SLOW TEST] [14.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:12.555
    Aug 17 03:36:12.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 03:36:12.556
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:12.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:12.578
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/17/23 03:36:12.584
    STEP: Ensuring job reaches completions 08/17/23 03:36:12.591
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:26.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4231" for this suite. 08/17/23 03:36:26.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:26.612
Aug 17 03:36:26.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename ephemeral-containers-test 08/17/23 03:36:26.613
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:26.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:26.633
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/17/23 03:36:26.638
Aug 17 03:36:26.647: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4333" to be "running and ready"
Aug 17 03:36:26.651: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.316017ms
Aug 17 03:36:26.651: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:36:28.657: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010013975s
Aug 17 03:36:28.657: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 17 03:36:28.657: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/17/23 03:36:28.661
Aug 17 03:36:28.671: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4333" to be "container debugger running"
Aug 17 03:36:28.675: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.236997ms
Aug 17 03:36:30.681: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010510065s
Aug 17 03:36:32.680: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00962362s
Aug 17 03:36:32.680: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/17/23 03:36:32.68
Aug 17 03:36:32.680: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4333 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:36:32.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:36:32.681: INFO: ExecWithOptions: Clientset creation
Aug 17 03:36:32.681: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-4333/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 17 03:36:32.983: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4333" for this suite. 08/17/23 03:36:33.069
------------------------------
• [SLOW TEST] [6.464 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:26.612
    Aug 17 03:36:26.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/17/23 03:36:26.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:26.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:26.633
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/17/23 03:36:26.638
    Aug 17 03:36:26.647: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4333" to be "running and ready"
    Aug 17 03:36:26.651: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.316017ms
    Aug 17 03:36:26.651: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:36:28.657: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010013975s
    Aug 17 03:36:28.657: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 17 03:36:28.657: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/17/23 03:36:28.661
    Aug 17 03:36:28.671: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4333" to be "container debugger running"
    Aug 17 03:36:28.675: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.236997ms
    Aug 17 03:36:30.681: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010510065s
    Aug 17 03:36:32.680: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00962362s
    Aug 17 03:36:32.680: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/17/23 03:36:32.68
    Aug 17 03:36:32.680: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4333 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:36:32.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:36:32.681: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:36:32.681: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-4333/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 17 03:36:32.983: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4333" for this suite. 08/17/23 03:36:33.069
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:33.078
Aug 17 03:36:33.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:36:33.079
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:33.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:33.1
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:36:33.121
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:36:33.7
STEP: Deploying the webhook pod 08/17/23 03:36:33.707
STEP: Wait for the deployment to be ready 08/17/23 03:36:33.719
Aug 17 03:36:33.727: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 03:36:35.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/17/23 03:36:37.753
STEP: Verifying the service has paired with the endpoint 08/17/23 03:36:37.765
Aug 17 03:36:38.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/17/23 03:36:38.772
STEP: create a configmap that should be updated by the webhook 08/17/23 03:36:38.877
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:36:39.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3437" for this suite. 08/17/23 03:36:39.071
STEP: Destroying namespace "webhook-3437-markers" for this suite. 08/17/23 03:36:39.08
------------------------------
• [SLOW TEST] [6.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:33.078
    Aug 17 03:36:33.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:36:33.079
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:33.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:33.1
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:36:33.121
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:36:33.7
    STEP: Deploying the webhook pod 08/17/23 03:36:33.707
    STEP: Wait for the deployment to be ready 08/17/23 03:36:33.719
    Aug 17 03:36:33.727: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 17 03:36:35.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 36, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/17/23 03:36:37.753
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:36:37.765
    Aug 17 03:36:38.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/17/23 03:36:38.772
    STEP: create a configmap that should be updated by the webhook 08/17/23 03:36:38.877
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:36:39.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3437" for this suite. 08/17/23 03:36:39.071
    STEP: Destroying namespace "webhook-3437-markers" for this suite. 08/17/23 03:36:39.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:36:39.113
Aug 17 03:36:39.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-watch 08/17/23 03:36:39.114
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:39.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:39.142
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 17 03:36:39.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Creating first CR  08/17/23 03:36:41.711
Aug 17 03:36:41.717: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:36:41Z]] name:name1 resourceVersion:224952 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/17/23 03:36:51.719
Aug 17 03:36:51.727: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:36:51Z]] name:name2 resourceVersion:224998 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/17/23 03:37:01.729
Aug 17 03:37:01.738: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:01Z]] name:name1 resourceVersion:225029 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/17/23 03:37:11.741
Aug 17 03:37:11.750: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:11Z]] name:name2 resourceVersion:225067 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/17/23 03:37:21.753
Aug 17 03:37:21.764: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:01Z]] name:name1 resourceVersion:225101 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/17/23 03:37:31.767
Aug 17 03:37:31.776: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:11Z]] name:name2 resourceVersion:225132 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:37:42.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2515" for this suite. 08/17/23 03:37:42.301
------------------------------
• [SLOW TEST] [63.196 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:36:39.113
    Aug 17 03:36:39.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-watch 08/17/23 03:36:39.114
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:36:39.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:36:39.142
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 17 03:36:39.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Creating first CR  08/17/23 03:36:41.711
    Aug 17 03:36:41.717: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:36:41Z]] name:name1 resourceVersion:224952 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/17/23 03:36:51.719
    Aug 17 03:36:51.727: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:36:51Z]] name:name2 resourceVersion:224998 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/17/23 03:37:01.729
    Aug 17 03:37:01.738: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:01Z]] name:name1 resourceVersion:225029 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/17/23 03:37:11.741
    Aug 17 03:37:11.750: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:11Z]] name:name2 resourceVersion:225067 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/17/23 03:37:21.753
    Aug 17 03:37:21.764: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:01Z]] name:name1 resourceVersion:225101 uid:119f6123-cd01-49fa-b5ef-1b58a0a17f06] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/17/23 03:37:31.767
    Aug 17 03:37:31.776: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-17T03:36:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-17T03:37:11Z]] name:name2 resourceVersion:225132 uid:1a55f977-63fb-4d29-859c-c67aeffafb39] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:37:42.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2515" for this suite. 08/17/23 03:37:42.301
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:37:42.31
Aug 17 03:37:42.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:37:42.31
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:42.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:42.328
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 17 03:37:42.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 create -f -'
Aug 17 03:37:43.163: INFO: stderr: ""
Aug 17 03:37:43.163: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 17 03:37:43.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 create -f -'
Aug 17 03:37:43.361: INFO: stderr: ""
Aug 17 03:37:43.361: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 03:37:43.361
Aug 17 03:37:44.367: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 03:37:44.368: INFO: Found 0 / 1
Aug 17 03:37:45.367: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 03:37:45.367: INFO: Found 1 / 1
Aug 17 03:37:45.367: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 03:37:45.372: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 03:37:45.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 03:37:45.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe pod agnhost-primary-lxw8n'
Aug 17 03:37:45.445: INFO: stderr: ""
Aug 17 03:37:45.445: INFO: stdout: "Name:             agnhost-primary-lxw8n\nNamespace:        kubectl-0\nPriority:         0\nService Account:  default\nNode:             ske-ubuntu-79fff84d86x69988-vjwlx/192.168.11.4\nStart Time:       Thu, 17 Aug 2023 03:37:43 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: adc3c452e72f425b27a2213617d6afc9c394e8219ab27629f1e24d26582f188b\n                  cni.projectcalico.org/podIP: 172.21.15.101/32\n                  cni.projectcalico.org/podIPs: 172.21.15.101/32\nStatus:           Running\nIP:               172.21.15.101\nIPs:\n  IP:           172.21.15.101\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://47988d11cf960841a0c3a82c5d26e1fd6574d84108d4b298f3de23ecad5d83cd\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Aug 2023 03:37:44 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-72vk5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-72vk5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-0/agnhost-primary-lxw8n to ske-ubuntu-79fff84d86x69988-vjwlx\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Aug 17 03:37:45.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe rc agnhost-primary'
Aug 17 03:37:45.519: INFO: stderr: ""
Aug 17 03:37:45.519: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-0\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lxw8n\n"
Aug 17 03:37:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe service agnhost-primary'
Aug 17 03:37:45.593: INFO: stderr: ""
Aug 17 03:37:45.593: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-0\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.20.17.162\nIPs:               172.20.17.162\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.15.101:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 17 03:37:45.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe node ske-rhel-6c9d465fc4xbjl6l-wz7jz'
Aug 17 03:37:45.705: INFO: stderr: ""
Aug 17 03:37:45.705: INFO: stdout: "Name:               ske-rhel-6c9d465fc4xbjl6l-wz7jz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=scp\n                    beta.kubernetes.io/os=linux\n                    cloud.samsungsds.com/nodepool=ske-rhel\n                    failure-domain.beta.kubernetes.io/region=KR-EAST-1\n                    failure-domain.beta.kubernetes.io/zone=KR-EAST-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ske-rhel-6c9d465fc4xbjl6l-wz7jz\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=scp\n                    topology.kubernetes.io/region=KR-EAST-1\n                    topology.kubernetes.io/zone=KR-EAST-1\nAnnotations:        cluster.x-k8s.io/cluster-name: sketest-g2dzi\n                    cluster.x-k8s.io/cluster-namespace: sketest-g2dzi\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: ske-rhel-6c9d465fc4xbjl6l-wz7jz\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: ske-rhel-6c9d465fc4xbjl6l\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.11.3/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.21.86.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 16 Aug 2023 08:28:36 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ske-rhel-6c9d465fc4xbjl6l-wz7jz\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Aug 2023 03:37:44 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 16 Aug 2023 08:29:15 +0000   Wed, 16 Aug 2023 08:29:15 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Aug 2023 03:34:19 +0000   Thu, 17 Aug 2023 02:12:42 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.11.3\n  Hostname:    ske-rhel-6c9d465fc4xbjl6l-wz7jz\nCapacity:\n  cpu:                2\n  ephemeral-storage:  67853956Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4022940Ki\n  pods:               110\nAllocatable:\n  cpu:                1930m\n  ephemeral-storage:  62534205747\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2871964Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c98fbd61e2354135840f5aa524a80bc9\n  System UUID:                dfd41442-8de0-1c96-4a75-f1027fa7f60a\n  Boot ID:                    417e6f24-9fdb-46c1-ba9f-1f6dbc200305\n  Kernel Version:             4.18.0-305.3.1.el8_4.x86_64\n  OS Image:                   Red Hat Enterprise Linux 8.3 (Ootpa)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.7-ske.p3\n  Kube-Proxy Version:         v1.26.7-ske.p3\nPodCIDR:                      172.21.1.0/24\nPodCIDRs:                     172.21.1.0/24\nProviderID:                   scp://INSTANCE-g91PtLdQtSpT97GJ7mliKl\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-77cc457ff7-x9r5c                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 calico-node-pbjkm                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 coredns-7b4f76cbb6-9cggt                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (6%)     19h\n  kube-system                 coredns-7b4f76cbb6-kvnv6                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (6%)     19h\n  kube-system                 kube-proxy-snplq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                 metrics-server-54cd6dc7f5-jkfs9                            100m (5%)     0 (0%)      200Mi (7%)       0 (0%)         19h\n  kube-system                 nfs-subdir-external-provisioner-68cdd7797d-466ng           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 node-exporter-6bjp9                                        100m (5%)     200m (10%)  50Mi (1%)        200Mi (7%)     19h\n  kube-system                 vpn-target-bcf545797-885zq                                 100m (5%)     0 (0%)      0 (0%)           0 (0%)         19h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                750m (38%)   200m (10%)\n  memory             390Mi (13%)  540Mi (19%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Aug 17 03:37:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe namespace kubectl-0'
Aug 17 03:37:45.781: INFO: stderr: ""
Aug 17 03:37:45.781: INFO: stdout: "Name:         kubectl-0\nLabels:       e2e-framework=kubectl\n              e2e-run=ced1975d-86f1-47d4-89a0-25054b775345\n              kubernetes.io/metadata.name=kubectl-0\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:37:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-0" for this suite. 08/17/23 03:37:45.788
------------------------------
• [3.487 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:37:42.31
    Aug 17 03:37:42.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:37:42.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:42.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:42.328
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 17 03:37:42.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 create -f -'
    Aug 17 03:37:43.163: INFO: stderr: ""
    Aug 17 03:37:43.163: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 17 03:37:43.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 create -f -'
    Aug 17 03:37:43.361: INFO: stderr: ""
    Aug 17 03:37:43.361: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 03:37:43.361
    Aug 17 03:37:44.367: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 03:37:44.368: INFO: Found 0 / 1
    Aug 17 03:37:45.367: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 03:37:45.367: INFO: Found 1 / 1
    Aug 17 03:37:45.367: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 17 03:37:45.372: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 03:37:45.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 03:37:45.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe pod agnhost-primary-lxw8n'
    Aug 17 03:37:45.445: INFO: stderr: ""
    Aug 17 03:37:45.445: INFO: stdout: "Name:             agnhost-primary-lxw8n\nNamespace:        kubectl-0\nPriority:         0\nService Account:  default\nNode:             ske-ubuntu-79fff84d86x69988-vjwlx/192.168.11.4\nStart Time:       Thu, 17 Aug 2023 03:37:43 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: adc3c452e72f425b27a2213617d6afc9c394e8219ab27629f1e24d26582f188b\n                  cni.projectcalico.org/podIP: 172.21.15.101/32\n                  cni.projectcalico.org/podIPs: 172.21.15.101/32\nStatus:           Running\nIP:               172.21.15.101\nIPs:\n  IP:           172.21.15.101\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://47988d11cf960841a0c3a82c5d26e1fd6574d84108d4b298f3de23ecad5d83cd\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Aug 2023 03:37:44 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-72vk5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-72vk5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-0/agnhost-primary-lxw8n to ske-ubuntu-79fff84d86x69988-vjwlx\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Aug 17 03:37:45.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe rc agnhost-primary'
    Aug 17 03:37:45.519: INFO: stderr: ""
    Aug 17 03:37:45.519: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-0\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lxw8n\n"
    Aug 17 03:37:45.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe service agnhost-primary'
    Aug 17 03:37:45.593: INFO: stderr: ""
    Aug 17 03:37:45.593: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-0\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.20.17.162\nIPs:               172.20.17.162\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.15.101:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 17 03:37:45.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe node ske-rhel-6c9d465fc4xbjl6l-wz7jz'
    Aug 17 03:37:45.705: INFO: stderr: ""
    Aug 17 03:37:45.705: INFO: stdout: "Name:               ske-rhel-6c9d465fc4xbjl6l-wz7jz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=scp\n                    beta.kubernetes.io/os=linux\n                    cloud.samsungsds.com/nodepool=ske-rhel\n                    failure-domain.beta.kubernetes.io/region=KR-EAST-1\n                    failure-domain.beta.kubernetes.io/zone=KR-EAST-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ske-rhel-6c9d465fc4xbjl6l-wz7jz\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=scp\n                    topology.kubernetes.io/region=KR-EAST-1\n                    topology.kubernetes.io/zone=KR-EAST-1\nAnnotations:        cluster.x-k8s.io/cluster-name: sketest-g2dzi\n                    cluster.x-k8s.io/cluster-namespace: sketest-g2dzi\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: ske-rhel-6c9d465fc4xbjl6l-wz7jz\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: ske-rhel-6c9d465fc4xbjl6l\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.11.3/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.21.86.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 16 Aug 2023 08:28:36 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ske-rhel-6c9d465fc4xbjl6l-wz7jz\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Aug 2023 03:37:44 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 16 Aug 2023 08:29:15 +0000   Wed, 16 Aug 2023 08:29:15 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Aug 2023 03:34:19 +0000   Wed, 16 Aug 2023 08:28:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Aug 2023 03:34:19 +0000   Thu, 17 Aug 2023 02:12:42 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.11.3\n  Hostname:    ske-rhel-6c9d465fc4xbjl6l-wz7jz\nCapacity:\n  cpu:                2\n  ephemeral-storage:  67853956Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4022940Ki\n  pods:               110\nAllocatable:\n  cpu:                1930m\n  ephemeral-storage:  62534205747\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2871964Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c98fbd61e2354135840f5aa524a80bc9\n  System UUID:                dfd41442-8de0-1c96-4a75-f1027fa7f60a\n  Boot ID:                    417e6f24-9fdb-46c1-ba9f-1f6dbc200305\n  Kernel Version:             4.18.0-305.3.1.el8_4.x86_64\n  OS Image:                   Red Hat Enterprise Linux 8.3 (Ootpa)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.26.7-ske.p3\n  Kube-Proxy Version:         v1.26.7-ske.p3\nPodCIDR:                      172.21.1.0/24\nPodCIDRs:                     172.21.1.0/24\nProviderID:                   scp://INSTANCE-g91PtLdQtSpT97GJ7mliKl\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-77cc457ff7-x9r5c                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 calico-node-pbjkm                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 coredns-7b4f76cbb6-9cggt                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (6%)     19h\n  kube-system                 coredns-7b4f76cbb6-kvnv6                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (6%)     19h\n  kube-system                 kube-proxy-snplq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                 metrics-server-54cd6dc7f5-jkfs9                            100m (5%)     0 (0%)      200Mi (7%)       0 (0%)         19h\n  kube-system                 nfs-subdir-external-provisioner-68cdd7797d-466ng           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h\n  kube-system                 node-exporter-6bjp9                                        100m (5%)     200m (10%)  50Mi (1%)        200Mi (7%)     19h\n  kube-system                 vpn-target-bcf545797-885zq                                 100m (5%)     0 (0%)      0 (0%)           0 (0%)         19h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                750m (38%)   200m (10%)\n  memory             390Mi (13%)  540Mi (19%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Aug 17 03:37:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-0 describe namespace kubectl-0'
    Aug 17 03:37:45.781: INFO: stderr: ""
    Aug 17 03:37:45.781: INFO: stdout: "Name:         kubectl-0\nLabels:       e2e-framework=kubectl\n              e2e-run=ced1975d-86f1-47d4-89a0-25054b775345\n              kubernetes.io/metadata.name=kubectl-0\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:37:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-0" for this suite. 08/17/23 03:37:45.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:37:45.798
Aug 17 03:37:45.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:37:45.799
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:45.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:45.818
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-29437504-5686-435f-8455-a64630527386 08/17/23 03:37:45.842
STEP: Creating a pod to test consume secrets 08/17/23 03:37:45.848
Aug 17 03:37:45.859: INFO: Waiting up to 5m0s for pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249" in namespace "secrets-9414" to be "Succeeded or Failed"
Aug 17 03:37:45.865: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 5.946893ms
Aug 17 03:37:47.870: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152256s
Aug 17 03:37:49.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012288904s
Aug 17 03:37:51.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012176493s
STEP: Saw pod success 08/17/23 03:37:51.871
Aug 17 03:37:51.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249" satisfied condition "Succeeded or Failed"
Aug 17 03:37:51.876: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:37:51.925
Aug 17 03:37:51.937: INFO: Waiting for pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 to disappear
Aug 17 03:37:51.942: INFO: Pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:37:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9414" for this suite. 08/17/23 03:37:51.95
STEP: Destroying namespace "secret-namespace-2557" for this suite. 08/17/23 03:37:51.959
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:37:45.798
    Aug 17 03:37:45.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:37:45.799
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:45.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:45.818
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-29437504-5686-435f-8455-a64630527386 08/17/23 03:37:45.842
    STEP: Creating a pod to test consume secrets 08/17/23 03:37:45.848
    Aug 17 03:37:45.859: INFO: Waiting up to 5m0s for pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249" in namespace "secrets-9414" to be "Succeeded or Failed"
    Aug 17 03:37:45.865: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 5.946893ms
    Aug 17 03:37:47.870: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152256s
    Aug 17 03:37:49.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012288904s
    Aug 17 03:37:51.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012176493s
    STEP: Saw pod success 08/17/23 03:37:51.871
    Aug 17 03:37:51.871: INFO: Pod "pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249" satisfied condition "Succeeded or Failed"
    Aug 17 03:37:51.876: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:37:51.925
    Aug 17 03:37:51.937: INFO: Waiting for pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 to disappear
    Aug 17 03:37:51.942: INFO: Pod pod-secrets-90efb80d-6b49-44e8-8c4d-695e83e9c249 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:37:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9414" for this suite. 08/17/23 03:37:51.95
    STEP: Destroying namespace "secret-namespace-2557" for this suite. 08/17/23 03:37:51.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:37:51.97
Aug 17 03:37:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:37:51.971
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:51.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:51.992
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:37:52.012
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:37:52.352
STEP: Deploying the webhook pod 08/17/23 03:37:52.362
STEP: Wait for the deployment to be ready 08/17/23 03:37:52.375
Aug 17 03:37:52.384: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:37:54.401
STEP: Verifying the service has paired with the endpoint 08/17/23 03:37:54.413
Aug 17 03:37:55.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 03:37:55.42
STEP: create a pod that should be denied by the webhook 08/17/23 03:37:55.526
STEP: create a pod that causes the webhook to hang 08/17/23 03:37:55.623
STEP: create a configmap that should be denied by the webhook 08/17/23 03:38:05.634
STEP: create a configmap that should be admitted by the webhook 08/17/23 03:38:05.766
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 03:38:05.825
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 03:38:05.879
STEP: create a namespace that bypass the webhook 08/17/23 03:38:05.93
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/17/23 03:38:05.939
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:38:05.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9685" for this suite. 08/17/23 03:38:06.016
STEP: Destroying namespace "webhook-9685-markers" for this suite. 08/17/23 03:38:06.036
------------------------------
• [SLOW TEST] [14.085 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:37:51.97
    Aug 17 03:37:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:37:51.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:37:51.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:37:51.992
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:37:52.012
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:37:52.352
    STEP: Deploying the webhook pod 08/17/23 03:37:52.362
    STEP: Wait for the deployment to be ready 08/17/23 03:37:52.375
    Aug 17 03:37:52.384: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:37:54.401
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:37:54.413
    Aug 17 03:37:55.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 03:37:55.42
    STEP: create a pod that should be denied by the webhook 08/17/23 03:37:55.526
    STEP: create a pod that causes the webhook to hang 08/17/23 03:37:55.623
    STEP: create a configmap that should be denied by the webhook 08/17/23 03:38:05.634
    STEP: create a configmap that should be admitted by the webhook 08/17/23 03:38:05.766
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 03:38:05.825
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/17/23 03:38:05.879
    STEP: create a namespace that bypass the webhook 08/17/23 03:38:05.93
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/17/23 03:38:05.939
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:38:05.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9685" for this suite. 08/17/23 03:38:06.016
    STEP: Destroying namespace "webhook-9685-markers" for this suite. 08/17/23 03:38:06.036
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:38:06.057
Aug 17 03:38:06.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename aggregator 08/17/23 03:38:06.06
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:06.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:06.101
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 17 03:38:06.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/17/23 03:38:06.109
Aug 17 03:38:06.558: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 17 03:38:08.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:10.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:12.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:14.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:16.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:18.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:20.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:22.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:24.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:26.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:28.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:30.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:32.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 03:38:34.831: INFO: Waited 208.53914ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/17/23 03:38:35.324
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/17/23 03:38:35.331
STEP: List APIServices 08/17/23 03:38:35.338
Aug 17 03:38:35.346: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 17 03:38:35.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-4661" for this suite. 08/17/23 03:38:35.555
------------------------------
• [SLOW TEST] [29.517 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:38:06.057
    Aug 17 03:38:06.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename aggregator 08/17/23 03:38:06.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:06.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:06.101
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 17 03:38:06.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/17/23 03:38:06.109
    Aug 17 03:38:06.558: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 17 03:38:08.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:10.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:12.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:14.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:16.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:18.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:20.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:22.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:24.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:26.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:28.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:30.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:32.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 38, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 03:38:34.831: INFO: Waited 208.53914ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/17/23 03:38:35.324
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/17/23 03:38:35.331
    STEP: List APIServices 08/17/23 03:38:35.338
    Aug 17 03:38:35.346: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:38:35.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-4661" for this suite. 08/17/23 03:38:35.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:38:35.585
Aug 17 03:38:35.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 03:38:35.59
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:35.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:35.62
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 17 03:38:35.649: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"42c5b9d9-87db-4faf-a30b-a88d63685da3", Controller:(*bool)(0xc00487c942), BlockOwnerDeletion:(*bool)(0xc00487c943)}}
Aug 17 03:38:35.656: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"08aa6803-5782-46b7-a14b-331bf9bdc1e2", Controller:(*bool)(0xc004ceac2a), BlockOwnerDeletion:(*bool)(0xc004ceac2b)}}
Aug 17 03:38:35.662: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2989c7dd-3d04-48be-b928-d39ecf4d4cda", Controller:(*bool)(0xc00487cb66), BlockOwnerDeletion:(*bool)(0xc00487cb67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 03:38:40.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-695" for this suite. 08/17/23 03:38:40.683
------------------------------
• [SLOW TEST] [5.106 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:38:35.585
    Aug 17 03:38:35.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 03:38:35.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:35.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:35.62
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 17 03:38:35.649: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"42c5b9d9-87db-4faf-a30b-a88d63685da3", Controller:(*bool)(0xc00487c942), BlockOwnerDeletion:(*bool)(0xc00487c943)}}
    Aug 17 03:38:35.656: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"08aa6803-5782-46b7-a14b-331bf9bdc1e2", Controller:(*bool)(0xc004ceac2a), BlockOwnerDeletion:(*bool)(0xc004ceac2b)}}
    Aug 17 03:38:35.662: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2989c7dd-3d04-48be-b928-d39ecf4d4cda", Controller:(*bool)(0xc00487cb66), BlockOwnerDeletion:(*bool)(0xc00487cb67)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:38:40.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-695" for this suite. 08/17/23 03:38:40.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:38:40.692
Aug 17 03:38:40.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:38:40.692
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:40.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:40.712
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/17/23 03:38:40.717
Aug 17 03:38:40.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: mark a version not serverd 08/17/23 03:38:44.478
STEP: check the unserved version gets removed 08/17/23 03:38:44.506
STEP: check the other version is not changed 08/17/23 03:38:45.989
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:38:48.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6828" for this suite. 08/17/23 03:38:48.963
------------------------------
• [SLOW TEST] [8.280 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:38:40.692
    Aug 17 03:38:40.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:38:40.692
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:40.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:40.712
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/17/23 03:38:40.717
    Aug 17 03:38:40.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: mark a version not serverd 08/17/23 03:38:44.478
    STEP: check the unserved version gets removed 08/17/23 03:38:44.506
    STEP: check the other version is not changed 08/17/23 03:38:45.989
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:38:48.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6828" for this suite. 08/17/23 03:38:48.963
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:38:48.973
Aug 17 03:38:48.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:38:48.973
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:48.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:48.991
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 03:38:49.009: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:39:49.070: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/17/23 03:39:49.074
Aug 17 03:39:49.099: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 17 03:39:49.104: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 17 03:39:49.121: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 17 03:39:49.127: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/17/23 03:39:49.127
Aug 17 03:39:49.127: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9048" to be "running"
Aug 17 03:39:49.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36614ms
Aug 17 03:39:51.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015894647s
Aug 17 03:39:51.144: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 17 03:39:51.144: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
Aug 17 03:39:51.149: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.827236ms
Aug 17 03:39:51.149: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 03:39:51.149: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
Aug 17 03:39:51.154: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247506ms
Aug 17 03:39:53.159: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009480829s
Aug 17 03:39:53.159: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 03:39:53.159: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
Aug 17 03:39:53.163: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.07477ms
Aug 17 03:39:53.163: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/17/23 03:39:53.163
Aug 17 03:39:53.169: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9048" to be "running"
Aug 17 03:39:53.174: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328279ms
Aug 17 03:39:55.180: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010083879s
Aug 17 03:39:57.181: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01108208s
Aug 17 03:39:57.181: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:39:57.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9048" for this suite. 08/17/23 03:39:57.255
------------------------------
• [SLOW TEST] [68.290 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:38:48.973
    Aug 17 03:38:48.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:38:48.973
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:38:48.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:38:48.991
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 03:38:49.009: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:39:49.070: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/17/23 03:39:49.074
    Aug 17 03:39:49.099: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 17 03:39:49.104: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 17 03:39:49.121: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 17 03:39:49.127: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/17/23 03:39:49.127
    Aug 17 03:39:49.127: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9048" to be "running"
    Aug 17 03:39:49.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.36614ms
    Aug 17 03:39:51.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015894647s
    Aug 17 03:39:51.144: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 17 03:39:51.144: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
    Aug 17 03:39:51.149: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.827236ms
    Aug 17 03:39:51.149: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 03:39:51.149: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
    Aug 17 03:39:51.154: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247506ms
    Aug 17 03:39:53.159: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009480829s
    Aug 17 03:39:53.159: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 03:39:53.159: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9048" to be "running"
    Aug 17 03:39:53.163: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.07477ms
    Aug 17 03:39:53.163: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/17/23 03:39:53.163
    Aug 17 03:39:53.169: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9048" to be "running"
    Aug 17 03:39:53.174: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328279ms
    Aug 17 03:39:55.180: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010083879s
    Aug 17 03:39:57.181: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01108208s
    Aug 17 03:39:57.181: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:39:57.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9048" for this suite. 08/17/23 03:39:57.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:39:57.263
Aug 17 03:39:57.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 03:39:57.265
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:57.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:57.285
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-msfl4" 08/17/23 03:39:57.29
Aug 17 03:39:57.305: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-msfl4-6494" 08/17/23 03:39:57.305
Aug 17 03:39:57.315: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-msfl4-6494" 08/17/23 03:39:57.315
Aug 17 03:39:57.326: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:39:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5053" for this suite. 08/17/23 03:39:57.332
STEP: Destroying namespace "e2e-ns-msfl4-6494" for this suite. 08/17/23 03:39:57.34
------------------------------
• [0.083 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:39:57.263
    Aug 17 03:39:57.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 03:39:57.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:57.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:57.285
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-msfl4" 08/17/23 03:39:57.29
    Aug 17 03:39:57.305: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-msfl4-6494" 08/17/23 03:39:57.305
    Aug 17 03:39:57.315: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-msfl4-6494" 08/17/23 03:39:57.315
    Aug 17 03:39:57.326: INFO: Namespace "e2e-ns-msfl4-6494" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:39:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5053" for this suite. 08/17/23 03:39:57.332
    STEP: Destroying namespace "e2e-ns-msfl4-6494" for this suite. 08/17/23 03:39:57.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:39:57.349
Aug 17 03:39:57.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:39:57.35
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:57.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:57.372
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 17 03:39:57.386: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722" in namespace "kubelet-test-1483" to be "running and ready"
Aug 17 03:39:57.390: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169116ms
Aug 17 03:39:57.390: INFO: The phase of Pod busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:39:59.396: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722": Phase="Running", Reason="", readiness=true. Elapsed: 2.010518565s
Aug 17 03:39:59.396: INFO: The phase of Pod busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722 is Running (Ready = true)
Aug 17 03:39:59.396: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:39:59.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1483" for this suite. 08/17/23 03:39:59.422
------------------------------
• [2.081 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:39:57.349
    Aug 17 03:39:57.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:39:57.35
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:57.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:57.372
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 17 03:39:57.386: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722" in namespace "kubelet-test-1483" to be "running and ready"
    Aug 17 03:39:57.390: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169116ms
    Aug 17 03:39:57.390: INFO: The phase of Pod busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:39:59.396: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722": Phase="Running", Reason="", readiness=true. Elapsed: 2.010518565s
    Aug 17 03:39:59.396: INFO: The phase of Pod busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722 is Running (Ready = true)
    Aug 17 03:39:59.396: INFO: Pod "busybox-scheduling-ec696f8c-6dca-4f66-9b3c-9ce437a1b722" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:39:59.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1483" for this suite. 08/17/23 03:39:59.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:39:59.43
Aug 17 03:39:59.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:39:59.431
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:59.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:59.451
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1157 08/17/23 03:39:59.456
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 03:39:59.473
STEP: creating service externalsvc in namespace services-1157 08/17/23 03:39:59.474
STEP: creating replication controller externalsvc in namespace services-1157 08/17/23 03:39:59.485
I0817 03:39:59.490749      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1157, replica count: 2
I0817 03:40:02.543737      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/17/23 03:40:02.549
Aug 17 03:40:02.569: INFO: Creating new exec pod
Aug 17 03:40:02.576: INFO: Waiting up to 5m0s for pod "execpodvz7mt" in namespace "services-1157" to be "running"
Aug 17 03:40:02.580: INFO: Pod "execpodvz7mt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900264ms
Aug 17 03:40:04.585: INFO: Pod "execpodvz7mt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008930518s
Aug 17 03:40:04.585: INFO: Pod "execpodvz7mt" satisfied condition "running"
Aug 17 03:40:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1157 exec execpodvz7mt -- /bin/sh -x -c nslookup nodeport-service.services-1157.svc.cluster.local'
Aug 17 03:40:05.142: INFO: stderr: "+ nslookup nodeport-service.services-1157.svc.cluster.local\n"
Aug 17 03:40:05.142: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nnodeport-service.services-1157.svc.cluster.local\tcanonical name = externalsvc.services-1157.svc.cluster.local.\nName:\texternalsvc.services-1157.svc.cluster.local\nAddress: 172.20.72.176\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1157, will wait for the garbage collector to delete the pods 08/17/23 03:40:05.142
Aug 17 03:40:05.205: INFO: Deleting ReplicationController externalsvc took: 7.396056ms
Aug 17 03:40:05.306: INFO: Terminating ReplicationController externalsvc pods took: 100.881564ms
Aug 17 03:40:07.223: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1157" for this suite. 08/17/23 03:40:07.244
------------------------------
• [SLOW TEST] [7.820 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:39:59.43
    Aug 17 03:39:59.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:39:59.431
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:39:59.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:39:59.451
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1157 08/17/23 03:39:59.456
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 03:39:59.473
    STEP: creating service externalsvc in namespace services-1157 08/17/23 03:39:59.474
    STEP: creating replication controller externalsvc in namespace services-1157 08/17/23 03:39:59.485
    I0817 03:39:59.490749      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1157, replica count: 2
    I0817 03:40:02.543737      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/17/23 03:40:02.549
    Aug 17 03:40:02.569: INFO: Creating new exec pod
    Aug 17 03:40:02.576: INFO: Waiting up to 5m0s for pod "execpodvz7mt" in namespace "services-1157" to be "running"
    Aug 17 03:40:02.580: INFO: Pod "execpodvz7mt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900264ms
    Aug 17 03:40:04.585: INFO: Pod "execpodvz7mt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008930518s
    Aug 17 03:40:04.585: INFO: Pod "execpodvz7mt" satisfied condition "running"
    Aug 17 03:40:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-1157 exec execpodvz7mt -- /bin/sh -x -c nslookup nodeport-service.services-1157.svc.cluster.local'
    Aug 17 03:40:05.142: INFO: stderr: "+ nslookup nodeport-service.services-1157.svc.cluster.local\n"
    Aug 17 03:40:05.142: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nnodeport-service.services-1157.svc.cluster.local\tcanonical name = externalsvc.services-1157.svc.cluster.local.\nName:\texternalsvc.services-1157.svc.cluster.local\nAddress: 172.20.72.176\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1157, will wait for the garbage collector to delete the pods 08/17/23 03:40:05.142
    Aug 17 03:40:05.205: INFO: Deleting ReplicationController externalsvc took: 7.396056ms
    Aug 17 03:40:05.306: INFO: Terminating ReplicationController externalsvc pods took: 100.881564ms
    Aug 17 03:40:07.223: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1157" for this suite. 08/17/23 03:40:07.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:07.258
Aug 17 03:40:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename controllerrevisions 08/17/23 03:40:07.258
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:07.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:07.28
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-tz27c-daemon-set" 08/17/23 03:40:07.311
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:40:07.318
Aug 17 03:40:07.329: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
Aug 17 03:40:07.330: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:40:08.348: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
Aug 17 03:40:08.348: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:40:09.345: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 2
Aug 17 03:40:09.345: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-tz27c-daemon-set
STEP: Confirm DaemonSet "e2e-tz27c-daemon-set" successfully created with "daemonset-name=e2e-tz27c-daemon-set" label 08/17/23 03:40:09.349
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tz27c-daemon-set" 08/17/23 03:40:09.36
Aug 17 03:40:09.366: INFO: Located ControllerRevision: "e2e-tz27c-daemon-set-78fddb78b7"
STEP: Patching ControllerRevision "e2e-tz27c-daemon-set-78fddb78b7" 08/17/23 03:40:09.371
Aug 17 03:40:09.380: INFO: e2e-tz27c-daemon-set-78fddb78b7 has been patched
STEP: Create a new ControllerRevision 08/17/23 03:40:09.38
Aug 17 03:40:09.387: INFO: Created ControllerRevision: e2e-tz27c-daemon-set-85749cb8cf
STEP: Confirm that there are two ControllerRevisions 08/17/23 03:40:09.387
Aug 17 03:40:09.387: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 03:40:09.393: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-tz27c-daemon-set-78fddb78b7" 08/17/23 03:40:09.393
STEP: Confirm that there is only one ControllerRevision 08/17/23 03:40:09.401
Aug 17 03:40:09.401: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 03:40:09.406: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-tz27c-daemon-set-85749cb8cf" 08/17/23 03:40:09.41
Aug 17 03:40:09.423: INFO: e2e-tz27c-daemon-set-85749cb8cf has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/17/23 03:40:09.424
W0817 03:40:09.431274      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/17/23 03:40:09.431
Aug 17 03:40:09.431: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 03:40:10.436: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 03:40:10.441: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tz27c-daemon-set-85749cb8cf=updated" 08/17/23 03:40:10.442
STEP: Confirm that there is only one ControllerRevision 08/17/23 03:40:10.452
Aug 17 03:40:10.452: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 17 03:40:10.457: INFO: Found 1 ControllerRevisions
Aug 17 03:40:10.462: INFO: ControllerRevision "e2e-tz27c-daemon-set-76d959c84b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-tz27c-daemon-set" 08/17/23 03:40:10.468
STEP: deleting DaemonSet.extensions e2e-tz27c-daemon-set in namespace controllerrevisions-2672, will wait for the garbage collector to delete the pods 08/17/23 03:40:10.468
Aug 17 03:40:10.530: INFO: Deleting DaemonSet.extensions e2e-tz27c-daemon-set took: 7.086506ms
Aug 17 03:40:10.631: INFO: Terminating DaemonSet.extensions e2e-tz27c-daemon-set pods took: 100.965103ms
Aug 17 03:40:12.237: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
Aug 17 03:40:12.237: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tz27c-daemon-set
Aug 17 03:40:12.241: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"226295"},"items":null}

Aug 17 03:40:12.245: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"226295"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-2672" for this suite. 08/17/23 03:40:12.268
------------------------------
• [SLOW TEST] [5.018 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:07.258
    Aug 17 03:40:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename controllerrevisions 08/17/23 03:40:07.258
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:07.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:07.28
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-tz27c-daemon-set" 08/17/23 03:40:07.311
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:40:07.318
    Aug 17 03:40:07.329: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
    Aug 17 03:40:07.330: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:40:08.348: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
    Aug 17 03:40:08.348: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:40:09.345: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 2
    Aug 17 03:40:09.345: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-tz27c-daemon-set
    STEP: Confirm DaemonSet "e2e-tz27c-daemon-set" successfully created with "daemonset-name=e2e-tz27c-daemon-set" label 08/17/23 03:40:09.349
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tz27c-daemon-set" 08/17/23 03:40:09.36
    Aug 17 03:40:09.366: INFO: Located ControllerRevision: "e2e-tz27c-daemon-set-78fddb78b7"
    STEP: Patching ControllerRevision "e2e-tz27c-daemon-set-78fddb78b7" 08/17/23 03:40:09.371
    Aug 17 03:40:09.380: INFO: e2e-tz27c-daemon-set-78fddb78b7 has been patched
    STEP: Create a new ControllerRevision 08/17/23 03:40:09.38
    Aug 17 03:40:09.387: INFO: Created ControllerRevision: e2e-tz27c-daemon-set-85749cb8cf
    STEP: Confirm that there are two ControllerRevisions 08/17/23 03:40:09.387
    Aug 17 03:40:09.387: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 03:40:09.393: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-tz27c-daemon-set-78fddb78b7" 08/17/23 03:40:09.393
    STEP: Confirm that there is only one ControllerRevision 08/17/23 03:40:09.401
    Aug 17 03:40:09.401: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 03:40:09.406: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-tz27c-daemon-set-85749cb8cf" 08/17/23 03:40:09.41
    Aug 17 03:40:09.423: INFO: e2e-tz27c-daemon-set-85749cb8cf has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/17/23 03:40:09.424
    W0817 03:40:09.431274      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/17/23 03:40:09.431
    Aug 17 03:40:09.431: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 03:40:10.436: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 03:40:10.441: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tz27c-daemon-set-85749cb8cf=updated" 08/17/23 03:40:10.442
    STEP: Confirm that there is only one ControllerRevision 08/17/23 03:40:10.452
    Aug 17 03:40:10.452: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 17 03:40:10.457: INFO: Found 1 ControllerRevisions
    Aug 17 03:40:10.462: INFO: ControllerRevision "e2e-tz27c-daemon-set-76d959c84b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-tz27c-daemon-set" 08/17/23 03:40:10.468
    STEP: deleting DaemonSet.extensions e2e-tz27c-daemon-set in namespace controllerrevisions-2672, will wait for the garbage collector to delete the pods 08/17/23 03:40:10.468
    Aug 17 03:40:10.530: INFO: Deleting DaemonSet.extensions e2e-tz27c-daemon-set took: 7.086506ms
    Aug 17 03:40:10.631: INFO: Terminating DaemonSet.extensions e2e-tz27c-daemon-set pods took: 100.965103ms
    Aug 17 03:40:12.237: INFO: Number of nodes with available pods controlled by daemonset e2e-tz27c-daemon-set: 0
    Aug 17 03:40:12.237: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tz27c-daemon-set
    Aug 17 03:40:12.241: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"226295"},"items":null}

    Aug 17 03:40:12.245: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"226295"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-2672" for this suite. 08/17/23 03:40:12.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:12.277
Aug 17 03:40:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:40:12.278
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:12.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:12.297
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 03:40:12.302
Aug 17 03:40:12.311: INFO: Waiting up to 5m0s for pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519" in namespace "emptydir-9112" to be "Succeeded or Failed"
Aug 17 03:40:12.316: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167901ms
Aug 17 03:40:14.321: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009215485s
Aug 17 03:40:16.323: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011188713s
STEP: Saw pod success 08/17/23 03:40:16.323
Aug 17 03:40:16.323: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519" satisfied condition "Succeeded or Failed"
Aug 17 03:40:16.329: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 container test-container: <nil>
STEP: delete the pod 08/17/23 03:40:16.345
Aug 17 03:40:16.357: INFO: Waiting for pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 to disappear
Aug 17 03:40:16.361: INFO: Pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:16.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9112" for this suite. 08/17/23 03:40:16.369
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:12.277
    Aug 17 03:40:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:40:12.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:12.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:12.297
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 03:40:12.302
    Aug 17 03:40:12.311: INFO: Waiting up to 5m0s for pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519" in namespace "emptydir-9112" to be "Succeeded or Failed"
    Aug 17 03:40:12.316: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167901ms
    Aug 17 03:40:14.321: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009215485s
    Aug 17 03:40:16.323: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011188713s
    STEP: Saw pod success 08/17/23 03:40:16.323
    Aug 17 03:40:16.323: INFO: Pod "pod-8c2fd679-2c40-4fca-af98-3c64466dd519" satisfied condition "Succeeded or Failed"
    Aug 17 03:40:16.329: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 container test-container: <nil>
    STEP: delete the pod 08/17/23 03:40:16.345
    Aug 17 03:40:16.357: INFO: Waiting for pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 to disappear
    Aug 17 03:40:16.361: INFO: Pod pod-8c2fd679-2c40-4fca-af98-3c64466dd519 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:16.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9112" for this suite. 08/17/23 03:40:16.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:16.378
Aug 17 03:40:16.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename csistoragecapacity 08/17/23 03:40:16.379
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:16.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:16.398
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/17/23 03:40:16.403
STEP: getting /apis/storage.k8s.io 08/17/23 03:40:16.409
STEP: getting /apis/storage.k8s.io/v1 08/17/23 03:40:16.411
STEP: creating 08/17/23 03:40:16.413
STEP: watching 08/17/23 03:40:16.434
Aug 17 03:40:16.434: INFO: starting watch
STEP: getting 08/17/23 03:40:16.445
STEP: listing in namespace 08/17/23 03:40:16.449
STEP: listing across namespaces 08/17/23 03:40:16.455
STEP: patching 08/17/23 03:40:16.459
STEP: updating 08/17/23 03:40:16.466
Aug 17 03:40:16.473: INFO: waiting for watch events with expected annotations in namespace
Aug 17 03:40:16.473: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/17/23 03:40:16.473
STEP: deleting a collection 08/17/23 03:40:16.488
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:16.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6512" for this suite. 08/17/23 03:40:16.514
------------------------------
• [0.143 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:16.378
    Aug 17 03:40:16.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename csistoragecapacity 08/17/23 03:40:16.379
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:16.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:16.398
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/17/23 03:40:16.403
    STEP: getting /apis/storage.k8s.io 08/17/23 03:40:16.409
    STEP: getting /apis/storage.k8s.io/v1 08/17/23 03:40:16.411
    STEP: creating 08/17/23 03:40:16.413
    STEP: watching 08/17/23 03:40:16.434
    Aug 17 03:40:16.434: INFO: starting watch
    STEP: getting 08/17/23 03:40:16.445
    STEP: listing in namespace 08/17/23 03:40:16.449
    STEP: listing across namespaces 08/17/23 03:40:16.455
    STEP: patching 08/17/23 03:40:16.459
    STEP: updating 08/17/23 03:40:16.466
    Aug 17 03:40:16.473: INFO: waiting for watch events with expected annotations in namespace
    Aug 17 03:40:16.473: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/17/23 03:40:16.473
    STEP: deleting a collection 08/17/23 03:40:16.488
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:16.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6512" for this suite. 08/17/23 03:40:16.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:16.524
Aug 17 03:40:16.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:40:16.525
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:16.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:16.547
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/17/23 03:40:16.552
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/17/23 03:40:16.559
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/17/23 03:40:16.559
STEP: creating a pod to probe DNS 08/17/23 03:40:16.559
STEP: submitting the pod to kubernetes 08/17/23 03:40:16.56
Aug 17 03:40:16.570: INFO: Waiting up to 15m0s for pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682" in namespace "dns-473" to be "running"
Aug 17 03:40:16.575: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682": Phase="Pending", Reason="", readiness=false. Elapsed: 4.862152ms
Aug 17 03:40:18.582: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682": Phase="Running", Reason="", readiness=true. Elapsed: 2.011359515s
Aug 17 03:40:18.582: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:40:18.582
STEP: looking for the results for each expected name from probers 08/17/23 03:40:18.586
Aug 17 03:40:18.735: INFO: DNS probes using dns-473/dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682 succeeded

STEP: deleting the pod 08/17/23 03:40:18.735
STEP: deleting the test headless service 08/17/23 03:40:18.748
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-473" for this suite. 08/17/23 03:40:18.769
------------------------------
• [2.252 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:16.524
    Aug 17 03:40:16.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:40:16.525
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:16.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:16.547
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/17/23 03:40:16.552
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/17/23 03:40:16.559
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/17/23 03:40:16.559
    STEP: creating a pod to probe DNS 08/17/23 03:40:16.559
    STEP: submitting the pod to kubernetes 08/17/23 03:40:16.56
    Aug 17 03:40:16.570: INFO: Waiting up to 15m0s for pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682" in namespace "dns-473" to be "running"
    Aug 17 03:40:16.575: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682": Phase="Pending", Reason="", readiness=false. Elapsed: 4.862152ms
    Aug 17 03:40:18.582: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682": Phase="Running", Reason="", readiness=true. Elapsed: 2.011359515s
    Aug 17 03:40:18.582: INFO: Pod "dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:40:18.582
    STEP: looking for the results for each expected name from probers 08/17/23 03:40:18.586
    Aug 17 03:40:18.735: INFO: DNS probes using dns-473/dns-test-2cafc0f9-dfc0-4d7e-8b0e-dc1837176682 succeeded

    STEP: deleting the pod 08/17/23 03:40:18.735
    STEP: deleting the test headless service 08/17/23 03:40:18.748
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-473" for this suite. 08/17/23 03:40:18.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:18.777
Aug 17 03:40:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:40:18.779
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:18.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:18.801
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 17 03:40:18.821: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7304 to be scheduled
Aug 17 03:40:18.826: INFO: 1 pods are not scheduled: [runtimeclass-7304/test-runtimeclass-runtimeclass-7304-preconfigured-handler-bxd9q(2f194174-889c-4e86-b4de-95c48698dad2)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:20.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7304" for this suite. 08/17/23 03:40:20.846
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:18.777
    Aug 17 03:40:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:40:18.779
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:18.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:18.801
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 17 03:40:18.821: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7304 to be scheduled
    Aug 17 03:40:18.826: INFO: 1 pods are not scheduled: [runtimeclass-7304/test-runtimeclass-runtimeclass-7304-preconfigured-handler-bxd9q(2f194174-889c-4e86-b4de-95c48698dad2)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:20.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7304" for this suite. 08/17/23 03:40:20.846
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:20.856
Aug 17 03:40:20.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-runtime 08/17/23 03:40:20.857
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:20.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:20.876
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/17/23 03:40:20.882
STEP: wait for the container to reach Succeeded 08/17/23 03:40:20.892
STEP: get the container status 08/17/23 03:40:24.92
STEP: the container should be terminated 08/17/23 03:40:24.924
STEP: the termination message should be set 08/17/23 03:40:24.924
Aug 17 03:40:24.924: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/17/23 03:40:24.924
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4389" for this suite. 08/17/23 03:40:24.947
------------------------------
• [4.098 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:20.856
    Aug 17 03:40:20.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-runtime 08/17/23 03:40:20.857
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:20.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:20.876
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/17/23 03:40:20.882
    STEP: wait for the container to reach Succeeded 08/17/23 03:40:20.892
    STEP: get the container status 08/17/23 03:40:24.92
    STEP: the container should be terminated 08/17/23 03:40:24.924
    STEP: the termination message should be set 08/17/23 03:40:24.924
    Aug 17 03:40:24.924: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/17/23 03:40:24.924
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4389" for this suite. 08/17/23 03:40:24.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:24.956
Aug 17 03:40:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-webhook 08/17/23 03:40:24.956
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:24.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:24.976
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/17/23 03:40:24.981
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 03:40:25.25
STEP: Deploying the custom resource conversion webhook pod 08/17/23 03:40:25.26
STEP: Wait for the deployment to be ready 08/17/23 03:40:25.274
Aug 17 03:40:25.284: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:40:27.3
STEP: Verifying the service has paired with the endpoint 08/17/23 03:40:27.313
Aug 17 03:40:28.313: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 17 03:40:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Creating a v1 custom resource 08/17/23 03:40:31.099
STEP: Create a v2 custom resource 08/17/23 03:40:31.118
STEP: List CRs in v1 08/17/23 03:40:31.206
STEP: List CRs in v2 08/17/23 03:40:31.251
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:31.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7265" for this suite. 08/17/23 03:40:31.87
------------------------------
• [SLOW TEST] [6.924 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:24.956
    Aug 17 03:40:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-webhook 08/17/23 03:40:24.956
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:24.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:24.976
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/17/23 03:40:24.981
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/17/23 03:40:25.25
    STEP: Deploying the custom resource conversion webhook pod 08/17/23 03:40:25.26
    STEP: Wait for the deployment to be ready 08/17/23 03:40:25.274
    Aug 17 03:40:25.284: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:40:27.3
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:40:27.313
    Aug 17 03:40:28.313: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 17 03:40:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Creating a v1 custom resource 08/17/23 03:40:31.099
    STEP: Create a v2 custom resource 08/17/23 03:40:31.118
    STEP: List CRs in v1 08/17/23 03:40:31.206
    STEP: List CRs in v2 08/17/23 03:40:31.251
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:31.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7265" for this suite. 08/17/23 03:40:31.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:31.896
Aug 17 03:40:31.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:40:31.903
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:31.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:31.922
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:40:31.926
Aug 17 03:40:31.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0" in namespace "downward-api-1773" to be "Succeeded or Failed"
Aug 17 03:40:31.941: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22625ms
Aug 17 03:40:33.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927642s
Aug 17 03:40:35.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009835673s
STEP: Saw pod success 08/17/23 03:40:35.946
Aug 17 03:40:35.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0" satisfied condition "Succeeded or Failed"
Aug 17 03:40:35.951: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 container client-container: <nil>
STEP: delete the pod 08/17/23 03:40:35.96
Aug 17 03:40:35.972: INFO: Waiting for pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 to disappear
Aug 17 03:40:35.976: INFO: Pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:35.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1773" for this suite. 08/17/23 03:40:35.983
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:31.896
    Aug 17 03:40:31.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:40:31.903
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:31.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:31.922
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:40:31.926
    Aug 17 03:40:31.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0" in namespace "downward-api-1773" to be "Succeeded or Failed"
    Aug 17 03:40:31.941: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22625ms
    Aug 17 03:40:33.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927642s
    Aug 17 03:40:35.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009835673s
    STEP: Saw pod success 08/17/23 03:40:35.946
    Aug 17 03:40:35.946: INFO: Pod "downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0" satisfied condition "Succeeded or Failed"
    Aug 17 03:40:35.951: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:40:35.96
    Aug 17 03:40:35.972: INFO: Waiting for pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 to disappear
    Aug 17 03:40:35.976: INFO: Pod downwardapi-volume-51c848c2-e00a-410e-b65c-1597595d9ff0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:35.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1773" for this suite. 08/17/23 03:40:35.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:35.993
Aug 17 03:40:35.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:40:35.994
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:36.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:36.012
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/17/23 03:40:36.018
Aug 17 03:40:36.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 create -f -'
Aug 17 03:40:36.747: INFO: stderr: ""
Aug 17 03:40:36.747: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:36.747
Aug 17 03:40:36.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:36.845: INFO: stderr: ""
Aug 17 03:40:36.845: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
Aug 17 03:40:36.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:36.912: INFO: stderr: ""
Aug 17 03:40:36.912: INFO: stdout: ""
Aug 17 03:40:36.912: INFO: update-demo-nautilus-q8xnc is created but not running
Aug 17 03:40:41.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:41.982: INFO: stderr: ""
Aug 17 03:40:41.982: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
Aug 17 03:40:41.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:42.051: INFO: stderr: ""
Aug 17 03:40:42.051: INFO: stdout: ""
Aug 17 03:40:42.051: INFO: update-demo-nautilus-q8xnc is created but not running
Aug 17 03:40:47.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:47.118: INFO: stderr: ""
Aug 17 03:40:47.118: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
Aug 17 03:40:47.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:47.183: INFO: stderr: ""
Aug 17 03:40:47.183: INFO: stdout: "true"
Aug 17 03:40:47.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:40:47.244: INFO: stderr: ""
Aug 17 03:40:47.244: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:40:47.244: INFO: validating pod update-demo-nautilus-q8xnc
Aug 17 03:40:47.336: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:40:47.336: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:40:47.336: INFO: update-demo-nautilus-q8xnc is verified up and running
Aug 17 03:40:47.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:47.399: INFO: stderr: ""
Aug 17 03:40:47.399: INFO: stdout: "true"
Aug 17 03:40:47.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:40:47.462: INFO: stderr: ""
Aug 17 03:40:47.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:40:47.462: INFO: validating pod update-demo-nautilus-qf57m
Aug 17 03:40:47.553: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:40:47.553: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:40:47.553: INFO: update-demo-nautilus-qf57m is verified up and running
STEP: scaling down the replication controller 08/17/23 03:40:47.553
Aug 17 03:40:47.554: INFO: scanned /root for discovery docs: <nil>
Aug 17 03:40:47.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 17 03:40:48.649: INFO: stderr: ""
Aug 17 03:40:48.649: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:48.649
Aug 17 03:40:48.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:48.762: INFO: stderr: ""
Aug 17 03:40:48.762: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/17/23 03:40:48.762
Aug 17 03:40:53.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:53.832: INFO: stderr: ""
Aug 17 03:40:53.832: INFO: stdout: "update-demo-nautilus-qf57m "
Aug 17 03:40:53.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:53.898: INFO: stderr: ""
Aug 17 03:40:53.898: INFO: stdout: "true"
Aug 17 03:40:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:40:53.965: INFO: stderr: ""
Aug 17 03:40:53.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:40:53.965: INFO: validating pod update-demo-nautilus-qf57m
Aug 17 03:40:53.972: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:40:53.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:40:53.972: INFO: update-demo-nautilus-qf57m is verified up and running
STEP: scaling up the replication controller 08/17/23 03:40:53.972
Aug 17 03:40:53.973: INFO: scanned /root for discovery docs: <nil>
Aug 17 03:40:53.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 17 03:40:55.062: INFO: stderr: ""
Aug 17 03:40:55.062: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:55.062
Aug 17 03:40:55.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:40:55.143: INFO: stderr: ""
Aug 17 03:40:55.143: INFO: stdout: "update-demo-nautilus-qf57m update-demo-nautilus-t5bmh "
Aug 17 03:40:55.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:55.223: INFO: stderr: ""
Aug 17 03:40:55.223: INFO: stdout: "true"
Aug 17 03:40:55.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:40:55.288: INFO: stderr: ""
Aug 17 03:40:55.288: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:40:55.288: INFO: validating pod update-demo-nautilus-qf57m
Aug 17 03:40:55.295: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:40:55.295: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:40:55.295: INFO: update-demo-nautilus-qf57m is verified up and running
Aug 17 03:40:55.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-t5bmh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:40:55.389: INFO: stderr: ""
Aug 17 03:40:55.389: INFO: stdout: "true"
Aug 17 03:40:55.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-t5bmh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:40:55.451: INFO: stderr: ""
Aug 17 03:40:55.451: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:40:55.451: INFO: validating pod update-demo-nautilus-t5bmh
Aug 17 03:40:55.543: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:40:55.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:40:55.543: INFO: update-demo-nautilus-t5bmh is verified up and running
STEP: using delete to clean up resources 08/17/23 03:40:55.543
Aug 17 03:40:55.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 delete --grace-period=0 --force -f -'
Aug 17 03:40:55.616: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:40:55.616: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 03:40:55.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get rc,svc -l name=update-demo --no-headers'
Aug 17 03:40:55.690: INFO: stderr: "No resources found in kubectl-6818 namespace.\n"
Aug 17 03:40:55.690: INFO: stdout: ""
Aug 17 03:40:55.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 03:40:55.765: INFO: stderr: ""
Aug 17 03:40:55.765: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:40:55.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6818" for this suite. 08/17/23 03:40:55.773
------------------------------
• [SLOW TEST] [19.789 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:35.993
    Aug 17 03:40:35.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:40:35.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:36.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:36.012
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/17/23 03:40:36.018
    Aug 17 03:40:36.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 create -f -'
    Aug 17 03:40:36.747: INFO: stderr: ""
    Aug 17 03:40:36.747: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:36.747
    Aug 17 03:40:36.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:36.845: INFO: stderr: ""
    Aug 17 03:40:36.845: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
    Aug 17 03:40:36.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:36.912: INFO: stderr: ""
    Aug 17 03:40:36.912: INFO: stdout: ""
    Aug 17 03:40:36.912: INFO: update-demo-nautilus-q8xnc is created but not running
    Aug 17 03:40:41.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:41.982: INFO: stderr: ""
    Aug 17 03:40:41.982: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
    Aug 17 03:40:41.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:42.051: INFO: stderr: ""
    Aug 17 03:40:42.051: INFO: stdout: ""
    Aug 17 03:40:42.051: INFO: update-demo-nautilus-q8xnc is created but not running
    Aug 17 03:40:47.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:47.118: INFO: stderr: ""
    Aug 17 03:40:47.118: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
    Aug 17 03:40:47.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:47.183: INFO: stderr: ""
    Aug 17 03:40:47.183: INFO: stdout: "true"
    Aug 17 03:40:47.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-q8xnc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:40:47.244: INFO: stderr: ""
    Aug 17 03:40:47.244: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:40:47.244: INFO: validating pod update-demo-nautilus-q8xnc
    Aug 17 03:40:47.336: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:40:47.336: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:40:47.336: INFO: update-demo-nautilus-q8xnc is verified up and running
    Aug 17 03:40:47.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:47.399: INFO: stderr: ""
    Aug 17 03:40:47.399: INFO: stdout: "true"
    Aug 17 03:40:47.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:40:47.462: INFO: stderr: ""
    Aug 17 03:40:47.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:40:47.462: INFO: validating pod update-demo-nautilus-qf57m
    Aug 17 03:40:47.553: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:40:47.553: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:40:47.553: INFO: update-demo-nautilus-qf57m is verified up and running
    STEP: scaling down the replication controller 08/17/23 03:40:47.553
    Aug 17 03:40:47.554: INFO: scanned /root for discovery docs: <nil>
    Aug 17 03:40:47.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 17 03:40:48.649: INFO: stderr: ""
    Aug 17 03:40:48.649: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:48.649
    Aug 17 03:40:48.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:48.762: INFO: stderr: ""
    Aug 17 03:40:48.762: INFO: stdout: "update-demo-nautilus-q8xnc update-demo-nautilus-qf57m "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/17/23 03:40:48.762
    Aug 17 03:40:53.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:53.832: INFO: stderr: ""
    Aug 17 03:40:53.832: INFO: stdout: "update-demo-nautilus-qf57m "
    Aug 17 03:40:53.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:53.898: INFO: stderr: ""
    Aug 17 03:40:53.898: INFO: stdout: "true"
    Aug 17 03:40:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:40:53.965: INFO: stderr: ""
    Aug 17 03:40:53.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:40:53.965: INFO: validating pod update-demo-nautilus-qf57m
    Aug 17 03:40:53.972: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:40:53.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:40:53.972: INFO: update-demo-nautilus-qf57m is verified up and running
    STEP: scaling up the replication controller 08/17/23 03:40:53.972
    Aug 17 03:40:53.973: INFO: scanned /root for discovery docs: <nil>
    Aug 17 03:40:53.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 17 03:40:55.062: INFO: stderr: ""
    Aug 17 03:40:55.062: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:40:55.062
    Aug 17 03:40:55.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:40:55.143: INFO: stderr: ""
    Aug 17 03:40:55.143: INFO: stdout: "update-demo-nautilus-qf57m update-demo-nautilus-t5bmh "
    Aug 17 03:40:55.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:55.223: INFO: stderr: ""
    Aug 17 03:40:55.223: INFO: stdout: "true"
    Aug 17 03:40:55.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-qf57m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:40:55.288: INFO: stderr: ""
    Aug 17 03:40:55.288: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:40:55.288: INFO: validating pod update-demo-nautilus-qf57m
    Aug 17 03:40:55.295: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:40:55.295: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:40:55.295: INFO: update-demo-nautilus-qf57m is verified up and running
    Aug 17 03:40:55.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-t5bmh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:40:55.389: INFO: stderr: ""
    Aug 17 03:40:55.389: INFO: stdout: "true"
    Aug 17 03:40:55.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods update-demo-nautilus-t5bmh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:40:55.451: INFO: stderr: ""
    Aug 17 03:40:55.451: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:40:55.451: INFO: validating pod update-demo-nautilus-t5bmh
    Aug 17 03:40:55.543: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:40:55.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:40:55.543: INFO: update-demo-nautilus-t5bmh is verified up and running
    STEP: using delete to clean up resources 08/17/23 03:40:55.543
    Aug 17 03:40:55.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 delete --grace-period=0 --force -f -'
    Aug 17 03:40:55.616: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:40:55.616: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 17 03:40:55.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get rc,svc -l name=update-demo --no-headers'
    Aug 17 03:40:55.690: INFO: stderr: "No resources found in kubectl-6818 namespace.\n"
    Aug 17 03:40:55.690: INFO: stdout: ""
    Aug 17 03:40:55.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-6818 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 03:40:55.765: INFO: stderr: ""
    Aug 17 03:40:55.765: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:40:55.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6818" for this suite. 08/17/23 03:40:55.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:40:55.784
Aug 17 03:40:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:40:55.785
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:55.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:55.804
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/17/23 03:40:55.809
Aug 17 03:40:55.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: rename a version 08/17/23 03:41:00.141
STEP: check the new version name is served 08/17/23 03:41:00.177
STEP: check the old version name is removed 08/17/23 03:41:01.674
STEP: check the other version is not changed 08/17/23 03:41:02.426
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3952" for this suite. 08/17/23 03:41:05.453
------------------------------
• [SLOW TEST] [9.677 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:40:55.784
    Aug 17 03:40:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 03:40:55.785
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:40:55.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:40:55.804
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/17/23 03:40:55.809
    Aug 17 03:40:55.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: rename a version 08/17/23 03:41:00.141
    STEP: check the new version name is served 08/17/23 03:41:00.177
    STEP: check the old version name is removed 08/17/23 03:41:01.674
    STEP: check the other version is not changed 08/17/23 03:41:02.426
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3952" for this suite. 08/17/23 03:41:05.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:05.463
Aug 17 03:41:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:41:05.464
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:05.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:05.484
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-7d61e41b-523d-4222-87b0-e9d957ba576b 08/17/23 03:41:05.49
STEP: Creating a pod to test consume secrets 08/17/23 03:41:05.497
Aug 17 03:41:05.508: INFO: Waiting up to 5m0s for pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2" in namespace "secrets-498" to be "Succeeded or Failed"
Aug 17 03:41:05.513: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878958ms
Aug 17 03:41:07.519: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010991256s
Aug 17 03:41:09.520: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011769319s
STEP: Saw pod success 08/17/23 03:41:09.52
Aug 17 03:41:09.520: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2" satisfied condition "Succeeded or Failed"
Aug 17 03:41:09.525: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 container secret-env-test: <nil>
STEP: delete the pod 08/17/23 03:41:09.576
Aug 17 03:41:09.590: INFO: Waiting for pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 to disappear
Aug 17 03:41:09.594: INFO: Pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-498" for this suite. 08/17/23 03:41:09.602
------------------------------
• [4.149 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:05.463
    Aug 17 03:41:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:41:05.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:05.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:05.484
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-7d61e41b-523d-4222-87b0-e9d957ba576b 08/17/23 03:41:05.49
    STEP: Creating a pod to test consume secrets 08/17/23 03:41:05.497
    Aug 17 03:41:05.508: INFO: Waiting up to 5m0s for pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2" in namespace "secrets-498" to be "Succeeded or Failed"
    Aug 17 03:41:05.513: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878958ms
    Aug 17 03:41:07.519: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010991256s
    Aug 17 03:41:09.520: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011769319s
    STEP: Saw pod success 08/17/23 03:41:09.52
    Aug 17 03:41:09.520: INFO: Pod "pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:09.525: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 container secret-env-test: <nil>
    STEP: delete the pod 08/17/23 03:41:09.576
    Aug 17 03:41:09.590: INFO: Waiting for pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 to disappear
    Aug 17 03:41:09.594: INFO: Pod pod-secrets-644fd7d7-9e11-4ee2-913a-36ad331c73c2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-498" for this suite. 08/17/23 03:41:09.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:09.618
Aug 17 03:41:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 03:41:09.619
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:09.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:09.642
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 17 03:41:09.669: INFO: Waiting up to 5m0s for pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c" in namespace "emptydir-wrapper-5412" to be "running and ready"
Aug 17 03:41:09.674: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138911ms
Aug 17 03:41:09.674: INFO: The phase of Pod pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:41:11.679: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010802268s
Aug 17 03:41:11.679: INFO: The phase of Pod pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c is Running (Ready = true)
Aug 17 03:41:11.679: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/17/23 03:41:11.684
STEP: Cleaning up the configmap 08/17/23 03:41:11.692
STEP: Cleaning up the pod 08/17/23 03:41:11.699
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5412" for this suite. 08/17/23 03:41:11.73
------------------------------
• [2.119 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:09.618
    Aug 17 03:41:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 03:41:09.619
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:09.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:09.642
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 17 03:41:09.669: INFO: Waiting up to 5m0s for pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c" in namespace "emptydir-wrapper-5412" to be "running and ready"
    Aug 17 03:41:09.674: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138911ms
    Aug 17 03:41:09.674: INFO: The phase of Pod pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:41:11.679: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010802268s
    Aug 17 03:41:11.679: INFO: The phase of Pod pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c is Running (Ready = true)
    Aug 17 03:41:11.679: INFO: Pod "pod-secrets-defc3b33-6568-4dbe-b5c1-71d0b0f5088c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/17/23 03:41:11.684
    STEP: Cleaning up the configmap 08/17/23 03:41:11.692
    STEP: Cleaning up the pod 08/17/23 03:41:11.699
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:11.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5412" for this suite. 08/17/23 03:41:11.73
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:11.738
Aug 17 03:41:11.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:41:11.739
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:11.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:11.76
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/17/23 03:41:11.766
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/17/23 03:41:11.772
STEP: patching the secret 08/17/23 03:41:11.777
STEP: deleting the secret using a LabelSelector 08/17/23 03:41:11.789
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/17/23 03:41:11.797
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1656" for this suite. 08/17/23 03:41:11.81
------------------------------
• [0.080 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:11.738
    Aug 17 03:41:11.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:41:11.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:11.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:11.76
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/17/23 03:41:11.766
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/17/23 03:41:11.772
    STEP: patching the secret 08/17/23 03:41:11.777
    STEP: deleting the secret using a LabelSelector 08/17/23 03:41:11.789
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/17/23 03:41:11.797
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1656" for this suite. 08/17/23 03:41:11.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:11.82
Aug 17 03:41:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context-test 08/17/23 03:41:11.821
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:11.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:11.844
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 17 03:41:11.862: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22" in namespace "security-context-test-3430" to be "Succeeded or Failed"
Aug 17 03:41:11.866: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.164025ms
Aug 17 03:41:13.874: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012430111s
Aug 17 03:41:15.872: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010837272s
Aug 17 03:41:15.872: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22" satisfied condition "Succeeded or Failed"
Aug 17 03:41:15.884: INFO: Got logs for pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:15.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3430" for this suite. 08/17/23 03:41:15.892
------------------------------
• [4.081 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:11.82
    Aug 17 03:41:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context-test 08/17/23 03:41:11.821
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:11.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:11.844
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 17 03:41:11.862: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22" in namespace "security-context-test-3430" to be "Succeeded or Failed"
    Aug 17 03:41:11.866: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.164025ms
    Aug 17 03:41:13.874: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012430111s
    Aug 17 03:41:15.872: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010837272s
    Aug 17 03:41:15.872: INFO: Pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:15.884: INFO: Got logs for pod "busybox-privileged-false-74a05941-54fc-4057-a250-523431958c22": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:15.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3430" for this suite. 08/17/23 03:41:15.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:15.902
Aug 17 03:41:15.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:41:15.902
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:15.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:15.921
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 17 03:41:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:16.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-193" for this suite. 08/17/23 03:41:16.477
------------------------------
• [0.583 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:15.902
    Aug 17 03:41:15.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:41:15.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:15.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:15.921
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 17 03:41:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:16.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-193" for this suite. 08/17/23 03:41:16.477
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:16.49
Aug 17 03:41:16.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 03:41:16.491
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:16.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:16.51
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 17 03:41:16.516: INFO: Creating simple deployment test-new-deployment
Aug 17 03:41:16.530: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/17/23 03:41:18.551
STEP: updating a scale subresource 08/17/23 03:41:18.556
STEP: verifying the deployment Spec.Replicas was modified 08/17/23 03:41:18.564
STEP: Patch a scale subresource 08/17/23 03:41:18.568
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 03:41:18.585: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4885  6e001a9b-ed05-4daf-bd0d-00a71c00c684 227073 3 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-17 03:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008e4278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 03:41:18 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-17 03:41:18 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 03:41:18.589: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4885  fd12f055-5abe-4a6f-a233-8e020067f562 227079 3 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6e001a9b-ed05-4daf-bd0d-00a71c00c684 0xc0008e4677 0xc0008e4678}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e001a9b-ed05-4daf-bd0d-00a71c00c684\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008e4708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 03:41:18.596: INFO: Pod "test-new-deployment-7f5969cbc7-5c767" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5c767 test-new-deployment-7f5969cbc7- deployment-4885  781a008b-40f3-44b8-9916-bc02c2d10222 227076 0 2023-08-17 03:41:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fd12f055-5abe-4a6f-a233-8e020067f562 0xc0008e55b7 0xc0008e55b8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd12f055-5abe-4a6f-a233-8e020067f562\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn5ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn5ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:18.596: INFO: Pod "test-new-deployment-7f5969cbc7-qhpg7" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qhpg7 test-new-deployment-7f5969cbc7- deployment-4885  598e50d2-57f3-484f-9c64-86a985ff6cea 227065 0 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c700925c17a8f71e74a0b17f6751166d859edaea40ed229527ce3a05853a58c cni.projectcalico.org/podIP:172.21.15.87/32 cni.projectcalico.org/podIPs:172.21.15.87/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fd12f055-5abe-4a6f-a233-8e020067f562 0xc0008e5c40 0xc0008e5c41}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd12f055-5abe-4a6f-a233-8e020067f562\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbl45,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbl45,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.87,StartTime:2023-08-17 03:41:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aeb6a2d50fb9b2790af5edbd79a1bb1c0a203879d5d1f6518399da4226b86887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:18.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4885" for this suite. 08/17/23 03:41:18.603
------------------------------
• [2.122 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:16.49
    Aug 17 03:41:16.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 03:41:16.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:16.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:16.51
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 17 03:41:16.516: INFO: Creating simple deployment test-new-deployment
    Aug 17 03:41:16.530: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/17/23 03:41:18.551
    STEP: updating a scale subresource 08/17/23 03:41:18.556
    STEP: verifying the deployment Spec.Replicas was modified 08/17/23 03:41:18.564
    STEP: Patch a scale subresource 08/17/23 03:41:18.568
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 03:41:18.585: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4885  6e001a9b-ed05-4daf-bd0d-00a71c00c684 227073 3 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-17 03:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008e4278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 03:41:18 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-17 03:41:18 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 03:41:18.589: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4885  fd12f055-5abe-4a6f-a233-8e020067f562 227079 3 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6e001a9b-ed05-4daf-bd0d-00a71c00c684 0xc0008e4677 0xc0008e4678}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e001a9b-ed05-4daf-bd0d-00a71c00c684\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008e4708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 03:41:18.596: INFO: Pod "test-new-deployment-7f5969cbc7-5c767" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-5c767 test-new-deployment-7f5969cbc7- deployment-4885  781a008b-40f3-44b8-9916-bc02c2d10222 227076 0 2023-08-17 03:41:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fd12f055-5abe-4a6f-a233-8e020067f562 0xc0008e55b7 0xc0008e55b8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd12f055-5abe-4a6f-a233-8e020067f562\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tn5ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tn5ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:18.596: INFO: Pod "test-new-deployment-7f5969cbc7-qhpg7" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qhpg7 test-new-deployment-7f5969cbc7- deployment-4885  598e50d2-57f3-484f-9c64-86a985ff6cea 227065 0 2023-08-17 03:41:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c700925c17a8f71e74a0b17f6751166d859edaea40ed229527ce3a05853a58c cni.projectcalico.org/podIP:172.21.15.87/32 cni.projectcalico.org/podIPs:172.21.15.87/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 fd12f055-5abe-4a6f-a233-8e020067f562 0xc0008e5c40 0xc0008e5c41}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd12f055-5abe-4a6f-a233-8e020067f562\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbl45,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbl45,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.87,StartTime:2023-08-17 03:41:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aeb6a2d50fb9b2790af5edbd79a1bb1c0a203879d5d1f6518399da4226b86887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:18.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4885" for this suite. 08/17/23 03:41:18.603
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:18.615
Aug 17 03:41:18.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 03:41:18.617
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:18.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:18.639
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/17/23 03:41:18.643
STEP: getting 08/17/23 03:41:18.661
STEP: listing in namespace 08/17/23 03:41:18.665
STEP: patching 08/17/23 03:41:18.67
STEP: deleting 08/17/23 03:41:18.679
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:18.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5782" for this suite. 08/17/23 03:41:18.701
------------------------------
• [0.094 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:18.615
    Aug 17 03:41:18.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 03:41:18.617
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:18.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:18.639
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/17/23 03:41:18.643
    STEP: getting 08/17/23 03:41:18.661
    STEP: listing in namespace 08/17/23 03:41:18.665
    STEP: patching 08/17/23 03:41:18.67
    STEP: deleting 08/17/23 03:41:18.679
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:18.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5782" for this suite. 08/17/23 03:41:18.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:18.711
Aug 17 03:41:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:41:18.712
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:18.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:18.732
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:41:18.754
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:41:19.555
STEP: Deploying the webhook pod 08/17/23 03:41:19.565
STEP: Wait for the deployment to be ready 08/17/23 03:41:19.578
Aug 17 03:41:19.589: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:41:21.602
STEP: Verifying the service has paired with the endpoint 08/17/23 03:41:21.614
Aug 17 03:41:22.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/17/23 03:41:22.62
STEP: create a pod that should be updated by the webhook 08/17/23 03:41:22.726
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:22.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8525" for this suite. 08/17/23 03:41:22.876
STEP: Destroying namespace "webhook-8525-markers" for this suite. 08/17/23 03:41:22.882
------------------------------
• [4.178 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:18.711
    Aug 17 03:41:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:41:18.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:18.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:18.732
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:41:18.754
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:41:19.555
    STEP: Deploying the webhook pod 08/17/23 03:41:19.565
    STEP: Wait for the deployment to be ready 08/17/23 03:41:19.578
    Aug 17 03:41:19.589: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:41:21.602
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:41:21.614
    Aug 17 03:41:22.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/17/23 03:41:22.62
    STEP: create a pod that should be updated by the webhook 08/17/23 03:41:22.726
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:22.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8525" for this suite. 08/17/23 03:41:22.876
    STEP: Destroying namespace "webhook-8525-markers" for this suite. 08/17/23 03:41:22.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:22.892
Aug 17 03:41:22.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:41:22.894
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:22.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:22.913
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-586628b1-497e-43a8-b8ad-cdce24e26b19 08/17/23 03:41:22.917
STEP: Creating a pod to test consume secrets 08/17/23 03:41:22.924
Aug 17 03:41:22.933: INFO: Waiting up to 5m0s for pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b" in namespace "secrets-2427" to be "Succeeded or Failed"
Aug 17 03:41:22.938: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075413ms
Aug 17 03:41:24.943: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496283s
Aug 17 03:41:26.943: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009818099s
STEP: Saw pod success 08/17/23 03:41:26.943
Aug 17 03:41:26.944: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b" satisfied condition "Succeeded or Failed"
Aug 17 03:41:26.948: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:41:26.958
Aug 17 03:41:26.970: INFO: Waiting for pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b to disappear
Aug 17 03:41:26.974: INFO: Pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:26.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2427" for this suite. 08/17/23 03:41:26.981
------------------------------
• [4.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:22.892
    Aug 17 03:41:22.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:41:22.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:22.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:22.913
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-586628b1-497e-43a8-b8ad-cdce24e26b19 08/17/23 03:41:22.917
    STEP: Creating a pod to test consume secrets 08/17/23 03:41:22.924
    Aug 17 03:41:22.933: INFO: Waiting up to 5m0s for pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b" in namespace "secrets-2427" to be "Succeeded or Failed"
    Aug 17 03:41:22.938: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075413ms
    Aug 17 03:41:24.943: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496283s
    Aug 17 03:41:26.943: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009818099s
    STEP: Saw pod success 08/17/23 03:41:26.943
    Aug 17 03:41:26.944: INFO: Pod "pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:26.948: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:41:26.958
    Aug 17 03:41:26.970: INFO: Waiting for pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b to disappear
    Aug 17 03:41:26.974: INFO: Pod pod-secrets-cde6b5f0-1cea-4ac8-a346-ec2a3f98315b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:26.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2427" for this suite. 08/17/23 03:41:26.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:26.992
Aug 17 03:41:26.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:41:26.992
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:27.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:27.011
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/17/23 03:41:27.016
Aug 17 03:41:27.026: INFO: Waiting up to 5m0s for pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b" in namespace "projected-963" to be "running and ready"
Aug 17 03:41:27.031: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.403754ms
Aug 17 03:41:27.031: INFO: The phase of Pod annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:41:29.036: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009627469s
Aug 17 03:41:29.036: INFO: The phase of Pod annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b is Running (Ready = true)
Aug 17 03:41:29.036: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b" satisfied condition "running and ready"
Aug 17 03:41:29.565: INFO: Successfully updated pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:33.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-963" for this suite. 08/17/23 03:41:33.654
------------------------------
• [SLOW TEST] [6.670 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:26.992
    Aug 17 03:41:26.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:41:26.992
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:27.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:27.011
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/17/23 03:41:27.016
    Aug 17 03:41:27.026: INFO: Waiting up to 5m0s for pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b" in namespace "projected-963" to be "running and ready"
    Aug 17 03:41:27.031: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.403754ms
    Aug 17 03:41:27.031: INFO: The phase of Pod annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:41:29.036: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009627469s
    Aug 17 03:41:29.036: INFO: The phase of Pod annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b is Running (Ready = true)
    Aug 17 03:41:29.036: INFO: Pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b" satisfied condition "running and ready"
    Aug 17 03:41:29.565: INFO: Successfully updated pod "annotationupdatec609a2e1-dc48-4cec-80a3-919dd785071b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:33.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-963" for this suite. 08/17/23 03:41:33.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:33.663
Aug 17 03:41:33.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context-test 08/17/23 03:41:33.663
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:33.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:33.683
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 17 03:41:33.696: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b" in namespace "security-context-test-4371" to be "Succeeded or Failed"
Aug 17 03:41:33.701: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240787ms
Aug 17 03:41:35.708: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011380566s
Aug 17 03:41:37.707: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01061611s
Aug 17 03:41:37.707: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:37.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4371" for this suite. 08/17/23 03:41:37.714
------------------------------
• [4.059 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:33.663
    Aug 17 03:41:33.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context-test 08/17/23 03:41:33.663
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:33.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:33.683
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 17 03:41:33.696: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b" in namespace "security-context-test-4371" to be "Succeeded or Failed"
    Aug 17 03:41:33.701: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240787ms
    Aug 17 03:41:35.708: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011380566s
    Aug 17 03:41:37.707: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01061611s
    Aug 17 03:41:37.707: INFO: Pod "busybox-readonly-false-4f7a2edd-456b-4162-ae14-60170fd53b1b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:37.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4371" for this suite. 08/17/23 03:41:37.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:37.724
Aug 17 03:41:37.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:41:37.724
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:37.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:37.745
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 03:41:37.749
Aug 17 03:41:37.761: INFO: Waiting up to 5m0s for pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95" in namespace "emptydir-528" to be "Succeeded or Failed"
Aug 17 03:41:37.765: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856127ms
Aug 17 03:41:39.770: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009823243s
Aug 17 03:41:41.772: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011952244s
STEP: Saw pod success 08/17/23 03:41:41.773
Aug 17 03:41:41.773: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95" satisfied condition "Succeeded or Failed"
Aug 17 03:41:41.777: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 container test-container: <nil>
STEP: delete the pod 08/17/23 03:41:41.786
Aug 17 03:41:41.796: INFO: Waiting for pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 to disappear
Aug 17 03:41:41.801: INFO: Pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-528" for this suite. 08/17/23 03:41:41.807
------------------------------
• [4.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:37.724
    Aug 17 03:41:37.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:41:37.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:37.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:37.745
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 03:41:37.749
    Aug 17 03:41:37.761: INFO: Waiting up to 5m0s for pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95" in namespace "emptydir-528" to be "Succeeded or Failed"
    Aug 17 03:41:37.765: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.856127ms
    Aug 17 03:41:39.770: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009823243s
    Aug 17 03:41:41.772: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011952244s
    STEP: Saw pod success 08/17/23 03:41:41.773
    Aug 17 03:41:41.773: INFO: Pod "pod-142a59d7-637b-4364-936b-a3dd6bff2f95" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:41.777: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 container test-container: <nil>
    STEP: delete the pod 08/17/23 03:41:41.786
    Aug 17 03:41:41.796: INFO: Waiting for pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 to disappear
    Aug 17 03:41:41.801: INFO: Pod pod-142a59d7-637b-4364-936b-a3dd6bff2f95 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:41.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-528" for this suite. 08/17/23 03:41:41.807
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:41.816
Aug 17 03:41:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:41:41.817
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:41.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:41.838
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:41:41.844
Aug 17 03:41:41.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084" in namespace "projected-190" to be "Succeeded or Failed"
Aug 17 03:41:41.858: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378116ms
Aug 17 03:41:43.864: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011102963s
Aug 17 03:41:45.865: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012627425s
STEP: Saw pod success 08/17/23 03:41:45.865
Aug 17 03:41:45.866: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084" satisfied condition "Succeeded or Failed"
Aug 17 03:41:45.871: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 container client-container: <nil>
STEP: delete the pod 08/17/23 03:41:45.88
Aug 17 03:41:45.893: INFO: Waiting for pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 to disappear
Aug 17 03:41:45.896: INFO: Pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:45.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-190" for this suite. 08/17/23 03:41:45.904
------------------------------
• [4.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:41.816
    Aug 17 03:41:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:41:41.817
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:41.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:41.838
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:41:41.844
    Aug 17 03:41:41.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084" in namespace "projected-190" to be "Succeeded or Failed"
    Aug 17 03:41:41.858: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378116ms
    Aug 17 03:41:43.864: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011102963s
    Aug 17 03:41:45.865: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012627425s
    STEP: Saw pod success 08/17/23 03:41:45.865
    Aug 17 03:41:45.866: INFO: Pod "downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:45.871: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:41:45.88
    Aug 17 03:41:45.893: INFO: Waiting for pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 to disappear
    Aug 17 03:41:45.896: INFO: Pod downwardapi-volume-687ba4b0-912a-422f-9b23-3caafad36084 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:45.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-190" for this suite. 08/17/23 03:41:45.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:45.914
Aug 17 03:41:45.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:41:45.914
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:45.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:45.932
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/17/23 03:41:45.936
STEP: getting /apis/node.k8s.io 08/17/23 03:41:45.942
STEP: getting /apis/node.k8s.io/v1 08/17/23 03:41:45.944
STEP: creating 08/17/23 03:41:45.946
STEP: watching 08/17/23 03:41:45.963
Aug 17 03:41:45.963: INFO: starting watch
STEP: getting 08/17/23 03:41:45.97
STEP: listing 08/17/23 03:41:45.974
STEP: patching 08/17/23 03:41:45.977
STEP: updating 08/17/23 03:41:45.982
Aug 17 03:41:45.988: INFO: waiting for watch events with expected annotations
STEP: deleting 08/17/23 03:41:45.988
STEP: deleting a collection 08/17/23 03:41:46.001
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:46.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7784" for this suite. 08/17/23 03:41:46.02
------------------------------
• [0.114 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:45.914
    Aug 17 03:41:45.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:41:45.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:45.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:45.932
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/17/23 03:41:45.936
    STEP: getting /apis/node.k8s.io 08/17/23 03:41:45.942
    STEP: getting /apis/node.k8s.io/v1 08/17/23 03:41:45.944
    STEP: creating 08/17/23 03:41:45.946
    STEP: watching 08/17/23 03:41:45.963
    Aug 17 03:41:45.963: INFO: starting watch
    STEP: getting 08/17/23 03:41:45.97
    STEP: listing 08/17/23 03:41:45.974
    STEP: patching 08/17/23 03:41:45.977
    STEP: updating 08/17/23 03:41:45.982
    Aug 17 03:41:45.988: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/17/23 03:41:45.988
    STEP: deleting a collection 08/17/23 03:41:46.001
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:46.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7784" for this suite. 08/17/23 03:41:46.02
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:46.029
Aug 17 03:41:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:41:46.03
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:46.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:46.051
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 03:41:46.057
Aug 17 03:41:46.067: INFO: Waiting up to 5m0s for pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d" in namespace "emptydir-9481" to be "Succeeded or Failed"
Aug 17 03:41:46.072: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845921ms
Aug 17 03:41:48.077: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010260812s
Aug 17 03:41:50.078: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0113027s
STEP: Saw pod success 08/17/23 03:41:50.078
Aug 17 03:41:50.079: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d" satisfied condition "Succeeded or Failed"
Aug 17 03:41:50.083: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d container test-container: <nil>
STEP: delete the pod 08/17/23 03:41:50.092
Aug 17 03:41:50.106: INFO: Waiting for pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d to disappear
Aug 17 03:41:50.110: INFO: Pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:50.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9481" for this suite. 08/17/23 03:41:50.117
------------------------------
• [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:46.029
    Aug 17 03:41:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:41:46.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:46.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:46.051
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 03:41:46.057
    Aug 17 03:41:46.067: INFO: Waiting up to 5m0s for pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d" in namespace "emptydir-9481" to be "Succeeded or Failed"
    Aug 17 03:41:46.072: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845921ms
    Aug 17 03:41:48.077: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010260812s
    Aug 17 03:41:50.078: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0113027s
    STEP: Saw pod success 08/17/23 03:41:50.078
    Aug 17 03:41:50.079: INFO: Pod "pod-9ed441e4-eb0c-451a-8ea2-72241eed704d" satisfied condition "Succeeded or Failed"
    Aug 17 03:41:50.083: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d container test-container: <nil>
    STEP: delete the pod 08/17/23 03:41:50.092
    Aug 17 03:41:50.106: INFO: Waiting for pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d to disappear
    Aug 17 03:41:50.110: INFO: Pod pod-9ed441e4-eb0c-451a-8ea2-72241eed704d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:50.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9481" for this suite. 08/17/23 03:41:50.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:50.125
Aug 17 03:41:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 03:41:50.126
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:50.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:50.143
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 17 03:41:50.148: INFO: Creating deployment "webserver-deployment"
Aug 17 03:41:50.154: INFO: Waiting for observed generation 1
Aug 17 03:41:52.164: INFO: Waiting for all required pods to come up
Aug 17 03:41:52.172: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/17/23 03:41:52.172
Aug 17 03:41:52.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zznc6" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6nkvr" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bqbd" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cj7lf" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dxtbm" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ffgqf" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fjzwd" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g2s2n" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k2gsv" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zvcm9" in namespace "deployment-2706" to be "running"
Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.83566ms
Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.10093ms
Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.877584ms
Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.134086ms
Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991824ms
Aug 17 03:41:52.181: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.552782ms
Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.686846ms
Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.726048ms
Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.149401ms
Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr": Phase="Pending", Reason="", readiness=false. Elapsed: 13.056648ms
Aug 17 03:41:54.185: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.012528563s
Aug 17 03:41:54.185: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf" satisfied condition "running"
Aug 17 03:41:54.186: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012622412s
Aug 17 03:41:54.186: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9" satisfied condition "running"
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013965418s
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd" satisfied condition "running"
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm": Phase="Running", Reason="", readiness=true. Elapsed: 2.014127489s
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm" satisfied condition "running"
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014100414s
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n" satisfied condition "running"
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv": Phase="Running", Reason="", readiness=true. Elapsed: 2.014081592s
Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv" satisfied condition "running"
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017766499s
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6" satisfied condition "running"
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.017610457s
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd" satisfied condition "running"
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017598435s
Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf" satisfied condition "running"
Aug 17 03:41:54.191: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018940762s
Aug 17 03:41:54.191: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr" satisfied condition "running"
Aug 17 03:41:54.191: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 17 03:41:54.200: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 17 03:41:54.211: INFO: Updating deployment webserver-deployment
Aug 17 03:41:54.211: INFO: Waiting for observed generation 2
Aug 17 03:41:56.221: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 17 03:41:56.226: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 17 03:41:56.231: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 03:41:56.241: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 17 03:41:56.241: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 17 03:41:56.245: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 17 03:41:56.251: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 17 03:41:56.251: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 17 03:41:56.262: INFO: Updating deployment webserver-deployment
Aug 17 03:41:56.262: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 17 03:41:56.270: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 17 03:41:56.273: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 03:41:56.282: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2706  01549479-3ca4-4421-b8d1-08875e6435ea 227791 3 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e6bb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-17 03:41:54 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 03:41:56 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 17 03:41:56.286: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2706  f8e1860e-1c82-4e80-93c9-be29900003f9 227785 3 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 01549479-3ca4-4421-b8d1-08875e6435ea 0xc005372007 0xc005372008}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01549479-3ca4-4421-b8d1-08875e6435ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053720a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 03:41:56.286: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 17 03:41:56.286: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2706  b082f1fe-11a8-48f7-b07f-58404b0c0283 227782 3 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 01549479-3ca4-4421-b8d1-08875e6435ea 0xc000e6bf17 0xc000e6bf18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01549479-3ca4-4421-b8d1-08875e6435ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e6bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-26vmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-26vmd webserver-deployment-7f5969cbc7- deployment-2706  f501ac32-7aa6-47db-b4e0-38efe49c733a 227787 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b367 0xc00397b368}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwg8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwg8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-49ptp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-49ptp webserver-deployment-7f5969cbc7- deployment-2706  0992ffb8-64eb-4dce-86c5-653518ab86c0 227799 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b5c0 0xc00397b5c1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8thg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8thg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-4ft4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4ft4z webserver-deployment-7f5969cbc7- deployment-2706  5284b0d1-544d-457b-92ad-fbbb8981dd07 227812 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b710 0xc00397b711}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh7dt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh7dt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-5mhlp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5mhlp webserver-deployment-7f5969cbc7- deployment-2706  76af3df5-b07a-4b1c-8a2f-5dfc558f3293 227793 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b870 0xc00397b871}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kr7jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kr7jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-6ckp9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6ckp9 webserver-deployment-7f5969cbc7- deployment-2706  8b3dd66d-a1e6-4020-a267-9d672b08ef24 227814 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b9e0 0xc00397b9e1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb64b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb64b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9bqbd webserver-deployment-7f5969cbc7- deployment-2706  794f7918-afef-4925-a4cd-7451ef6925f3 227655 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c9982392e2734f2391e05a17f1a9e6c043acae14006aa03b014ed2dc21710463 cni.projectcalico.org/podIP:172.21.15.115/32 cni.projectcalico.org/podIPs:172.21.15.115/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397bb50 0xc00397bb51}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcq7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcq7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.115,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cff6c8da22a089f178f504433ab0b185eed4b60e226c43df57f6b9606a09a5eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-cbpmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbpmn webserver-deployment-7f5969cbc7- deployment-2706  bffd02f6-90a8-4f9d-a95a-36f5a6548b86 227808 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397bd47 0xc00397bd48}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l49g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l49g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dxtbm webserver-deployment-7f5969cbc7- deployment-2706  8d59fbb4-d7aa-4b76-9c34-72ea24241e45 227685 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:293b92f6cb6315cfe1bb5f75395a14620479367baf33202f12c804e005044958 cni.projectcalico.org/podIP:172.21.86.150/32 cni.projectcalico.org/podIPs:172.21.86.150/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397beb0 0xc00397beb1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4smnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4smnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.150,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff835a023b63c95bdea77693d17583058cdeebba550407892b4e138934a3cc2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ffgqf webserver-deployment-7f5969cbc7- deployment-2706  51280101-e350-436c-a954-673d81795248 227669 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aeb3718709b2de82c630b7ef5bbca682520db9ea17b55157e06c9587964de044 cni.projectcalico.org/podIP:172.21.86.180/32 cni.projectcalico.org/podIPs:172.21.86.180/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace0d7 0xc004ace0d8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lvfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lvfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.180,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ed3d82b2339f72e95490f037d55b0d1a0e6658514106e7632c560e903fdc95c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjzwd webserver-deployment-7f5969cbc7- deployment-2706  51310561-4581-47ff-b248-e5f2a39b8320 227667 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bcc52085f05adb33a3792c9a84551d0fe41757f10539bff857587e7008bc4578 cni.projectcalico.org/podIP:172.21.15.71/32 cni.projectcalico.org/podIPs:172.21.15.71/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace2f7 0xc004ace2f8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzjrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzjrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.71,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ec9182ac31eaf732152b2f66d85881a1caf32c60f9d088a64855b8a680e4d5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g2s2n webserver-deployment-7f5969cbc7- deployment-2706  49417edd-05d2-47c1-873a-95815c83d744 227640 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:690553deada6b00dae6b380527d1f932f6991bb473c8e92493b880ea6b02e749 cni.projectcalico.org/podIP:172.21.86.182/32 cni.projectcalico.org/podIPs:172.21.86.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace517 0xc004ace518}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdbv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdbv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.182,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2aa3334049d184b46f3e4885cc73458a7fc6b6a5d2b2e7650ac7addedd9540e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k2gsv webserver-deployment-7f5969cbc7- deployment-2706  fc4e41a8-51cf-4e19-afb9-ac6a97b8eb44 227677 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2ddf7cc9cd8f729236d86b2a325cba5e03f0077f5909cdb49602a25f09dcfd95 cni.projectcalico.org/podIP:172.21.86.135/32 cni.projectcalico.org/podIPs:172.21.86.135/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004aced77 0xc004aced78}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r26xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r26xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.135,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://35163c28772ab8fa64697007cddd447b7052b3fe49a318762ef42964520d3b71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-l56fp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l56fp webserver-deployment-7f5969cbc7- deployment-2706  55a9d622-0b85-4821-bb37-8c241e765e54 227816 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acef87 0xc004acef88}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhvhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhvhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-lvkwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lvkwk webserver-deployment-7f5969cbc7- deployment-2706  66cbf246-0b1c-4e85-8c5d-f1d566980237 227810 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf0e0 0xc004acf0e1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzfs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzfs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-p8w88" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p8w88 webserver-deployment-7f5969cbc7- deployment-2706  3dd9edcb-4171-43c8-adb1-24ba6fa37767 227807 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf220 0xc004acf221}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sb8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sb8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-pppck" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pppck webserver-deployment-7f5969cbc7- deployment-2706  30d2104f-d6e8-4f1a-b61e-64d7b76a4d1f 227809 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf360 0xc004acf361}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcmb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcmb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-vv5l4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vv5l4 webserver-deployment-7f5969cbc7- deployment-2706  7dedd5c4-b31e-47af-8b6b-b5f4d7197a09 227821 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf4a0 0xc004acf4a1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxlc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxlc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-xj4rw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xj4rw webserver-deployment-7f5969cbc7- deployment-2706  748a420f-33d2-4580-a9bd-afd18836a814 227806 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf600 0xc004acf601}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxrkr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxrkr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvcm9 webserver-deployment-7f5969cbc7- deployment-2706  9d60dd57-5bcf-4fe8-8e46-509c8d7c1029 227650 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:897772a9133372c581003954516b7ddabab7666a07dcff38cd81cc1a6b52842e cni.projectcalico.org/podIP:172.21.15.118/32 cni.projectcalico.org/podIPs:172.21.15.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf760 0xc004acf761}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2p9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2p9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.118,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d087aeeb3c6bafa573da03985c804de79afe5b4c206c741b661d2fbb98c9b08b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zznc6 webserver-deployment-7f5969cbc7- deployment-2706  b9df23f5-e4c3-459e-9891-ec0ac8ed72a0 227657 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ddf190b8e7e99f96cbf48767e6c44928c490018335d9bc13505f615589c3d992 cni.projectcalico.org/podIP:172.21.86.172/32 cni.projectcalico.org/podIPs:172.21.86.172/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf987 0xc004acf988}] [] [{calico Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-829cb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-829cb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.172,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3514e0d79f8e0c885b24b3c9db91a24f3cddf754dd5a74c35f3da1afda4a810a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-2h99c" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2h99c webserver-deployment-d9f79cb5- deployment-2706  a246653e-8a3d-4edb-b482-586145de0909 227742 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:130e66f7d0be4125f04d72664d9b3c72499a4f44a2036a20c6ac49902c3fbfef cni.projectcalico.org/podIP:172.21.86.156/32 cni.projectcalico.org/podIPs:172.21.86.156/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfb97 0xc004acfb98}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr8ds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr8ds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-4mtzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4mtzn webserver-deployment-d9f79cb5- deployment-2706  59a37595-272f-4935-abbd-f4a84ef37d82 227815 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfd97 0xc004acfd98}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7ztg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7ztg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-64n8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-64n8d webserver-deployment-d9f79cb5- deployment-2706  601bed6d-868b-41b5-9e96-7f2d35f4fe40 227822 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfedf 0xc004acfef0}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m76mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m76mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-68vjh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-68vjh webserver-deployment-d9f79cb5- deployment-2706  2ac2a847-1b8b-43ad-bc94-a051517a8061 227817 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492803f 0xc004928050}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6jgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6jgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-6tqjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6tqjn webserver-deployment-d9f79cb5- deployment-2706  a67dae70-6ebb-4125-8274-4099ef7ef545 227760 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:689a939e0e4b06790b64092a440ba6662a6c549b41c450534a1fcf509b32d99b cni.projectcalico.org/podIP:172.21.86.137/32 cni.projectcalico.org/podIPs:172.21.86.137/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492819f 0xc0049281d0}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfzlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfzlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-8b56d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8b56d webserver-deployment-d9f79cb5- deployment-2706  c74e3c43-2423-4127-871c-f6e733d8d4dc 227818 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc0049283c7 0xc0049283c8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2m2s4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2m2s4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-dsvmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dsvmx webserver-deployment-d9f79cb5- deployment-2706  4ca430c4-0be7-49f8-979a-a41edfbafb6e 227813 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492850f 0xc004928520}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cww4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cww4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-lt8wg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lt8wg webserver-deployment-d9f79cb5- deployment-2706  5359f728-f4d4-4836-b37c-03219be906e2 227773 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3813af664e45f6f5b284fc965d055232177ee2facca1ced985943405c73e57ea cni.projectcalico.org/podIP:172.21.15.77/32 cni.projectcalico.org/podIPs:172.21.15.77/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492865f 0xc004928690}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xn97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xn97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-srt57" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-srt57 webserver-deployment-d9f79cb5- deployment-2706  4c425fd7-4b17-4284-8429-4a0fbefdd0ac 227819 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928887 0xc004928888}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr4k9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr4k9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-tjsvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tjsvh webserver-deployment-d9f79cb5- deployment-2706  4ff0dc8c-b69a-48cc-b0c5-b4a0ca24dc5a 227765 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a59e16d0361c6af0610e7f5f7f502562abd0a5c6abf6e7ad4adc9b5a97e42b5e cni.projectcalico.org/podIP:172.21.15.75/32 cni.projectcalico.org/podIPs:172.21.15.75/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc0049289cf 0xc004928a00}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-xjj9t" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xjj9t webserver-deployment-d9f79cb5- deployment-2706  1fab22f9-c053-47c7-886e-42181ab3131d 227800 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928bf7 0xc004928bf8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkfks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkfks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-xlv4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xlv4x webserver-deployment-d9f79cb5- deployment-2706  f953cdd3-46b8-4b7a-8b18-bf26aae4b852 227754 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9e2d6ce40d61c46f1e153dc2afd813a90b76de1ed040f323d887e8e5e22c991b cni.projectcalico.org/podIP:172.21.15.100/32 cni.projectcalico.org/podIPs:172.21.15.100/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928d4f 0xc004928d80}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdljj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdljj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 03:41:56.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2706" for this suite. 08/17/23 03:41:56.313
------------------------------
• [SLOW TEST] [6.196 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:50.125
    Aug 17 03:41:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 03:41:50.126
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:50.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:50.143
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 17 03:41:50.148: INFO: Creating deployment "webserver-deployment"
    Aug 17 03:41:50.154: INFO: Waiting for observed generation 1
    Aug 17 03:41:52.164: INFO: Waiting for all required pods to come up
    Aug 17 03:41:52.172: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/17/23 03:41:52.172
    Aug 17 03:41:52.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zznc6" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6nkvr" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bqbd" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cj7lf" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dxtbm" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ffgqf" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fjzwd" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g2s2n" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k2gsv" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-zvcm9" in namespace "deployment-2706" to be "running"
    Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.83566ms
    Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.10093ms
    Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.877584ms
    Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.134086ms
    Aug 17 03:41:52.180: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991824ms
    Aug 17 03:41:52.181: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.552782ms
    Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.686846ms
    Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.726048ms
    Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.149401ms
    Aug 17 03:41:52.185: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr": Phase="Pending", Reason="", readiness=false. Elapsed: 13.056648ms
    Aug 17 03:41:54.185: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.012528563s
    Aug 17 03:41:54.185: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf" satisfied condition "running"
    Aug 17 03:41:54.186: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012622412s
    Aug 17 03:41:54.186: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9" satisfied condition "running"
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013965418s
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd" satisfied condition "running"
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm": Phase="Running", Reason="", readiness=true. Elapsed: 2.014127489s
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm" satisfied condition "running"
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014100414s
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n" satisfied condition "running"
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv": Phase="Running", Reason="", readiness=true. Elapsed: 2.014081592s
    Aug 17 03:41:54.187: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv" satisfied condition "running"
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017766499s
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6" satisfied condition "running"
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.017610457s
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd" satisfied condition "running"
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017598435s
    Aug 17 03:41:54.190: INFO: Pod "webserver-deployment-7f5969cbc7-cj7lf" satisfied condition "running"
    Aug 17 03:41:54.191: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018940762s
    Aug 17 03:41:54.191: INFO: Pod "webserver-deployment-7f5969cbc7-6nkvr" satisfied condition "running"
    Aug 17 03:41:54.191: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 17 03:41:54.200: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 17 03:41:54.211: INFO: Updating deployment webserver-deployment
    Aug 17 03:41:54.211: INFO: Waiting for observed generation 2
    Aug 17 03:41:56.221: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 17 03:41:56.226: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 17 03:41:56.231: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 03:41:56.241: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 17 03:41:56.241: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 17 03:41:56.245: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 03:41:56.251: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 17 03:41:56.251: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 17 03:41:56.262: INFO: Updating deployment webserver-deployment
    Aug 17 03:41:56.262: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 17 03:41:56.270: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 17 03:41:56.273: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 03:41:56.282: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2706  01549479-3ca4-4421-b8d1-08875e6435ea 227791 3 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e6bb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-17 03:41:54 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 03:41:56 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 17 03:41:56.286: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2706  f8e1860e-1c82-4e80-93c9-be29900003f9 227785 3 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 01549479-3ca4-4421-b8d1-08875e6435ea 0xc005372007 0xc005372008}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01549479-3ca4-4421-b8d1-08875e6435ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053720a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 03:41:56.286: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 17 03:41:56.286: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2706  b082f1fe-11a8-48f7-b07f-58404b0c0283 227782 3 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 01549479-3ca4-4421-b8d1-08875e6435ea 0xc000e6bf17 0xc000e6bf18}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01549479-3ca4-4421-b8d1-08875e6435ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e6bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-26vmd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-26vmd webserver-deployment-7f5969cbc7- deployment-2706  f501ac32-7aa6-47db-b4e0-38efe49c733a 227787 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b367 0xc00397b368}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwg8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwg8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-49ptp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-49ptp webserver-deployment-7f5969cbc7- deployment-2706  0992ffb8-64eb-4dce-86c5-653518ab86c0 227799 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b5c0 0xc00397b5c1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8thg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8thg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-4ft4z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4ft4z webserver-deployment-7f5969cbc7- deployment-2706  5284b0d1-544d-457b-92ad-fbbb8981dd07 227812 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b710 0xc00397b711}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh7dt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh7dt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.302: INFO: Pod "webserver-deployment-7f5969cbc7-5mhlp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5mhlp webserver-deployment-7f5969cbc7- deployment-2706  76af3df5-b07a-4b1c-8a2f-5dfc558f3293 227793 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b870 0xc00397b871}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kr7jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kr7jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-6ckp9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6ckp9 webserver-deployment-7f5969cbc7- deployment-2706  8b3dd66d-a1e6-4020-a267-9d672b08ef24 227814 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397b9e0 0xc00397b9e1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb64b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb64b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-9bqbd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9bqbd webserver-deployment-7f5969cbc7- deployment-2706  794f7918-afef-4925-a4cd-7451ef6925f3 227655 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c9982392e2734f2391e05a17f1a9e6c043acae14006aa03b014ed2dc21710463 cni.projectcalico.org/podIP:172.21.15.115/32 cni.projectcalico.org/podIPs:172.21.15.115/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397bb50 0xc00397bb51}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcq7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcq7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.115,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cff6c8da22a089f178f504433ab0b185eed4b60e226c43df57f6b9606a09a5eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-cbpmn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbpmn webserver-deployment-7f5969cbc7- deployment-2706  bffd02f6-90a8-4f9d-a95a-36f5a6548b86 227808 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397bd47 0xc00397bd48}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l49g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l49g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.303: INFO: Pod "webserver-deployment-7f5969cbc7-dxtbm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dxtbm webserver-deployment-7f5969cbc7- deployment-2706  8d59fbb4-d7aa-4b76-9c34-72ea24241e45 227685 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:293b92f6cb6315cfe1bb5f75395a14620479367baf33202f12c804e005044958 cni.projectcalico.org/podIP:172.21.86.150/32 cni.projectcalico.org/podIPs:172.21.86.150/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc00397beb0 0xc00397beb1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4smnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4smnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.150,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff835a023b63c95bdea77693d17583058cdeebba550407892b4e138934a3cc2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-ffgqf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ffgqf webserver-deployment-7f5969cbc7- deployment-2706  51280101-e350-436c-a954-673d81795248 227669 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aeb3718709b2de82c630b7ef5bbca682520db9ea17b55157e06c9587964de044 cni.projectcalico.org/podIP:172.21.86.180/32 cni.projectcalico.org/podIPs:172.21.86.180/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace0d7 0xc004ace0d8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lvfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lvfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.180,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ed3d82b2339f72e95490f037d55b0d1a0e6658514106e7632c560e903fdc95c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-fjzwd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjzwd webserver-deployment-7f5969cbc7- deployment-2706  51310561-4581-47ff-b248-e5f2a39b8320 227667 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bcc52085f05adb33a3792c9a84551d0fe41757f10539bff857587e7008bc4578 cni.projectcalico.org/podIP:172.21.15.71/32 cni.projectcalico.org/podIPs:172.21.15.71/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace2f7 0xc004ace2f8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzjrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzjrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.71,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ec9182ac31eaf732152b2f66d85881a1caf32c60f9d088a64855b8a680e4d5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-g2s2n" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g2s2n webserver-deployment-7f5969cbc7- deployment-2706  49417edd-05d2-47c1-873a-95815c83d744 227640 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:690553deada6b00dae6b380527d1f932f6991bb473c8e92493b880ea6b02e749 cni.projectcalico.org/podIP:172.21.86.182/32 cni.projectcalico.org/podIPs:172.21.86.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004ace517 0xc004ace518}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdbv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdbv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.182,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2aa3334049d184b46f3e4885cc73458a7fc6b6a5d2b2e7650ac7addedd9540e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-k2gsv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k2gsv webserver-deployment-7f5969cbc7- deployment-2706  fc4e41a8-51cf-4e19-afb9-ac6a97b8eb44 227677 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2ddf7cc9cd8f729236d86b2a325cba5e03f0077f5909cdb49602a25f09dcfd95 cni.projectcalico.org/podIP:172.21.86.135/32 cni.projectcalico.org/podIPs:172.21.86.135/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004aced77 0xc004aced78}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r26xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r26xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.135,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://35163c28772ab8fa64697007cddd447b7052b3fe49a318762ef42964520d3b71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-l56fp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l56fp webserver-deployment-7f5969cbc7- deployment-2706  55a9d622-0b85-4821-bb37-8c241e765e54 227816 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acef87 0xc004acef88}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhvhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhvhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.304: INFO: Pod "webserver-deployment-7f5969cbc7-lvkwk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lvkwk webserver-deployment-7f5969cbc7- deployment-2706  66cbf246-0b1c-4e85-8c5d-f1d566980237 227810 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf0e0 0xc004acf0e1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzfs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzfs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-p8w88" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p8w88 webserver-deployment-7f5969cbc7- deployment-2706  3dd9edcb-4171-43c8-adb1-24ba6fa37767 227807 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf220 0xc004acf221}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sb8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sb8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-pppck" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pppck webserver-deployment-7f5969cbc7- deployment-2706  30d2104f-d6e8-4f1a-b61e-64d7b76a4d1f 227809 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf360 0xc004acf361}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcmb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcmb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-vv5l4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vv5l4 webserver-deployment-7f5969cbc7- deployment-2706  7dedd5c4-b31e-47af-8b6b-b5f4d7197a09 227821 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf4a0 0xc004acf4a1}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxlc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxlc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-xj4rw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xj4rw webserver-deployment-7f5969cbc7- deployment-2706  748a420f-33d2-4580-a9bd-afd18836a814 227806 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf600 0xc004acf601}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxrkr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxrkr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-zvcm9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvcm9 webserver-deployment-7f5969cbc7- deployment-2706  9d60dd57-5bcf-4fe8-8e46-509c8d7c1029 227650 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:897772a9133372c581003954516b7ddabab7666a07dcff38cd81cc1a6b52842e cni.projectcalico.org/podIP:172.21.15.118/32 cni.projectcalico.org/podIPs:172.21.15.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf760 0xc004acf761}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2p9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2p9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.118,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d087aeeb3c6bafa573da03985c804de79afe5b4c206c741b661d2fbb98c9b08b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.305: INFO: Pod "webserver-deployment-7f5969cbc7-zznc6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zznc6 webserver-deployment-7f5969cbc7- deployment-2706  b9df23f5-e4c3-459e-9891-ec0ac8ed72a0 227657 0 2023-08-17 03:41:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ddf190b8e7e99f96cbf48767e6c44928c490018335d9bc13505f615589c3d992 cni.projectcalico.org/podIP:172.21.86.172/32 cni.projectcalico.org/podIPs:172.21.86.172/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b082f1fe-11a8-48f7-b07f-58404b0c0283 0xc004acf987 0xc004acf988}] [] [{calico Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 03:41:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b082f1fe-11a8-48f7-b07f-58404b0c0283\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-829cb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-829cb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.172,StartTime:2023-08-17 03:41:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:41:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3514e0d79f8e0c885b24b3c9db91a24f3cddf754dd5a74c35f3da1afda4a810a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-2h99c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2h99c webserver-deployment-d9f79cb5- deployment-2706  a246653e-8a3d-4edb-b482-586145de0909 227742 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:130e66f7d0be4125f04d72664d9b3c72499a4f44a2036a20c6ac49902c3fbfef cni.projectcalico.org/podIP:172.21.86.156/32 cni.projectcalico.org/podIPs:172.21.86.156/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfb97 0xc004acfb98}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr8ds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr8ds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-4mtzn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4mtzn webserver-deployment-d9f79cb5- deployment-2706  59a37595-272f-4935-abbd-f4a84ef37d82 227815 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfd97 0xc004acfd98}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7ztg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7ztg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-64n8d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-64n8d webserver-deployment-d9f79cb5- deployment-2706  601bed6d-868b-41b5-9e96-7f2d35f4fe40 227822 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004acfedf 0xc004acfef0}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m76mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m76mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-68vjh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-68vjh webserver-deployment-d9f79cb5- deployment-2706  2ac2a847-1b8b-43ad-bc94-a051517a8061 227817 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492803f 0xc004928050}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6jgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6jgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-6tqjn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6tqjn webserver-deployment-d9f79cb5- deployment-2706  a67dae70-6ebb-4125-8274-4099ef7ef545 227760 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:689a939e0e4b06790b64092a440ba6662a6c549b41c450534a1fcf509b32d99b cni.projectcalico.org/podIP:172.21.86.137/32 cni.projectcalico.org/podIPs:172.21.86.137/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492819f 0xc0049281d0}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfzlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfzlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.306: INFO: Pod "webserver-deployment-d9f79cb5-8b56d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8b56d webserver-deployment-d9f79cb5- deployment-2706  c74e3c43-2423-4127-871c-f6e733d8d4dc 227818 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc0049283c7 0xc0049283c8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2m2s4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2m2s4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-dsvmx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dsvmx webserver-deployment-d9f79cb5- deployment-2706  4ca430c4-0be7-49f8-979a-a41edfbafb6e 227813 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492850f 0xc004928520}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cww4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cww4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-lt8wg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lt8wg webserver-deployment-d9f79cb5- deployment-2706  5359f728-f4d4-4836-b37c-03219be906e2 227773 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3813af664e45f6f5b284fc965d055232177ee2facca1ced985943405c73e57ea cni.projectcalico.org/podIP:172.21.15.77/32 cni.projectcalico.org/podIPs:172.21.15.77/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc00492865f 0xc004928690}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xn97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xn97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-srt57" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-srt57 webserver-deployment-d9f79cb5- deployment-2706  4c425fd7-4b17-4284-8429-4a0fbefdd0ac 227819 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928887 0xc004928888}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr4k9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr4k9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-tjsvh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tjsvh webserver-deployment-d9f79cb5- deployment-2706  4ff0dc8c-b69a-48cc-b0c5-b4a0ca24dc5a 227765 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a59e16d0361c6af0610e7f5f7f502562abd0a5c6abf6e7ad4adc9b5a97e42b5e cni.projectcalico.org/podIP:172.21.15.75/32 cni.projectcalico.org/podIPs:172.21.15.75/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc0049289cf 0xc004928a00}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-xjj9t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xjj9t webserver-deployment-d9f79cb5- deployment-2706  1fab22f9-c053-47c7-886e-42181ab3131d 227800 0 2023-08-17 03:41:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928bf7 0xc004928bf8}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkfks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkfks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:41:56.307: INFO: Pod "webserver-deployment-d9f79cb5-xlv4x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xlv4x webserver-deployment-d9f79cb5- deployment-2706  f953cdd3-46b8-4b7a-8b18-bf26aae4b852 227754 0 2023-08-17 03:41:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9e2d6ce40d61c46f1e153dc2afd813a90b76de1ed040f323d887e8e5e22c991b cni.projectcalico.org/podIP:172.21.15.100/32 cni.projectcalico.org/podIPs:172.21.15.100/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 f8e1860e-1c82-4e80-93c9-be29900003f9 0xc004928d4f 0xc004928d80}] [] [{kube-controller-manager Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8e1860e-1c82-4e80-93c9-be29900003f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 03:41:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-17 03:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdljj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdljj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 03:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:41:56.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2706" for this suite. 08/17/23 03:41:56.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:41:56.341
Aug 17 03:41:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 03:41:56.345
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:56.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:56.369
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/17/23 03:41:56.373
STEP: Creating a ResourceQuota 08/17/23 03:42:01.379
STEP: Ensuring resource quota status is calculated 08/17/23 03:42:01.385
STEP: Creating a ReplicationController 08/17/23 03:42:03.391
STEP: Ensuring resource quota status captures replication controller creation 08/17/23 03:42:03.406
STEP: Deleting a ReplicationController 08/17/23 03:42:05.412
STEP: Ensuring resource quota status released usage 08/17/23 03:42:05.418
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 03:42:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8644" for this suite. 08/17/23 03:42:07.431
------------------------------
• [SLOW TEST] [11.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:41:56.341
    Aug 17 03:41:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 03:41:56.345
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:41:56.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:41:56.369
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/17/23 03:41:56.373
    STEP: Creating a ResourceQuota 08/17/23 03:42:01.379
    STEP: Ensuring resource quota status is calculated 08/17/23 03:42:01.385
    STEP: Creating a ReplicationController 08/17/23 03:42:03.391
    STEP: Ensuring resource quota status captures replication controller creation 08/17/23 03:42:03.406
    STEP: Deleting a ReplicationController 08/17/23 03:42:05.412
    STEP: Ensuring resource quota status released usage 08/17/23 03:42:05.418
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:42:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8644" for this suite. 08/17/23 03:42:07.431
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:42:07.44
Aug 17 03:42:07.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename containers 08/17/23 03:42:07.441
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:42:07.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:42:07.46
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/17/23 03:42:07.465
Aug 17 03:42:07.474: INFO: Waiting up to 5m0s for pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0" in namespace "containers-9303" to be "Succeeded or Failed"
Aug 17 03:42:07.477: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504582ms
Aug 17 03:42:09.482: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008589198s
Aug 17 03:42:11.482: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008284522s
Aug 17 03:42:13.483: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00916503s
STEP: Saw pod success 08/17/23 03:42:13.483
Aug 17 03:42:13.483: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0" satisfied condition "Succeeded or Failed"
Aug 17 03:42:13.487: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:42:13.502
Aug 17 03:42:13.512: INFO: Waiting for pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 to disappear
Aug 17 03:42:13.515: INFO: Pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:42:13.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9303" for this suite. 08/17/23 03:42:13.522
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:42:07.44
    Aug 17 03:42:07.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename containers 08/17/23 03:42:07.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:42:07.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:42:07.46
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/17/23 03:42:07.465
    Aug 17 03:42:07.474: INFO: Waiting up to 5m0s for pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0" in namespace "containers-9303" to be "Succeeded or Failed"
    Aug 17 03:42:07.477: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.504582ms
    Aug 17 03:42:09.482: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008589198s
    Aug 17 03:42:11.482: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008284522s
    Aug 17 03:42:13.483: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00916503s
    STEP: Saw pod success 08/17/23 03:42:13.483
    Aug 17 03:42:13.483: INFO: Pod "client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0" satisfied condition "Succeeded or Failed"
    Aug 17 03:42:13.487: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:42:13.502
    Aug 17 03:42:13.512: INFO: Waiting for pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 to disappear
    Aug 17 03:42:13.515: INFO: Pod client-containers-5a963ca4-efe3-4998-8c51-34df1a0208e0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:42:13.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9303" for this suite. 08/17/23 03:42:13.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:42:13.531
Aug 17 03:42:13.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 03:42:13.531
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:42:13.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:42:13.55
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/17/23 03:42:13.555
Aug 17 03:42:13.565: INFO: Waiting up to 2m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342" to be "running"
Aug 17 03:42:13.570: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926475ms
Aug 17 03:42:15.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009910099s
Aug 17 03:42:17.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010497758s
Aug 17 03:42:19.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009996068s
Aug 17 03:42:21.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012185244s
Aug 17 03:42:23.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010820748s
Aug 17 03:42:25.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012449522s
Aug 17 03:42:27.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011051408s
Aug 17 03:42:29.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010555633s
Aug 17 03:42:31.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01083165s
Aug 17 03:42:33.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010175541s
Aug 17 03:42:35.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009956327s
Aug 17 03:42:37.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009705142s
Aug 17 03:42:39.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010629154s
Aug 17 03:42:41.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010645872s
Aug 17 03:42:43.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011515442s
Aug 17 03:42:45.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011221083s
Aug 17 03:42:47.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010310856s
Aug 17 03:42:49.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010160296s
Aug 17 03:42:51.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010516553s
Aug 17 03:42:53.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009717251s
Aug 17 03:42:55.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009659087s
Aug 17 03:42:57.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010723803s
Aug 17 03:42:59.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009941079s
Aug 17 03:43:01.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011540105s
Aug 17 03:43:03.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012458676s
Aug 17 03:43:05.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010384953s
Aug 17 03:43:07.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010521941s
Aug 17 03:43:09.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01019324s
Aug 17 03:43:11.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010223257s
Aug 17 03:43:13.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010085096s
Aug 17 03:43:15.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010768078s
Aug 17 03:43:17.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011383642s
Aug 17 03:43:19.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012655986s
Aug 17 03:43:21.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011273349s
Aug 17 03:43:23.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010299283s
Aug 17 03:43:25.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010652765s
Aug 17 03:43:27.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010695896s
Aug 17 03:43:29.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010874801s
Aug 17 03:43:31.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010606298s
Aug 17 03:43:33.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01054924s
Aug 17 03:43:35.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010709815s
Aug 17 03:43:37.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010536157s
Aug 17 03:43:39.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010518596s
Aug 17 03:43:41.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010461289s
Aug 17 03:43:43.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01060235s
Aug 17 03:43:45.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010749539s
Aug 17 03:43:47.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010378381s
Aug 17 03:43:49.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011126469s
Aug 17 03:43:51.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011042289s
Aug 17 03:43:53.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009819446s
Aug 17 03:43:55.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011600172s
Aug 17 03:43:57.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011012061s
Aug 17 03:43:59.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009803017s
Aug 17 03:44:01.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011725783s
Aug 17 03:44:03.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010929574s
Aug 17 03:44:05.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010684799s
Aug 17 03:44:07.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010221062s
Aug 17 03:44:09.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01060286s
Aug 17 03:44:11.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010601853s
Aug 17 03:44:13.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011803082s
Aug 17 03:44:13.581: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016740137s
STEP: updating the pod 08/17/23 03:44:13.581
Aug 17 03:44:14.098: INFO: Successfully updated pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71"
STEP: waiting for pod running 08/17/23 03:44:14.098
Aug 17 03:44:14.098: INFO: Waiting up to 2m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342" to be "running"
Aug 17 03:44:14.110: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973052ms
Aug 17 03:44:16.116: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Running", Reason="", readiness=true. Elapsed: 2.017579675s
Aug 17 03:44:16.116: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 03:44:16.116
Aug 17 03:44:16.116: INFO: Deleting pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342"
Aug 17 03:44:16.131: INFO: Wait up to 5m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 03:44:48.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3342" for this suite. 08/17/23 03:44:48.147
------------------------------
• [SLOW TEST] [154.625 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:42:13.531
    Aug 17 03:42:13.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 03:42:13.531
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:42:13.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:42:13.55
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/17/23 03:42:13.555
    Aug 17 03:42:13.565: INFO: Waiting up to 2m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342" to be "running"
    Aug 17 03:42:13.570: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.926475ms
    Aug 17 03:42:15.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009910099s
    Aug 17 03:42:17.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010497758s
    Aug 17 03:42:19.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009996068s
    Aug 17 03:42:21.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012185244s
    Aug 17 03:42:23.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010820748s
    Aug 17 03:42:25.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012449522s
    Aug 17 03:42:27.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011051408s
    Aug 17 03:42:29.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010555633s
    Aug 17 03:42:31.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01083165s
    Aug 17 03:42:33.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010175541s
    Aug 17 03:42:35.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009956327s
    Aug 17 03:42:37.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009705142s
    Aug 17 03:42:39.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010629154s
    Aug 17 03:42:41.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010645872s
    Aug 17 03:42:43.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011515442s
    Aug 17 03:42:45.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011221083s
    Aug 17 03:42:47.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010310856s
    Aug 17 03:42:49.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010160296s
    Aug 17 03:42:51.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010516553s
    Aug 17 03:42:53.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009717251s
    Aug 17 03:42:55.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009659087s
    Aug 17 03:42:57.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010723803s
    Aug 17 03:42:59.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009941079s
    Aug 17 03:43:01.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011540105s
    Aug 17 03:43:03.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012458676s
    Aug 17 03:43:05.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010384953s
    Aug 17 03:43:07.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010521941s
    Aug 17 03:43:09.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01019324s
    Aug 17 03:43:11.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010223257s
    Aug 17 03:43:13.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010085096s
    Aug 17 03:43:15.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010768078s
    Aug 17 03:43:17.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011383642s
    Aug 17 03:43:19.577: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012655986s
    Aug 17 03:43:21.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011273349s
    Aug 17 03:43:23.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010299283s
    Aug 17 03:43:25.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010652765s
    Aug 17 03:43:27.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010695896s
    Aug 17 03:43:29.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010874801s
    Aug 17 03:43:31.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010606298s
    Aug 17 03:43:33.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01054924s
    Aug 17 03:43:35.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010709815s
    Aug 17 03:43:37.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010536157s
    Aug 17 03:43:39.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010518596s
    Aug 17 03:43:41.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010461289s
    Aug 17 03:43:43.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01060235s
    Aug 17 03:43:45.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010749539s
    Aug 17 03:43:47.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010378381s
    Aug 17 03:43:49.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011126469s
    Aug 17 03:43:51.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011042289s
    Aug 17 03:43:53.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009819446s
    Aug 17 03:43:55.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011600172s
    Aug 17 03:43:57.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011012061s
    Aug 17 03:43:59.574: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009803017s
    Aug 17 03:44:01.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011725783s
    Aug 17 03:44:03.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010929574s
    Aug 17 03:44:05.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010684799s
    Aug 17 03:44:07.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010221062s
    Aug 17 03:44:09.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01060286s
    Aug 17 03:44:11.575: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010601853s
    Aug 17 03:44:13.576: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011803082s
    Aug 17 03:44:13.581: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016740137s
    STEP: updating the pod 08/17/23 03:44:13.581
    Aug 17 03:44:14.098: INFO: Successfully updated pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71"
    STEP: waiting for pod running 08/17/23 03:44:14.098
    Aug 17 03:44:14.098: INFO: Waiting up to 2m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342" to be "running"
    Aug 17 03:44:14.110: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973052ms
    Aug 17 03:44:16.116: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71": Phase="Running", Reason="", readiness=true. Elapsed: 2.017579675s
    Aug 17 03:44:16.116: INFO: Pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 03:44:16.116
    Aug 17 03:44:16.116: INFO: Deleting pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" in namespace "var-expansion-3342"
    Aug 17 03:44:16.131: INFO: Wait up to 5m0s for pod "var-expansion-c478898e-bb3e-4b0d-989c-d96d44164e71" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:44:48.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3342" for this suite. 08/17/23 03:44:48.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:44:48.158
Aug 17 03:44:48.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:44:48.159
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:48.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:48.178
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 17 03:44:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: creating the pod 08/17/23 03:44:48.183
STEP: submitting the pod to kubernetes 08/17/23 03:44:48.183
Aug 17 03:44:48.192: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e" in namespace "pods-2672" to be "running and ready"
Aug 17 03:44:48.198: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.500065ms
Aug 17 03:44:48.198: INFO: The phase of Pod pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:44:50.204: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011367705s
Aug 17 03:44:50.204: INFO: The phase of Pod pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e is Running (Ready = true)
Aug 17 03:44:50.204: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:44:50.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2672" for this suite. 08/17/23 03:44:50.377
------------------------------
• [2.227 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:44:48.158
    Aug 17 03:44:48.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:44:48.159
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:48.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:48.178
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 17 03:44:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: creating the pod 08/17/23 03:44:48.183
    STEP: submitting the pod to kubernetes 08/17/23 03:44:48.183
    Aug 17 03:44:48.192: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e" in namespace "pods-2672" to be "running and ready"
    Aug 17 03:44:48.198: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.500065ms
    Aug 17 03:44:48.198: INFO: The phase of Pod pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:44:50.204: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011367705s
    Aug 17 03:44:50.204: INFO: The phase of Pod pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e is Running (Ready = true)
    Aug 17 03:44:50.204: INFO: Pod "pod-exec-websocket-31270c21-caf1-4f0e-8ffb-d2e2a88bf54e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:44:50.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2672" for this suite. 08/17/23 03:44:50.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:44:50.389
Aug 17 03:44:50.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:44:50.39
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:50.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:50.41
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:44:50.415
Aug 17 03:44:50.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 17 03:44:50.483: INFO: stderr: ""
Aug 17 03:44:50.483: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/17/23 03:44:50.483
STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 03:44:55.537
Aug 17 03:44:55.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 get pod e2e-test-httpd-pod -o json'
Aug 17 03:44:55.606: INFO: stderr: ""
Aug 17 03:44:55.606: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"9a6eb09569cf9ddea6351d67ffb40644dde71d0743f6a3a7bcd36655061486c6\",\n            \"cni.projectcalico.org/podIP\": \"172.21.15.92/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.15.92/32\"\n        },\n        \"creationTimestamp\": \"2023-08-17T03:44:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9053\",\n        \"resourceVersion\": \"228787\",\n        \"uid\": \"14b2e9f1-a3ee-4af6-b98b-04f66e244527\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9lr6z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ske-ubuntu-79fff84d86x69988-vjwlx\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9lr6z\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e5bf0653adfe72b66166a73dafdf378f3af3a4622bb00163e7d66cf127b17263\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-17T03:44:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.11.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.15.92\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.15.92\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-17T03:44:50Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/17/23 03:44:55.606
Aug 17 03:44:55.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 replace -f -'
Aug 17 03:44:56.450: INFO: stderr: ""
Aug 17 03:44:56.450: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/17/23 03:44:56.45
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 17 03:44:56.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 delete pods e2e-test-httpd-pod'
Aug 17 03:44:57.967: INFO: stderr: ""
Aug 17 03:44:57.967: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:44:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9053" for this suite. 08/17/23 03:44:57.974
------------------------------
• [SLOW TEST] [7.593 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:44:50.389
    Aug 17 03:44:50.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:44:50.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:50.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:50.41
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:44:50.415
    Aug 17 03:44:50.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 17 03:44:50.483: INFO: stderr: ""
    Aug 17 03:44:50.483: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/17/23 03:44:50.483
    STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 03:44:55.537
    Aug 17 03:44:55.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 get pod e2e-test-httpd-pod -o json'
    Aug 17 03:44:55.606: INFO: stderr: ""
    Aug 17 03:44:55.606: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"9a6eb09569cf9ddea6351d67ffb40644dde71d0743f6a3a7bcd36655061486c6\",\n            \"cni.projectcalico.org/podIP\": \"172.21.15.92/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.15.92/32\"\n        },\n        \"creationTimestamp\": \"2023-08-17T03:44:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9053\",\n        \"resourceVersion\": \"228787\",\n        \"uid\": \"14b2e9f1-a3ee-4af6-b98b-04f66e244527\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9lr6z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ske-ubuntu-79fff84d86x69988-vjwlx\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9lr6z\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-17T03:44:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e5bf0653adfe72b66166a73dafdf378f3af3a4622bb00163e7d66cf127b17263\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-17T03:44:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.11.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.15.92\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.15.92\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-17T03:44:50Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/17/23 03:44:55.606
    Aug 17 03:44:55.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 replace -f -'
    Aug 17 03:44:56.450: INFO: stderr: ""
    Aug 17 03:44:56.450: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/17/23 03:44:56.45
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 17 03:44:56.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9053 delete pods e2e-test-httpd-pod'
    Aug 17 03:44:57.967: INFO: stderr: ""
    Aug 17 03:44:57.967: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:44:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9053" for this suite. 08/17/23 03:44:57.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:44:57.983
Aug 17 03:44:57.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:44:57.984
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:57.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:58.004
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/17/23 03:44:58.009
Aug 17 03:44:58.009: INFO: Creating e2e-svc-a-b4vhg
Aug 17 03:44:58.019: INFO: Creating e2e-svc-b-vpmxn
Aug 17 03:44:58.031: INFO: Creating e2e-svc-c-m5pz2
STEP: deleting service collection 08/17/23 03:44:58.046
Aug 17 03:44:58.073: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:44:58.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2532" for this suite. 08/17/23 03:44:58.08
------------------------------
• [0.105 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:44:57.983
    Aug 17 03:44:57.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:44:57.984
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:57.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:58.004
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/17/23 03:44:58.009
    Aug 17 03:44:58.009: INFO: Creating e2e-svc-a-b4vhg
    Aug 17 03:44:58.019: INFO: Creating e2e-svc-b-vpmxn
    Aug 17 03:44:58.031: INFO: Creating e2e-svc-c-m5pz2
    STEP: deleting service collection 08/17/23 03:44:58.046
    Aug 17 03:44:58.073: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:44:58.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2532" for this suite. 08/17/23 03:44:58.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:44:58.091
Aug 17 03:44:58.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:44:58.092
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:58.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:58.11
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:44:58.115
Aug 17 03:44:58.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb" in namespace "downward-api-8068" to be "Succeeded or Failed"
Aug 17 03:44:58.129: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882741ms
Aug 17 03:45:00.136: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01076314s
Aug 17 03:45:02.135: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010227963s
STEP: Saw pod success 08/17/23 03:45:02.135
Aug 17 03:45:02.135: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb" satisfied condition "Succeeded or Failed"
Aug 17 03:45:02.140: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb container client-container: <nil>
STEP: delete the pod 08/17/23 03:45:02.19
Aug 17 03:45:02.203: INFO: Waiting for pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb to disappear
Aug 17 03:45:02.207: INFO: Pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8068" for this suite. 08/17/23 03:45:02.214
------------------------------
• [4.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:44:58.091
    Aug 17 03:44:58.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:44:58.092
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:44:58.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:44:58.11
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:44:58.115
    Aug 17 03:44:58.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb" in namespace "downward-api-8068" to be "Succeeded or Failed"
    Aug 17 03:44:58.129: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882741ms
    Aug 17 03:45:00.136: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01076314s
    Aug 17 03:45:02.135: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010227963s
    STEP: Saw pod success 08/17/23 03:45:02.135
    Aug 17 03:45:02.135: INFO: Pod "downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb" satisfied condition "Succeeded or Failed"
    Aug 17 03:45:02.140: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb container client-container: <nil>
    STEP: delete the pod 08/17/23 03:45:02.19
    Aug 17 03:45:02.203: INFO: Waiting for pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb to disappear
    Aug 17 03:45:02.207: INFO: Pod downwardapi-volume-3c739517-d7cb-4087-a1eb-e249346521fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8068" for this suite. 08/17/23 03:45:02.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:02.223
Aug 17 03:45:02.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename subpath 08/17/23 03:45:02.224
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:02.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:02.247
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 03:45:02.252
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-5l5s 08/17/23 03:45:02.265
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:45:02.265
Aug 17 03:45:02.276: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5l5s" in namespace "subpath-332" to be "Succeeded or Failed"
Aug 17 03:45:02.281: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76163ms
Aug 17 03:45:04.289: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.012754711s
Aug 17 03:45:06.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.012621715s
Aug 17 03:45:08.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.010426092s
Aug 17 03:45:10.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.010393518s
Aug 17 03:45:12.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.010962792s
Aug 17 03:45:14.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.01265026s
Aug 17 03:45:16.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.010321589s
Aug 17 03:45:18.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.010759452s
Aug 17 03:45:20.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.012623022s
Aug 17 03:45:22.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.010588107s
Aug 17 03:45:24.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.010984751s
Aug 17 03:45:26.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010653304s
STEP: Saw pod success 08/17/23 03:45:26.286
Aug 17 03:45:26.287: INFO: Pod "pod-subpath-test-configmap-5l5s" satisfied condition "Succeeded or Failed"
Aug 17 03:45:26.291: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-configmap-5l5s container test-container-subpath-configmap-5l5s: <nil>
STEP: delete the pod 08/17/23 03:45:26.303
Aug 17 03:45:26.314: INFO: Waiting for pod pod-subpath-test-configmap-5l5s to disappear
Aug 17 03:45:26.319: INFO: Pod pod-subpath-test-configmap-5l5s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5l5s 08/17/23 03:45:26.319
Aug 17 03:45:26.319: INFO: Deleting pod "pod-subpath-test-configmap-5l5s" in namespace "subpath-332"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-332" for this suite. 08/17/23 03:45:26.33
------------------------------
• [SLOW TEST] [24.116 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:02.223
    Aug 17 03:45:02.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename subpath 08/17/23 03:45:02.224
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:02.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:02.247
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 03:45:02.252
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-5l5s 08/17/23 03:45:02.265
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:45:02.265
    Aug 17 03:45:02.276: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5l5s" in namespace "subpath-332" to be "Succeeded or Failed"
    Aug 17 03:45:02.281: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76163ms
    Aug 17 03:45:04.289: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.012754711s
    Aug 17 03:45:06.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.012621715s
    Aug 17 03:45:08.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.010426092s
    Aug 17 03:45:10.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.010393518s
    Aug 17 03:45:12.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.010962792s
    Aug 17 03:45:14.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.01265026s
    Aug 17 03:45:16.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.010321589s
    Aug 17 03:45:18.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.010759452s
    Aug 17 03:45:20.288: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.012623022s
    Aug 17 03:45:22.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.010588107s
    Aug 17 03:45:24.287: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Running", Reason="", readiness=false. Elapsed: 22.010984751s
    Aug 17 03:45:26.286: INFO: Pod "pod-subpath-test-configmap-5l5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010653304s
    STEP: Saw pod success 08/17/23 03:45:26.286
    Aug 17 03:45:26.287: INFO: Pod "pod-subpath-test-configmap-5l5s" satisfied condition "Succeeded or Failed"
    Aug 17 03:45:26.291: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-configmap-5l5s container test-container-subpath-configmap-5l5s: <nil>
    STEP: delete the pod 08/17/23 03:45:26.303
    Aug 17 03:45:26.314: INFO: Waiting for pod pod-subpath-test-configmap-5l5s to disappear
    Aug 17 03:45:26.319: INFO: Pod pod-subpath-test-configmap-5l5s no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-5l5s 08/17/23 03:45:26.319
    Aug 17 03:45:26.319: INFO: Deleting pod "pod-subpath-test-configmap-5l5s" in namespace "subpath-332"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-332" for this suite. 08/17/23 03:45:26.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:26.34
Aug 17 03:45:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 03:45:26.341
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:26.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:26.361
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-3ca09f83-8693-42de-a3b1-f5512a7cb4ba 08/17/23 03:45:26.366
STEP: Creating a pod to test consume secrets 08/17/23 03:45:26.371
Aug 17 03:45:26.381: INFO: Waiting up to 5m0s for pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425" in namespace "secrets-2900" to be "Succeeded or Failed"
Aug 17 03:45:26.386: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73153ms
Aug 17 03:45:28.392: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324554s
Aug 17 03:45:30.391: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010844634s
STEP: Saw pod success 08/17/23 03:45:30.392
Aug 17 03:45:30.392: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425" satisfied condition "Succeeded or Failed"
Aug 17 03:45:30.396: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 03:45:30.405
Aug 17 03:45:30.416: INFO: Waiting for pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 to disappear
Aug 17 03:45:30.420: INFO: Pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:30.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2900" for this suite. 08/17/23 03:45:30.427
------------------------------
• [4.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:26.34
    Aug 17 03:45:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 03:45:26.341
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:26.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:26.361
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-3ca09f83-8693-42de-a3b1-f5512a7cb4ba 08/17/23 03:45:26.366
    STEP: Creating a pod to test consume secrets 08/17/23 03:45:26.371
    Aug 17 03:45:26.381: INFO: Waiting up to 5m0s for pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425" in namespace "secrets-2900" to be "Succeeded or Failed"
    Aug 17 03:45:26.386: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73153ms
    Aug 17 03:45:28.392: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324554s
    Aug 17 03:45:30.391: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010844634s
    STEP: Saw pod success 08/17/23 03:45:30.392
    Aug 17 03:45:30.392: INFO: Pod "pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425" satisfied condition "Succeeded or Failed"
    Aug 17 03:45:30.396: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 03:45:30.405
    Aug 17 03:45:30.416: INFO: Waiting for pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 to disappear
    Aug 17 03:45:30.420: INFO: Pod pod-secrets-bc321d21-7401-4642-a431-9f09e5f09425 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:30.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2900" for this suite. 08/17/23 03:45:30.427
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:30.437
Aug 17 03:45:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:45:30.437
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:30.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:30.455
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-f1828d5e-e595-4231-a26c-1aefb45b4bc3 08/17/23 03:45:30.465
STEP: Creating the pod 08/17/23 03:45:30.471
Aug 17 03:45:30.481: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d" in namespace "configmap-2929" to be "running"
Aug 17 03:45:30.485: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90621ms
Aug 17 03:45:32.491: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d": Phase="Running", Reason="", readiness=false. Elapsed: 2.010409569s
Aug 17 03:45:32.491: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d" satisfied condition "running"
STEP: Waiting for pod with text data 08/17/23 03:45:32.491
STEP: Waiting for pod with binary data 08/17/23 03:45:32.501
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:32.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2929" for this suite. 08/17/23 03:45:32.592
------------------------------
• [2.163 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:30.437
    Aug 17 03:45:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:45:30.437
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:30.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:30.455
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-f1828d5e-e595-4231-a26c-1aefb45b4bc3 08/17/23 03:45:30.465
    STEP: Creating the pod 08/17/23 03:45:30.471
    Aug 17 03:45:30.481: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d" in namespace "configmap-2929" to be "running"
    Aug 17 03:45:30.485: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90621ms
    Aug 17 03:45:32.491: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d": Phase="Running", Reason="", readiness=false. Elapsed: 2.010409569s
    Aug 17 03:45:32.491: INFO: Pod "pod-configmaps-2d5a82d5-ceff-48a7-bbd2-e3730695fe4d" satisfied condition "running"
    STEP: Waiting for pod with text data 08/17/23 03:45:32.491
    STEP: Waiting for pod with binary data 08/17/23 03:45:32.501
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:32.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2929" for this suite. 08/17/23 03:45:32.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:32.601
Aug 17 03:45:32.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 03:45:32.601
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:32.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:32.62
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Aug 17 03:45:32.649: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/17/23 03:45:32.656
Aug 17 03:45:32.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:32.659: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/17/23 03:45:32.659
Aug 17 03:45:32.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:32.683: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:33.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:33.689: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:34.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:45:34.689: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/17/23 03:45:34.693
Aug 17 03:45:34.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:34.720: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/17/23 03:45:34.72
Aug 17 03:45:34.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:34.731: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:35.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:35.736: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:36.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:36.738: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:37.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:37.735: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:38.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:38.737: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 03:45:39.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:45:39.736: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:45:39.743
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4830, will wait for the garbage collector to delete the pods 08/17/23 03:45:39.743
Aug 17 03:45:39.805: INFO: Deleting DaemonSet.extensions daemon-set took: 7.132221ms
Aug 17 03:45:39.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.456228ms
Aug 17 03:45:42.110: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:45:42.110: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 03:45:42.114: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"229209"},"items":null}

Aug 17 03:45:42.118: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"229209"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4830" for this suite. 08/17/23 03:45:42.155
------------------------------
• [SLOW TEST] [9.562 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:32.601
    Aug 17 03:45:32.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 03:45:32.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:32.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:32.62
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Aug 17 03:45:32.649: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/17/23 03:45:32.656
    Aug 17 03:45:32.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:32.659: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/17/23 03:45:32.659
    Aug 17 03:45:32.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:32.683: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:33.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:33.689: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:34.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:45:34.689: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/17/23 03:45:34.693
    Aug 17 03:45:34.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:34.720: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/17/23 03:45:34.72
    Aug 17 03:45:34.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:34.731: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:35.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:35.736: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:36.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:36.738: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:37.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:37.735: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:38.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:38.737: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 03:45:39.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:45:39.736: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:45:39.743
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4830, will wait for the garbage collector to delete the pods 08/17/23 03:45:39.743
    Aug 17 03:45:39.805: INFO: Deleting DaemonSet.extensions daemon-set took: 7.132221ms
    Aug 17 03:45:39.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.456228ms
    Aug 17 03:45:42.110: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:45:42.110: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 03:45:42.114: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"229209"},"items":null}

    Aug 17 03:45:42.118: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"229209"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4830" for this suite. 08/17/23 03:45:42.155
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:42.165
Aug 17 03:45:42.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:45:42.166
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:42.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:42.185
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/17/23 03:45:42.191
Aug 17 03:45:42.201: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e" in namespace "emptydir-3110" to be "running"
Aug 17 03:45:42.211: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.086012ms
Aug 17 03:45:44.217: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e": Phase="Running", Reason="", readiness=false. Elapsed: 2.01562319s
Aug 17 03:45:44.217: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/17/23 03:45:44.217
Aug 17 03:45:44.217: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3110 PodName:pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:45:44.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:45:44.217: INFO: ExecWithOptions: Clientset creation
Aug 17 03:45:44.217: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/emptydir-3110/pods/pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 17 03:45:44.618: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3110" for this suite. 08/17/23 03:45:44.626
------------------------------
• [2.468 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:42.165
    Aug 17 03:45:42.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:45:42.166
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:42.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:42.185
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/17/23 03:45:42.191
    Aug 17 03:45:42.201: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e" in namespace "emptydir-3110" to be "running"
    Aug 17 03:45:42.211: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.086012ms
    Aug 17 03:45:44.217: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e": Phase="Running", Reason="", readiness=false. Elapsed: 2.01562319s
    Aug 17 03:45:44.217: INFO: Pod "pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/17/23 03:45:44.217
    Aug 17 03:45:44.217: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3110 PodName:pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:45:44.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:45:44.217: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:45:44.217: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/emptydir-3110/pods/pod-sharedvolume-6d023761-7a1f-41cd-8c4e-08d055016c6e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 17 03:45:44.618: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3110" for this suite. 08/17/23 03:45:44.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:44.636
Aug 17 03:45:44.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 03:45:44.637
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:44.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:44.653
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-1838" 08/17/23 03:45:44.658
Aug 17 03:45:44.668: INFO: Namespace "namespaces-1838" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ced1975d-86f1-47d4-89a0-25054b775345", "kubernetes.io/metadata.name":"namespaces-1838", "namespaces-1838":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:44.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1838" for this suite. 08/17/23 03:45:44.674
------------------------------
• [0.045 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:44.636
    Aug 17 03:45:44.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 03:45:44.637
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:44.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:44.653
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-1838" 08/17/23 03:45:44.658
    Aug 17 03:45:44.668: INFO: Namespace "namespaces-1838" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ced1975d-86f1-47d4-89a0-25054b775345", "kubernetes.io/metadata.name":"namespaces-1838", "namespaces-1838":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:44.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1838" for this suite. 08/17/23 03:45:44.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:44.682
Aug 17 03:45:44.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:45:44.683
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:44.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:44.7
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5331 08/17/23 03:45:44.704
STEP: creating service affinity-clusterip in namespace services-5331 08/17/23 03:45:44.704
STEP: creating replication controller affinity-clusterip in namespace services-5331 08/17/23 03:45:44.714
I0817 03:45:44.720216      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5331, replica count: 3
I0817 03:45:47.771223      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:45:47.780: INFO: Creating new exec pod
Aug 17 03:45:47.786: INFO: Waiting up to 5m0s for pod "execpod-affinityj6282" in namespace "services-5331" to be "running"
Aug 17 03:45:47.792: INFO: Pod "execpod-affinityj6282": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750739ms
Aug 17 03:45:49.800: INFO: Pod "execpod-affinityj6282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014087008s
Aug 17 03:45:51.798: INFO: Pod "execpod-affinityj6282": Phase="Running", Reason="", readiness=true. Elapsed: 4.012767697s
Aug 17 03:45:51.798: INFO: Pod "execpod-affinityj6282" satisfied condition "running"
Aug 17 03:45:52.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 17 03:45:53.245: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 17 03:45:53.245: INFO: stdout: ""
Aug 17 03:45:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c nc -v -z -w 2 172.20.28.243 80'
Aug 17 03:45:53.675: INFO: stderr: "+ nc -v -z -w 2 172.20.28.243 80\nConnection to 172.20.28.243 80 port [tcp/http] succeeded!\n"
Aug 17 03:45:53.675: INFO: stdout: ""
Aug 17 03:45:53.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.28.243:80/ ; done'
Aug 17 03:45:54.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n"
Aug 17 03:45:54.159: INFO: stdout: "\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g"
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
Aug 17 03:45:54.159: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5331, will wait for the garbage collector to delete the pods 08/17/23 03:45:54.173
Aug 17 03:45:54.241: INFO: Deleting ReplicationController affinity-clusterip took: 6.586527ms
Aug 17 03:45:54.342: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.871653ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:45:56.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5331" for this suite. 08/17/23 03:45:56.567
------------------------------
• [SLOW TEST] [11.896 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:44.682
    Aug 17 03:45:44.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:45:44.683
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:44.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:44.7
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5331 08/17/23 03:45:44.704
    STEP: creating service affinity-clusterip in namespace services-5331 08/17/23 03:45:44.704
    STEP: creating replication controller affinity-clusterip in namespace services-5331 08/17/23 03:45:44.714
    I0817 03:45:44.720216      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5331, replica count: 3
    I0817 03:45:47.771223      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:45:47.780: INFO: Creating new exec pod
    Aug 17 03:45:47.786: INFO: Waiting up to 5m0s for pod "execpod-affinityj6282" in namespace "services-5331" to be "running"
    Aug 17 03:45:47.792: INFO: Pod "execpod-affinityj6282": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750739ms
    Aug 17 03:45:49.800: INFO: Pod "execpod-affinityj6282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014087008s
    Aug 17 03:45:51.798: INFO: Pod "execpod-affinityj6282": Phase="Running", Reason="", readiness=true. Elapsed: 4.012767697s
    Aug 17 03:45:51.798: INFO: Pod "execpod-affinityj6282" satisfied condition "running"
    Aug 17 03:45:52.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 17 03:45:53.245: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 17 03:45:53.245: INFO: stdout: ""
    Aug 17 03:45:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c nc -v -z -w 2 172.20.28.243 80'
    Aug 17 03:45:53.675: INFO: stderr: "+ nc -v -z -w 2 172.20.28.243 80\nConnection to 172.20.28.243 80 port [tcp/http] succeeded!\n"
    Aug 17 03:45:53.675: INFO: stdout: ""
    Aug 17 03:45:53.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-5331 exec execpod-affinityj6282 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.28.243:80/ ; done'
    Aug 17 03:45:54.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.28.243:80/\n"
    Aug 17 03:45:54.159: INFO: stdout: "\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g\naffinity-clusterip-grn5g"
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Received response from host: affinity-clusterip-grn5g
    Aug 17 03:45:54.159: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5331, will wait for the garbage collector to delete the pods 08/17/23 03:45:54.173
    Aug 17 03:45:54.241: INFO: Deleting ReplicationController affinity-clusterip took: 6.586527ms
    Aug 17 03:45:54.342: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.871653ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:45:56.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5331" for this suite. 08/17/23 03:45:56.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:45:56.58
Aug 17 03:45:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:45:56.58
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:56.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:56.599
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-157/configmap-test-02563084-1a49-4594-9807-f214a9229638 08/17/23 03:45:56.604
STEP: Creating a pod to test consume configMaps 08/17/23 03:45:56.611
Aug 17 03:45:56.619: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1" in namespace "configmap-157" to be "Succeeded or Failed"
Aug 17 03:45:56.624: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.974006ms
Aug 17 03:45:58.630: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010192443s
Aug 17 03:46:00.629: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009511234s
STEP: Saw pod success 08/17/23 03:46:00.629
Aug 17 03:46:00.629: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1" satisfied condition "Succeeded or Failed"
Aug 17 03:46:00.634: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 container env-test: <nil>
STEP: delete the pod 08/17/23 03:46:00.645
Aug 17 03:46:00.658: INFO: Waiting for pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 to disappear
Aug 17 03:46:00.662: INFO: Pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:00.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-157" for this suite. 08/17/23 03:46:00.669
------------------------------
• [4.098 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:45:56.58
    Aug 17 03:45:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:45:56.58
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:45:56.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:45:56.599
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-157/configmap-test-02563084-1a49-4594-9807-f214a9229638 08/17/23 03:45:56.604
    STEP: Creating a pod to test consume configMaps 08/17/23 03:45:56.611
    Aug 17 03:45:56.619: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1" in namespace "configmap-157" to be "Succeeded or Failed"
    Aug 17 03:45:56.624: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.974006ms
    Aug 17 03:45:58.630: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010192443s
    Aug 17 03:46:00.629: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009511234s
    STEP: Saw pod success 08/17/23 03:46:00.629
    Aug 17 03:46:00.629: INFO: Pod "pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1" satisfied condition "Succeeded or Failed"
    Aug 17 03:46:00.634: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 container env-test: <nil>
    STEP: delete the pod 08/17/23 03:46:00.645
    Aug 17 03:46:00.658: INFO: Waiting for pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 to disappear
    Aug 17 03:46:00.662: INFO: Pod pod-configmaps-c6e6ea3f-7ff0-46bd-9be9-3c10f55baff1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:00.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-157" for this suite. 08/17/23 03:46:00.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:00.678
Aug 17 03:46:00.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:46:00.679
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:00.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:00.698
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:00.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7658" for this suite. 08/17/23 03:46:00.733
------------------------------
• [0.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:00.678
    Aug 17 03:46:00.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 03:46:00.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:00.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:00.698
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:00.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7658" for this suite. 08/17/23 03:46:00.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:00.743
Aug 17 03:46:00.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:46:00.743
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:00.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:00.762
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/17/23 03:46:00.766
STEP: setting up watch 08/17/23 03:46:00.766
STEP: submitting the pod to kubernetes 08/17/23 03:46:00.871
STEP: verifying the pod is in kubernetes 08/17/23 03:46:00.882
STEP: verifying pod creation was observed 08/17/23 03:46:00.888
Aug 17 03:46:00.888: INFO: Waiting up to 5m0s for pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77" in namespace "pods-5953" to be "running"
Aug 17 03:46:00.892: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231138ms
Aug 17 03:46:02.898: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77": Phase="Running", Reason="", readiness=true. Elapsed: 2.009677052s
Aug 17 03:46:02.898: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 03:46:02.902
STEP: verifying pod deletion was observed 08/17/23 03:46:02.912
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:05.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5953" for this suite. 08/17/23 03:46:05.175
------------------------------
• [4.440 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:00.743
    Aug 17 03:46:00.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:46:00.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:00.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:00.762
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/17/23 03:46:00.766
    STEP: setting up watch 08/17/23 03:46:00.766
    STEP: submitting the pod to kubernetes 08/17/23 03:46:00.871
    STEP: verifying the pod is in kubernetes 08/17/23 03:46:00.882
    STEP: verifying pod creation was observed 08/17/23 03:46:00.888
    Aug 17 03:46:00.888: INFO: Waiting up to 5m0s for pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77" in namespace "pods-5953" to be "running"
    Aug 17 03:46:00.892: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231138ms
    Aug 17 03:46:02.898: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77": Phase="Running", Reason="", readiness=true. Elapsed: 2.009677052s
    Aug 17 03:46:02.898: INFO: Pod "pod-submit-remove-aeac3e4e-b76e-4b92-af6a-e56bf2154a77" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 03:46:02.902
    STEP: verifying pod deletion was observed 08/17/23 03:46:02.912
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:05.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5953" for this suite. 08/17/23 03:46:05.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:05.186
Aug 17 03:46:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:46:05.187
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:05.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:05.206
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8278 08/17/23 03:46:05.21
STEP: changing the ExternalName service to type=ClusterIP 08/17/23 03:46:05.216
STEP: creating replication controller externalname-service in namespace services-8278 08/17/23 03:46:05.234
I0817 03:46:05.240190      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8278, replica count: 2
I0817 03:46:08.291015      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:46:08.291: INFO: Creating new exec pod
Aug 17 03:46:08.297: INFO: Waiting up to 5m0s for pod "execpodq2jbn" in namespace "services-8278" to be "running"
Aug 17 03:46:08.302: INFO: Pod "execpodq2jbn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36308ms
Aug 17 03:46:10.306: INFO: Pod "execpodq2jbn": Phase="Running", Reason="", readiness=true. Elapsed: 2.008634068s
Aug 17 03:46:10.306: INFO: Pod "execpodq2jbn" satisfied condition "running"
Aug 17 03:46:11.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8278 exec execpodq2jbn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 17 03:46:11.742: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 17 03:46:11.742: INFO: stdout: ""
Aug 17 03:46:11.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8278 exec execpodq2jbn -- /bin/sh -x -c nc -v -z -w 2 172.20.46.164 80'
Aug 17 03:46:12.217: INFO: stderr: "+ nc -v -z -w 2 172.20.46.164 80\nConnection to 172.20.46.164 80 port [tcp/http] succeeded!\n"
Aug 17 03:46:12.217: INFO: stdout: ""
Aug 17 03:46:12.217: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:12.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8278" for this suite. 08/17/23 03:46:12.242
------------------------------
• [SLOW TEST] [7.063 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:05.186
    Aug 17 03:46:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:46:05.187
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:05.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:05.206
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8278 08/17/23 03:46:05.21
    STEP: changing the ExternalName service to type=ClusterIP 08/17/23 03:46:05.216
    STEP: creating replication controller externalname-service in namespace services-8278 08/17/23 03:46:05.234
    I0817 03:46:05.240190      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8278, replica count: 2
    I0817 03:46:08.291015      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:46:08.291: INFO: Creating new exec pod
    Aug 17 03:46:08.297: INFO: Waiting up to 5m0s for pod "execpodq2jbn" in namespace "services-8278" to be "running"
    Aug 17 03:46:08.302: INFO: Pod "execpodq2jbn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36308ms
    Aug 17 03:46:10.306: INFO: Pod "execpodq2jbn": Phase="Running", Reason="", readiness=true. Elapsed: 2.008634068s
    Aug 17 03:46:10.306: INFO: Pod "execpodq2jbn" satisfied condition "running"
    Aug 17 03:46:11.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8278 exec execpodq2jbn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 17 03:46:11.742: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 17 03:46:11.742: INFO: stdout: ""
    Aug 17 03:46:11.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8278 exec execpodq2jbn -- /bin/sh -x -c nc -v -z -w 2 172.20.46.164 80'
    Aug 17 03:46:12.217: INFO: stderr: "+ nc -v -z -w 2 172.20.46.164 80\nConnection to 172.20.46.164 80 port [tcp/http] succeeded!\n"
    Aug 17 03:46:12.217: INFO: stdout: ""
    Aug 17 03:46:12.217: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:12.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8278" for this suite. 08/17/23 03:46:12.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:12.251
Aug 17 03:46:12.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 03:46:12.252
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:12.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:12.271
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/17/23 03:46:12.276
Aug 17 03:46:12.281: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/17/23 03:46:12.281
Aug 17 03:46:12.288: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/17/23 03:46:12.288
Aug 17 03:46:12.299: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:12.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2067" for this suite. 08/17/23 03:46:12.306
------------------------------
• [0.064 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:12.251
    Aug 17 03:46:12.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 03:46:12.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:12.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:12.271
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/17/23 03:46:12.276
    Aug 17 03:46:12.281: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/17/23 03:46:12.281
    Aug 17 03:46:12.288: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/17/23 03:46:12.288
    Aug 17 03:46:12.299: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:12.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2067" for this suite. 08/17/23 03:46:12.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:12.316
Aug 17 03:46:12.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 03:46:12.317
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:12.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:12.336
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/17/23 03:46:12.346
Aug 17 03:46:12.346: INFO: Creating simple deployment test-deployment-9kd5s
Aug 17 03:46:12.361: INFO: deployment "test-deployment-9kd5s" doesn't have the required revision set
STEP: Getting /status 08/17/23 03:46:14.381
Aug 17 03:46:14.386: INFO: Deployment test-deployment-9kd5s has Conditions: [{Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/17/23 03:46:14.386
Aug 17 03:46:14.399: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 46, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-9kd5s-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/17/23 03:46:14.399
Aug 17 03:46:14.402: INFO: Observed &Deployment event: ADDED
Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
Aug 17 03:46:14.402: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 03:46:14.402: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9kd5s-54bc444df" is progressing.}
Aug 17 03:46:14.403: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
Aug 17 03:46:14.403: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
Aug 17 03:46:14.403: INFO: Found Deployment test-deployment-9kd5s in namespace deployment-2000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 03:46:14.403: INFO: Deployment test-deployment-9kd5s has an updated status
STEP: patching the Statefulset Status 08/17/23 03:46:14.403
Aug 17 03:46:14.403: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 17 03:46:14.410: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/17/23 03:46:14.41
Aug 17 03:46:14.414: INFO: Observed &Deployment event: ADDED
Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
Aug 17 03:46:14.414: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 03:46:14.414: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9kd5s-54bc444df" is progressing.}
Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
Aug 17 03:46:14.415: INFO: Found deployment test-deployment-9kd5s in namespace deployment-2000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 17 03:46:14.415: INFO: Deployment test-deployment-9kd5s has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 03:46:14.419: INFO: Deployment "test-deployment-9kd5s":
&Deployment{ObjectMeta:{test-deployment-9kd5s  deployment-2000  ce360bb4-bc05-4245-b30d-b91b1256f368 229687 1 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00484fc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-9kd5s-54bc444df",LastUpdateTime:2023-08-17 03:46:14 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 03:46:14.424: INFO: New ReplicaSet "test-deployment-9kd5s-54bc444df" of Deployment "test-deployment-9kd5s":
&ReplicaSet{ObjectMeta:{test-deployment-9kd5s-54bc444df  deployment-2000  a195c680-45b6-4bf6-85ea-c8154357611e 229682 1 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-9kd5s ce360bb4-bc05-4245-b30d-b91b1256f368 0xc0007fc277 0xc0007fc278}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce360bb4-bc05-4245-b30d-b91b1256f368\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007fc548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 03:46:14.429: INFO: Pod "test-deployment-9kd5s-54bc444df-29cgp" is available:
&Pod{ObjectMeta:{test-deployment-9kd5s-54bc444df-29cgp test-deployment-9kd5s-54bc444df- deployment-2000  b7c380c4-cc75-41b0-bbb0-aa4fa3b839a3 229681 0 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:69fa306657c09c35a75ad9dd934ad2b8a6218b2f4761fc821d038b949d90b175 cni.projectcalico.org/podIP:172.21.15.72/32 cni.projectcalico.org/podIPs:172.21.15.72/32] [{apps/v1 ReplicaSet test-deployment-9kd5s-54bc444df a195c680-45b6-4bf6-85ea-c8154357611e 0xc0048b25a0 0xc0048b25a1}] [] [{kube-controller-manager Update v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a195c680-45b6-4bf6-85ea-c8154357611e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:46:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gckw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gckw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.72,StartTime:2023-08-17 03:46:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:46:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fff5961362476857cf291f629ba072bac001d9d7f031e27fb0e13d02826542ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2000" for this suite. 08/17/23 03:46:14.436
------------------------------
• [2.128 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:12.316
    Aug 17 03:46:12.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 03:46:12.317
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:12.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:12.336
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/17/23 03:46:12.346
    Aug 17 03:46:12.346: INFO: Creating simple deployment test-deployment-9kd5s
    Aug 17 03:46:12.361: INFO: deployment "test-deployment-9kd5s" doesn't have the required revision set
    STEP: Getting /status 08/17/23 03:46:14.381
    Aug 17 03:46:14.386: INFO: Deployment test-deployment-9kd5s has Conditions: [{Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/17/23 03:46:14.386
    Aug 17 03:46:14.399: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 3, 46, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 3, 46, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-9kd5s-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/17/23 03:46:14.399
    Aug 17 03:46:14.402: INFO: Observed &Deployment event: ADDED
    Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
    Aug 17 03:46:14.402: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
    Aug 17 03:46:14.402: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 03:46:14.402: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9kd5s-54bc444df" is progressing.}
    Aug 17 03:46:14.403: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
    Aug 17 03:46:14.403: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 03:46:14.403: INFO: Observed Deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
    Aug 17 03:46:14.403: INFO: Found Deployment test-deployment-9kd5s in namespace deployment-2000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 03:46:14.403: INFO: Deployment test-deployment-9kd5s has an updated status
    STEP: patching the Statefulset Status 08/17/23 03:46:14.403
    Aug 17 03:46:14.403: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 17 03:46:14.410: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/17/23 03:46:14.41
    Aug 17 03:46:14.414: INFO: Observed &Deployment event: ADDED
    Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
    Aug 17 03:46:14.414: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9kd5s-54bc444df"}
    Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 03:46:14.414: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 17 03:46:14.414: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:12 +0000 UTC 2023-08-17 03:46:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9kd5s-54bc444df" is progressing.}
    Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
    Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-17 03:46:14 +0000 UTC 2023-08-17 03:46:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9kd5s-54bc444df" has successfully progressed.}
    Aug 17 03:46:14.415: INFO: Observed deployment test-deployment-9kd5s in namespace deployment-2000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 17 03:46:14.415: INFO: Observed &Deployment event: MODIFIED
    Aug 17 03:46:14.415: INFO: Found deployment test-deployment-9kd5s in namespace deployment-2000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 17 03:46:14.415: INFO: Deployment test-deployment-9kd5s has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 03:46:14.419: INFO: Deployment "test-deployment-9kd5s":
    &Deployment{ObjectMeta:{test-deployment-9kd5s  deployment-2000  ce360bb4-bc05-4245-b30d-b91b1256f368 229687 1 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00484fc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-9kd5s-54bc444df",LastUpdateTime:2023-08-17 03:46:14 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 03:46:14.424: INFO: New ReplicaSet "test-deployment-9kd5s-54bc444df" of Deployment "test-deployment-9kd5s":
    &ReplicaSet{ObjectMeta:{test-deployment-9kd5s-54bc444df  deployment-2000  a195c680-45b6-4bf6-85ea-c8154357611e 229682 1 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-9kd5s ce360bb4-bc05-4245-b30d-b91b1256f368 0xc0007fc277 0xc0007fc278}] [] [{kube-controller-manager Update apps/v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce360bb4-bc05-4245-b30d-b91b1256f368\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007fc548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 03:46:14.429: INFO: Pod "test-deployment-9kd5s-54bc444df-29cgp" is available:
    &Pod{ObjectMeta:{test-deployment-9kd5s-54bc444df-29cgp test-deployment-9kd5s-54bc444df- deployment-2000  b7c380c4-cc75-41b0-bbb0-aa4fa3b839a3 229681 0 2023-08-17 03:46:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:69fa306657c09c35a75ad9dd934ad2b8a6218b2f4761fc821d038b949d90b175 cni.projectcalico.org/podIP:172.21.15.72/32 cni.projectcalico.org/podIPs:172.21.15.72/32] [{apps/v1 ReplicaSet test-deployment-9kd5s-54bc444df a195c680-45b6-4bf6-85ea-c8154357611e 0xc0048b25a0 0xc0048b25a1}] [] [{kube-controller-manager Update v1 2023-08-17 03:46:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a195c680-45b6-4bf6-85ea-c8154357611e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 03:46:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 03:46:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gckw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gckw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 03:46:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.72,StartTime:2023-08-17 03:46:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 03:46:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fff5961362476857cf291f629ba072bac001d9d7f031e27fb0e13d02826542ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2000" for this suite. 08/17/23 03:46:14.436
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:14.444
Aug 17 03:46:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:46:14.445
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:14.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:14.464
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 17 03:46:14.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9126 version'
Aug 17 03:46:14.529: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 17 03:46:14.529: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9126" for this suite. 08/17/23 03:46:14.536
------------------------------
• [0.101 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:14.444
    Aug 17 03:46:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:46:14.445
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:14.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:14.464
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 17 03:46:14.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9126 version'
    Aug 17 03:46:14.529: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 17 03:46:14.529: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9126" for this suite. 08/17/23 03:46:14.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:14.546
Aug 17 03:46:14.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:46:14.548
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:14.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:14.566
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:46:14.571
Aug 17 03:46:14.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6" in namespace "projected-8197" to be "Succeeded or Failed"
Aug 17 03:46:14.585: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275036ms
Aug 17 03:46:16.591: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009400612s
Aug 17 03:46:18.590: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00925409s
STEP: Saw pod success 08/17/23 03:46:18.59
Aug 17 03:46:18.590: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6" satisfied condition "Succeeded or Failed"
Aug 17 03:46:18.594: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 container client-container: <nil>
STEP: delete the pod 08/17/23 03:46:18.603
Aug 17 03:46:18.616: INFO: Waiting for pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 to disappear
Aug 17 03:46:18.629: INFO: Pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8197" for this suite. 08/17/23 03:46:18.636
------------------------------
• [4.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:14.546
    Aug 17 03:46:14.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:46:14.548
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:14.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:14.566
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:46:14.571
    Aug 17 03:46:14.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6" in namespace "projected-8197" to be "Succeeded or Failed"
    Aug 17 03:46:14.585: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275036ms
    Aug 17 03:46:16.591: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009400612s
    Aug 17 03:46:18.590: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00925409s
    STEP: Saw pod success 08/17/23 03:46:18.59
    Aug 17 03:46:18.590: INFO: Pod "downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6" satisfied condition "Succeeded or Failed"
    Aug 17 03:46:18.594: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:46:18.603
    Aug 17 03:46:18.616: INFO: Waiting for pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 to disappear
    Aug 17 03:46:18.629: INFO: Pod downwardapi-volume-7f064238-e9fb-458c-8f2c-416f2bde62f6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8197" for this suite. 08/17/23 03:46:18.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:18.643
Aug 17 03:46:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 03:46:18.644
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.661
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/17/23 03:46:18.665
STEP: watching for the ServiceAccount to be added 08/17/23 03:46:18.674
STEP: patching the ServiceAccount 08/17/23 03:46:18.676
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/17/23 03:46:18.682
STEP: deleting the ServiceAccount 08/17/23 03:46:18.687
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-971" for this suite. 08/17/23 03:46:18.706
------------------------------
• [0.070 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:18.643
    Aug 17 03:46:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 03:46:18.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.661
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/17/23 03:46:18.665
    STEP: watching for the ServiceAccount to be added 08/17/23 03:46:18.674
    STEP: patching the ServiceAccount 08/17/23 03:46:18.676
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/17/23 03:46:18.682
    STEP: deleting the ServiceAccount 08/17/23 03:46:18.687
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-971" for this suite. 08/17/23 03:46:18.706
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:18.715
Aug 17 03:46:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:46:18.716
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.734
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-618-delete-me 08/17/23 03:46:18.743
STEP: Waiting for the RuntimeClass to disappear 08/17/23 03:46:18.749
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-618" for this suite. 08/17/23 03:46:18.774
------------------------------
• [0.066 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:18.715
    Aug 17 03:46:18.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:46:18.716
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.734
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-618-delete-me 08/17/23 03:46:18.743
    STEP: Waiting for the RuntimeClass to disappear 08/17/23 03:46:18.749
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-618" for this suite. 08/17/23 03:46:18.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:18.784
Aug 17 03:46:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:46:18.786
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.804
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/17/23 03:46:18.809
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +notcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_tcp@PTR;sleep 1; done
 08/17/23 03:46:18.825
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +notcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_tcp@PTR;sleep 1; done
 08/17/23 03:46:18.825
STEP: creating a pod to probe DNS 08/17/23 03:46:18.825
STEP: submitting the pod to kubernetes 08/17/23 03:46:18.825
Aug 17 03:46:18.837: INFO: Waiting up to 15m0s for pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39" in namespace "dns-1851" to be "running"
Aug 17 03:46:18.842: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.665482ms
Aug 17 03:46:20.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700655s
Aug 17 03:46:22.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Running", Reason="", readiness=true. Elapsed: 4.011033505s
Aug 17 03:46:22.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:46:22.848
STEP: looking for the results for each expected name from probers 08/17/23 03:46:22.854
Aug 17 03:46:22.945: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:22.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:22.995: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.009: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.058: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.068: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.074: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.086: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:23.130: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:28.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.182: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.255: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.261: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.268: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.279: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:28.320: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:33.179: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.223: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.241: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.247: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.288: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.294: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.300: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.307: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.312: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:33.360: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:38.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.181: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.246: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.252: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.257: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.263: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.268: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:38.307: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:43.138: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.182: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.205: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.250: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.256: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.262: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.267: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.273: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.279: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:43.315: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:48.138: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.181: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.257: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.264: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.270: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.284: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.291: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
Aug 17 03:46:48.331: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

Aug 17 03:46:53.369: INFO: DNS probes using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 succeeded

STEP: deleting the pod 08/17/23 03:46:53.369
STEP: deleting the test service 08/17/23 03:46:53.388
STEP: deleting the test headless service 08/17/23 03:46:53.405
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:53.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1851" for this suite. 08/17/23 03:46:53.449
------------------------------
• [SLOW TEST] [34.673 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:18.784
    Aug 17 03:46:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:46:18.786
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:18.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:18.804
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/17/23 03:46:18.809
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +notcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_tcp@PTR;sleep 1; done
     08/17/23 03:46:18.825
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1851.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1851.svc;check="$$(dig +notcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.119.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.119.175_tcp@PTR;sleep 1; done
     08/17/23 03:46:18.825
    STEP: creating a pod to probe DNS 08/17/23 03:46:18.825
    STEP: submitting the pod to kubernetes 08/17/23 03:46:18.825
    Aug 17 03:46:18.837: INFO: Waiting up to 15m0s for pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39" in namespace "dns-1851" to be "running"
    Aug 17 03:46:18.842: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.665482ms
    Aug 17 03:46:20.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010700655s
    Aug 17 03:46:22.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39": Phase="Running", Reason="", readiness=true. Elapsed: 4.011033505s
    Aug 17 03:46:22.848: INFO: Pod "dns-test-70609ff7-2cf9-4370-b041-885101574e39" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:46:22.848
    STEP: looking for the results for each expected name from probers 08/17/23 03:46:22.854
    Aug 17 03:46:22.945: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:22.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:22.995: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.009: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.058: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.068: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.074: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.086: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:23.130: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:28.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.182: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.255: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.261: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.268: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.279: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:28.320: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:33.179: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.223: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.229: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.241: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.247: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.288: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.294: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.300: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.307: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.312: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:33.360: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:38.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.181: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.246: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.252: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.257: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.263: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.268: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:38.307: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:43.138: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.182: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.199: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.205: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.250: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.256: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.262: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.267: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.273: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.279: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:43.315: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:48.138: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.181: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.257: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.264: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.270: INFO: Unable to read jessie_udp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851 from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.284: INFO: Unable to read jessie_udp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.291: INFO: Unable to read jessie_tcp@dns-test-service.dns-1851.svc from pod dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39: the server could not find the requested resource (get pods dns-test-70609ff7-2cf9-4370-b041-885101574e39)
    Aug 17 03:46:48.331: INFO: Lookups using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1851 wheezy_tcp@dns-test-service.dns-1851 wheezy_udp@dns-test-service.dns-1851.svc wheezy_tcp@dns-test-service.dns-1851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1851 jessie_tcp@dns-test-service.dns-1851 jessie_udp@dns-test-service.dns-1851.svc jessie_tcp@dns-test-service.dns-1851.svc]

    Aug 17 03:46:53.369: INFO: DNS probes using dns-1851/dns-test-70609ff7-2cf9-4370-b041-885101574e39 succeeded

    STEP: deleting the pod 08/17/23 03:46:53.369
    STEP: deleting the test service 08/17/23 03:46:53.388
    STEP: deleting the test headless service 08/17/23 03:46:53.405
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:53.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1851" for this suite. 08/17/23 03:46:53.449
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:53.458
Aug 17 03:46:53.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:46:53.459
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:53.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:53.48
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/17/23 03:46:53.485
Aug 17 03:46:53.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 create -f -'
Aug 17 03:46:53.721: INFO: stderr: ""
Aug 17 03:46:53.721: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:46:53.721
Aug 17 03:46:53.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:46:53.801: INFO: stderr: ""
Aug 17 03:46:53.801: INFO: stdout: "update-demo-nautilus-9b4pl update-demo-nautilus-g9n6j "
Aug 17 03:46:53.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:46:53.865: INFO: stderr: ""
Aug 17 03:46:53.865: INFO: stdout: ""
Aug 17 03:46:53.865: INFO: update-demo-nautilus-9b4pl is created but not running
Aug 17 03:46:58.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 17 03:46:58.936: INFO: stderr: ""
Aug 17 03:46:58.936: INFO: stdout: "update-demo-nautilus-9b4pl update-demo-nautilus-g9n6j "
Aug 17 03:46:58.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:46:59.000: INFO: stderr: ""
Aug 17 03:46:59.000: INFO: stdout: "true"
Aug 17 03:46:59.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:46:59.061: INFO: stderr: ""
Aug 17 03:46:59.061: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:46:59.061: INFO: validating pod update-demo-nautilus-9b4pl
Aug 17 03:46:59.153: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:46:59.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:46:59.153: INFO: update-demo-nautilus-9b4pl is verified up and running
Aug 17 03:46:59.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-g9n6j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 17 03:46:59.216: INFO: stderr: ""
Aug 17 03:46:59.216: INFO: stdout: "true"
Aug 17 03:46:59.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-g9n6j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 17 03:46:59.285: INFO: stderr: ""
Aug 17 03:46:59.285: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 17 03:46:59.286: INFO: validating pod update-demo-nautilus-g9n6j
Aug 17 03:46:59.376: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 17 03:46:59.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 17 03:46:59.376: INFO: update-demo-nautilus-g9n6j is verified up and running
STEP: using delete to clean up resources 08/17/23 03:46:59.376
Aug 17 03:46:59.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 delete --grace-period=0 --force -f -'
Aug 17 03:46:59.440: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 03:46:59.440: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 17 03:46:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get rc,svc -l name=update-demo --no-headers'
Aug 17 03:46:59.529: INFO: stderr: "No resources found in kubectl-1855 namespace.\n"
Aug 17 03:46:59.529: INFO: stdout: ""
Aug 17 03:46:59.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 03:46:59.607: INFO: stderr: ""
Aug 17 03:46:59.607: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:46:59.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1855" for this suite. 08/17/23 03:46:59.616
------------------------------
• [SLOW TEST] [6.178 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:53.458
    Aug 17 03:46:53.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:46:53.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:53.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:53.48
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/17/23 03:46:53.485
    Aug 17 03:46:53.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 create -f -'
    Aug 17 03:46:53.721: INFO: stderr: ""
    Aug 17 03:46:53.721: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/17/23 03:46:53.721
    Aug 17 03:46:53.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:46:53.801: INFO: stderr: ""
    Aug 17 03:46:53.801: INFO: stdout: "update-demo-nautilus-9b4pl update-demo-nautilus-g9n6j "
    Aug 17 03:46:53.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:46:53.865: INFO: stderr: ""
    Aug 17 03:46:53.865: INFO: stdout: ""
    Aug 17 03:46:53.865: INFO: update-demo-nautilus-9b4pl is created but not running
    Aug 17 03:46:58.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 17 03:46:58.936: INFO: stderr: ""
    Aug 17 03:46:58.936: INFO: stdout: "update-demo-nautilus-9b4pl update-demo-nautilus-g9n6j "
    Aug 17 03:46:58.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:46:59.000: INFO: stderr: ""
    Aug 17 03:46:59.000: INFO: stdout: "true"
    Aug 17 03:46:59.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-9b4pl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:46:59.061: INFO: stderr: ""
    Aug 17 03:46:59.061: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:46:59.061: INFO: validating pod update-demo-nautilus-9b4pl
    Aug 17 03:46:59.153: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:46:59.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:46:59.153: INFO: update-demo-nautilus-9b4pl is verified up and running
    Aug 17 03:46:59.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-g9n6j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 17 03:46:59.216: INFO: stderr: ""
    Aug 17 03:46:59.216: INFO: stdout: "true"
    Aug 17 03:46:59.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods update-demo-nautilus-g9n6j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 17 03:46:59.285: INFO: stderr: ""
    Aug 17 03:46:59.285: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 17 03:46:59.286: INFO: validating pod update-demo-nautilus-g9n6j
    Aug 17 03:46:59.376: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 17 03:46:59.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 17 03:46:59.376: INFO: update-demo-nautilus-g9n6j is verified up and running
    STEP: using delete to clean up resources 08/17/23 03:46:59.376
    Aug 17 03:46:59.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 delete --grace-period=0 --force -f -'
    Aug 17 03:46:59.440: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 03:46:59.440: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 17 03:46:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get rc,svc -l name=update-demo --no-headers'
    Aug 17 03:46:59.529: INFO: stderr: "No resources found in kubectl-1855 namespace.\n"
    Aug 17 03:46:59.529: INFO: stdout: ""
    Aug 17 03:46:59.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1855 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 03:46:59.607: INFO: stderr: ""
    Aug 17 03:46:59.607: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:46:59.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1855" for this suite. 08/17/23 03:46:59.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:46:59.636
Aug 17 03:46:59.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:46:59.636
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:59.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:59.664
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:46:59.67
Aug 17 03:46:59.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301" in namespace "downward-api-9380" to be "Succeeded or Failed"
Aug 17 03:46:59.687: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Pending", Reason="", readiness=false. Elapsed: 5.366135ms
Aug 17 03:47:01.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012050959s
Aug 17 03:47:03.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012374451s
STEP: Saw pod success 08/17/23 03:47:03.694
Aug 17 03:47:03.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301" satisfied condition "Succeeded or Failed"
Aug 17 03:47:03.699: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 container client-container: <nil>
STEP: delete the pod 08/17/23 03:47:03.71
Aug 17 03:47:03.723: INFO: Waiting for pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 to disappear
Aug 17 03:47:03.729: INFO: Pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9380" for this suite. 08/17/23 03:47:03.737
------------------------------
• [4.110 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:46:59.636
    Aug 17 03:46:59.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:46:59.636
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:46:59.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:46:59.664
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:46:59.67
    Aug 17 03:46:59.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301" in namespace "downward-api-9380" to be "Succeeded or Failed"
    Aug 17 03:46:59.687: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Pending", Reason="", readiness=false. Elapsed: 5.366135ms
    Aug 17 03:47:01.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012050959s
    Aug 17 03:47:03.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012374451s
    STEP: Saw pod success 08/17/23 03:47:03.694
    Aug 17 03:47:03.694: INFO: Pod "downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301" satisfied condition "Succeeded or Failed"
    Aug 17 03:47:03.699: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:47:03.71
    Aug 17 03:47:03.723: INFO: Waiting for pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 to disappear
    Aug 17 03:47:03.729: INFO: Pod downwardapi-volume-be8685a6-d7e9-4930-a558-419a00b61301 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9380" for this suite. 08/17/23 03:47:03.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:03.747
Aug 17 03:47:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:47:03.748
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:03.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:03.768
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-992 08/17/23 03:47:03.774
STEP: creating replication controller nodeport-test in namespace services-992 08/17/23 03:47:03.79
I0817 03:47:03.796666      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-992, replica count: 2
I0817 03:47:06.847896      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:47:06.847: INFO: Creating new exec pod
Aug 17 03:47:06.854: INFO: Waiting up to 5m0s for pod "execpodkrtzs" in namespace "services-992" to be "running"
Aug 17 03:47:06.861: INFO: Pod "execpodkrtzs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.168028ms
Aug 17 03:47:08.866: INFO: Pod "execpodkrtzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.011748239s
Aug 17 03:47:08.866: INFO: Pod "execpodkrtzs" satisfied condition "running"
Aug 17 03:47:09.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 17 03:47:10.356: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 17 03:47:10.356: INFO: stdout: ""
Aug 17 03:47:10.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 172.20.123.192 80'
Aug 17 03:47:10.838: INFO: stderr: "+ nc -v -z -w 2 172.20.123.192 80\nConnection to 172.20.123.192 80 port [tcp/http] succeeded!\n"
Aug 17 03:47:10.838: INFO: stdout: ""
Aug 17 03:47:10.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 31304'
Aug 17 03:47:11.364: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 31304\nConnection to 192.168.11.3 31304 port [tcp/*] succeeded!\n"
Aug 17 03:47:11.364: INFO: stdout: ""
Aug 17 03:47:11.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 31304'
Aug 17 03:47:11.887: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 31304\nConnection to 192.168.11.4 31304 port [tcp/*] succeeded!\n"
Aug 17 03:47:11.887: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:11.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-992" for this suite. 08/17/23 03:47:11.895
------------------------------
• [SLOW TEST] [8.156 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:03.747
    Aug 17 03:47:03.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:47:03.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:03.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:03.768
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-992 08/17/23 03:47:03.774
    STEP: creating replication controller nodeport-test in namespace services-992 08/17/23 03:47:03.79
    I0817 03:47:03.796666      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-992, replica count: 2
    I0817 03:47:06.847896      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:47:06.847: INFO: Creating new exec pod
    Aug 17 03:47:06.854: INFO: Waiting up to 5m0s for pod "execpodkrtzs" in namespace "services-992" to be "running"
    Aug 17 03:47:06.861: INFO: Pod "execpodkrtzs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.168028ms
    Aug 17 03:47:08.866: INFO: Pod "execpodkrtzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.011748239s
    Aug 17 03:47:08.866: INFO: Pod "execpodkrtzs" satisfied condition "running"
    Aug 17 03:47:09.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 17 03:47:10.356: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 17 03:47:10.356: INFO: stdout: ""
    Aug 17 03:47:10.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 172.20.123.192 80'
    Aug 17 03:47:10.838: INFO: stderr: "+ nc -v -z -w 2 172.20.123.192 80\nConnection to 172.20.123.192 80 port [tcp/http] succeeded!\n"
    Aug 17 03:47:10.838: INFO: stdout: ""
    Aug 17 03:47:10.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 31304'
    Aug 17 03:47:11.364: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 31304\nConnection to 192.168.11.3 31304 port [tcp/*] succeeded!\n"
    Aug 17 03:47:11.364: INFO: stdout: ""
    Aug 17 03:47:11.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-992 exec execpodkrtzs -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 31304'
    Aug 17 03:47:11.887: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 31304\nConnection to 192.168.11.4 31304 port [tcp/*] succeeded!\n"
    Aug 17 03:47:11.887: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:11.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-992" for this suite. 08/17/23 03:47:11.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:11.904
Aug 17 03:47:11.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 03:47:11.905
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:11.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:11.924
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/17/23 03:47:11.929
Aug 17 03:47:11.941: INFO: Waiting up to 5m0s for pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3" in namespace "var-expansion-2928" to be "Succeeded or Failed"
Aug 17 03:47:11.946: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693147ms
Aug 17 03:47:13.953: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012030948s
Aug 17 03:47:15.951: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010153935s
STEP: Saw pod success 08/17/23 03:47:15.951
Aug 17 03:47:15.951: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3" satisfied condition "Succeeded or Failed"
Aug 17 03:47:15.955: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 container dapi-container: <nil>
STEP: delete the pod 08/17/23 03:47:15.965
Aug 17 03:47:15.977: INFO: Waiting for pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 to disappear
Aug 17 03:47:15.981: INFO: Pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2928" for this suite. 08/17/23 03:47:15.988
------------------------------
• [4.091 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:11.904
    Aug 17 03:47:11.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 03:47:11.905
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:11.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:11.924
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/17/23 03:47:11.929
    Aug 17 03:47:11.941: INFO: Waiting up to 5m0s for pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3" in namespace "var-expansion-2928" to be "Succeeded or Failed"
    Aug 17 03:47:11.946: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693147ms
    Aug 17 03:47:13.953: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012030948s
    Aug 17 03:47:15.951: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010153935s
    STEP: Saw pod success 08/17/23 03:47:15.951
    Aug 17 03:47:15.951: INFO: Pod "var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3" satisfied condition "Succeeded or Failed"
    Aug 17 03:47:15.955: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 03:47:15.965
    Aug 17 03:47:15.977: INFO: Waiting for pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 to disappear
    Aug 17 03:47:15.981: INFO: Pod var-expansion-114561c2-9c1d-4c6d-a9cd-9dce9e8c3de3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2928" for this suite. 08/17/23 03:47:15.988
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:15.996
Aug 17 03:47:15.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:47:15.996
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:16.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:16.015
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/17/23 03:47:16.019
Aug 17 03:47:16.029: INFO: Waiting up to 5m0s for pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4" in namespace "emptydir-1802" to be "Succeeded or Failed"
Aug 17 03:47:16.034: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211915ms
Aug 17 03:47:18.038: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008437242s
Aug 17 03:47:20.038: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009120133s
STEP: Saw pod success 08/17/23 03:47:20.039
Aug 17 03:47:20.039: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4" satisfied condition "Succeeded or Failed"
Aug 17 03:47:20.043: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 container test-container: <nil>
STEP: delete the pod 08/17/23 03:47:20.053
Aug 17 03:47:20.065: INFO: Waiting for pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 to disappear
Aug 17 03:47:20.068: INFO: Pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:20.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1802" for this suite. 08/17/23 03:47:20.075
------------------------------
• [4.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:15.996
    Aug 17 03:47:15.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:47:15.996
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:16.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:16.015
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/17/23 03:47:16.019
    Aug 17 03:47:16.029: INFO: Waiting up to 5m0s for pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4" in namespace "emptydir-1802" to be "Succeeded or Failed"
    Aug 17 03:47:16.034: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211915ms
    Aug 17 03:47:18.038: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008437242s
    Aug 17 03:47:20.038: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009120133s
    STEP: Saw pod success 08/17/23 03:47:20.039
    Aug 17 03:47:20.039: INFO: Pod "pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4" satisfied condition "Succeeded or Failed"
    Aug 17 03:47:20.043: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 container test-container: <nil>
    STEP: delete the pod 08/17/23 03:47:20.053
    Aug 17 03:47:20.065: INFO: Waiting for pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 to disappear
    Aug 17 03:47:20.068: INFO: Pod pod-9c806272-fb55-4d07-82dc-9ad9aa2346b4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:20.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1802" for this suite. 08/17/23 03:47:20.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:20.087
Aug 17 03:47:20.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename watch 08/17/23 03:47:20.088
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:20.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:20.106
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/17/23 03:47:20.11
STEP: creating a new configmap 08/17/23 03:47:20.113
STEP: modifying the configmap once 08/17/23 03:47:20.118
STEP: changing the label value of the configmap 08/17/23 03:47:20.128
STEP: Expecting to observe a delete notification for the watched object 08/17/23 03:47:20.138
Aug 17 03:47:20.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230336 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:47:20.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230337 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:47:20.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230338 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/17/23 03:47:20.138
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/17/23 03:47:20.147
STEP: changing the label value of the configmap back 08/17/23 03:47:30.148
STEP: modifying the configmap a third time 08/17/23 03:47:30.159
STEP: deleting the configmap 08/17/23 03:47:30.169
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/17/23 03:47:30.175
Aug 17 03:47:30.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230387 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:47:30.175: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230388 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 03:47:30.175: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230389 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:30.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4281" for this suite. 08/17/23 03:47:30.181
------------------------------
• [SLOW TEST] [10.101 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:20.087
    Aug 17 03:47:20.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename watch 08/17/23 03:47:20.088
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:20.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:20.106
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/17/23 03:47:20.11
    STEP: creating a new configmap 08/17/23 03:47:20.113
    STEP: modifying the configmap once 08/17/23 03:47:20.118
    STEP: changing the label value of the configmap 08/17/23 03:47:20.128
    STEP: Expecting to observe a delete notification for the watched object 08/17/23 03:47:20.138
    Aug 17 03:47:20.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230336 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:47:20.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230337 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:47:20.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230338 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/17/23 03:47:20.138
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/17/23 03:47:20.147
    STEP: changing the label value of the configmap back 08/17/23 03:47:30.148
    STEP: modifying the configmap a third time 08/17/23 03:47:30.159
    STEP: deleting the configmap 08/17/23 03:47:30.169
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/17/23 03:47:30.175
    Aug 17 03:47:30.175: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230387 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:47:30.175: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230388 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 03:47:30.175: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4281  fe2bdf28-70e0-412b-b726-500e9f824456 230389 0 2023-08-17 03:47:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-17 03:47:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:30.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4281" for this suite. 08/17/23 03:47:30.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:30.19
Aug 17 03:47:30.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:47:30.191
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:30.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:30.208
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:47:30.213
Aug 17 03:47:30.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746" in namespace "projected-9712" to be "Succeeded or Failed"
Aug 17 03:47:30.226: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635616ms
Aug 17 03:47:32.231: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Running", Reason="", readiness=true. Elapsed: 2.009021906s
Aug 17 03:47:34.231: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Running", Reason="", readiness=false. Elapsed: 4.009308137s
Aug 17 03:47:36.232: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010106984s
STEP: Saw pod success 08/17/23 03:47:36.232
Aug 17 03:47:36.232: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746" satisfied condition "Succeeded or Failed"
Aug 17 03:47:36.237: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 container client-container: <nil>
STEP: delete the pod 08/17/23 03:47:36.285
Aug 17 03:47:36.297: INFO: Waiting for pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 to disappear
Aug 17 03:47:36.301: INFO: Pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:36.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9712" for this suite. 08/17/23 03:47:36.308
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:30.19
    Aug 17 03:47:30.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:47:30.191
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:30.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:30.208
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:47:30.213
    Aug 17 03:47:30.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746" in namespace "projected-9712" to be "Succeeded or Failed"
    Aug 17 03:47:30.226: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635616ms
    Aug 17 03:47:32.231: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Running", Reason="", readiness=true. Elapsed: 2.009021906s
    Aug 17 03:47:34.231: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Running", Reason="", readiness=false. Elapsed: 4.009308137s
    Aug 17 03:47:36.232: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010106984s
    STEP: Saw pod success 08/17/23 03:47:36.232
    Aug 17 03:47:36.232: INFO: Pod "downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746" satisfied condition "Succeeded or Failed"
    Aug 17 03:47:36.237: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:47:36.285
    Aug 17 03:47:36.297: INFO: Waiting for pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 to disappear
    Aug 17 03:47:36.301: INFO: Pod downwardapi-volume-1c029d7f-0e3c-40a3-9b36-ec32d7511746 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:36.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9712" for this suite. 08/17/23 03:47:36.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:36.316
Aug 17 03:47:36.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:47:36.317
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:36.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:36.335
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 03:47:36.339
Aug 17 03:47:36.348: INFO: Waiting up to 5m0s for pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740" in namespace "emptydir-2073" to be "Succeeded or Failed"
Aug 17 03:47:36.355: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Pending", Reason="", readiness=false. Elapsed: 6.69499ms
Aug 17 03:47:38.361: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Running", Reason="", readiness=true. Elapsed: 2.013220377s
Aug 17 03:47:40.360: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Running", Reason="", readiness=false. Elapsed: 4.011625622s
Aug 17 03:47:42.361: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013028468s
STEP: Saw pod success 08/17/23 03:47:42.361
Aug 17 03:47:42.362: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740" satisfied condition "Succeeded or Failed"
Aug 17 03:47:42.366: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 container test-container: <nil>
STEP: delete the pod 08/17/23 03:47:42.377
Aug 17 03:47:42.388: INFO: Waiting for pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 to disappear
Aug 17 03:47:42.392: INFO: Pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:42.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2073" for this suite. 08/17/23 03:47:42.398
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:36.316
    Aug 17 03:47:36.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:47:36.317
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:36.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:36.335
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/17/23 03:47:36.339
    Aug 17 03:47:36.348: INFO: Waiting up to 5m0s for pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740" in namespace "emptydir-2073" to be "Succeeded or Failed"
    Aug 17 03:47:36.355: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Pending", Reason="", readiness=false. Elapsed: 6.69499ms
    Aug 17 03:47:38.361: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Running", Reason="", readiness=true. Elapsed: 2.013220377s
    Aug 17 03:47:40.360: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Running", Reason="", readiness=false. Elapsed: 4.011625622s
    Aug 17 03:47:42.361: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013028468s
    STEP: Saw pod success 08/17/23 03:47:42.361
    Aug 17 03:47:42.362: INFO: Pod "pod-e93424b0-6070-4fbf-a6a1-df232433b740" satisfied condition "Succeeded or Failed"
    Aug 17 03:47:42.366: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 container test-container: <nil>
    STEP: delete the pod 08/17/23 03:47:42.377
    Aug 17 03:47:42.388: INFO: Waiting for pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 to disappear
    Aug 17 03:47:42.392: INFO: Pod pod-e93424b0-6070-4fbf-a6a1-df232433b740 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:42.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2073" for this suite. 08/17/23 03:47:42.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:42.412
Aug 17 03:47:42.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context-test 08/17/23 03:47:42.413
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:42.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:42.43
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 17 03:47:42.444: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2" in namespace "security-context-test-7947" to be "Succeeded or Failed"
Aug 17 03:47:42.449: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.318034ms
Aug 17 03:47:44.455: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010628847s
Aug 17 03:47:46.455: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011195757s
Aug 17 03:47:48.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Running", Reason="", readiness=false. Elapsed: 6.01231046s
Aug 17 03:47:50.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012481491s
Aug 17 03:47:50.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 03:47:50.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7947" for this suite. 08/17/23 03:47:50.515
------------------------------
• [SLOW TEST] [8.118 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:42.412
    Aug 17 03:47:42.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context-test 08/17/23 03:47:42.413
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:42.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:42.43
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 17 03:47:42.444: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2" in namespace "security-context-test-7947" to be "Succeeded or Failed"
    Aug 17 03:47:42.449: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.318034ms
    Aug 17 03:47:44.455: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010628847s
    Aug 17 03:47:46.455: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011195757s
    Aug 17 03:47:48.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Running", Reason="", readiness=false. Elapsed: 6.01231046s
    Aug 17 03:47:50.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012481491s
    Aug 17 03:47:50.456: INFO: Pod "alpine-nnp-false-d7ef93cd-581e-45ae-9dff-0087874022d2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:47:50.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7947" for this suite. 08/17/23 03:47:50.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:47:50.532
Aug 17 03:47:50.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:47:50.532
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:50.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:50.556
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:47:50.576
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:47:50.897
STEP: Deploying the webhook pod 08/17/23 03:47:50.908
STEP: Wait for the deployment to be ready 08/17/23 03:47:50.922
Aug 17 03:47:50.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:47:52.947
STEP: Verifying the service has paired with the endpoint 08/17/23 03:47:52.958
Aug 17 03:47:53.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/17/23 03:47:53.965
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:53.965
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/17/23 03:47:54.069
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/17/23 03:47:55.083
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:55.083
STEP: Having no error when timeout is longer than webhook latency 08/17/23 03:47:56.121
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:56.121
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/17/23 03:48:01.297
STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:48:01.297
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5264" for this suite. 08/17/23 03:48:06.438
STEP: Destroying namespace "webhook-5264-markers" for this suite. 08/17/23 03:48:06.446
------------------------------
• [SLOW TEST] [15.934 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:47:50.532
    Aug 17 03:47:50.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:47:50.532
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:47:50.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:47:50.556
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:47:50.576
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:47:50.897
    STEP: Deploying the webhook pod 08/17/23 03:47:50.908
    STEP: Wait for the deployment to be ready 08/17/23 03:47:50.922
    Aug 17 03:47:50.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:47:52.947
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:47:52.958
    Aug 17 03:47:53.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/17/23 03:47:53.965
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:53.965
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/17/23 03:47:54.069
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/17/23 03:47:55.083
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:55.083
    STEP: Having no error when timeout is longer than webhook latency 08/17/23 03:47:56.121
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:47:56.121
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/17/23 03:48:01.297
    STEP: Registering slow webhook via the AdmissionRegistration API 08/17/23 03:48:01.297
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5264" for this suite. 08/17/23 03:48:06.438
    STEP: Destroying namespace "webhook-5264-markers" for this suite. 08/17/23 03:48:06.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:06.473
Aug 17 03:48:06.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:48:06.478
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:06.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:06.51
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-685465b7-7542-49ff-acdf-234857f20cb1 08/17/23 03:48:06.521
STEP: Creating a pod to test consume configMaps 08/17/23 03:48:06.529
Aug 17 03:48:06.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63" in namespace "projected-9418" to be "Succeeded or Failed"
Aug 17 03:48:06.547: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.697414ms
Aug 17 03:48:08.554: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011799781s
Aug 17 03:48:10.555: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012879423s
STEP: Saw pod success 08/17/23 03:48:10.555
Aug 17 03:48:10.555: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63" satisfied condition "Succeeded or Failed"
Aug 17 03:48:10.560: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:48:10.57
Aug 17 03:48:10.583: INFO: Waiting for pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 to disappear
Aug 17 03:48:10.587: INFO: Pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:10.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9418" for this suite. 08/17/23 03:48:10.594
------------------------------
• [4.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:06.473
    Aug 17 03:48:06.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:48:06.478
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:06.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:06.51
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-685465b7-7542-49ff-acdf-234857f20cb1 08/17/23 03:48:06.521
    STEP: Creating a pod to test consume configMaps 08/17/23 03:48:06.529
    Aug 17 03:48:06.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63" in namespace "projected-9418" to be "Succeeded or Failed"
    Aug 17 03:48:06.547: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.697414ms
    Aug 17 03:48:08.554: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011799781s
    Aug 17 03:48:10.555: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012879423s
    STEP: Saw pod success 08/17/23 03:48:10.555
    Aug 17 03:48:10.555: INFO: Pod "pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63" satisfied condition "Succeeded or Failed"
    Aug 17 03:48:10.560: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:48:10.57
    Aug 17 03:48:10.583: INFO: Waiting for pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 to disappear
    Aug 17 03:48:10.587: INFO: Pod pod-projected-configmaps-65e30dfb-70ae-466f-a363-8e71bff5eb63 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:10.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9418" for this suite. 08/17/23 03:48:10.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:10.605
Aug 17 03:48:10.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 03:48:10.606
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:10.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:10.628
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d 08/17/23 03:48:10.635
Aug 17 03:48:10.645: INFO: Pod name my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Found 0 pods out of 1
Aug 17 03:48:15.651: INFO: Pod name my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Found 1 pods out of 1
Aug 17 03:48:15.651: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d" are running
Aug 17 03:48:15.651: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" in namespace "replication-controller-5526" to be "running"
Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.445611ms
Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" satisfied condition "running"
Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:10 +0000 UTC Reason: Message:}])
Aug 17 03:48:15.656: INFO: Trying to dial the pod
Aug 17 03:48:20.759: INFO: Controller my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Got expected result from replica 1 [my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b]: "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5526" for this suite. 08/17/23 03:48:20.769
------------------------------
• [SLOW TEST] [10.173 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:10.605
    Aug 17 03:48:10.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 03:48:10.606
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:10.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:10.628
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d 08/17/23 03:48:10.635
    Aug 17 03:48:10.645: INFO: Pod name my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Found 0 pods out of 1
    Aug 17 03:48:15.651: INFO: Pod name my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Found 1 pods out of 1
    Aug 17 03:48:15.651: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d" are running
    Aug 17 03:48:15.651: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" in namespace "replication-controller-5526" to be "running"
    Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.445611ms
    Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" satisfied condition "running"
    Aug 17 03:48:15.656: INFO: Pod "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 03:48:10 +0000 UTC Reason: Message:}])
    Aug 17 03:48:15.656: INFO: Trying to dial the pod
    Aug 17 03:48:20.759: INFO: Controller my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d: Got expected result from replica 1 [my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b]: "my-hostname-basic-5791590f-28ba-44ae-b8d0-b139bd498f2d-65r2b", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5526" for this suite. 08/17/23 03:48:20.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:20.78
Aug 17 03:48:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:48:20.781
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:20.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:20.801
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/17/23 03:48:20.806
STEP: submitting the pod to kubernetes 08/17/23 03:48:20.806
Aug 17 03:48:20.818: INFO: Waiting up to 5m0s for pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" in namespace "pods-7796" to be "running and ready"
Aug 17 03:48:20.823: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163436ms
Aug 17 03:48:20.823: INFO: The phase of Pod pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:48:22.829: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Running", Reason="", readiness=true. Elapsed: 2.010443748s
Aug 17 03:48:22.829: INFO: The phase of Pod pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875 is Running (Ready = true)
Aug 17 03:48:22.829: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/17/23 03:48:22.833
STEP: updating the pod 08/17/23 03:48:22.837
Aug 17 03:48:23.353: INFO: Successfully updated pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875"
Aug 17 03:48:23.354: INFO: Waiting up to 5m0s for pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" in namespace "pods-7796" to be "running"
Aug 17 03:48:23.358: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Running", Reason="", readiness=true. Elapsed: 4.259737ms
Aug 17 03:48:23.358: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/17/23 03:48:23.358
Aug 17 03:48:23.365: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:23.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7796" for this suite. 08/17/23 03:48:23.373
------------------------------
• [2.601 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:20.78
    Aug 17 03:48:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:48:20.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:20.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:20.801
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/17/23 03:48:20.806
    STEP: submitting the pod to kubernetes 08/17/23 03:48:20.806
    Aug 17 03:48:20.818: INFO: Waiting up to 5m0s for pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" in namespace "pods-7796" to be "running and ready"
    Aug 17 03:48:20.823: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163436ms
    Aug 17 03:48:20.823: INFO: The phase of Pod pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:48:22.829: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Running", Reason="", readiness=true. Elapsed: 2.010443748s
    Aug 17 03:48:22.829: INFO: The phase of Pod pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875 is Running (Ready = true)
    Aug 17 03:48:22.829: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/17/23 03:48:22.833
    STEP: updating the pod 08/17/23 03:48:22.837
    Aug 17 03:48:23.353: INFO: Successfully updated pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875"
    Aug 17 03:48:23.354: INFO: Waiting up to 5m0s for pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" in namespace "pods-7796" to be "running"
    Aug 17 03:48:23.358: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875": Phase="Running", Reason="", readiness=true. Elapsed: 4.259737ms
    Aug 17 03:48:23.358: INFO: Pod "pod-update-28cef9d3-6c59-4a12-a999-1a5b3d1fd875" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/17/23 03:48:23.358
    Aug 17 03:48:23.365: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:23.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7796" for this suite. 08/17/23 03:48:23.373
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:23.382
Aug 17 03:48:23.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:48:23.383
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:23.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:23.402
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 17 03:48:23.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:29.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6058" for this suite. 08/17/23 03:48:29.708
------------------------------
• [SLOW TEST] [6.335 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:23.382
    Aug 17 03:48:23.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 03:48:23.383
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:23.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:23.402
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 17 03:48:23.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:29.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6058" for this suite. 08/17/23 03:48:29.708
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:29.718
Aug 17 03:48:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:48:29.719
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:29.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:29.739
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/17/23 03:48:29.744
Aug 17 03:48:29.753: INFO: Waiting up to 5m0s for pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5" in namespace "downward-api-5116" to be "running and ready"
Aug 17 03:48:29.757: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386667ms
Aug 17 03:48:29.757: INFO: The phase of Pod labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:48:31.764: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010780525s
Aug 17 03:48:31.764: INFO: The phase of Pod labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5 is Running (Ready = true)
Aug 17 03:48:31.764: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5" satisfied condition "running and ready"
Aug 17 03:48:32.291: INFO: Successfully updated pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5116" for this suite. 08/17/23 03:48:34.321
------------------------------
• [4.612 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:29.718
    Aug 17 03:48:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:48:29.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:29.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:29.739
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/17/23 03:48:29.744
    Aug 17 03:48:29.753: INFO: Waiting up to 5m0s for pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5" in namespace "downward-api-5116" to be "running and ready"
    Aug 17 03:48:29.757: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386667ms
    Aug 17 03:48:29.757: INFO: The phase of Pod labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:48:31.764: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010780525s
    Aug 17 03:48:31.764: INFO: The phase of Pod labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5 is Running (Ready = true)
    Aug 17 03:48:31.764: INFO: Pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5" satisfied condition "running and ready"
    Aug 17 03:48:32.291: INFO: Successfully updated pod "labelsupdate45921964-b8c5-4cac-9e1e-53434690ccd5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5116" for this suite. 08/17/23 03:48:34.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:34.332
Aug 17 03:48:34.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 03:48:34.333
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:34.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:34.354
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-2598 08/17/23 03:48:34.358
STEP: creating service affinity-clusterip-transition in namespace services-2598 08/17/23 03:48:34.358
STEP: creating replication controller affinity-clusterip-transition in namespace services-2598 08/17/23 03:48:34.368
I0817 03:48:34.373503      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2598, replica count: 3
I0817 03:48:37.425526      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 03:48:37.440: INFO: Creating new exec pod
Aug 17 03:48:37.450: INFO: Waiting up to 5m0s for pod "execpod-affinitydn74n" in namespace "services-2598" to be "running"
Aug 17 03:48:37.459: INFO: Pod "execpod-affinitydn74n": Phase="Pending", Reason="", readiness=false. Elapsed: 9.09523ms
Aug 17 03:48:39.474: INFO: Pod "execpod-affinitydn74n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023626636s
Aug 17 03:48:41.490: INFO: Pod "execpod-affinitydn74n": Phase="Running", Reason="", readiness=true. Elapsed: 4.040381953s
Aug 17 03:48:41.490: INFO: Pod "execpod-affinitydn74n" satisfied condition "running"
Aug 17 03:48:42.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 17 03:48:42.983: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 17 03:48:42.983: INFO: stdout: ""
Aug 17 03:48:42.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c nc -v -z -w 2 172.20.119.119 80'
Aug 17 03:48:43.582: INFO: stderr: "+ nc -v -z -w 2 172.20.119.119 80\nConnection to 172.20.119.119 80 port [tcp/http] succeeded!\n"
Aug 17 03:48:43.582: INFO: stdout: ""
Aug 17 03:48:43.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.119.119:80/ ; done'
Aug 17 03:48:44.314: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n"
Aug 17 03:48:44.314: INFO: stdout: "\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k"
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:44.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.119.119:80/ ; done'
Aug 17 03:48:45.033: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n"
Aug 17 03:48:45.033: INFO: stdout: "\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k"
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
Aug 17 03:48:45.033: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2598, will wait for the garbage collector to delete the pods 08/17/23 03:48:45.077
Aug 17 03:48:45.256: INFO: Deleting ReplicationController affinity-clusterip-transition took: 40.836309ms
Aug 17 03:48:45.358: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.348908ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 03:48:48.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2598" for this suite. 08/17/23 03:48:48.066
------------------------------
• [SLOW TEST] [13.774 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:34.332
    Aug 17 03:48:34.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 03:48:34.333
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:34.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:34.354
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-2598 08/17/23 03:48:34.358
    STEP: creating service affinity-clusterip-transition in namespace services-2598 08/17/23 03:48:34.358
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2598 08/17/23 03:48:34.368
    I0817 03:48:34.373503      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2598, replica count: 3
    I0817 03:48:37.425526      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 03:48:37.440: INFO: Creating new exec pod
    Aug 17 03:48:37.450: INFO: Waiting up to 5m0s for pod "execpod-affinitydn74n" in namespace "services-2598" to be "running"
    Aug 17 03:48:37.459: INFO: Pod "execpod-affinitydn74n": Phase="Pending", Reason="", readiness=false. Elapsed: 9.09523ms
    Aug 17 03:48:39.474: INFO: Pod "execpod-affinitydn74n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023626636s
    Aug 17 03:48:41.490: INFO: Pod "execpod-affinitydn74n": Phase="Running", Reason="", readiness=true. Elapsed: 4.040381953s
    Aug 17 03:48:41.490: INFO: Pod "execpod-affinitydn74n" satisfied condition "running"
    Aug 17 03:48:42.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 17 03:48:42.983: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 17 03:48:42.983: INFO: stdout: ""
    Aug 17 03:48:42.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c nc -v -z -w 2 172.20.119.119 80'
    Aug 17 03:48:43.582: INFO: stderr: "+ nc -v -z -w 2 172.20.119.119 80\nConnection to 172.20.119.119 80 port [tcp/http] succeeded!\n"
    Aug 17 03:48:43.582: INFO: stdout: ""
    Aug 17 03:48:43.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.119.119:80/ ; done'
    Aug 17 03:48:44.314: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n"
    Aug 17 03:48:44.314: INFO: stdout: "\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-858xf\naffinity-clusterip-transition-4zfpv\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k"
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-858xf
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-4zfpv
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.314: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:44.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-2598 exec execpod-affinitydn74n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.119.119:80/ ; done'
    Aug 17 03:48:45.033: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.119.119:80/\n"
    Aug 17 03:48:45.033: INFO: stdout: "\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k\naffinity-clusterip-transition-dz96k"
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Received response from host: affinity-clusterip-transition-dz96k
    Aug 17 03:48:45.033: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2598, will wait for the garbage collector to delete the pods 08/17/23 03:48:45.077
    Aug 17 03:48:45.256: INFO: Deleting ReplicationController affinity-clusterip-transition took: 40.836309ms
    Aug 17 03:48:45.358: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.348908ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:48:48.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2598" for this suite. 08/17/23 03:48:48.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:48:48.11
Aug 17 03:48:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 03:48:48.11
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:48.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:48.273
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 in namespace container-probe-676 08/17/23 03:48:48.335
Aug 17 03:48:48.376: INFO: Waiting up to 5m0s for pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2" in namespace "container-probe-676" to be "not pending"
Aug 17 03:48:48.404: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 27.639978ms
Aug 17 03:48:50.418: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042345351s
Aug 17 03:48:52.425: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.048975215s
Aug 17 03:48:52.425: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2" satisfied condition "not pending"
Aug 17 03:48:52.425: INFO: Started pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 in namespace container-probe-676
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:48:52.425
Aug 17 03:48:52.443: INFO: Initial restart count of pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 is 0
STEP: deleting the pod 08/17/23 03:52:53.315
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 03:52:53.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-676" for this suite. 08/17/23 03:52:53.338
------------------------------
• [SLOW TEST] [245.240 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:48:48.11
    Aug 17 03:48:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 03:48:48.11
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:48:48.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:48:48.273
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 in namespace container-probe-676 08/17/23 03:48:48.335
    Aug 17 03:48:48.376: INFO: Waiting up to 5m0s for pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2" in namespace "container-probe-676" to be "not pending"
    Aug 17 03:48:48.404: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 27.639978ms
    Aug 17 03:48:50.418: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042345351s
    Aug 17 03:48:52.425: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.048975215s
    Aug 17 03:48:52.425: INFO: Pod "test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2" satisfied condition "not pending"
    Aug 17 03:48:52.425: INFO: Started pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 in namespace container-probe-676
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 03:48:52.425
    Aug 17 03:48:52.443: INFO: Initial restart count of pod test-webserver-e85fcb8f-fd15-4dec-9593-bd72dd7ad3e2 is 0
    STEP: deleting the pod 08/17/23 03:52:53.315
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:52:53.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-676" for this suite. 08/17/23 03:52:53.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:52:53.351
Aug 17 03:52:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename containers 08/17/23 03:52:53.352
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:53.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:53.375
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 17 03:52:53.391: INFO: Waiting up to 5m0s for pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff" in namespace "containers-4798" to be "running"
Aug 17 03:52:53.396: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935787ms
Aug 17 03:52:55.401: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff": Phase="Running", Reason="", readiness=true. Elapsed: 2.010101162s
Aug 17 03:52:55.401: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 17 03:52:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4798" for this suite. 08/17/23 03:52:55.461
------------------------------
• [2.117 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:52:53.351
    Aug 17 03:52:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename containers 08/17/23 03:52:53.352
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:53.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:53.375
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 17 03:52:53.391: INFO: Waiting up to 5m0s for pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff" in namespace "containers-4798" to be "running"
    Aug 17 03:52:53.396: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935787ms
    Aug 17 03:52:55.401: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff": Phase="Running", Reason="", readiness=true. Elapsed: 2.010101162s
    Aug 17 03:52:55.401: INFO: Pod "client-containers-2bf702ba-ff0f-47bb-8904-811cf6edcfff" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:52:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4798" for this suite. 08/17/23 03:52:55.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:52:55.469
Aug 17 03:52:55.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:52:55.47
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:55.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:55.494
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 03:52:55.498
Aug 17 03:52:55.506: INFO: Waiting up to 5m0s for pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913" in namespace "emptydir-6507" to be "Succeeded or Failed"
Aug 17 03:52:55.510: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043588ms
Aug 17 03:52:57.515: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008969177s
Aug 17 03:52:59.516: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009207745s
STEP: Saw pod success 08/17/23 03:52:59.516
Aug 17 03:52:59.516: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913" satisfied condition "Succeeded or Failed"
Aug 17 03:52:59.519: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 container test-container: <nil>
STEP: delete the pod 08/17/23 03:52:59.528
Aug 17 03:52:59.539: INFO: Waiting for pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 to disappear
Aug 17 03:52:59.542: INFO: Pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:52:59.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6507" for this suite. 08/17/23 03:52:59.549
------------------------------
• [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:52:55.469
    Aug 17 03:52:55.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:52:55.47
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:55.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:55.494
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 03:52:55.498
    Aug 17 03:52:55.506: INFO: Waiting up to 5m0s for pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913" in namespace "emptydir-6507" to be "Succeeded or Failed"
    Aug 17 03:52:55.510: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043588ms
    Aug 17 03:52:57.515: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008969177s
    Aug 17 03:52:59.516: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009207745s
    STEP: Saw pod success 08/17/23 03:52:59.516
    Aug 17 03:52:59.516: INFO: Pod "pod-d3430fda-c926-4a9e-b4c0-4e059cddf913" satisfied condition "Succeeded or Failed"
    Aug 17 03:52:59.519: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 container test-container: <nil>
    STEP: delete the pod 08/17/23 03:52:59.528
    Aug 17 03:52:59.539: INFO: Waiting for pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 to disappear
    Aug 17 03:52:59.542: INFO: Pod pod-d3430fda-c926-4a9e-b4c0-4e059cddf913 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:52:59.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6507" for this suite. 08/17/23 03:52:59.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:52:59.557
Aug 17 03:52:59.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:52:59.558
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:59.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:59.573
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/17/23 03:52:59.577
Aug 17 03:52:59.586: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2938  33575c94-87d1-460d-971c-dd8db21b10c2 231967 0 2023-08-17 03:52:59 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-17 03:52:59 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnlsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnlsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 17 03:52:59.586: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2938" to be "running and ready"
Aug 17 03:52:59.589: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442993ms
Aug 17 03:52:59.589: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:53:01.595: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009003412s
Aug 17 03:53:01.595: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 17 03:53:01.595: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/17/23 03:53:01.595
Aug 17 03:53:01.595: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2938 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:53:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:53:01.596: INFO: ExecWithOptions: Clientset creation
Aug 17 03:53:01.596: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-2938/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/17/23 03:53:01.82
Aug 17 03:53:01.820: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2938 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 03:53:01.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 03:53:01.821: INFO: ExecWithOptions: Clientset creation
Aug 17 03:53:01.821: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-2938/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 17 03:53:02.152: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:53:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2938" for this suite. 08/17/23 03:53:02.17
------------------------------
• [2.620 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:52:59.557
    Aug 17 03:52:59.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:52:59.558
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:52:59.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:52:59.573
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/17/23 03:52:59.577
    Aug 17 03:52:59.586: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2938  33575c94-87d1-460d-971c-dd8db21b10c2 231967 0 2023-08-17 03:52:59 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-17 03:52:59 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnlsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnlsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 17 03:52:59.586: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2938" to be "running and ready"
    Aug 17 03:52:59.589: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442993ms
    Aug 17 03:52:59.589: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:53:01.595: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009003412s
    Aug 17 03:53:01.595: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 17 03:53:01.595: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/17/23 03:53:01.595
    Aug 17 03:53:01.595: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2938 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:53:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:53:01.596: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:53:01.596: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-2938/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/17/23 03:53:01.82
    Aug 17 03:53:01.820: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2938 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 03:53:01.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 03:53:01.821: INFO: ExecWithOptions: Clientset creation
    Aug 17 03:53:01.821: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-2938/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 17 03:53:02.152: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:53:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2938" for this suite. 08/17/23 03:53:02.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:53:02.182
Aug 17 03:53:02.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:53:02.183
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:53:02.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:53:02.199
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-f6c0e697-286d-4747-971e-4e8ac8c8e013 08/17/23 03:53:02.21
STEP: Creating secret with name s-test-opt-upd-d9f519da-60fa-4d44-ae6a-a959c1e18f4b 08/17/23 03:53:02.215
STEP: Creating the pod 08/17/23 03:53:02.221
Aug 17 03:53:02.231: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92" in namespace "projected-8587" to be "running and ready"
Aug 17 03:53:02.236: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329178ms
Aug 17 03:53:02.236: INFO: The phase of Pod pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:53:04.243: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92": Phase="Running", Reason="", readiness=true. Elapsed: 2.011089426s
Aug 17 03:53:04.243: INFO: The phase of Pod pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92 is Running (Ready = true)
Aug 17 03:53:04.243: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f6c0e697-286d-4747-971e-4e8ac8c8e013 08/17/23 03:53:04.391
STEP: Updating secret s-test-opt-upd-d9f519da-60fa-4d44-ae6a-a959c1e18f4b 08/17/23 03:53:04.399
STEP: Creating secret with name s-test-opt-create-8f20e6ce-9340-4e9c-9798-c13da5bf576d 08/17/23 03:53:04.406
STEP: waiting to observe update in volume 08/17/23 03:53:04.411
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 03:53:06.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8587" for this suite. 08/17/23 03:53:06.624
------------------------------
• [4.450 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:53:02.182
    Aug 17 03:53:02.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:53:02.183
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:53:02.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:53:02.199
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-f6c0e697-286d-4747-971e-4e8ac8c8e013 08/17/23 03:53:02.21
    STEP: Creating secret with name s-test-opt-upd-d9f519da-60fa-4d44-ae6a-a959c1e18f4b 08/17/23 03:53:02.215
    STEP: Creating the pod 08/17/23 03:53:02.221
    Aug 17 03:53:02.231: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92" in namespace "projected-8587" to be "running and ready"
    Aug 17 03:53:02.236: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329178ms
    Aug 17 03:53:02.236: INFO: The phase of Pod pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:53:04.243: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92": Phase="Running", Reason="", readiness=true. Elapsed: 2.011089426s
    Aug 17 03:53:04.243: INFO: The phase of Pod pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92 is Running (Ready = true)
    Aug 17 03:53:04.243: INFO: Pod "pod-projected-secrets-10292ea1-38ee-4590-be88-6d8d879aff92" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f6c0e697-286d-4747-971e-4e8ac8c8e013 08/17/23 03:53:04.391
    STEP: Updating secret s-test-opt-upd-d9f519da-60fa-4d44-ae6a-a959c1e18f4b 08/17/23 03:53:04.399
    STEP: Creating secret with name s-test-opt-create-8f20e6ce-9340-4e9c-9798-c13da5bf576d 08/17/23 03:53:04.406
    STEP: waiting to observe update in volume 08/17/23 03:53:04.411
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:53:06.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8587" for this suite. 08/17/23 03:53:06.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:53:06.634
Aug 17 03:53:06.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 03:53:06.635
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:53:06.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:53:06.652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/17/23 03:53:06.656
STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:06.936
Aug 17 03:53:07.013: INFO: Pod name wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 03:53:07.013
Aug 17 03:53:07.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:07.041: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.687981ms
Aug 17 03:53:09.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035452245s
Aug 17 03:53:11.048: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034815293s
Aug 17 03:53:13.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035190225s
Aug 17 03:53:15.050: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036036459s
Aug 17 03:53:17.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035497539s
Aug 17 03:53:19.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035155978s
Aug 17 03:53:21.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Running", Reason="", readiness=true. Elapsed: 14.035091798s
Aug 17 03:53:21.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc" satisfied condition "running"
Aug 17 03:53:21.049: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:21.055: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680993ms
Aug 17 03:53:23.063: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013930715s
Aug 17 03:53:23.063: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp" satisfied condition "running"
Aug 17 03:53:23.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:23.069: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2": Phase="Running", Reason="", readiness=true. Elapsed: 6.357453ms
Aug 17 03:53:23.069: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2" satisfied condition "running"
Aug 17 03:53:23.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:23.075: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c": Phase="Running", Reason="", readiness=true. Elapsed: 6.350123ms
Aug 17 03:53:23.075: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c" satisfied condition "running"
Aug 17 03:53:23.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:23.082: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.046666ms
Aug 17 03:53:23.082: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:23.082
Aug 17 03:53:23.145: INFO: Deleting ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 took: 7.3649ms
Aug 17 03:53:23.347: INFO: Terminating ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 pods took: 202.119678ms
STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:26.054
Aug 17 03:53:26.071: INFO: Pod name wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a: Found 0 pods out of 5
Aug 17 03:53:31.091: INFO: Pod name wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 03:53:31.091
Aug 17 03:53:31.091: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:31.097: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69706ms
Aug 17 03:53:33.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012285973s
Aug 17 03:53:35.106: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014528677s
Aug 17 03:53:37.105: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013964377s
Aug 17 03:53:39.105: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013661669s
Aug 17 03:53:41.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Running", Reason="", readiness=true. Elapsed: 10.012893635s
Aug 17 03:53:41.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb" satisfied condition "running"
Aug 17 03:53:41.104: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:41.111: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb": Phase="Running", Reason="", readiness=true. Elapsed: 6.663303ms
Aug 17 03:53:41.111: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb" satisfied condition "running"
Aug 17 03:53:41.111: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:41.117: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6": Phase="Running", Reason="", readiness=true. Elapsed: 6.234185ms
Aug 17 03:53:41.117: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6" satisfied condition "running"
Aug 17 03:53:41.117: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:41.123: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r": Phase="Running", Reason="", readiness=true. Elapsed: 6.28443ms
Aug 17 03:53:41.123: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r" satisfied condition "running"
Aug 17 03:53:41.123: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:41.130: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2": Phase="Running", Reason="", readiness=true. Elapsed: 6.954431ms
Aug 17 03:53:41.130: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:41.13
Aug 17 03:53:41.195: INFO: Deleting ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a took: 8.463975ms
Aug 17 03:53:41.295: INFO: Terminating ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a pods took: 100.579953ms
STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:44.404
Aug 17 03:53:44.422: INFO: Pod name wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118: Found 0 pods out of 5
Aug 17 03:53:49.439: INFO: Pod name wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/17/23 03:53:49.439
Aug 17 03:53:49.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:49.445: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.809789ms
Aug 17 03:53:51.452: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390067s
Aug 17 03:53:53.451: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012477897s
Aug 17 03:53:55.454: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01484707s
Aug 17 03:53:57.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013971633s
Aug 17 03:53:59.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013883292s
Aug 17 03:53:59.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp" satisfied condition "running"
Aug 17 03:53:59.453: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:59.459: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp": Phase="Running", Reason="", readiness=true. Elapsed: 6.386361ms
Aug 17 03:53:59.459: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp" satisfied condition "running"
Aug 17 03:53:59.459: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:59.465: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj": Phase="Running", Reason="", readiness=true. Elapsed: 5.961288ms
Aug 17 03:53:59.465: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj" satisfied condition "running"
Aug 17 03:53:59.465: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:59.471: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg": Phase="Running", Reason="", readiness=true. Elapsed: 5.367824ms
Aug 17 03:53:59.471: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg" satisfied condition "running"
Aug 17 03:53:59.471: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5" in namespace "emptydir-wrapper-9021" to be "running"
Aug 17 03:53:59.476: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5": Phase="Running", Reason="", readiness=true. Elapsed: 5.400302ms
Aug 17 03:53:59.476: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:59.476
Aug 17 03:53:59.540: INFO: Deleting ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 took: 7.945468ms
Aug 17 03:53:59.640: INFO: Terminating ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 pods took: 100.399811ms
STEP: Cleaning up the configMaps 08/17/23 03:54:03.341
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:54:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9021" for this suite. 08/17/23 03:54:03.687
------------------------------
• [SLOW TEST] [57.060 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:53:06.634
    Aug 17 03:53:06.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir-wrapper 08/17/23 03:53:06.635
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:53:06.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:53:06.652
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/17/23 03:53:06.656
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:06.936
    Aug 17 03:53:07.013: INFO: Pod name wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 03:53:07.013
    Aug 17 03:53:07.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:07.041: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.687981ms
    Aug 17 03:53:09.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035452245s
    Aug 17 03:53:11.048: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034815293s
    Aug 17 03:53:13.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035190225s
    Aug 17 03:53:15.050: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036036459s
    Aug 17 03:53:17.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.035497539s
    Aug 17 03:53:19.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035155978s
    Aug 17 03:53:21.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc": Phase="Running", Reason="", readiness=true. Elapsed: 14.035091798s
    Aug 17 03:53:21.049: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4cfcc" satisfied condition "running"
    Aug 17 03:53:21.049: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:21.055: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680993ms
    Aug 17 03:53:23.063: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013930715s
    Aug 17 03:53:23.063: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-4gmxp" satisfied condition "running"
    Aug 17 03:53:23.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:23.069: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2": Phase="Running", Reason="", readiness=true. Elapsed: 6.357453ms
    Aug 17 03:53:23.069: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-c8sb2" satisfied condition "running"
    Aug 17 03:53:23.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:23.075: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c": Phase="Running", Reason="", readiness=true. Elapsed: 6.350123ms
    Aug 17 03:53:23.075: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-chf7c" satisfied condition "running"
    Aug 17 03:53:23.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:23.082: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.046666ms
    Aug 17 03:53:23.082: INFO: Pod "wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261-sgbxx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:23.082
    Aug 17 03:53:23.145: INFO: Deleting ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 took: 7.3649ms
    Aug 17 03:53:23.347: INFO: Terminating ReplicationController wrapped-volume-race-d7e71eca-ccde-45c4-a891-a2db65d6c261 pods took: 202.119678ms
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:26.054
    Aug 17 03:53:26.071: INFO: Pod name wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a: Found 0 pods out of 5
    Aug 17 03:53:31.091: INFO: Pod name wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 03:53:31.091
    Aug 17 03:53:31.091: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:31.097: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69706ms
    Aug 17 03:53:33.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012285973s
    Aug 17 03:53:35.106: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014528677s
    Aug 17 03:53:37.105: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013964377s
    Aug 17 03:53:39.105: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013661669s
    Aug 17 03:53:41.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb": Phase="Running", Reason="", readiness=true. Elapsed: 10.012893635s
    Aug 17 03:53:41.104: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-2r9rb" satisfied condition "running"
    Aug 17 03:53:41.104: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:41.111: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb": Phase="Running", Reason="", readiness=true. Elapsed: 6.663303ms
    Aug 17 03:53:41.111: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-7vscb" satisfied condition "running"
    Aug 17 03:53:41.111: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:41.117: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6": Phase="Running", Reason="", readiness=true. Elapsed: 6.234185ms
    Aug 17 03:53:41.117: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-b76s6" satisfied condition "running"
    Aug 17 03:53:41.117: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:41.123: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r": Phase="Running", Reason="", readiness=true. Elapsed: 6.28443ms
    Aug 17 03:53:41.123: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-md89r" satisfied condition "running"
    Aug 17 03:53:41.123: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:41.130: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2": Phase="Running", Reason="", readiness=true. Elapsed: 6.954431ms
    Aug 17 03:53:41.130: INFO: Pod "wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a-qvmm2" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:41.13
    Aug 17 03:53:41.195: INFO: Deleting ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a took: 8.463975ms
    Aug 17 03:53:41.295: INFO: Terminating ReplicationController wrapped-volume-race-1fe259d5-9c81-4537-a9ab-35cf6d6ae78a pods took: 100.579953ms
    STEP: Creating RC which spawns configmap-volume pods 08/17/23 03:53:44.404
    Aug 17 03:53:44.422: INFO: Pod name wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118: Found 0 pods out of 5
    Aug 17 03:53:49.439: INFO: Pod name wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/17/23 03:53:49.439
    Aug 17 03:53:49.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:49.445: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.809789ms
    Aug 17 03:53:51.452: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390067s
    Aug 17 03:53:53.451: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012477897s
    Aug 17 03:53:55.454: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01484707s
    Aug 17 03:53:57.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013971633s
    Aug 17 03:53:59.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp": Phase="Running", Reason="", readiness=true. Elapsed: 10.013883292s
    Aug 17 03:53:59.453: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-57nwp" satisfied condition "running"
    Aug 17 03:53:59.453: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:59.459: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp": Phase="Running", Reason="", readiness=true. Elapsed: 6.386361ms
    Aug 17 03:53:59.459: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-9sgwp" satisfied condition "running"
    Aug 17 03:53:59.459: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:59.465: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj": Phase="Running", Reason="", readiness=true. Elapsed: 5.961288ms
    Aug 17 03:53:59.465: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-kbvbj" satisfied condition "running"
    Aug 17 03:53:59.465: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:59.471: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg": Phase="Running", Reason="", readiness=true. Elapsed: 5.367824ms
    Aug 17 03:53:59.471: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-lvchg" satisfied condition "running"
    Aug 17 03:53:59.471: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5" in namespace "emptydir-wrapper-9021" to be "running"
    Aug 17 03:53:59.476: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5": Phase="Running", Reason="", readiness=true. Elapsed: 5.400302ms
    Aug 17 03:53:59.476: INFO: Pod "wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118-rtmz5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 in namespace emptydir-wrapper-9021, will wait for the garbage collector to delete the pods 08/17/23 03:53:59.476
    Aug 17 03:53:59.540: INFO: Deleting ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 took: 7.945468ms
    Aug 17 03:53:59.640: INFO: Terminating ReplicationController wrapped-volume-race-e07d20aa-eb8c-4f4e-8774-f7a0ab2c5118 pods took: 100.399811ms
    STEP: Cleaning up the configMaps 08/17/23 03:54:03.341
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:54:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9021" for this suite. 08/17/23 03:54:03.687
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:54:03.695
Aug 17 03:54:03.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:54:03.696
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:03.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:03.715
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-6b348b58-6b72-42d2-aa95-f5b6b7b8ba68 08/17/23 03:54:03.72
STEP: Creating a pod to test consume configMaps 08/17/23 03:54:03.725
Aug 17 03:54:03.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f" in namespace "projected-3241" to be "Succeeded or Failed"
Aug 17 03:54:03.739: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026136ms
Aug 17 03:54:05.745: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010156685s
Aug 17 03:54:07.745: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010029824s
Aug 17 03:54:09.744: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008716708s
STEP: Saw pod success 08/17/23 03:54:09.744
Aug 17 03:54:09.744: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f" satisfied condition "Succeeded or Failed"
Aug 17 03:54:09.748: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:54:09.758
Aug 17 03:54:09.771: INFO: Waiting for pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f to disappear
Aug 17 03:54:09.775: INFO: Pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:54:09.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3241" for this suite. 08/17/23 03:54:09.783
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:54:03.695
    Aug 17 03:54:03.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:54:03.696
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:03.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:03.715
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-6b348b58-6b72-42d2-aa95-f5b6b7b8ba68 08/17/23 03:54:03.72
    STEP: Creating a pod to test consume configMaps 08/17/23 03:54:03.725
    Aug 17 03:54:03.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f" in namespace "projected-3241" to be "Succeeded or Failed"
    Aug 17 03:54:03.739: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026136ms
    Aug 17 03:54:05.745: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010156685s
    Aug 17 03:54:07.745: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010029824s
    Aug 17 03:54:09.744: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008716708s
    STEP: Saw pod success 08/17/23 03:54:09.744
    Aug 17 03:54:09.744: INFO: Pod "pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f" satisfied condition "Succeeded or Failed"
    Aug 17 03:54:09.748: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:54:09.758
    Aug 17 03:54:09.771: INFO: Waiting for pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f to disappear
    Aug 17 03:54:09.775: INFO: Pod pod-projected-configmaps-dab5880f-8fcb-4f07-b711-fb65b978629f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:54:09.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3241" for this suite. 08/17/23 03:54:09.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:54:09.792
Aug 17 03:54:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-pred 08/17/23 03:54:09.793
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:09.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:09.813
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 03:54:09.818: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 03:54:09.831: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 03:54:09.835: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
Aug 17 03:54:09.848: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 03:54:09.849: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 03:54:09.849: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container coredns ready: true, restart count 0
Aug 17 03:54:09.849: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container coredns ready: true, restart count 0
Aug 17 03:54:09.849: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 03:54:09.849: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container metrics-server ready: true, restart count 0
Aug 17 03:54:09.849: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Aug 17 03:54:09.849: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 03:54:09.849: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container vpn-target ready: true, restart count 0
Aug 17 03:54:09.849: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:54:09.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:54:09.849: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 03:54:09.849: INFO: 
Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
Aug 17 03:54:09.868: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 03:54:09.868: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 03:54:09.868: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 03:54:09.868: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 03:54:09.868: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container e2e ready: true, restart count 0
Aug 17 03:54:09.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:54:09.868: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 03:54:09.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 03:54:09.868: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:54:09.868
Aug 17 03:54:09.878: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7878" to be "running"
Aug 17 03:54:09.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940825ms
Aug 17 03:54:11.889: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010221459s
Aug 17 03:54:11.889: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:54:11.893
STEP: Trying to apply a random label on the found node. 08/17/23 03:54:11.911
STEP: verifying the node has the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 42 08/17/23 03:54:11.923
STEP: Trying to relaunch the pod, now with labels. 08/17/23 03:54:11.928
Aug 17 03:54:11.934: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7878" to be "not pending"
Aug 17 03:54:11.941: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59681ms
Aug 17 03:54:13.947: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012649865s
Aug 17 03:54:13.947: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 03:54:13.951
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 08/17/23 03:54:13.969
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:54:13.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7878" for this suite. 08/17/23 03:54:13.982
------------------------------
• [4.198 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:54:09.792
    Aug 17 03:54:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-pred 08/17/23 03:54:09.793
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:09.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:09.813
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 03:54:09.818: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 03:54:09.831: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 03:54:09.835: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
    Aug 17 03:54:09.848: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Aug 17 03:54:09.849: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container vpn-target ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:54:09.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 03:54:09.849: INFO: 
    Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
    Aug 17 03:54:09.868: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 03:54:09.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 03:54:09.868: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/17/23 03:54:09.868
    Aug 17 03:54:09.878: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7878" to be "running"
    Aug 17 03:54:09.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940825ms
    Aug 17 03:54:11.889: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010221459s
    Aug 17 03:54:11.889: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/17/23 03:54:11.893
    STEP: Trying to apply a random label on the found node. 08/17/23 03:54:11.911
    STEP: verifying the node has the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 42 08/17/23 03:54:11.923
    STEP: Trying to relaunch the pod, now with labels. 08/17/23 03:54:11.928
    Aug 17 03:54:11.934: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7878" to be "not pending"
    Aug 17 03:54:11.941: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59681ms
    Aug 17 03:54:13.947: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012649865s
    Aug 17 03:54:13.947: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 03:54:13.951
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f2603cc7-3860-413c-bf59-d6e82f460be8 08/17/23 03:54:13.969
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:54:13.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7878" for this suite. 08/17/23 03:54:13.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:54:13.994
Aug 17 03:54:13.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 03:54:13.994
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:14.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:14.014
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:14.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8568" for this suite. 08/17/23 03:55:14.041
------------------------------
• [SLOW TEST] [60.056 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:54:13.994
    Aug 17 03:54:13.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 03:54:13.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:54:14.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:54:14.014
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:14.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8568" for this suite. 08/17/23 03:55:14.041
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:14.05
Aug 17 03:55:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:55:14.051
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:14.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:14.07
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 17 03:55:14.089: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5971 to be scheduled
Aug 17 03:55:14.093: INFO: 1 pods are not scheduled: [runtimeclass-5971/test-runtimeclass-runtimeclass-5971-preconfigured-handler-566wt(d05f0f94-5659-4390-a473-5ae9973e6001)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:16.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5971" for this suite. 08/17/23 03:55:16.113
------------------------------
• [2.071 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:14.05
    Aug 17 03:55:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename runtimeclass 08/17/23 03:55:14.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:14.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:14.07
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 17 03:55:14.089: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5971 to be scheduled
    Aug 17 03:55:14.093: INFO: 1 pods are not scheduled: [runtimeclass-5971/test-runtimeclass-runtimeclass-5971-preconfigured-handler-566wt(d05f0f94-5659-4390-a473-5ae9973e6001)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:16.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5971" for this suite. 08/17/23 03:55:16.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:16.121
Aug 17 03:55:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:55:16.122
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:16.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:16.141
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/17/23 03:55:16.145
Aug 17 03:55:16.154: INFO: created test-pod-1
Aug 17 03:55:16.160: INFO: created test-pod-2
Aug 17 03:55:16.167: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/17/23 03:55:16.167
Aug 17 03:55:16.167: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5084' to be running and ready
Aug 17 03:55:16.181: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 03:55:16.181: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 03:55:16.181: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 03:55:16.181: INFO: 0 / 3 pods in namespace 'pods-5084' are running and ready (0 seconds elapsed)
Aug 17 03:55:16.181: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
Aug 17 03:55:16.181: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
Aug 17 03:55:16.181: INFO: test-pod-1  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
Aug 17 03:55:16.182: INFO: test-pod-2  ske-rhel-6c9d465fc4xbjl6l-wz7jz    Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
Aug 17 03:55:16.182: INFO: test-pod-3  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
Aug 17 03:55:16.182: INFO: 
Aug 17 03:55:18.196: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 17 03:55:18.196: INFO: 2 / 3 pods in namespace 'pods-5084' are running and ready (2 seconds elapsed)
Aug 17 03:55:18.196: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
Aug 17 03:55:18.196: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
Aug 17 03:55:18.196: INFO: test-pod-3  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
Aug 17 03:55:18.196: INFO: 
Aug 17 03:55:20.196: INFO: 3 / 3 pods in namespace 'pods-5084' are running and ready (4 seconds elapsed)
Aug 17 03:55:20.196: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/17/23 03:55:20.216
Aug 17 03:55:20.227: INFO: Pod quantity 3 is different from expected quantity 0
Aug 17 03:55:21.233: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:22.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5084" for this suite. 08/17/23 03:55:22.241
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:16.121
    Aug 17 03:55:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:55:16.122
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:16.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:16.141
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/17/23 03:55:16.145
    Aug 17 03:55:16.154: INFO: created test-pod-1
    Aug 17 03:55:16.160: INFO: created test-pod-2
    Aug 17 03:55:16.167: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/17/23 03:55:16.167
    Aug 17 03:55:16.167: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5084' to be running and ready
    Aug 17 03:55:16.181: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 03:55:16.181: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 03:55:16.181: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 03:55:16.181: INFO: 0 / 3 pods in namespace 'pods-5084' are running and ready (0 seconds elapsed)
    Aug 17 03:55:16.181: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
    Aug 17 03:55:16.181: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
    Aug 17 03:55:16.181: INFO: test-pod-1  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
    Aug 17 03:55:16.182: INFO: test-pod-2  ske-rhel-6c9d465fc4xbjl6l-wz7jz    Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
    Aug 17 03:55:16.182: INFO: test-pod-3  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
    Aug 17 03:55:16.182: INFO: 
    Aug 17 03:55:18.196: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 17 03:55:18.196: INFO: 2 / 3 pods in namespace 'pods-5084' are running and ready (2 seconds elapsed)
    Aug 17 03:55:18.196: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
    Aug 17 03:55:18.196: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
    Aug 17 03:55:18.196: INFO: test-pod-3  ske-ubuntu-79fff84d86x69988-vjwlx  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 03:55:16 +0000 UTC  }]
    Aug 17 03:55:18.196: INFO: 
    Aug 17 03:55:20.196: INFO: 3 / 3 pods in namespace 'pods-5084' are running and ready (4 seconds elapsed)
    Aug 17 03:55:20.196: INFO: expected 0 pod replicas in namespace 'pods-5084', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/17/23 03:55:20.216
    Aug 17 03:55:20.227: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 17 03:55:21.233: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:22.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5084" for this suite. 08/17/23 03:55:22.241
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:22.251
Aug 17 03:55:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:55:22.252
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:22.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:22.275
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:55:22.293
Aug 17 03:55:22.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5983" to be "running and ready"
Aug 17 03:55:22.308: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46421ms
Aug 17 03:55:22.308: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:55:24.312: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009098177s
Aug 17 03:55:24.312: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 03:55:24.312: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/17/23 03:55:24.316
Aug 17 03:55:24.324: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5983" to be "running and ready"
Aug 17 03:55:24.329: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52222ms
Aug 17 03:55:24.329: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:55:26.336: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012090643s
Aug 17 03:55:26.336: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 17 03:55:26.336: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/17/23 03:55:26.341
STEP: delete the pod with lifecycle hook 08/17/23 03:55:26.356
Aug 17 03:55:26.366: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 03:55:26.371: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 03:55:28.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 03:55:28.378: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 17 03:55:30.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 17 03:55:30.379: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:30.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5983" for this suite. 08/17/23 03:55:30.386
------------------------------
• [SLOW TEST] [8.143 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:22.251
    Aug 17 03:55:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 03:55:22.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:22.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:22.275
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 03:55:22.293
    Aug 17 03:55:22.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5983" to be "running and ready"
    Aug 17 03:55:22.308: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46421ms
    Aug 17 03:55:22.308: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:55:24.312: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009098177s
    Aug 17 03:55:24.312: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 03:55:24.312: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/17/23 03:55:24.316
    Aug 17 03:55:24.324: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5983" to be "running and ready"
    Aug 17 03:55:24.329: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52222ms
    Aug 17 03:55:24.329: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:55:26.336: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012090643s
    Aug 17 03:55:26.336: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 17 03:55:26.336: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/17/23 03:55:26.341
    STEP: delete the pod with lifecycle hook 08/17/23 03:55:26.356
    Aug 17 03:55:26.366: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 03:55:26.371: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 17 03:55:28.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 03:55:28.378: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 17 03:55:30.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 17 03:55:30.379: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:30.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5983" for this suite. 08/17/23 03:55:30.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:30.396
Aug 17 03:55:30.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 03:55:30.397
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:30.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:30.414
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 03:55:30.433
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:55:30.792
STEP: Deploying the webhook pod 08/17/23 03:55:30.801
STEP: Wait for the deployment to be ready 08/17/23 03:55:30.815
Aug 17 03:55:30.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 03:55:32.838
STEP: Verifying the service has paired with the endpoint 08/17/23 03:55:32.849
Aug 17 03:55:33.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/17/23 03:55:33.855
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/17/23 03:55:33.857
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 03:55:33.858
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/17/23 03:55:33.858
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/17/23 03:55:33.86
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 03:55:33.86
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 03:55:33.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:33.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4463" for this suite. 08/17/23 03:55:33.915
STEP: Destroying namespace "webhook-4463-markers" for this suite. 08/17/23 03:55:33.922
------------------------------
• [3.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:30.396
    Aug 17 03:55:30.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 03:55:30.397
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:30.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:30.414
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 03:55:30.433
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 03:55:30.792
    STEP: Deploying the webhook pod 08/17/23 03:55:30.801
    STEP: Wait for the deployment to be ready 08/17/23 03:55:30.815
    Aug 17 03:55:30.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 03:55:32.838
    STEP: Verifying the service has paired with the endpoint 08/17/23 03:55:32.849
    Aug 17 03:55:33.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/17/23 03:55:33.855
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/17/23 03:55:33.857
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 03:55:33.858
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/17/23 03:55:33.858
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/17/23 03:55:33.86
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 03:55:33.86
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/17/23 03:55:33.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:33.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4463" for this suite. 08/17/23 03:55:33.915
    STEP: Destroying namespace "webhook-4463-markers" for this suite. 08/17/23 03:55:33.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:33.94
Aug 17 03:55:33.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename proxy 08/17/23 03:55:33.94
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:33.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:33.961
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 17 03:55:33.966: INFO: Creating pod...
Aug 17 03:55:33.976: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6292" to be "running"
Aug 17 03:55:33.980: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88657ms
Aug 17 03:55:35.987: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011419154s
Aug 17 03:55:35.987: INFO: Pod "agnhost" satisfied condition "running"
Aug 17 03:55:35.987: INFO: Creating service...
Aug 17 03:55:35.998: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=DELETE
Aug 17 03:55:36.091: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 03:55:36.091: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=OPTIONS
Aug 17 03:55:36.135: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 03:55:36.135: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=PATCH
Aug 17 03:55:36.141: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 03:55:36.141: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=POST
Aug 17 03:55:36.147: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 03:55:36.148: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=PUT
Aug 17 03:55:36.154: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 03:55:36.154: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 17 03:55:36.162: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 17 03:55:36.162: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 17 03:55:36.170: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 17 03:55:36.170: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 17 03:55:36.178: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 17 03:55:36.178: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=POST
Aug 17 03:55:36.186: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 17 03:55:36.187: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=PUT
Aug 17 03:55:36.194: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 17 03:55:36.194: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=GET
Aug 17 03:55:36.198: INFO: http.Client request:GET StatusCode:301
Aug 17 03:55:36.198: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=GET
Aug 17 03:55:36.203: INFO: http.Client request:GET StatusCode:301
Aug 17 03:55:36.203: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=HEAD
Aug 17 03:55:36.207: INFO: http.Client request:HEAD StatusCode:301
Aug 17 03:55:36.207: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 17 03:55:36.213: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:36.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6292" for this suite. 08/17/23 03:55:36.221
------------------------------
• [2.289 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:33.94
    Aug 17 03:55:33.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename proxy 08/17/23 03:55:33.94
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:33.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:33.961
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 17 03:55:33.966: INFO: Creating pod...
    Aug 17 03:55:33.976: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6292" to be "running"
    Aug 17 03:55:33.980: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88657ms
    Aug 17 03:55:35.987: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011419154s
    Aug 17 03:55:35.987: INFO: Pod "agnhost" satisfied condition "running"
    Aug 17 03:55:35.987: INFO: Creating service...
    Aug 17 03:55:35.998: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=DELETE
    Aug 17 03:55:36.091: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 03:55:36.091: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=OPTIONS
    Aug 17 03:55:36.135: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 03:55:36.135: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=PATCH
    Aug 17 03:55:36.141: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 03:55:36.141: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=POST
    Aug 17 03:55:36.147: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 03:55:36.148: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=PUT
    Aug 17 03:55:36.154: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 03:55:36.154: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 17 03:55:36.162: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 17 03:55:36.162: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 17 03:55:36.170: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 17 03:55:36.170: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 17 03:55:36.178: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 17 03:55:36.178: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=POST
    Aug 17 03:55:36.186: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 17 03:55:36.187: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 17 03:55:36.194: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 17 03:55:36.194: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=GET
    Aug 17 03:55:36.198: INFO: http.Client request:GET StatusCode:301
    Aug 17 03:55:36.198: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=GET
    Aug 17 03:55:36.203: INFO: http.Client request:GET StatusCode:301
    Aug 17 03:55:36.203: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/pods/agnhost/proxy?method=HEAD
    Aug 17 03:55:36.207: INFO: http.Client request:HEAD StatusCode:301
    Aug 17 03:55:36.207: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-6292/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 17 03:55:36.213: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:36.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6292" for this suite. 08/17/23 03:55:36.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:36.234
Aug 17 03:55:36.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 03:55:36.235
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:36.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:36.253
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 03:55:36.259
Aug 17 03:55:36.269: INFO: Waiting up to 5m0s for pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e" in namespace "emptydir-8543" to be "Succeeded or Failed"
Aug 17 03:55:36.274: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802289ms
Aug 17 03:55:38.279: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009863551s
Aug 17 03:55:40.281: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352034s
STEP: Saw pod success 08/17/23 03:55:40.281
Aug 17 03:55:40.281: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e" satisfied condition "Succeeded or Failed"
Aug 17 03:55:40.285: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e container test-container: <nil>
STEP: delete the pod 08/17/23 03:55:40.295
Aug 17 03:55:40.308: INFO: Waiting for pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e to disappear
Aug 17 03:55:40.312: INFO: Pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:40.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8543" for this suite. 08/17/23 03:55:40.318
------------------------------
• [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:36.234
    Aug 17 03:55:36.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 03:55:36.235
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:36.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:36.253
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/17/23 03:55:36.259
    Aug 17 03:55:36.269: INFO: Waiting up to 5m0s for pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e" in namespace "emptydir-8543" to be "Succeeded or Failed"
    Aug 17 03:55:36.274: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802289ms
    Aug 17 03:55:38.279: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009863551s
    Aug 17 03:55:40.281: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352034s
    STEP: Saw pod success 08/17/23 03:55:40.281
    Aug 17 03:55:40.281: INFO: Pod "pod-838ce345-6a3f-476c-ae72-7e5da722fd4e" satisfied condition "Succeeded or Failed"
    Aug 17 03:55:40.285: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e container test-container: <nil>
    STEP: delete the pod 08/17/23 03:55:40.295
    Aug 17 03:55:40.308: INFO: Waiting for pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e to disappear
    Aug 17 03:55:40.312: INFO: Pod pod-838ce345-6a3f-476c-ae72-7e5da722fd4e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:40.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8543" for this suite. 08/17/23 03:55:40.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:40.33
Aug 17 03:55:40.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:55:40.331
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:40.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:40.353
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/17/23 03:55:40.36
Aug 17 03:55:40.369: INFO: Waiting up to 5m0s for pod "pod-jv6ck" in namespace "pods-8112" to be "running"
Aug 17 03:55:40.374: INFO: Pod "pod-jv6ck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.572093ms
Aug 17 03:55:42.379: INFO: Pod "pod-jv6ck": Phase="Running", Reason="", readiness=true. Elapsed: 2.010248282s
Aug 17 03:55:42.379: INFO: Pod "pod-jv6ck" satisfied condition "running"
STEP: patching /status 08/17/23 03:55:42.379
Aug 17 03:55:42.396: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8112" for this suite. 08/17/23 03:55:42.403
------------------------------
• [2.081 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:40.33
    Aug 17 03:55:40.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:55:40.331
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:40.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:40.353
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/17/23 03:55:40.36
    Aug 17 03:55:40.369: INFO: Waiting up to 5m0s for pod "pod-jv6ck" in namespace "pods-8112" to be "running"
    Aug 17 03:55:40.374: INFO: Pod "pod-jv6ck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.572093ms
    Aug 17 03:55:42.379: INFO: Pod "pod-jv6ck": Phase="Running", Reason="", readiness=true. Elapsed: 2.010248282s
    Aug 17 03:55:42.379: INFO: Pod "pod-jv6ck" satisfied condition "running"
    STEP: patching /status 08/17/23 03:55:42.379
    Aug 17 03:55:42.396: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8112" for this suite. 08/17/23 03:55:42.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:42.413
Aug 17 03:55:42.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:55:42.414
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:42.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:42.433
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:55:42.438
Aug 17 03:55:42.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5060 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 17 03:55:42.507: INFO: stderr: ""
Aug 17 03:55:42.507: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 03:55:42.507
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 17 03:55:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5060 delete pods e2e-test-httpd-pod'
Aug 17 03:55:44.620: INFO: stderr: ""
Aug 17 03:55:44.620: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5060" for this suite. 08/17/23 03:55:44.628
------------------------------
• [2.223 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:42.413
    Aug 17 03:55:42.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:55:42.414
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:42.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:42.433
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/17/23 03:55:42.438
    Aug 17 03:55:42.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5060 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 17 03:55:42.507: INFO: stderr: ""
    Aug 17 03:55:42.507: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/17/23 03:55:42.507
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 17 03:55:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5060 delete pods e2e-test-httpd-pod'
    Aug 17 03:55:44.620: INFO: stderr: ""
    Aug 17 03:55:44.620: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5060" for this suite. 08/17/23 03:55:44.628
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:44.637
Aug 17 03:55:44.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 03:55:44.638
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:44.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:44.654
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/17/23 03:55:44.66
Aug 17 03:55:44.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 17 03:55:44.737: INFO: stderr: ""
Aug 17 03:55:44.737: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/17/23 03:55:44.737
Aug 17 03:55:44.737: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 17 03:55:44.737: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5002" to be "running and ready, or succeeded"
Aug 17 03:55:44.740: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569472ms
Aug 17 03:55:44.744: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ske-ubuntu-79fff84d86x69988-vjwlx' to be 'Running' but was 'Pending'
Aug 17 03:55:46.752: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015156883s
Aug 17 03:55:46.752: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 17 03:55:46.752: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/17/23 03:55:46.752
Aug 17 03:55:46.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator'
Aug 17 03:55:46.857: INFO: stderr: ""
Aug 17 03:55:46.857: INFO: stdout: "I0817 03:55:45.839467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8m7 366\nI0817 03:55:46.039558       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/gbcs 531\nI0817 03:55:46.240166       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4k2 319\nI0817 03:55:46.440529       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/lgw 212\nI0817 03:55:46.639717       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/szs 304\nI0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
STEP: limiting log lines 08/17/23 03:55:46.857
Aug 17 03:55:46.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --tail=1'
Aug 17 03:55:46.978: INFO: stderr: ""
Aug 17 03:55:46.978: INFO: stdout: "I0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
Aug 17 03:55:46.978: INFO: got output "I0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
STEP: limiting log bytes 08/17/23 03:55:46.978
Aug 17 03:55:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --limit-bytes=1'
Aug 17 03:55:47.050: INFO: stderr: ""
Aug 17 03:55:47.050: INFO: stdout: "I"
Aug 17 03:55:47.050: INFO: got output "I"
STEP: exposing timestamps 08/17/23 03:55:47.05
Aug 17 03:55:47.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 17 03:55:47.122: INFO: stderr: ""
Aug 17 03:55:47.122: INFO: stdout: "2023-08-17T12:55:47.040402105+09:00 I0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\n"
Aug 17 03:55:47.122: INFO: got output "2023-08-17T12:55:47.040402105+09:00 I0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\n"
STEP: restricting to a time range 08/17/23 03:55:47.122
Aug 17 03:55:49.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --since=1s'
Aug 17 03:55:49.698: INFO: stderr: ""
Aug 17 03:55:49.698: INFO: stdout: "I0817 03:55:48.840224       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/fmkb 212\nI0817 03:55:49.040530       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2cjh 574\nI0817 03:55:49.239860       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/w8bg 569\nI0817 03:55:49.440268       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/5n4c 258\nI0817 03:55:49.639508       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f62 322\n"
Aug 17 03:55:49.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --since=24h'
Aug 17 03:55:49.771: INFO: stderr: ""
Aug 17 03:55:49.771: INFO: stdout: "I0817 03:55:45.839467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8m7 366\nI0817 03:55:46.039558       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/gbcs 531\nI0817 03:55:46.240166       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4k2 319\nI0817 03:55:46.440529       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/lgw 212\nI0817 03:55:46.639717       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/szs 304\nI0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\nI0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\nI0817 03:55:47.239558       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/zmcx 505\nI0817 03:55:47.439898       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/rfm 433\nI0817 03:55:47.639967       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/qgsj 344\nI0817 03:55:47.839922       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/bg55 272\nI0817 03:55:48.040183       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/vk8l 431\nI0817 03:55:48.239494       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/28n 413\nI0817 03:55:48.439794       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/682 261\nI0817 03:55:48.640092       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/4h72 289\nI0817 03:55:48.840224       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/fmkb 212\nI0817 03:55:49.040530       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2cjh 574\nI0817 03:55:49.239860       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/w8bg 569\nI0817 03:55:49.440268       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/5n4c 258\nI0817 03:55:49.639508       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f62 322\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 17 03:55:49.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 delete pod logs-generator'
Aug 17 03:55:50.642: INFO: stderr: ""
Aug 17 03:55:50.642: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5002" for this suite. 08/17/23 03:55:50.649
------------------------------
• [SLOW TEST] [6.021 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:44.637
    Aug 17 03:55:44.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 03:55:44.638
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:44.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:44.654
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/17/23 03:55:44.66
    Aug 17 03:55:44.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 17 03:55:44.737: INFO: stderr: ""
    Aug 17 03:55:44.737: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/17/23 03:55:44.737
    Aug 17 03:55:44.737: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 17 03:55:44.737: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5002" to be "running and ready, or succeeded"
    Aug 17 03:55:44.740: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569472ms
    Aug 17 03:55:44.744: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ske-ubuntu-79fff84d86x69988-vjwlx' to be 'Running' but was 'Pending'
    Aug 17 03:55:46.752: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015156883s
    Aug 17 03:55:46.752: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 17 03:55:46.752: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/17/23 03:55:46.752
    Aug 17 03:55:46.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator'
    Aug 17 03:55:46.857: INFO: stderr: ""
    Aug 17 03:55:46.857: INFO: stdout: "I0817 03:55:45.839467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8m7 366\nI0817 03:55:46.039558       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/gbcs 531\nI0817 03:55:46.240166       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4k2 319\nI0817 03:55:46.440529       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/lgw 212\nI0817 03:55:46.639717       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/szs 304\nI0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
    STEP: limiting log lines 08/17/23 03:55:46.857
    Aug 17 03:55:46.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --tail=1'
    Aug 17 03:55:46.978: INFO: stderr: ""
    Aug 17 03:55:46.978: INFO: stdout: "I0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
    Aug 17 03:55:46.978: INFO: got output "I0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\n"
    STEP: limiting log bytes 08/17/23 03:55:46.978
    Aug 17 03:55:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --limit-bytes=1'
    Aug 17 03:55:47.050: INFO: stderr: ""
    Aug 17 03:55:47.050: INFO: stdout: "I"
    Aug 17 03:55:47.050: INFO: got output "I"
    STEP: exposing timestamps 08/17/23 03:55:47.05
    Aug 17 03:55:47.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 17 03:55:47.122: INFO: stderr: ""
    Aug 17 03:55:47.122: INFO: stdout: "2023-08-17T12:55:47.040402105+09:00 I0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\n"
    Aug 17 03:55:47.122: INFO: got output "2023-08-17T12:55:47.040402105+09:00 I0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\n"
    STEP: restricting to a time range 08/17/23 03:55:47.122
    Aug 17 03:55:49.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --since=1s'
    Aug 17 03:55:49.698: INFO: stderr: ""
    Aug 17 03:55:49.698: INFO: stdout: "I0817 03:55:48.840224       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/fmkb 212\nI0817 03:55:49.040530       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2cjh 574\nI0817 03:55:49.239860       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/w8bg 569\nI0817 03:55:49.440268       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/5n4c 258\nI0817 03:55:49.639508       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f62 322\n"
    Aug 17 03:55:49.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 logs logs-generator logs-generator --since=24h'
    Aug 17 03:55:49.771: INFO: stderr: ""
    Aug 17 03:55:49.771: INFO: stdout: "I0817 03:55:45.839467       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8m7 366\nI0817 03:55:46.039558       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/gbcs 531\nI0817 03:55:46.240166       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4k2 319\nI0817 03:55:46.440529       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/lgw 212\nI0817 03:55:46.639717       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/szs 304\nI0817 03:55:46.840092       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9x2q 389\nI0817 03:55:47.040294       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qfd4 506\nI0817 03:55:47.239558       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/zmcx 505\nI0817 03:55:47.439898       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/rfm 433\nI0817 03:55:47.639967       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/qgsj 344\nI0817 03:55:47.839922       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/bg55 272\nI0817 03:55:48.040183       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/vk8l 431\nI0817 03:55:48.239494       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/28n 413\nI0817 03:55:48.439794       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/682 261\nI0817 03:55:48.640092       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/4h72 289\nI0817 03:55:48.840224       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/fmkb 212\nI0817 03:55:49.040530       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2cjh 574\nI0817 03:55:49.239860       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/w8bg 569\nI0817 03:55:49.440268       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/5n4c 258\nI0817 03:55:49.639508       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/f62 322\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 17 03:55:49.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-5002 delete pod logs-generator'
    Aug 17 03:55:50.642: INFO: stderr: ""
    Aug 17 03:55:50.642: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5002" for this suite. 08/17/23 03:55:50.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:50.658
Aug 17 03:55:50.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 03:55:50.659
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:50.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:50.676
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/17/23 03:55:50.681
Aug 17 03:55:50.690: INFO: Waiting up to 5m0s for pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223" in namespace "var-expansion-9220" to be "Succeeded or Failed"
Aug 17 03:55:50.696: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368764ms
Aug 17 03:55:52.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012980906s
Aug 17 03:55:54.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012687337s
STEP: Saw pod success 08/17/23 03:55:54.703
Aug 17 03:55:54.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223" satisfied condition "Succeeded or Failed"
Aug 17 03:55:54.708: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 container dapi-container: <nil>
STEP: delete the pod 08/17/23 03:55:54.719
Aug 17 03:55:54.732: INFO: Waiting for pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 to disappear
Aug 17 03:55:54.736: INFO: Pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:54.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9220" for this suite. 08/17/23 03:55:54.743
------------------------------
• [4.095 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:50.658
    Aug 17 03:55:50.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 03:55:50.659
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:50.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:50.676
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/17/23 03:55:50.681
    Aug 17 03:55:50.690: INFO: Waiting up to 5m0s for pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223" in namespace "var-expansion-9220" to be "Succeeded or Failed"
    Aug 17 03:55:50.696: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368764ms
    Aug 17 03:55:52.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012980906s
    Aug 17 03:55:54.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012687337s
    STEP: Saw pod success 08/17/23 03:55:54.703
    Aug 17 03:55:54.703: INFO: Pod "var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223" satisfied condition "Succeeded or Failed"
    Aug 17 03:55:54.708: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 03:55:54.719
    Aug 17 03:55:54.732: INFO: Waiting for pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 to disappear
    Aug 17 03:55:54.736: INFO: Pod var-expansion-074c94b3-a1ec-4196-afd3-e1eb8485b223 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:54.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9220" for this suite. 08/17/23 03:55:54.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:54.757
Aug 17 03:55:54.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 03:55:54.758
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:54.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:54.777
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-70214518-7751-4509-86ed-f6cc19f2f0c6 08/17/23 03:55:54.781
STEP: Creating a pod to test consume configMaps 08/17/23 03:55:54.787
Aug 17 03:55:54.797: INFO: Waiting up to 5m0s for pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432" in namespace "configmap-8085" to be "Succeeded or Failed"
Aug 17 03:55:54.802: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337522ms
Aug 17 03:55:56.807: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00947821s
Aug 17 03:55:58.808: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010696211s
STEP: Saw pod success 08/17/23 03:55:58.808
Aug 17 03:55:58.808: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432" satisfied condition "Succeeded or Failed"
Aug 17 03:55:58.812: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 03:55:58.822
Aug 17 03:55:58.835: INFO: Waiting for pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 to disappear
Aug 17 03:55:58.839: INFO: Pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 03:55:58.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8085" for this suite. 08/17/23 03:55:58.846
------------------------------
• [4.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:54.757
    Aug 17 03:55:54.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 03:55:54.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:54.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:54.777
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-70214518-7751-4509-86ed-f6cc19f2f0c6 08/17/23 03:55:54.781
    STEP: Creating a pod to test consume configMaps 08/17/23 03:55:54.787
    Aug 17 03:55:54.797: INFO: Waiting up to 5m0s for pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432" in namespace "configmap-8085" to be "Succeeded or Failed"
    Aug 17 03:55:54.802: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337522ms
    Aug 17 03:55:56.807: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00947821s
    Aug 17 03:55:58.808: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010696211s
    STEP: Saw pod success 08/17/23 03:55:58.808
    Aug 17 03:55:58.808: INFO: Pod "pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432" satisfied condition "Succeeded or Failed"
    Aug 17 03:55:58.812: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 03:55:58.822
    Aug 17 03:55:58.835: INFO: Waiting for pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 to disappear
    Aug 17 03:55:58.839: INFO: Pod pod-configmaps-3878b124-bdf7-4657-aa55-cb6bb4c79432 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:55:58.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8085" for this suite. 08/17/23 03:55:58.846
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:55:58.853
Aug 17 03:55:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:55:58.855
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:58.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:58.876
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/17/23 03:55:58.881
Aug 17 03:55:58.891: INFO: Waiting up to 5m0s for pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2" in namespace "downward-api-5273" to be "running and ready"
Aug 17 03:55:58.895: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678619ms
Aug 17 03:55:58.895: INFO: The phase of Pod annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:56:00.901: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764309s
Aug 17 03:56:00.902: INFO: The phase of Pod annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2 is Running (Ready = true)
Aug 17 03:56:00.902: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2" satisfied condition "running and ready"
Aug 17 03:56:01.435: INFO: Successfully updated pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:56:03.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5273" for this suite. 08/17/23 03:56:03.471
------------------------------
• [4.628 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:55:58.853
    Aug 17 03:55:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:55:58.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:55:58.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:55:58.876
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/17/23 03:55:58.881
    Aug 17 03:55:58.891: INFO: Waiting up to 5m0s for pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2" in namespace "downward-api-5273" to be "running and ready"
    Aug 17 03:55:58.895: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678619ms
    Aug 17 03:55:58.895: INFO: The phase of Pod annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:56:00.901: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764309s
    Aug 17 03:56:00.902: INFO: The phase of Pod annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2 is Running (Ready = true)
    Aug 17 03:56:00.902: INFO: Pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2" satisfied condition "running and ready"
    Aug 17 03:56:01.435: INFO: Successfully updated pod "annotationupdate147a929d-3dab-4939-9b80-08a0c22d01e2"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:56:03.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5273" for this suite. 08/17/23 03:56:03.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:56:03.484
Aug 17 03:56:03.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 03:56:03.485
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:03.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:03.504
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:56:03.51
Aug 17 03:56:03.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390" in namespace "downward-api-9465" to be "Succeeded or Failed"
Aug 17 03:56:03.529: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Pending", Reason="", readiness=false. Elapsed: 8.758419ms
Aug 17 03:56:05.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Running", Reason="", readiness=true. Elapsed: 2.014574049s
Aug 17 03:56:07.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Running", Reason="", readiness=false. Elapsed: 4.01443051s
Aug 17 03:56:09.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014630036s
STEP: Saw pod success 08/17/23 03:56:09.535
Aug 17 03:56:09.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390" satisfied condition "Succeeded or Failed"
Aug 17 03:56:09.541: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 container client-container: <nil>
STEP: delete the pod 08/17/23 03:56:09.553
Aug 17 03:56:09.567: INFO: Waiting for pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 to disappear
Aug 17 03:56:09.572: INFO: Pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 03:56:09.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9465" for this suite. 08/17/23 03:56:09.582
------------------------------
• [SLOW TEST] [6.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:56:03.484
    Aug 17 03:56:03.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 03:56:03.485
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:03.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:03.504
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:56:03.51
    Aug 17 03:56:03.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390" in namespace "downward-api-9465" to be "Succeeded or Failed"
    Aug 17 03:56:03.529: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Pending", Reason="", readiness=false. Elapsed: 8.758419ms
    Aug 17 03:56:05.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Running", Reason="", readiness=true. Elapsed: 2.014574049s
    Aug 17 03:56:07.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Running", Reason="", readiness=false. Elapsed: 4.01443051s
    Aug 17 03:56:09.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014630036s
    STEP: Saw pod success 08/17/23 03:56:09.535
    Aug 17 03:56:09.535: INFO: Pod "downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390" satisfied condition "Succeeded or Failed"
    Aug 17 03:56:09.541: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:56:09.553
    Aug 17 03:56:09.567: INFO: Waiting for pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 to disappear
    Aug 17 03:56:09.572: INFO: Pod downwardapi-volume-a7f213c9-8e68-487b-96de-b1816ebd0390 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:56:09.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9465" for this suite. 08/17/23 03:56:09.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:56:09.601
Aug 17 03:56:09.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 03:56:09.602
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:09.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:09.623
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/17/23 03:56:09.629
Aug 17 03:56:09.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852" in namespace "projected-2951" to be "Succeeded or Failed"
Aug 17 03:56:09.647: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128535ms
Aug 17 03:56:11.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011553579s
Aug 17 03:56:13.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012007915s
Aug 17 03:56:15.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011268127s
STEP: Saw pod success 08/17/23 03:56:15.652
Aug 17 03:56:15.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852" satisfied condition "Succeeded or Failed"
Aug 17 03:56:15.656: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 container client-container: <nil>
STEP: delete the pod 08/17/23 03:56:15.666
Aug 17 03:56:15.679: INFO: Waiting for pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 to disappear
Aug 17 03:56:15.683: INFO: Pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 03:56:15.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2951" for this suite. 08/17/23 03:56:15.69
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:56:09.601
    Aug 17 03:56:09.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 03:56:09.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:09.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:09.623
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/17/23 03:56:09.629
    Aug 17 03:56:09.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852" in namespace "projected-2951" to be "Succeeded or Failed"
    Aug 17 03:56:09.647: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128535ms
    Aug 17 03:56:11.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011553579s
    Aug 17 03:56:13.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012007915s
    Aug 17 03:56:15.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011268127s
    STEP: Saw pod success 08/17/23 03:56:15.652
    Aug 17 03:56:15.652: INFO: Pod "downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852" satisfied condition "Succeeded or Failed"
    Aug 17 03:56:15.656: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 container client-container: <nil>
    STEP: delete the pod 08/17/23 03:56:15.666
    Aug 17 03:56:15.679: INFO: Waiting for pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 to disappear
    Aug 17 03:56:15.683: INFO: Pod downwardapi-volume-a6844b06-dad1-4255-84c0-c91edcaf9852 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:56:15.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2951" for this suite. 08/17/23 03:56:15.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:56:15.704
Aug 17 03:56:15.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 03:56:15.705
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:15.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:15.725
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3164 08/17/23 03:56:15.731
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/17/23 03:56:15.738
Aug 17 03:56:15.747: INFO: Found 0 stateful pods, waiting for 3
Aug 17 03:56:25.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 03:56:25.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 03:56:25.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 03:56:25.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 03:56:26.200: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 03:56:26.200: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 03:56:26.200: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 03:56:36.22
Aug 17 03:56:36.241: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/17/23 03:56:36.241
STEP: Updating Pods in reverse ordinal order 08/17/23 03:56:46.263
Aug 17 03:56:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 03:56:46.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 03:56:46.728: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 03:56:46.728: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/17/23 03:56:56.76
Aug 17 03:56:56.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 03:56:57.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 03:56:57.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 03:56:57.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 03:57:07.251: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/17/23 03:57:17.273
Aug 17 03:57:17.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 03:57:17.751: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 03:57:17.751: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 03:57:17.751: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 03:57:27.783: INFO: Deleting all statefulset in ns statefulset-3164
Aug 17 03:57:27.788: INFO: Scaling statefulset ss2 to 0
Aug 17 03:57:37.813: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 03:57:37.818: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 03:57:37.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3164" for this suite. 08/17/23 03:57:37.843
------------------------------
• [SLOW TEST] [82.147 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:56:15.704
    Aug 17 03:56:15.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 03:56:15.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:56:15.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:56:15.725
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3164 08/17/23 03:56:15.731
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/17/23 03:56:15.738
    Aug 17 03:56:15.747: INFO: Found 0 stateful pods, waiting for 3
    Aug 17 03:56:25.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 03:56:25.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 03:56:25.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 03:56:25.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 03:56:26.200: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 03:56:26.200: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 03:56:26.200: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 03:56:36.22
    Aug 17 03:56:36.241: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/17/23 03:56:36.241
    STEP: Updating Pods in reverse ordinal order 08/17/23 03:56:46.263
    Aug 17 03:56:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 03:56:46.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 03:56:46.728: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 03:56:46.728: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/17/23 03:56:56.76
    Aug 17 03:56:56.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 03:56:57.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 03:56:57.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 03:56:57.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 03:57:07.251: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/17/23 03:57:17.273
    Aug 17 03:57:17.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3164 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 03:57:17.751: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 03:57:17.751: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 03:57:17.751: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 03:57:27.783: INFO: Deleting all statefulset in ns statefulset-3164
    Aug 17 03:57:27.788: INFO: Scaling statefulset ss2 to 0
    Aug 17 03:57:37.813: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 03:57:37.818: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:57:37.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3164" for this suite. 08/17/23 03:57:37.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:57:37.855
Aug 17 03:57:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 03:57:37.856
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:57:37.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:57:37.875
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Aug 17 03:57:37.906: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:57:37.912
Aug 17 03:57:37.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:57:37.925: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:57:38.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:57:38.939: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:57:39.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:57:39.944: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 08/17/23 03:57:39.966
STEP: Check that daemon pods images are updated. 08/17/23 03:57:39.983
Aug 17 03:57:39.990: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 03:57:41.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 03:57:42.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 03:57:43.003: INFO: Pod daemon-set-hp5tl is not available
Aug 17 03:57:43.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 03:57:44.002: INFO: Pod daemon-set-hp5tl is not available
Aug 17 03:57:44.002: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 17 03:57:47.004: INFO: Pod daemon-set-fdtgf is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/17/23 03:57:47.011
Aug 17 03:57:47.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 03:57:47.021: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 03:57:48.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 03:57:48.037: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:57:48.059
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3664, will wait for the garbage collector to delete the pods 08/17/23 03:57:48.059
Aug 17 03:57:48.121: INFO: Deleting DaemonSet.extensions daemon-set took: 6.780618ms
Aug 17 03:57:48.221: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.68229ms
Aug 17 03:57:50.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 03:57:50.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 03:57:50.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"235160"},"items":null}

Aug 17 03:57:50.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"235160"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 03:57:50.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3664" for this suite. 08/17/23 03:57:50.161
------------------------------
• [SLOW TEST] [12.314 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:57:37.855
    Aug 17 03:57:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 03:57:37.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:57:37.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:57:37.875
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Aug 17 03:57:37.906: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 03:57:37.912
    Aug 17 03:57:37.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:57:37.925: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:57:38.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:57:38.939: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:57:39.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:57:39.944: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 08/17/23 03:57:39.966
    STEP: Check that daemon pods images are updated. 08/17/23 03:57:39.983
    Aug 17 03:57:39.990: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 03:57:41.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 03:57:42.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 03:57:43.003: INFO: Pod daemon-set-hp5tl is not available
    Aug 17 03:57:43.003: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 03:57:44.002: INFO: Pod daemon-set-hp5tl is not available
    Aug 17 03:57:44.002: INFO: Wrong image for pod: daemon-set-zq4bx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 17 03:57:47.004: INFO: Pod daemon-set-fdtgf is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/17/23 03:57:47.011
    Aug 17 03:57:47.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 03:57:47.021: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 03:57:48.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 03:57:48.037: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 03:57:48.059
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3664, will wait for the garbage collector to delete the pods 08/17/23 03:57:48.059
    Aug 17 03:57:48.121: INFO: Deleting DaemonSet.extensions daemon-set took: 6.780618ms
    Aug 17 03:57:48.221: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.68229ms
    Aug 17 03:57:50.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 03:57:50.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 03:57:50.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"235160"},"items":null}

    Aug 17 03:57:50.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"235160"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:57:50.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3664" for this suite. 08/17/23 03:57:50.161
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:57:50.17
Aug 17 03:57:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename subpath 08/17/23 03:57:50.171
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:57:50.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:57:50.191
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 03:57:50.195
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-qjgw 08/17/23 03:57:50.208
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:57:50.208
Aug 17 03:57:50.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qjgw" in namespace "subpath-1851" to be "Succeeded or Failed"
Aug 17 03:57:50.225: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.921747ms
Aug 17 03:57:52.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878432s
Aug 17 03:57:54.230: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 4.010246187s
Aug 17 03:57:56.230: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 6.010151328s
Aug 17 03:57:58.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 8.011670682s
Aug 17 03:58:00.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 10.011615702s
Aug 17 03:58:02.233: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 12.012481955s
Aug 17 03:58:04.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 14.011488773s
Aug 17 03:58:06.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 16.010692183s
Aug 17 03:58:08.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 18.010382212s
Aug 17 03:58:10.233: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 20.012612821s
Aug 17 03:58:12.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=false. Elapsed: 22.011124562s
Aug 17 03:58:14.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010494537s
STEP: Saw pod success 08/17/23 03:58:14.231
Aug 17 03:58:14.231: INFO: Pod "pod-subpath-test-projected-qjgw" satisfied condition "Succeeded or Failed"
Aug 17 03:58:14.236: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-projected-qjgw container test-container-subpath-projected-qjgw: <nil>
STEP: delete the pod 08/17/23 03:58:14.251
Aug 17 03:58:14.264: INFO: Waiting for pod pod-subpath-test-projected-qjgw to disappear
Aug 17 03:58:14.268: INFO: Pod pod-subpath-test-projected-qjgw no longer exists
STEP: Deleting pod pod-subpath-test-projected-qjgw 08/17/23 03:58:14.268
Aug 17 03:58:14.268: INFO: Deleting pod "pod-subpath-test-projected-qjgw" in namespace "subpath-1851"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 03:58:14.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1851" for this suite. 08/17/23 03:58:14.28
------------------------------
• [SLOW TEST] [24.122 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:57:50.17
    Aug 17 03:57:50.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename subpath 08/17/23 03:57:50.171
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:57:50.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:57:50.191
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 03:57:50.195
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-qjgw 08/17/23 03:57:50.208
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 03:57:50.208
    Aug 17 03:57:50.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qjgw" in namespace "subpath-1851" to be "Succeeded or Failed"
    Aug 17 03:57:50.225: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.921747ms
    Aug 17 03:57:52.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878432s
    Aug 17 03:57:54.230: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 4.010246187s
    Aug 17 03:57:56.230: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 6.010151328s
    Aug 17 03:57:58.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 8.011670682s
    Aug 17 03:58:00.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 10.011615702s
    Aug 17 03:58:02.233: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 12.012481955s
    Aug 17 03:58:04.232: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 14.011488773s
    Aug 17 03:58:06.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 16.010692183s
    Aug 17 03:58:08.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 18.010382212s
    Aug 17 03:58:10.233: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=true. Elapsed: 20.012612821s
    Aug 17 03:58:12.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Running", Reason="", readiness=false. Elapsed: 22.011124562s
    Aug 17 03:58:14.231: INFO: Pod "pod-subpath-test-projected-qjgw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010494537s
    STEP: Saw pod success 08/17/23 03:58:14.231
    Aug 17 03:58:14.231: INFO: Pod "pod-subpath-test-projected-qjgw" satisfied condition "Succeeded or Failed"
    Aug 17 03:58:14.236: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-projected-qjgw container test-container-subpath-projected-qjgw: <nil>
    STEP: delete the pod 08/17/23 03:58:14.251
    Aug 17 03:58:14.264: INFO: Waiting for pod pod-subpath-test-projected-qjgw to disappear
    Aug 17 03:58:14.268: INFO: Pod pod-subpath-test-projected-qjgw no longer exists
    STEP: Deleting pod pod-subpath-test-projected-qjgw 08/17/23 03:58:14.268
    Aug 17 03:58:14.268: INFO: Deleting pod "pod-subpath-test-projected-qjgw" in namespace "subpath-1851"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:58:14.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1851" for this suite. 08/17/23 03:58:14.28
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:58:14.296
Aug 17 03:58:14.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 03:58:14.298
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:14.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:14.316
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 17 03:58:14.331: INFO: Waiting up to 5m0s for pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5" in namespace "pods-2387" to be "running and ready"
Aug 17 03:58:14.336: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.677586ms
Aug 17 03:58:14.336: INFO: The phase of Pod server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 03:58:16.343: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012345701s
Aug 17 03:58:16.343: INFO: The phase of Pod server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5 is Running (Ready = true)
Aug 17 03:58:16.343: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5" satisfied condition "running and ready"
Aug 17 03:58:16.370: INFO: Waiting up to 5m0s for pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a" in namespace "pods-2387" to be "Succeeded or Failed"
Aug 17 03:58:16.375: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265292ms
Aug 17 03:58:18.381: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011504536s
Aug 17 03:58:20.382: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011647329s
STEP: Saw pod success 08/17/23 03:58:20.382
Aug 17 03:58:20.382: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a" satisfied condition "Succeeded or Failed"
Aug 17 03:58:20.387: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a container env3cont: <nil>
STEP: delete the pod 08/17/23 03:58:20.401
Aug 17 03:58:20.413: INFO: Waiting for pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a to disappear
Aug 17 03:58:20.417: INFO: Pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 03:58:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2387" for this suite. 08/17/23 03:58:20.424
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:58:14.296
    Aug 17 03:58:14.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 03:58:14.298
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:14.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:14.316
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 17 03:58:14.331: INFO: Waiting up to 5m0s for pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5" in namespace "pods-2387" to be "running and ready"
    Aug 17 03:58:14.336: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.677586ms
    Aug 17 03:58:14.336: INFO: The phase of Pod server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 03:58:16.343: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012345701s
    Aug 17 03:58:16.343: INFO: The phase of Pod server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5 is Running (Ready = true)
    Aug 17 03:58:16.343: INFO: Pod "server-envvars-4fb917ff-5f90-4db3-a2d8-38da7c0c92c5" satisfied condition "running and ready"
    Aug 17 03:58:16.370: INFO: Waiting up to 5m0s for pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a" in namespace "pods-2387" to be "Succeeded or Failed"
    Aug 17 03:58:16.375: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265292ms
    Aug 17 03:58:18.381: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011504536s
    Aug 17 03:58:20.382: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011647329s
    STEP: Saw pod success 08/17/23 03:58:20.382
    Aug 17 03:58:20.382: INFO: Pod "client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a" satisfied condition "Succeeded or Failed"
    Aug 17 03:58:20.387: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a container env3cont: <nil>
    STEP: delete the pod 08/17/23 03:58:20.401
    Aug 17 03:58:20.413: INFO: Waiting for pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a to disappear
    Aug 17 03:58:20.417: INFO: Pod client-envvars-d873529e-e577-46dc-b3aa-f51a8eb0ab9a no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:58:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2387" for this suite. 08/17/23 03:58:20.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:58:20.434
Aug 17 03:58:20.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 03:58:20.435
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:20.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:20.452
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/17/23 03:58:20.457
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_tcp@PTR;sleep 1; done
 08/17/23 03:58:20.474
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_tcp@PTR;sleep 1; done
 08/17/23 03:58:20.474
STEP: creating a pod to probe DNS 08/17/23 03:58:20.474
STEP: submitting the pod to kubernetes 08/17/23 03:58:20.474
Aug 17 03:58:20.484: INFO: Waiting up to 15m0s for pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347" in namespace "dns-1226" to be "running"
Aug 17 03:58:20.492: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347": Phase="Pending", Reason="", readiness=false. Elapsed: 8.663948ms
Aug 17 03:58:22.498: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347": Phase="Running", Reason="", readiness=true. Elapsed: 2.014482743s
Aug 17 03:58:22.498: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347" satisfied condition "running"
STEP: retrieving the pod 08/17/23 03:58:22.498
STEP: looking for the results for each expected name from probers 08/17/23 03:58:22.503
Aug 17 03:58:22.594: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.651: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.682: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.695: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.701: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:22.725: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:27.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:27.775: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:27.829: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:27.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:27.875: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:32.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:32.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:32.818: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:32.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:32.861: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:37.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:37.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:37.838: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:37.845: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:37.889: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:42.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:42.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:42.822: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:42.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:42.863: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:47.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:47.776: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:47.822: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:47.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
Aug 17 03:58:47.864: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

Aug 17 03:58:52.871: INFO: DNS probes using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 succeeded

STEP: deleting the pod 08/17/23 03:58:52.871
STEP: deleting the test service 08/17/23 03:58:52.921
STEP: deleting the test headless service 08/17/23 03:58:52.938
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 03:58:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1226" for this suite. 08/17/23 03:58:52.963
------------------------------
• [SLOW TEST] [32.537 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:58:20.434
    Aug 17 03:58:20.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 03:58:20.435
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:20.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:20.452
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/17/23 03:58:20.457
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_tcp@PTR;sleep 1; done
     08/17/23 03:58:20.474
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1226.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.31.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.31.226_tcp@PTR;sleep 1; done
     08/17/23 03:58:20.474
    STEP: creating a pod to probe DNS 08/17/23 03:58:20.474
    STEP: submitting the pod to kubernetes 08/17/23 03:58:20.474
    Aug 17 03:58:20.484: INFO: Waiting up to 15m0s for pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347" in namespace "dns-1226" to be "running"
    Aug 17 03:58:20.492: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347": Phase="Pending", Reason="", readiness=false. Elapsed: 8.663948ms
    Aug 17 03:58:22.498: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347": Phase="Running", Reason="", readiness=true. Elapsed: 2.014482743s
    Aug 17 03:58:22.498: INFO: Pod "dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 03:58:22.498
    STEP: looking for the results for each expected name from probers 08/17/23 03:58:22.503
    Aug 17 03:58:22.594: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.651: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.682: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.695: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.701: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:22.725: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:27.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:27.775: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:27.829: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:27.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:27.875: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:32.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:32.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:32.818: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:32.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:32.861: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:37.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:37.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:37.838: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:37.845: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:37.889: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:42.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:42.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:42.822: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:42.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:42.863: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:47.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:47.776: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:47.822: INFO: Unable to read jessie_udp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:47.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-1226.svc.cluster.local from pod dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347: the server could not find the requested resource (get pods dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347)
    Aug 17 03:58:47.864: INFO: Lookups using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 failed for: [wheezy_udp@dns-test-service.dns-1226.svc.cluster.local wheezy_tcp@dns-test-service.dns-1226.svc.cluster.local jessie_udp@dns-test-service.dns-1226.svc.cluster.local jessie_tcp@dns-test-service.dns-1226.svc.cluster.local]

    Aug 17 03:58:52.871: INFO: DNS probes using dns-1226/dns-test-6b4b21af-a4af-4e55-8248-1d3d3b0b2347 succeeded

    STEP: deleting the pod 08/17/23 03:58:52.871
    STEP: deleting the test service 08/17/23 03:58:52.921
    STEP: deleting the test headless service 08/17/23 03:58:52.938
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 03:58:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1226" for this suite. 08/17/23 03:58:52.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 03:58:52.973
Aug 17 03:58:52.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:58:52.974
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:52.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:52.996
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 17 03:58:53.019: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 17 03:59:53.080: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/17/23 03:59:53.084
Aug 17 03:59:53.104: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 17 03:59:53.110: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 17 03:59:53.125: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 17 03:59:53.140: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/17/23 03:59:53.14
Aug 17 03:59:53.140: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6049" to be "running"
Aug 17 03:59:53.144: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103071ms
Aug 17 03:59:55.150: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009272993s
Aug 17 03:59:55.150: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 17 03:59:55.150: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
Aug 17 03:59:55.154: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.826463ms
Aug 17 03:59:55.154: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 03:59:55.154: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
Aug 17 03:59:55.157: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521407ms
Aug 17 03:59:57.162: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008818213s
Aug 17 03:59:57.162: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 17 03:59:57.162: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
Aug 17 03:59:57.167: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.57349ms
Aug 17 03:59:57.167: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/17/23 03:59:57.167
Aug 17 03:59:57.178: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 17 03:59:57.182: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767078ms
Aug 17 03:59:59.188: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009727151s
Aug 17 04:00:01.189: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010970913s
Aug 17 04:00:01.189: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:01.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6049" for this suite. 08/17/23 04:00:01.31
------------------------------
• [SLOW TEST] [68.348 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 03:58:52.973
    Aug 17 03:58:52.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-preemption 08/17/23 03:58:52.974
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 03:58:52.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 03:58:52.996
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 17 03:58:53.019: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 17 03:59:53.080: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/17/23 03:59:53.084
    Aug 17 03:59:53.104: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 17 03:59:53.110: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 17 03:59:53.125: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 17 03:59:53.140: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/17/23 03:59:53.14
    Aug 17 03:59:53.140: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6049" to be "running"
    Aug 17 03:59:53.144: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103071ms
    Aug 17 03:59:55.150: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009272993s
    Aug 17 03:59:55.150: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 17 03:59:55.150: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
    Aug 17 03:59:55.154: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.826463ms
    Aug 17 03:59:55.154: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 03:59:55.154: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
    Aug 17 03:59:55.157: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521407ms
    Aug 17 03:59:57.162: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008818213s
    Aug 17 03:59:57.162: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 17 03:59:57.162: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6049" to be "running"
    Aug 17 03:59:57.167: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.57349ms
    Aug 17 03:59:57.167: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/17/23 03:59:57.167
    Aug 17 03:59:57.178: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 17 03:59:57.182: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767078ms
    Aug 17 03:59:59.188: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009727151s
    Aug 17 04:00:01.189: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010970913s
    Aug 17 04:00:01.189: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:01.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6049" for this suite. 08/17/23 04:00:01.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:01.323
Aug 17 04:00:01.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:00:01.324
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:01.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:01.352
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:00:01.381
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:00:01.913
STEP: Deploying the webhook pod 08/17/23 04:00:01.924
STEP: Wait for the deployment to be ready 08/17/23 04:00:01.948
Aug 17 04:00:01.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:00:03.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/17/23 04:00:05.992
STEP: Verifying the service has paired with the endpoint 08/17/23 04:00:06.004
Aug 17 04:00:07.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/17/23 04:00:07.01
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/17/23 04:00:07.125
STEP: Creating a configMap that should not be mutated 08/17/23 04:00:07.132
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/17/23 04:00:07.144
STEP: Creating a configMap that should be mutated 08/17/23 04:00:07.152
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2609" for this suite. 08/17/23 04:00:07.356
STEP: Destroying namespace "webhook-2609-markers" for this suite. 08/17/23 04:00:07.364
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:01.323
    Aug 17 04:00:01.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:00:01.324
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:01.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:01.352
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:00:01.381
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:00:01.913
    STEP: Deploying the webhook pod 08/17/23 04:00:01.924
    STEP: Wait for the deployment to be ready 08/17/23 04:00:01.948
    Aug 17 04:00:01.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 17 04:00:03.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/17/23 04:00:05.992
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:00:06.004
    Aug 17 04:00:07.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/17/23 04:00:07.01
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/17/23 04:00:07.125
    STEP: Creating a configMap that should not be mutated 08/17/23 04:00:07.132
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/17/23 04:00:07.144
    STEP: Creating a configMap that should be mutated 08/17/23 04:00:07.152
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2609" for this suite. 08/17/23 04:00:07.356
    STEP: Destroying namespace "webhook-2609-markers" for this suite. 08/17/23 04:00:07.364
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:07.373
Aug 17 04:00:07.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename podtemplate 08/17/23 04:00:07.375
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:07.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:07.402
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:07.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-931" for this suite. 08/17/23 04:00:07.469
------------------------------
• [0.105 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:07.373
    Aug 17 04:00:07.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename podtemplate 08/17/23 04:00:07.375
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:07.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:07.402
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:07.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-931" for this suite. 08/17/23 04:00:07.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:07.478
Aug 17 04:00:07.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 04:00:07.479
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:07.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:07.496
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/17/23 04:00:07.5
STEP: Ensure pods equal to parallelism count is attached to the job 08/17/23 04:00:07.506
STEP: patching /status 08/17/23 04:00:09.513
STEP: updating /status 08/17/23 04:00:09.521
STEP: get /status 08/17/23 04:00:09.555
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:09.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9282" for this suite. 08/17/23 04:00:09.568
------------------------------
• [2.097 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:07.478
    Aug 17 04:00:07.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 04:00:07.479
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:07.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:07.496
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/17/23 04:00:07.5
    STEP: Ensure pods equal to parallelism count is attached to the job 08/17/23 04:00:07.506
    STEP: patching /status 08/17/23 04:00:09.513
    STEP: updating /status 08/17/23 04:00:09.521
    STEP: get /status 08/17/23 04:00:09.555
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:09.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9282" for this suite. 08/17/23 04:00:09.568
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:09.576
Aug 17 04:00:09.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename lease-test 08/17/23 04:00:09.577
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:09.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:09.595
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:09.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-797" for this suite. 08/17/23 04:00:09.677
------------------------------
• [0.110 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:09.576
    Aug 17 04:00:09.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename lease-test 08/17/23 04:00:09.577
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:09.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:09.595
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:09.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-797" for this suite. 08/17/23 04:00:09.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:09.688
Aug 17 04:00:09.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 04:00:09.689
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:09.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:09.708
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/17/23 04:00:09.714
Aug 17 04:00:09.724: INFO: Waiting up to 5m0s for pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4" in namespace "var-expansion-2709" to be "Succeeded or Failed"
Aug 17 04:00:09.729: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703117ms
Aug 17 04:00:11.736: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011434411s
Aug 17 04:00:13.737: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012258388s
STEP: Saw pod success 08/17/23 04:00:13.737
Aug 17 04:00:13.737: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4" satisfied condition "Succeeded or Failed"
Aug 17 04:00:13.742: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 container dapi-container: <nil>
STEP: delete the pod 08/17/23 04:00:13.752
Aug 17 04:00:13.766: INFO: Waiting for pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 to disappear
Aug 17 04:00:13.771: INFO: Pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2709" for this suite. 08/17/23 04:00:13.778
------------------------------
• [4.099 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:09.688
    Aug 17 04:00:09.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 04:00:09.689
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:09.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:09.708
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/17/23 04:00:09.714
    Aug 17 04:00:09.724: INFO: Waiting up to 5m0s for pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4" in namespace "var-expansion-2709" to be "Succeeded or Failed"
    Aug 17 04:00:09.729: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.703117ms
    Aug 17 04:00:11.736: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011434411s
    Aug 17 04:00:13.737: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012258388s
    STEP: Saw pod success 08/17/23 04:00:13.737
    Aug 17 04:00:13.737: INFO: Pod "var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4" satisfied condition "Succeeded or Failed"
    Aug 17 04:00:13.742: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 04:00:13.752
    Aug 17 04:00:13.766: INFO: Waiting for pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 to disappear
    Aug 17 04:00:13.771: INFO: Pod var-expansion-627a12c6-a498-420f-a35a-50dbe11d8eb4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2709" for this suite. 08/17/23 04:00:13.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:13.789
Aug 17 04:00:13.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:00:13.79
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:13.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:13.816
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:00:13.844
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:00:14.243
STEP: Deploying the webhook pod 08/17/23 04:00:14.251
STEP: Wait for the deployment to be ready 08/17/23 04:00:14.269
Aug 17 04:00:14.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 17 04:00:16.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/17/23 04:00:18.304
STEP: Verifying the service has paired with the endpoint 08/17/23 04:00:18.316
Aug 17 04:00:19.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 04:00:19.323
STEP: create a pod 08/17/23 04:00:19.429
Aug 17 04:00:19.436: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1455" to be "running"
Aug 17 04:00:19.441: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73046ms
Aug 17 04:00:21.446: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009943359s
Aug 17 04:00:21.446: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/17/23 04:00:21.446
Aug 17 04:00:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=webhook-1455 attach --namespace=webhook-1455 to-be-attached-pod -i -c=container1'
Aug 17 04:00:21.624: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:21.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1455" for this suite. 08/17/23 04:00:21.674
STEP: Destroying namespace "webhook-1455-markers" for this suite. 08/17/23 04:00:21.681
------------------------------
• [SLOW TEST] [7.901 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:13.789
    Aug 17 04:00:13.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:00:13.79
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:13.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:13.816
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:00:13.844
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:00:14.243
    STEP: Deploying the webhook pod 08/17/23 04:00:14.251
    STEP: Wait for the deployment to be ready 08/17/23 04:00:14.269
    Aug 17 04:00:14.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 17 04:00:16.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/17/23 04:00:18.304
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:00:18.316
    Aug 17 04:00:19.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/17/23 04:00:19.323
    STEP: create a pod 08/17/23 04:00:19.429
    Aug 17 04:00:19.436: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1455" to be "running"
    Aug 17 04:00:19.441: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73046ms
    Aug 17 04:00:21.446: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009943359s
    Aug 17 04:00:21.446: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/17/23 04:00:21.446
    Aug 17 04:00:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=webhook-1455 attach --namespace=webhook-1455 to-be-attached-pod -i -c=container1'
    Aug 17 04:00:21.624: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:21.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1455" for this suite. 08/17/23 04:00:21.674
    STEP: Destroying namespace "webhook-1455-markers" for this suite. 08/17/23 04:00:21.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:21.699
Aug 17 04:00:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 04:00:21.7
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:21.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:21.724
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/17/23 04:00:21.74
STEP: create the rc2 08/17/23 04:00:21.746
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/17/23 04:00:26.758
STEP: delete the rc simpletest-rc-to-be-deleted 08/17/23 04:00:27.325
STEP: wait for the rc to be deleted 08/17/23 04:00:27.335
Aug 17 04:00:32.418: INFO: 70 pods remaining
Aug 17 04:00:32.418: INFO: 70 pods has nil DeletionTimestamp
Aug 17 04:00:32.418: INFO: 
STEP: Gathering metrics 08/17/23 04:00:37.387
W0817 04:00:37.405454      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 04:00:37.405: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 17 04:00:37.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-229bg" in namespace "gc-4228"
Aug 17 04:00:37.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jwlw" in namespace "gc-4228"
Aug 17 04:00:37.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-2slfd" in namespace "gc-4228"
Aug 17 04:00:37.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-42jww" in namespace "gc-4228"
Aug 17 04:00:37.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cl2q" in namespace "gc-4228"
Aug 17 04:00:37.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hns2" in namespace "gc-4228"
Aug 17 04:00:37.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q9kl" in namespace "gc-4228"
Aug 17 04:00:37.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wsp2" in namespace "gc-4228"
Aug 17 04:00:37.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nps8" in namespace "gc-4228"
Aug 17 04:00:37.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-68l4f" in namespace "gc-4228"
Aug 17 04:00:37.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f7lv" in namespace "gc-4228"
Aug 17 04:00:37.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjxt" in namespace "gc-4228"
Aug 17 04:00:37.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksgf" in namespace "gc-4228"
Aug 17 04:00:37.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pgg9" in namespace "gc-4228"
Aug 17 04:00:37.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-6thrq" in namespace "gc-4228"
Aug 17 04:00:37.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-76l6s" in namespace "gc-4228"
Aug 17 04:00:37.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-77kww" in namespace "gc-4228"
Aug 17 04:00:37.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fgjd" in namespace "gc-4228"
Aug 17 04:00:37.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hflz" in namespace "gc-4228"
Aug 17 04:00:37.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mczz" in namespace "gc-4228"
Aug 17 04:00:37.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p29m" in namespace "gc-4228"
Aug 17 04:00:37.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-86ddp" in namespace "gc-4228"
Aug 17 04:00:37.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tqx" in namespace "gc-4228"
Aug 17 04:00:37.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bmmk" in namespace "gc-4228"
Aug 17 04:00:37.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cr5p" in namespace "gc-4228"
Aug 17 04:00:37.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g7wm" in namespace "gc-4228"
Aug 17 04:00:37.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7rk" in namespace "gc-4228"
Aug 17 04:00:37.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xklk" in namespace "gc-4228"
Aug 17 04:00:37.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zs6c" in namespace "gc-4228"
Aug 17 04:00:37.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fmft" in namespace "gc-4228"
Aug 17 04:00:37.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgj89" in namespace "gc-4228"
Aug 17 04:00:37.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2jhh" in namespace "gc-4228"
Aug 17 04:00:37.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7sq2" in namespace "gc-4228"
Aug 17 04:00:37.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxzdv" in namespace "gc-4228"
Aug 17 04:00:37.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7pzc" in namespace "gc-4228"
Aug 17 04:00:37.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgprw" in namespace "gc-4228"
Aug 17 04:00:37.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-dll9k" in namespace "gc-4228"
Aug 17 04:00:37.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-drlgv" in namespace "gc-4228"
Aug 17 04:00:37.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjrwd" in namespace "gc-4228"
Aug 17 04:00:37.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9756" in namespace "gc-4228"
Aug 17 04:00:37.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9nfw" in namespace "gc-4228"
Aug 17 04:00:37.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd67n" in namespace "gc-4228"
Aug 17 04:00:38.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnb9c" in namespace "gc-4228"
Aug 17 04:00:38.018: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqz4f" in namespace "gc-4228"
Aug 17 04:00:38.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzgjm" in namespace "gc-4228"
Aug 17 04:00:38.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9p7v" in namespace "gc-4228"
Aug 17 04:00:38.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfrkd" in namespace "gc-4228"
Aug 17 04:00:38.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkzk7" in namespace "gc-4228"
Aug 17 04:00:38.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-lbrcb" in namespace "gc-4228"
Aug 17 04:00:38.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-lnr8z" in namespace "gc-4228"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 04:00:38.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4228" for this suite. 08/17/23 04:00:38.132
------------------------------
• [SLOW TEST] [16.441 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:21.699
    Aug 17 04:00:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 04:00:21.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:21.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:21.724
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/17/23 04:00:21.74
    STEP: create the rc2 08/17/23 04:00:21.746
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/17/23 04:00:26.758
    STEP: delete the rc simpletest-rc-to-be-deleted 08/17/23 04:00:27.325
    STEP: wait for the rc to be deleted 08/17/23 04:00:27.335
    Aug 17 04:00:32.418: INFO: 70 pods remaining
    Aug 17 04:00:32.418: INFO: 70 pods has nil DeletionTimestamp
    Aug 17 04:00:32.418: INFO: 
    STEP: Gathering metrics 08/17/23 04:00:37.387
    W0817 04:00:37.405454      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 04:00:37.405: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 17 04:00:37.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-229bg" in namespace "gc-4228"
    Aug 17 04:00:37.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jwlw" in namespace "gc-4228"
    Aug 17 04:00:37.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-2slfd" in namespace "gc-4228"
    Aug 17 04:00:37.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-42jww" in namespace "gc-4228"
    Aug 17 04:00:37.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cl2q" in namespace "gc-4228"
    Aug 17 04:00:37.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hns2" in namespace "gc-4228"
    Aug 17 04:00:37.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q9kl" in namespace "gc-4228"
    Aug 17 04:00:37.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wsp2" in namespace "gc-4228"
    Aug 17 04:00:37.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nps8" in namespace "gc-4228"
    Aug 17 04:00:37.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-68l4f" in namespace "gc-4228"
    Aug 17 04:00:37.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f7lv" in namespace "gc-4228"
    Aug 17 04:00:37.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hjxt" in namespace "gc-4228"
    Aug 17 04:00:37.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksgf" in namespace "gc-4228"
    Aug 17 04:00:37.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pgg9" in namespace "gc-4228"
    Aug 17 04:00:37.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-6thrq" in namespace "gc-4228"
    Aug 17 04:00:37.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-76l6s" in namespace "gc-4228"
    Aug 17 04:00:37.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-77kww" in namespace "gc-4228"
    Aug 17 04:00:37.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fgjd" in namespace "gc-4228"
    Aug 17 04:00:37.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hflz" in namespace "gc-4228"
    Aug 17 04:00:37.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mczz" in namespace "gc-4228"
    Aug 17 04:00:37.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p29m" in namespace "gc-4228"
    Aug 17 04:00:37.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-86ddp" in namespace "gc-4228"
    Aug 17 04:00:37.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tqx" in namespace "gc-4228"
    Aug 17 04:00:37.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bmmk" in namespace "gc-4228"
    Aug 17 04:00:37.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cr5p" in namespace "gc-4228"
    Aug 17 04:00:37.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g7wm" in namespace "gc-4228"
    Aug 17 04:00:37.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7rk" in namespace "gc-4228"
    Aug 17 04:00:37.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xklk" in namespace "gc-4228"
    Aug 17 04:00:37.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zs6c" in namespace "gc-4228"
    Aug 17 04:00:37.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fmft" in namespace "gc-4228"
    Aug 17 04:00:37.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgj89" in namespace "gc-4228"
    Aug 17 04:00:37.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2jhh" in namespace "gc-4228"
    Aug 17 04:00:37.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7sq2" in namespace "gc-4228"
    Aug 17 04:00:37.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxzdv" in namespace "gc-4228"
    Aug 17 04:00:37.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7pzc" in namespace "gc-4228"
    Aug 17 04:00:37.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgprw" in namespace "gc-4228"
    Aug 17 04:00:37.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-dll9k" in namespace "gc-4228"
    Aug 17 04:00:37.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-drlgv" in namespace "gc-4228"
    Aug 17 04:00:37.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjrwd" in namespace "gc-4228"
    Aug 17 04:00:37.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9756" in namespace "gc-4228"
    Aug 17 04:00:37.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9nfw" in namespace "gc-4228"
    Aug 17 04:00:37.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd67n" in namespace "gc-4228"
    Aug 17 04:00:38.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnb9c" in namespace "gc-4228"
    Aug 17 04:00:38.018: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqz4f" in namespace "gc-4228"
    Aug 17 04:00:38.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzgjm" in namespace "gc-4228"
    Aug 17 04:00:38.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9p7v" in namespace "gc-4228"
    Aug 17 04:00:38.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfrkd" in namespace "gc-4228"
    Aug 17 04:00:38.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkzk7" in namespace "gc-4228"
    Aug 17 04:00:38.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-lbrcb" in namespace "gc-4228"
    Aug 17 04:00:38.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-lnr8z" in namespace "gc-4228"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:00:38.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4228" for this suite. 08/17/23 04:00:38.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:00:38.141
Aug 17 04:00:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:00:38.142
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:38.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:38.187
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7515 08/17/23 04:00:38.22
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/17/23 04:00:38.246
Aug 17 04:00:38.323: INFO: Found 1 stateful pods, waiting for 3
Aug 17 04:00:48.329: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:00:48.329: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:00:48.330: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 17 04:00:58.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:00:58.331: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:00:58.331: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 04:00:58.347
Aug 17 04:00:58.370: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/17/23 04:00:58.37
STEP: Not applying an update when the partition is greater than the number of replicas 08/17/23 04:01:08.402
STEP: Performing a canary update 08/17/23 04:01:08.402
Aug 17 04:01:08.425: INFO: Updating stateful set ss2
Aug 17 04:01:08.440: INFO: Waiting for Pod statefulset-7515/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/17/23 04:01:18.452
Aug 17 04:01:18.492: INFO: Found 2 stateful pods, waiting for 3
Aug 17 04:01:28.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:01:28.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:01:28.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/17/23 04:01:28.511
Aug 17 04:01:28.533: INFO: Updating stateful set ss2
Aug 17 04:01:28.542: INFO: Waiting for Pod statefulset-7515/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 17 04:01:38.577: INFO: Updating stateful set ss2
Aug 17 04:01:38.588: INFO: Waiting for StatefulSet statefulset-7515/ss2 to complete update
Aug 17 04:01:38.588: INFO: Waiting for Pod statefulset-7515/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:01:48.601: INFO: Deleting all statefulset in ns statefulset-7515
Aug 17 04:01:48.605: INFO: Scaling statefulset ss2 to 0
Aug 17 04:01:58.627: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:01:58.632: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:01:58.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7515" for this suite. 08/17/23 04:01:58.654
------------------------------
• [SLOW TEST] [80.521 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:00:38.141
    Aug 17 04:00:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:00:38.142
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:00:38.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:00:38.187
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7515 08/17/23 04:00:38.22
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/17/23 04:00:38.246
    Aug 17 04:00:38.323: INFO: Found 1 stateful pods, waiting for 3
    Aug 17 04:00:48.329: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:00:48.329: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:00:48.330: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Aug 17 04:00:58.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:00:58.331: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:00:58.331: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/17/23 04:00:58.347
    Aug 17 04:00:58.370: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/17/23 04:00:58.37
    STEP: Not applying an update when the partition is greater than the number of replicas 08/17/23 04:01:08.402
    STEP: Performing a canary update 08/17/23 04:01:08.402
    Aug 17 04:01:08.425: INFO: Updating stateful set ss2
    Aug 17 04:01:08.440: INFO: Waiting for Pod statefulset-7515/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/17/23 04:01:18.452
    Aug 17 04:01:18.492: INFO: Found 2 stateful pods, waiting for 3
    Aug 17 04:01:28.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:01:28.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:01:28.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/17/23 04:01:28.511
    Aug 17 04:01:28.533: INFO: Updating stateful set ss2
    Aug 17 04:01:28.542: INFO: Waiting for Pod statefulset-7515/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 17 04:01:38.577: INFO: Updating stateful set ss2
    Aug 17 04:01:38.588: INFO: Waiting for StatefulSet statefulset-7515/ss2 to complete update
    Aug 17 04:01:38.588: INFO: Waiting for Pod statefulset-7515/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:01:48.601: INFO: Deleting all statefulset in ns statefulset-7515
    Aug 17 04:01:48.605: INFO: Scaling statefulset ss2 to 0
    Aug 17 04:01:58.627: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:01:58.632: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:01:58.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7515" for this suite. 08/17/23 04:01:58.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:01:58.664
Aug 17 04:01:58.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 04:01:58.665
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:01:58.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:01:58.686
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 17 04:01:58.702: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 17 04:02:03.706: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 04:02:03.706
Aug 17 04:02:03.707: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 17 04:02:05.712: INFO: Creating deployment "test-rollover-deployment"
Aug 17 04:02:05.726: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 17 04:02:07.735: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 17 04:02:07.744: INFO: Ensure that both replica sets have 1 created replica
Aug 17 04:02:07.752: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 17 04:02:07.763: INFO: Updating deployment test-rollover-deployment
Aug 17 04:02:07.763: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 17 04:02:09.774: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 17 04:02:09.783: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 17 04:02:09.792: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 04:02:09.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 04:02:11.805: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 04:02:11.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 04:02:13.804: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 04:02:13.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 04:02:15.805: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 04:02:15.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 04:02:17.803: INFO: all replica sets need to contain the pod-template-hash label
Aug 17 04:02:17.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 17 04:02:19.803: INFO: 
Aug 17 04:02:19.803: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 04:02:19.816: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9759  57c0e792-1af2-4e60-a959-227f406f2212 238790 2 2023-08-17 04:02:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004755028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 04:02:05 +0000 UTC,LastTransitionTime:2023-08-17 04:02:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-17 04:02:19 +0000 UTC,LastTransitionTime:2023-08-17 04:02:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 04:02:19.821: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9759  53e7c5d6-9dd3-48a7-baa9-7e20368b6b63 238780 2 2023-08-17 04:02:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc0047554f7 0xc0047554f8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047555a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:02:19.821: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 17 04:02:19.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9759  b1f322c3-5f74-434a-92d5-09ef3a98df61 238789 2 2023-08-17 04:01:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc0047553c7 0xc0047553c8}] [] [{e2e.test Update apps/v1 2023-08-17 04:01:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004755488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:02:19.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9759  e8bd8c7b-84eb-4d9f-882b-60d60e668545 238724 2 2023-08-17 04:02:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc004755617 0xc004755618}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047556c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:02:19.826: INFO: Pod "test-rollover-deployment-6c6df9974f-rmd4b" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rmd4b test-rollover-deployment-6c6df9974f- deployment-9759  4df273fa-d19d-49ec-8e89-e0ca50fcaf97 238744 0 2023-08-17 04:02:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:f3400a85735b9596a3c3c5c2c8a1abc7669f8dc172bf99f9c2c6c464107112f3 cni.projectcalico.org/podIP:172.21.15.126/32 cni.projectcalico.org/podIPs:172.21.15.126/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 53e7c5d6-9dd3-48a7-baa9-7e20368b6b63 0xc004755c17 0xc004755c18}] [] [{kube-controller-manager Update v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"53e7c5d6-9dd3-48a7-baa9-7e20368b6b63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:02:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:02:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2r6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2r6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.126,StartTime:2023-08-17 04:02:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:02:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9548813afc972a3e47f086166095635f1c1f30a06a1703b8d96e6e752017cbe4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:19.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9759" for this suite. 08/17/23 04:02:19.833
------------------------------
• [SLOW TEST] [21.178 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:01:58.664
    Aug 17 04:01:58.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 04:01:58.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:01:58.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:01:58.686
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 17 04:01:58.702: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 17 04:02:03.706: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 04:02:03.706
    Aug 17 04:02:03.707: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 17 04:02:05.712: INFO: Creating deployment "test-rollover-deployment"
    Aug 17 04:02:05.726: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 17 04:02:07.735: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 17 04:02:07.744: INFO: Ensure that both replica sets have 1 created replica
    Aug 17 04:02:07.752: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 17 04:02:07.763: INFO: Updating deployment test-rollover-deployment
    Aug 17 04:02:07.763: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 17 04:02:09.774: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 17 04:02:09.783: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 17 04:02:09.792: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 04:02:09.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 04:02:11.805: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 04:02:11.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 04:02:13.804: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 04:02:13.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 04:02:15.805: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 04:02:15.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 04:02:17.803: INFO: all replica sets need to contain the pod-template-hash label
    Aug 17 04:02:17.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 17, 4, 2, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 17, 4, 2, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 17 04:02:19.803: INFO: 
    Aug 17 04:02:19.803: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 04:02:19.816: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9759  57c0e792-1af2-4e60-a959-227f406f2212 238790 2 2023-08-17 04:02:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004755028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 04:02:05 +0000 UTC,LastTransitionTime:2023-08-17 04:02:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-17 04:02:19 +0000 UTC,LastTransitionTime:2023-08-17 04:02:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 04:02:19.821: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9759  53e7c5d6-9dd3-48a7-baa9-7e20368b6b63 238780 2 2023-08-17 04:02:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc0047554f7 0xc0047554f8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047555a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:02:19.821: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 17 04:02:19.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9759  b1f322c3-5f74-434a-92d5-09ef3a98df61 238789 2 2023-08-17 04:01:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc0047553c7 0xc0047553c8}] [] [{e2e.test Update apps/v1 2023-08-17 04:01:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004755488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:02:19.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9759  e8bd8c7b-84eb-4d9f-882b-60d60e668545 238724 2 2023-08-17 04:02:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 57c0e792-1af2-4e60-a959-227f406f2212 0xc004755617 0xc004755618}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c0e792-1af2-4e60-a959-227f406f2212\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047556c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:02:19.826: INFO: Pod "test-rollover-deployment-6c6df9974f-rmd4b" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rmd4b test-rollover-deployment-6c6df9974f- deployment-9759  4df273fa-d19d-49ec-8e89-e0ca50fcaf97 238744 0 2023-08-17 04:02:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:f3400a85735b9596a3c3c5c2c8a1abc7669f8dc172bf99f9c2c6c464107112f3 cni.projectcalico.org/podIP:172.21.15.126/32 cni.projectcalico.org/podIPs:172.21.15.126/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 53e7c5d6-9dd3-48a7-baa9-7e20368b6b63 0xc004755c17 0xc004755c18}] [] [{kube-controller-manager Update v1 2023-08-17 04:02:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"53e7c5d6-9dd3-48a7-baa9-7e20368b6b63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:02:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:02:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2r6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2r6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:02:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.126,StartTime:2023-08-17 04:02:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:02:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9548813afc972a3e47f086166095635f1c1f30a06a1703b8d96e6e752017cbe4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:19.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9759" for this suite. 08/17/23 04:02:19.833
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:19.842
Aug 17 04:02:19.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:02:19.843
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:19.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:19.863
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:19.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6491" for this suite. 08/17/23 04:02:19.927
------------------------------
• [0.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:19.842
    Aug 17 04:02:19.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:02:19.843
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:19.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:19.863
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:19.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6491" for this suite. 08/17/23 04:02:19.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:19.937
Aug 17 04:02:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:02:19.938
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:19.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:19.961
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/17/23 04:02:19.966
Aug 17 04:02:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 04:02:21.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:27.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8584" for this suite. 08/17/23 04:02:27.457
------------------------------
• [SLOW TEST] [7.528 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:19.937
    Aug 17 04:02:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:02:19.938
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:19.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:19.961
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/17/23 04:02:19.966
    Aug 17 04:02:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 04:02:21.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:27.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8584" for this suite. 08/17/23 04:02:27.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:27.466
Aug 17 04:02:27.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 04:02:27.466
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:27.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:27.486
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/17/23 04:02:27.491
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/17/23 04:02:27.493
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 04:02:27.493
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/17/23 04:02:27.494
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/17/23 04:02:27.495
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 04:02:27.495
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 04:02:27.498
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:27.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3748" for this suite. 08/17/23 04:02:27.504
------------------------------
• [0.045 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:27.466
    Aug 17 04:02:27.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 04:02:27.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:27.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:27.486
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/17/23 04:02:27.491
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/17/23 04:02:27.493
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/17/23 04:02:27.493
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/17/23 04:02:27.494
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/17/23 04:02:27.495
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 04:02:27.495
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/17/23 04:02:27.498
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:27.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3748" for this suite. 08/17/23 04:02:27.504
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:27.513
Aug 17 04:02:27.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:02:27.513
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:27.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:27.532
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 17 04:02:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 04:02:29.027
Aug 17 04:02:29.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 create -f -'
Aug 17 04:02:29.622: INFO: stderr: ""
Aug 17 04:02:29.622: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 04:02:29.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 delete e2e-test-crd-publish-openapi-2723-crds test-cr'
Aug 17 04:02:29.692: INFO: stderr: ""
Aug 17 04:02:29.692: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 17 04:02:29.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 apply -f -'
Aug 17 04:02:29.907: INFO: stderr: ""
Aug 17 04:02:29.907: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 17 04:02:29.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 delete e2e-test-crd-publish-openapi-2723-crds test-cr'
Aug 17 04:02:29.974: INFO: stderr: ""
Aug 17 04:02:29.974: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/17/23 04:02:29.974
Aug 17 04:02:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 explain e2e-test-crd-publish-openapi-2723-crds'
Aug 17 04:02:30.168: INFO: stderr: ""
Aug 17 04:02:30.168: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2723-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:31.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4738" for this suite. 08/17/23 04:02:31.675
------------------------------
• [4.170 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:27.513
    Aug 17 04:02:27.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:02:27.513
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:27.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:27.532
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 17 04:02:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/17/23 04:02:29.027
    Aug 17 04:02:29.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 create -f -'
    Aug 17 04:02:29.622: INFO: stderr: ""
    Aug 17 04:02:29.622: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 17 04:02:29.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 delete e2e-test-crd-publish-openapi-2723-crds test-cr'
    Aug 17 04:02:29.692: INFO: stderr: ""
    Aug 17 04:02:29.692: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 17 04:02:29.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 apply -f -'
    Aug 17 04:02:29.907: INFO: stderr: ""
    Aug 17 04:02:29.907: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 17 04:02:29.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 --namespace=crd-publish-openapi-4738 delete e2e-test-crd-publish-openapi-2723-crds test-cr'
    Aug 17 04:02:29.974: INFO: stderr: ""
    Aug 17 04:02:29.974: INFO: stdout: "e2e-test-crd-publish-openapi-2723-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/17/23 04:02:29.974
    Aug 17 04:02:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-4738 explain e2e-test-crd-publish-openapi-2723-crds'
    Aug 17 04:02:30.168: INFO: stderr: ""
    Aug 17 04:02:30.168: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2723-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:31.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4738" for this suite. 08/17/23 04:02:31.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:31.684
Aug 17 04:02:31.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename watch 08/17/23 04:02:31.684
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:31.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:31.703
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/17/23 04:02:31.708
STEP: creating a watch on configmaps with label B 08/17/23 04:02:31.71
STEP: creating a watch on configmaps with label A or B 08/17/23 04:02:31.712
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.715
Aug 17 04:02:31.720: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238926 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:31.720: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238926 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.721
Aug 17 04:02:31.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238928 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:31.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238928 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/17/23 04:02:31.73
Aug 17 04:02:31.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238929 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:31.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238929 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.74
Aug 17 04:02:31.747: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238930 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:31.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238930 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/17/23 04:02:31.748
Aug 17 04:02:31.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238931 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:31.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238931 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/17/23 04:02:41.755
Aug 17 04:02:41.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238973 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:02:41.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238973 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:51.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2262" for this suite. 08/17/23 04:02:51.773
------------------------------
• [SLOW TEST] [20.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:31.684
    Aug 17 04:02:31.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename watch 08/17/23 04:02:31.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:31.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:31.703
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/17/23 04:02:31.708
    STEP: creating a watch on configmaps with label B 08/17/23 04:02:31.71
    STEP: creating a watch on configmaps with label A or B 08/17/23 04:02:31.712
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.715
    Aug 17 04:02:31.720: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238926 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:31.720: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238926 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.721
    Aug 17 04:02:31.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238928 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:31.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238928 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/17/23 04:02:31.73
    Aug 17 04:02:31.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238929 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:31.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238929 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/17/23 04:02:31.74
    Aug 17 04:02:31.747: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238930 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:31.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2262  cbe93223-433a-480e-b1f9-087bb4c6c93d 238930 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/17/23 04:02:31.748
    Aug 17 04:02:31.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238931 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:31.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238931 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/17/23 04:02:41.755
    Aug 17 04:02:41.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238973 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:02:41.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2262  eefafe47-090e-46ac-a3cf-00cc682564fd 238973 0 2023-08-17 04:02:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-17 04:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:51.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2262" for this suite. 08/17/23 04:02:51.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:51.783
Aug 17 04:02:51.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:02:51.783
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:51.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:51.806
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:02:51.823
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:02:52.171
STEP: Deploying the webhook pod 08/17/23 04:02:52.18
STEP: Wait for the deployment to be ready 08/17/23 04:02:52.193
Aug 17 04:02:52.202: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 04:02:54.217
STEP: Verifying the service has paired with the endpoint 08/17/23 04:02:54.227
Aug 17 04:02:55.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/17/23 04:02:55.233
STEP: create a namespace for the webhook 08/17/23 04:02:55.345
STEP: create a configmap should be unconditionally rejected by the webhook 08/17/23 04:02:55.354
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:02:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4876" for this suite. 08/17/23 04:02:55.425
STEP: Destroying namespace "webhook-4876-markers" for this suite. 08/17/23 04:02:55.432
------------------------------
• [3.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:51.783
    Aug 17 04:02:51.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:02:51.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:51.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:51.806
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:02:51.823
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:02:52.171
    STEP: Deploying the webhook pod 08/17/23 04:02:52.18
    STEP: Wait for the deployment to be ready 08/17/23 04:02:52.193
    Aug 17 04:02:52.202: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 04:02:54.217
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:02:54.227
    Aug 17 04:02:55.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/17/23 04:02:55.233
    STEP: create a namespace for the webhook 08/17/23 04:02:55.345
    STEP: create a configmap should be unconditionally rejected by the webhook 08/17/23 04:02:55.354
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:02:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4876" for this suite. 08/17/23 04:02:55.425
    STEP: Destroying namespace "webhook-4876-markers" for this suite. 08/17/23 04:02:55.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:02:55.443
Aug 17 04:02:55.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 04:02:55.444
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:55.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:55.471
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-470d9aef-56ba-4a5a-99ba-c7444eceac52 08/17/23 04:02:55.476
STEP: Creating a pod to test consume secrets 08/17/23 04:02:55.481
Aug 17 04:02:55.490: INFO: Waiting up to 5m0s for pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778" in namespace "secrets-4632" to be "Succeeded or Failed"
Aug 17 04:02:55.495: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593521ms
Aug 17 04:02:57.500: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009502735s
Aug 17 04:02:59.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01134229s
Aug 17 04:03:01.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010595238s
STEP: Saw pod success 08/17/23 04:03:01.501
Aug 17 04:03:01.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778" satisfied condition "Succeeded or Failed"
Aug 17 04:03:01.506: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:03:01.517
Aug 17 04:03:01.529: INFO: Waiting for pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 to disappear
Aug 17 04:03:01.533: INFO: Pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:01.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4632" for this suite. 08/17/23 04:03:01.54
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:02:55.443
    Aug 17 04:02:55.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 04:02:55.444
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:02:55.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:02:55.471
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-470d9aef-56ba-4a5a-99ba-c7444eceac52 08/17/23 04:02:55.476
    STEP: Creating a pod to test consume secrets 08/17/23 04:02:55.481
    Aug 17 04:02:55.490: INFO: Waiting up to 5m0s for pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778" in namespace "secrets-4632" to be "Succeeded or Failed"
    Aug 17 04:02:55.495: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593521ms
    Aug 17 04:02:57.500: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009502735s
    Aug 17 04:02:59.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01134229s
    Aug 17 04:03:01.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010595238s
    STEP: Saw pod success 08/17/23 04:03:01.501
    Aug 17 04:03:01.501: INFO: Pod "pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778" satisfied condition "Succeeded or Failed"
    Aug 17 04:03:01.506: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:03:01.517
    Aug 17 04:03:01.529: INFO: Waiting for pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 to disappear
    Aug 17 04:03:01.533: INFO: Pod pod-secrets-292e251d-8c88-486a-adf5-f6e6e6f51778 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:01.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4632" for this suite. 08/17/23 04:03:01.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:01.548
Aug 17 04:03:01.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:03:01.549
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:01.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:01.568
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/17/23 04:03:01.572
Aug 17 04:03:01.573: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3881 proxy --unix-socket=/tmp/kubectl-proxy-unix573937537/test'
STEP: retrieving proxy /api/ output 08/17/23 04:03:01.617
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:01.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3881" for this suite. 08/17/23 04:03:01.625
------------------------------
• [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:01.548
    Aug 17 04:03:01.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:03:01.549
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:01.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:01.568
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/17/23 04:03:01.572
    Aug 17 04:03:01.573: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3881 proxy --unix-socket=/tmp/kubectl-proxy-unix573937537/test'
    STEP: retrieving proxy /api/ output 08/17/23 04:03:01.617
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:01.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3881" for this suite. 08/17/23 04:03:01.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:01.635
Aug 17 04:03:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:03:01.635
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:01.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:01.659
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-5ef6322c-4bf4-4da9-b6fd-6a92e190fb27 08/17/23 04:03:01.666
STEP: Creating a pod to test consume configMaps 08/17/23 04:03:01.672
Aug 17 04:03:01.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22" in namespace "configmap-7742" to be "Succeeded or Failed"
Aug 17 04:03:01.688: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439117ms
Aug 17 04:03:03.694: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010487285s
Aug 17 04:03:05.695: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733665s
STEP: Saw pod success 08/17/23 04:03:05.695
Aug 17 04:03:05.695: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22" satisfied condition "Succeeded or Failed"
Aug 17 04:03:05.699: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 04:03:05.749
Aug 17 04:03:05.761: INFO: Waiting for pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 to disappear
Aug 17 04:03:05.765: INFO: Pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:05.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7742" for this suite. 08/17/23 04:03:05.772
------------------------------
• [4.148 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:01.635
    Aug 17 04:03:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:03:01.635
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:01.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:01.659
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-5ef6322c-4bf4-4da9-b6fd-6a92e190fb27 08/17/23 04:03:01.666
    STEP: Creating a pod to test consume configMaps 08/17/23 04:03:01.672
    Aug 17 04:03:01.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22" in namespace "configmap-7742" to be "Succeeded or Failed"
    Aug 17 04:03:01.688: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439117ms
    Aug 17 04:03:03.694: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010487285s
    Aug 17 04:03:05.695: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733665s
    STEP: Saw pod success 08/17/23 04:03:05.695
    Aug 17 04:03:05.695: INFO: Pod "pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22" satisfied condition "Succeeded or Failed"
    Aug 17 04:03:05.699: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 04:03:05.749
    Aug 17 04:03:05.761: INFO: Waiting for pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 to disappear
    Aug 17 04:03:05.765: INFO: Pod pod-configmaps-5a726510-5a05-49d2-935b-122857eabb22 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:05.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7742" for this suite. 08/17/23 04:03:05.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:05.784
Aug 17 04:03:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:03:05.784
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:05.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:05.804
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/17/23 04:03:05.808
Aug 17 04:03:05.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-8871 create -f -'
Aug 17 04:03:06.377: INFO: stderr: ""
Aug 17 04:03:06.377: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 04:03:06.377
Aug 17 04:03:07.384: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:03:07.384: INFO: Found 0 / 1
Aug 17 04:03:08.383: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:03:08.383: INFO: Found 1 / 1
Aug 17 04:03:08.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/17/23 04:03:08.383
Aug 17 04:03:08.388: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:03:08.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 04:03:08.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-8871 patch pod agnhost-primary-jcsdz -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 17 04:03:08.460: INFO: stderr: ""
Aug 17 04:03:08.460: INFO: stdout: "pod/agnhost-primary-jcsdz patched\n"
STEP: checking annotations 08/17/23 04:03:08.46
Aug 17 04:03:08.465: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:03:08.465: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8871" for this suite. 08/17/23 04:03:08.473
------------------------------
• [2.696 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:05.784
    Aug 17 04:03:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:03:05.784
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:05.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:05.804
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/17/23 04:03:05.808
    Aug 17 04:03:05.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-8871 create -f -'
    Aug 17 04:03:06.377: INFO: stderr: ""
    Aug 17 04:03:06.377: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 04:03:06.377
    Aug 17 04:03:07.384: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:03:07.384: INFO: Found 0 / 1
    Aug 17 04:03:08.383: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:03:08.383: INFO: Found 1 / 1
    Aug 17 04:03:08.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/17/23 04:03:08.383
    Aug 17 04:03:08.388: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:03:08.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 04:03:08.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-8871 patch pod agnhost-primary-jcsdz -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 17 04:03:08.460: INFO: stderr: ""
    Aug 17 04:03:08.460: INFO: stdout: "pod/agnhost-primary-jcsdz patched\n"
    STEP: checking annotations 08/17/23 04:03:08.46
    Aug 17 04:03:08.465: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:03:08.465: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8871" for this suite. 08/17/23 04:03:08.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:08.48
Aug 17 04:03:08.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:03:08.481
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:08.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:08.499
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-113d6748-87fd-47ff-82ab-2937497a6d4b 08/17/23 04:03:08.504
STEP: Creating a pod to test consume configMaps 08/17/23 04:03:08.511
Aug 17 04:03:08.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585" in namespace "configmap-4366" to be "Succeeded or Failed"
Aug 17 04:03:08.525: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Pending", Reason="", readiness=false. Elapsed: 4.242207ms
Aug 17 04:03:10.533: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012329772s
Aug 17 04:03:12.531: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010910486s
STEP: Saw pod success 08/17/23 04:03:12.531
Aug 17 04:03:12.532: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585" satisfied condition "Succeeded or Failed"
Aug 17 04:03:12.536: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 container configmap-volume-test: <nil>
STEP: delete the pod 08/17/23 04:03:12.546
Aug 17 04:03:12.574: INFO: Waiting for pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 to disappear
Aug 17 04:03:12.579: INFO: Pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:12.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4366" for this suite. 08/17/23 04:03:12.585
------------------------------
• [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:08.48
    Aug 17 04:03:08.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:03:08.481
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:08.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:08.499
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-113d6748-87fd-47ff-82ab-2937497a6d4b 08/17/23 04:03:08.504
    STEP: Creating a pod to test consume configMaps 08/17/23 04:03:08.511
    Aug 17 04:03:08.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585" in namespace "configmap-4366" to be "Succeeded or Failed"
    Aug 17 04:03:08.525: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Pending", Reason="", readiness=false. Elapsed: 4.242207ms
    Aug 17 04:03:10.533: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012329772s
    Aug 17 04:03:12.531: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010910486s
    STEP: Saw pod success 08/17/23 04:03:12.531
    Aug 17 04:03:12.532: INFO: Pod "pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585" satisfied condition "Succeeded or Failed"
    Aug 17 04:03:12.536: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 container configmap-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:03:12.546
    Aug 17 04:03:12.574: INFO: Waiting for pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 to disappear
    Aug 17 04:03:12.579: INFO: Pod pod-configmaps-cff73f3b-2d1e-4bac-8754-f369fea1c585 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:12.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4366" for this suite. 08/17/23 04:03:12.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:12.594
Aug 17 04:03:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename limitrange 08/17/23 04:03:12.595
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:12.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:12.614
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/17/23 04:03:12.619
STEP: Setting up watch 08/17/23 04:03:12.619
STEP: Submitting a LimitRange 08/17/23 04:03:12.724
STEP: Verifying LimitRange creation was observed 08/17/23 04:03:12.731
STEP: Fetching the LimitRange to ensure it has proper values 08/17/23 04:03:12.731
Aug 17 04:03:12.736: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 17 04:03:12.736: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/17/23 04:03:12.736
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/17/23 04:03:12.742
Aug 17 04:03:12.746: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 17 04:03:12.747: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/17/23 04:03:12.747
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/17/23 04:03:12.752
Aug 17 04:03:12.755: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 17 04:03:12.756: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/17/23 04:03:12.756
STEP: Failing to create a Pod with more than max resources 08/17/23 04:03:12.758
STEP: Updating a LimitRange 08/17/23 04:03:12.762
STEP: Verifying LimitRange updating is effective 08/17/23 04:03:12.767
STEP: Creating a Pod with less than former min resources 08/17/23 04:03:14.774
STEP: Failing to create a Pod with more than max resources 08/17/23 04:03:14.781
STEP: Deleting a LimitRange 08/17/23 04:03:14.784
STEP: Verifying the LimitRange was deleted 08/17/23 04:03:14.792
Aug 17 04:03:19.798: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/17/23 04:03:19.798
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:19.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7165" for this suite. 08/17/23 04:03:19.817
------------------------------
• [SLOW TEST] [7.231 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:12.594
    Aug 17 04:03:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename limitrange 08/17/23 04:03:12.595
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:12.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:12.614
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/17/23 04:03:12.619
    STEP: Setting up watch 08/17/23 04:03:12.619
    STEP: Submitting a LimitRange 08/17/23 04:03:12.724
    STEP: Verifying LimitRange creation was observed 08/17/23 04:03:12.731
    STEP: Fetching the LimitRange to ensure it has proper values 08/17/23 04:03:12.731
    Aug 17 04:03:12.736: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 17 04:03:12.736: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/17/23 04:03:12.736
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/17/23 04:03:12.742
    Aug 17 04:03:12.746: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 17 04:03:12.747: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/17/23 04:03:12.747
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/17/23 04:03:12.752
    Aug 17 04:03:12.755: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 17 04:03:12.756: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/17/23 04:03:12.756
    STEP: Failing to create a Pod with more than max resources 08/17/23 04:03:12.758
    STEP: Updating a LimitRange 08/17/23 04:03:12.762
    STEP: Verifying LimitRange updating is effective 08/17/23 04:03:12.767
    STEP: Creating a Pod with less than former min resources 08/17/23 04:03:14.774
    STEP: Failing to create a Pod with more than max resources 08/17/23 04:03:14.781
    STEP: Deleting a LimitRange 08/17/23 04:03:14.784
    STEP: Verifying the LimitRange was deleted 08/17/23 04:03:14.792
    Aug 17 04:03:19.798: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/17/23 04:03:19.798
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:19.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7165" for this suite. 08/17/23 04:03:19.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:19.828
Aug 17 04:03:19.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 04:03:19.829
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:19.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:19.849
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 04:03:19.855
Aug 17 04:03:19.864: INFO: Waiting up to 5m0s for pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3" in namespace "emptydir-5065" to be "Succeeded or Failed"
Aug 17 04:03:19.869: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501044ms
Aug 17 04:03:21.874: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010221065s
Aug 17 04:03:23.875: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010480257s
STEP: Saw pod success 08/17/23 04:03:23.875
Aug 17 04:03:23.875: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3" satisfied condition "Succeeded or Failed"
Aug 17 04:03:23.880: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 container test-container: <nil>
STEP: delete the pod 08/17/23 04:03:23.893
Aug 17 04:03:23.904: INFO: Waiting for pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 to disappear
Aug 17 04:03:23.907: INFO: Pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5065" for this suite. 08/17/23 04:03:23.914
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:19.828
    Aug 17 04:03:19.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 04:03:19.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:19.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:19.849
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/17/23 04:03:19.855
    Aug 17 04:03:19.864: INFO: Waiting up to 5m0s for pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3" in namespace "emptydir-5065" to be "Succeeded or Failed"
    Aug 17 04:03:19.869: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501044ms
    Aug 17 04:03:21.874: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010221065s
    Aug 17 04:03:23.875: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010480257s
    STEP: Saw pod success 08/17/23 04:03:23.875
    Aug 17 04:03:23.875: INFO: Pod "pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3" satisfied condition "Succeeded or Failed"
    Aug 17 04:03:23.880: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 container test-container: <nil>
    STEP: delete the pod 08/17/23 04:03:23.893
    Aug 17 04:03:23.904: INFO: Waiting for pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 to disappear
    Aug 17 04:03:23.907: INFO: Pod pod-ff5f32ba-cc29-43f5-ab80-7698f62426e3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5065" for this suite. 08/17/23 04:03:23.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:23.922
Aug 17 04:03:23.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 04:03:23.923
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:23.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:23.944
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/17/23 04:03:23.948
STEP: Creating a ResourceQuota 08/17/23 04:03:28.952
STEP: Ensuring resource quota status is calculated 08/17/23 04:03:28.957
STEP: Creating a ReplicaSet 08/17/23 04:03:30.963
STEP: Ensuring resource quota status captures replicaset creation 08/17/23 04:03:30.975
STEP: Deleting a ReplicaSet 08/17/23 04:03:32.981
STEP: Ensuring resource quota status released usage 08/17/23 04:03:32.988
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:34.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2119" for this suite. 08/17/23 04:03:35.003
------------------------------
• [SLOW TEST] [11.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:23.922
    Aug 17 04:03:23.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 04:03:23.923
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:23.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:23.944
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/17/23 04:03:23.948
    STEP: Creating a ResourceQuota 08/17/23 04:03:28.952
    STEP: Ensuring resource quota status is calculated 08/17/23 04:03:28.957
    STEP: Creating a ReplicaSet 08/17/23 04:03:30.963
    STEP: Ensuring resource quota status captures replicaset creation 08/17/23 04:03:30.975
    STEP: Deleting a ReplicaSet 08/17/23 04:03:32.981
    STEP: Ensuring resource quota status released usage 08/17/23 04:03:32.988
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:34.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2119" for this suite. 08/17/23 04:03:35.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:35.012
Aug 17 04:03:35.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename endpointslice 08/17/23 04:03:35.013
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:35.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:35.031
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 17 04:03:35.047: INFO: Endpoints addresses: [198.19.86.35] , ports: [6443]
Aug 17 04:03:35.047: INFO: EndpointSlices addresses: [198.19.86.35] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5337" for this suite. 08/17/23 04:03:35.053
------------------------------
• [0.049 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:35.012
    Aug 17 04:03:35.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename endpointslice 08/17/23 04:03:35.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:35.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:35.031
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 17 04:03:35.047: INFO: Endpoints addresses: [198.19.86.35] , ports: [6443]
    Aug 17 04:03:35.047: INFO: EndpointSlices addresses: [198.19.86.35] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5337" for this suite. 08/17/23 04:03:35.053
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:35.063
Aug 17 04:03:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:03:35.064
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:35.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:35.08
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:03:35.1
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:03:35.391
STEP: Deploying the webhook pod 08/17/23 04:03:35.401
STEP: Wait for the deployment to be ready 08/17/23 04:03:35.414
Aug 17 04:03:35.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 04:03:37.438
STEP: Verifying the service has paired with the endpoint 08/17/23 04:03:37.448
Aug 17 04:03:38.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/17/23 04:03:38.514
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:03:38.633
STEP: Deleting the collection of validation webhooks 08/17/23 04:03:38.73
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:03:38.781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:38.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5213" for this suite. 08/17/23 04:03:38.837
STEP: Destroying namespace "webhook-5213-markers" for this suite. 08/17/23 04:03:38.845
------------------------------
• [3.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:35.063
    Aug 17 04:03:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:03:35.064
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:35.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:35.08
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:03:35.1
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:03:35.391
    STEP: Deploying the webhook pod 08/17/23 04:03:35.401
    STEP: Wait for the deployment to be ready 08/17/23 04:03:35.414
    Aug 17 04:03:35.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 04:03:37.438
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:03:37.448
    Aug 17 04:03:38.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/17/23 04:03:38.514
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:03:38.633
    STEP: Deleting the collection of validation webhooks 08/17/23 04:03:38.73
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:03:38.781
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:38.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5213" for this suite. 08/17/23 04:03:38.837
    STEP: Destroying namespace "webhook-5213-markers" for this suite. 08/17/23 04:03:38.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:38.856
Aug 17 04:03:38.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svc-latency 08/17/23 04:03:38.857
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:38.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:38.874
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 17 04:03:38.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4406 08/17/23 04:03:38.879
I0817 04:03:38.885388      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4406, replica count: 1
I0817 04:03:39.937272      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:03:40.938076      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:03:41.050: INFO: Created: latency-svc-mfj6c
Aug 17 04:03:41.054: INFO: Got endpoints: latency-svc-mfj6c [16.100936ms]
Aug 17 04:03:41.065: INFO: Created: latency-svc-brs6z
Aug 17 04:03:41.068: INFO: Got endpoints: latency-svc-brs6z [13.489358ms]
Aug 17 04:03:41.069: INFO: Created: latency-svc-sv7zp
Aug 17 04:03:41.072: INFO: Got endpoints: latency-svc-sv7zp [17.121928ms]
Aug 17 04:03:41.073: INFO: Created: latency-svc-qzn26
Aug 17 04:03:41.076: INFO: Got endpoints: latency-svc-qzn26 [20.639878ms]
Aug 17 04:03:41.077: INFO: Created: latency-svc-844w8
Aug 17 04:03:41.081: INFO: Got endpoints: latency-svc-844w8 [26.246661ms]
Aug 17 04:03:41.082: INFO: Created: latency-svc-qqrkv
Aug 17 04:03:41.085: INFO: Got endpoints: latency-svc-qqrkv [29.513453ms]
Aug 17 04:03:41.086: INFO: Created: latency-svc-9fkdt
Aug 17 04:03:41.089: INFO: Got endpoints: latency-svc-9fkdt [33.357373ms]
Aug 17 04:03:41.091: INFO: Created: latency-svc-dj8jx
Aug 17 04:03:41.094: INFO: Got endpoints: latency-svc-dj8jx [38.364341ms]
Aug 17 04:03:41.096: INFO: Created: latency-svc-c9tq2
Aug 17 04:03:41.099: INFO: Got endpoints: latency-svc-c9tq2 [43.892475ms]
Aug 17 04:03:41.102: INFO: Created: latency-svc-ktjxr
Aug 17 04:03:41.106: INFO: Got endpoints: latency-svc-ktjxr [50.017614ms]
Aug 17 04:03:41.107: INFO: Created: latency-svc-v92vg
Aug 17 04:03:41.111: INFO: Got endpoints: latency-svc-v92vg [55.508615ms]
Aug 17 04:03:41.116: INFO: Created: latency-svc-kn8lq
Aug 17 04:03:41.126: INFO: Created: latency-svc-pgw9h
Aug 17 04:03:41.126: INFO: Got endpoints: latency-svc-pgw9h [70.851659ms]
Aug 17 04:03:41.126: INFO: Created: latency-svc-gfktg
Aug 17 04:03:41.126: INFO: Got endpoints: latency-svc-kn8lq [70.629518ms]
Aug 17 04:03:41.127: INFO: Got endpoints: latency-svc-gfktg [71.58399ms]
Aug 17 04:03:41.127: INFO: Created: latency-svc-c8x7b
Aug 17 04:03:41.131: INFO: Got endpoints: latency-svc-c8x7b [75.605156ms]
Aug 17 04:03:41.132: INFO: Created: latency-svc-v7qz5
Aug 17 04:03:41.135: INFO: Got endpoints: latency-svc-v7qz5 [79.539158ms]
Aug 17 04:03:41.137: INFO: Created: latency-svc-gqbf7
Aug 17 04:03:41.140: INFO: Got endpoints: latency-svc-gqbf7 [71.319522ms]
Aug 17 04:03:41.141: INFO: Created: latency-svc-h95dx
Aug 17 04:03:41.146: INFO: Got endpoints: latency-svc-h95dx [73.84958ms]
Aug 17 04:03:41.147: INFO: Created: latency-svc-glskw
Aug 17 04:03:41.150: INFO: Got endpoints: latency-svc-glskw [74.362809ms]
Aug 17 04:03:41.152: INFO: Created: latency-svc-kn9xx
Aug 17 04:03:41.155: INFO: Got endpoints: latency-svc-kn9xx [73.580478ms]
Aug 17 04:03:41.157: INFO: Created: latency-svc-wpd97
Aug 17 04:03:41.159: INFO: Got endpoints: latency-svc-wpd97 [74.725301ms]
Aug 17 04:03:41.161: INFO: Created: latency-svc-v9nm5
Aug 17 04:03:41.165: INFO: Got endpoints: latency-svc-v9nm5 [75.969729ms]
Aug 17 04:03:41.167: INFO: Created: latency-svc-gqnwn
Aug 17 04:03:41.169: INFO: Got endpoints: latency-svc-gqnwn [75.632782ms]
Aug 17 04:03:41.176: INFO: Created: latency-svc-chtj7
Aug 17 04:03:41.177: INFO: Created: latency-svc-rlx7v
Aug 17 04:03:41.180: INFO: Got endpoints: latency-svc-chtj7 [15.008964ms]
Aug 17 04:03:41.180: INFO: Got endpoints: latency-svc-rlx7v [81.038435ms]
Aug 17 04:03:41.181: INFO: Created: latency-svc-x7vlx
Aug 17 04:03:41.184: INFO: Got endpoints: latency-svc-x7vlx [73.126421ms]
Aug 17 04:03:41.185: INFO: Created: latency-svc-gpxzz
Aug 17 04:03:41.188: INFO: Got endpoints: latency-svc-gpxzz [61.584026ms]
Aug 17 04:03:41.191: INFO: Created: latency-svc-flnp2
Aug 17 04:03:41.195: INFO: Created: latency-svc-8gwfw
Aug 17 04:03:41.195: INFO: Got endpoints: latency-svc-flnp2 [68.458877ms]
Aug 17 04:03:41.198: INFO: Got endpoints: latency-svc-8gwfw [71.035538ms]
Aug 17 04:03:41.199: INFO: Created: latency-svc-9q482
Aug 17 04:03:41.203: INFO: Got endpoints: latency-svc-9q482 [71.2148ms]
Aug 17 04:03:41.205: INFO: Created: latency-svc-r6nxc
Aug 17 04:03:41.208: INFO: Created: latency-svc-svm26
Aug 17 04:03:41.209: INFO: Got endpoints: latency-svc-r6nxc [73.398263ms]
Aug 17 04:03:41.211: INFO: Got endpoints: latency-svc-svm26 [71.364601ms]
Aug 17 04:03:41.212: INFO: Created: latency-svc-m7p5t
Aug 17 04:03:41.215: INFO: Got endpoints: latency-svc-m7p5t [69.412086ms]
Aug 17 04:03:41.218: INFO: Created: latency-svc-bvg5f
Aug 17 04:03:41.222: INFO: Created: latency-svc-52wpg
Aug 17 04:03:41.227: INFO: Created: latency-svc-4zff5
Aug 17 04:03:41.231: INFO: Created: latency-svc-6lzlp
Aug 17 04:03:41.235: INFO: Created: latency-svc-6vv45
Aug 17 04:03:41.240: INFO: Created: latency-svc-zdt9r
Aug 17 04:03:41.244: INFO: Created: latency-svc-xg5w8
Aug 17 04:03:41.248: INFO: Created: latency-svc-h5zrz
Aug 17 04:03:41.253: INFO: Created: latency-svc-g7ph4
Aug 17 04:03:41.254: INFO: Got endpoints: latency-svc-bvg5f [104.025941ms]
Aug 17 04:03:41.257: INFO: Created: latency-svc-cftpk
Aug 17 04:03:41.262: INFO: Created: latency-svc-qp2kt
Aug 17 04:03:41.266: INFO: Created: latency-svc-fkmvm
Aug 17 04:03:41.270: INFO: Created: latency-svc-vl8rm
Aug 17 04:03:41.278: INFO: Created: latency-svc-5rql5
Aug 17 04:03:41.283: INFO: Created: latency-svc-mkgnp
Aug 17 04:03:41.286: INFO: Created: latency-svc-84wwd
Aug 17 04:03:41.306: INFO: Got endpoints: latency-svc-52wpg [150.624962ms]
Aug 17 04:03:41.317: INFO: Created: latency-svc-qhhx5
Aug 17 04:03:41.356: INFO: Got endpoints: latency-svc-4zff5 [196.863028ms]
Aug 17 04:03:41.367: INFO: Created: latency-svc-4fh86
Aug 17 04:03:41.406: INFO: Got endpoints: latency-svc-6lzlp [300.301795ms]
Aug 17 04:03:41.416: INFO: Created: latency-svc-92h2x
Aug 17 04:03:41.458: INFO: Got endpoints: latency-svc-6vv45 [288.437292ms]
Aug 17 04:03:41.469: INFO: Created: latency-svc-xj7m2
Aug 17 04:03:41.506: INFO: Got endpoints: latency-svc-zdt9r [325.789632ms]
Aug 17 04:03:41.517: INFO: Created: latency-svc-8nnpn
Aug 17 04:03:41.556: INFO: Got endpoints: latency-svc-xg5w8 [375.080195ms]
Aug 17 04:03:41.566: INFO: Created: latency-svc-xct2t
Aug 17 04:03:41.605: INFO: Got endpoints: latency-svc-h5zrz [420.643279ms]
Aug 17 04:03:41.616: INFO: Created: latency-svc-hzmgn
Aug 17 04:03:41.655: INFO: Got endpoints: latency-svc-g7ph4 [467.262383ms]
Aug 17 04:03:41.665: INFO: Created: latency-svc-j7h5v
Aug 17 04:03:41.706: INFO: Got endpoints: latency-svc-cftpk [511.019055ms]
Aug 17 04:03:41.718: INFO: Created: latency-svc-bhxwv
Aug 17 04:03:41.755: INFO: Got endpoints: latency-svc-qp2kt [556.593071ms]
Aug 17 04:03:41.766: INFO: Created: latency-svc-qnsjn
Aug 17 04:03:41.806: INFO: Got endpoints: latency-svc-fkmvm [603.015693ms]
Aug 17 04:03:41.817: INFO: Created: latency-svc-qlmc7
Aug 17 04:03:41.855: INFO: Got endpoints: latency-svc-vl8rm [646.745986ms]
Aug 17 04:03:41.867: INFO: Created: latency-svc-5rw94
Aug 17 04:03:41.906: INFO: Got endpoints: latency-svc-5rql5 [694.925938ms]
Aug 17 04:03:41.918: INFO: Created: latency-svc-7s9mh
Aug 17 04:03:41.956: INFO: Got endpoints: latency-svc-mkgnp [740.13488ms]
Aug 17 04:03:41.966: INFO: Created: latency-svc-8khmx
Aug 17 04:03:42.005: INFO: Got endpoints: latency-svc-84wwd [750.786704ms]
Aug 17 04:03:42.015: INFO: Created: latency-svc-b95xt
Aug 17 04:03:42.056: INFO: Got endpoints: latency-svc-qhhx5 [749.729728ms]
Aug 17 04:03:42.066: INFO: Created: latency-svc-66j5r
Aug 17 04:03:42.105: INFO: Got endpoints: latency-svc-4fh86 [748.962921ms]
Aug 17 04:03:42.116: INFO: Created: latency-svc-g7nkm
Aug 17 04:03:42.154: INFO: Got endpoints: latency-svc-92h2x [748.366987ms]
Aug 17 04:03:42.167: INFO: Created: latency-svc-krlxq
Aug 17 04:03:42.206: INFO: Got endpoints: latency-svc-xj7m2 [747.412787ms]
Aug 17 04:03:42.217: INFO: Created: latency-svc-kt97d
Aug 17 04:03:42.255: INFO: Got endpoints: latency-svc-8nnpn [749.574793ms]
Aug 17 04:03:42.265: INFO: Created: latency-svc-lk72j
Aug 17 04:03:42.306: INFO: Got endpoints: latency-svc-xct2t [749.97178ms]
Aug 17 04:03:42.317: INFO: Created: latency-svc-md2mb
Aug 17 04:03:42.355: INFO: Got endpoints: latency-svc-hzmgn [749.99821ms]
Aug 17 04:03:42.367: INFO: Created: latency-svc-8pxsd
Aug 17 04:03:42.406: INFO: Got endpoints: latency-svc-j7h5v [750.446023ms]
Aug 17 04:03:42.416: INFO: Created: latency-svc-zsgvj
Aug 17 04:03:42.455: INFO: Got endpoints: latency-svc-bhxwv [748.542191ms]
Aug 17 04:03:42.468: INFO: Created: latency-svc-4g48p
Aug 17 04:03:42.505: INFO: Got endpoints: latency-svc-qnsjn [749.956272ms]
Aug 17 04:03:42.515: INFO: Created: latency-svc-6dk8k
Aug 17 04:03:42.556: INFO: Got endpoints: latency-svc-qlmc7 [750.307933ms]
Aug 17 04:03:42.566: INFO: Created: latency-svc-7r4gl
Aug 17 04:03:42.606: INFO: Got endpoints: latency-svc-5rw94 [750.248839ms]
Aug 17 04:03:42.617: INFO: Created: latency-svc-nt5sl
Aug 17 04:03:42.656: INFO: Got endpoints: latency-svc-7s9mh [750.009014ms]
Aug 17 04:03:42.666: INFO: Created: latency-svc-v6pj2
Aug 17 04:03:42.705: INFO: Got endpoints: latency-svc-8khmx [749.703211ms]
Aug 17 04:03:42.716: INFO: Created: latency-svc-6qggf
Aug 17 04:03:42.756: INFO: Got endpoints: latency-svc-b95xt [750.227228ms]
Aug 17 04:03:42.767: INFO: Created: latency-svc-khbt5
Aug 17 04:03:42.806: INFO: Got endpoints: latency-svc-66j5r [750.441146ms]
Aug 17 04:03:42.817: INFO: Created: latency-svc-9szgt
Aug 17 04:03:42.856: INFO: Got endpoints: latency-svc-g7nkm [750.372425ms]
Aug 17 04:03:42.867: INFO: Created: latency-svc-jslt9
Aug 17 04:03:42.906: INFO: Got endpoints: latency-svc-krlxq [751.225299ms]
Aug 17 04:03:42.918: INFO: Created: latency-svc-2fd4h
Aug 17 04:03:42.956: INFO: Got endpoints: latency-svc-kt97d [750.374507ms]
Aug 17 04:03:42.967: INFO: Created: latency-svc-zfbhd
Aug 17 04:03:43.007: INFO: Got endpoints: latency-svc-lk72j [751.360372ms]
Aug 17 04:03:43.018: INFO: Created: latency-svc-vlhqb
Aug 17 04:03:43.055: INFO: Got endpoints: latency-svc-md2mb [749.498558ms]
Aug 17 04:03:43.066: INFO: Created: latency-svc-2mw5h
Aug 17 04:03:43.106: INFO: Got endpoints: latency-svc-8pxsd [750.480108ms]
Aug 17 04:03:43.117: INFO: Created: latency-svc-975zp
Aug 17 04:03:43.155: INFO: Got endpoints: latency-svc-zsgvj [749.417634ms]
Aug 17 04:03:43.166: INFO: Created: latency-svc-zmxwr
Aug 17 04:03:43.205: INFO: Got endpoints: latency-svc-4g48p [750.61387ms]
Aug 17 04:03:43.218: INFO: Created: latency-svc-wtkm2
Aug 17 04:03:43.257: INFO: Got endpoints: latency-svc-6dk8k [751.677507ms]
Aug 17 04:03:43.269: INFO: Created: latency-svc-m5mkd
Aug 17 04:03:43.306: INFO: Got endpoints: latency-svc-7r4gl [749.926435ms]
Aug 17 04:03:43.321: INFO: Created: latency-svc-cjqfj
Aug 17 04:03:43.356: INFO: Got endpoints: latency-svc-nt5sl [750.321264ms]
Aug 17 04:03:43.367: INFO: Created: latency-svc-bt5v9
Aug 17 04:03:43.406: INFO: Got endpoints: latency-svc-v6pj2 [749.440822ms]
Aug 17 04:03:43.417: INFO: Created: latency-svc-zr8rb
Aug 17 04:03:43.455: INFO: Got endpoints: latency-svc-6qggf [749.571986ms]
Aug 17 04:03:43.466: INFO: Created: latency-svc-6llcp
Aug 17 04:03:43.505: INFO: Got endpoints: latency-svc-khbt5 [749.853337ms]
Aug 17 04:03:43.517: INFO: Created: latency-svc-5l8rg
Aug 17 04:03:43.556: INFO: Got endpoints: latency-svc-9szgt [749.398192ms]
Aug 17 04:03:43.567: INFO: Created: latency-svc-bjxx9
Aug 17 04:03:43.606: INFO: Got endpoints: latency-svc-jslt9 [749.821633ms]
Aug 17 04:03:43.617: INFO: Created: latency-svc-t5wz2
Aug 17 04:03:43.656: INFO: Got endpoints: latency-svc-2fd4h [750.669838ms]
Aug 17 04:03:43.667: INFO: Created: latency-svc-lnpdh
Aug 17 04:03:43.706: INFO: Got endpoints: latency-svc-zfbhd [749.715031ms]
Aug 17 04:03:43.720: INFO: Created: latency-svc-mc656
Aug 17 04:03:43.756: INFO: Got endpoints: latency-svc-vlhqb [748.582972ms]
Aug 17 04:03:43.766: INFO: Created: latency-svc-fq7p8
Aug 17 04:03:43.806: INFO: Got endpoints: latency-svc-2mw5h [750.6345ms]
Aug 17 04:03:43.818: INFO: Created: latency-svc-44xvg
Aug 17 04:03:43.856: INFO: Got endpoints: latency-svc-975zp [749.759486ms]
Aug 17 04:03:43.867: INFO: Created: latency-svc-fsq2z
Aug 17 04:03:43.905: INFO: Got endpoints: latency-svc-zmxwr [749.590231ms]
Aug 17 04:03:43.916: INFO: Created: latency-svc-rj9bd
Aug 17 04:03:43.955: INFO: Got endpoints: latency-svc-wtkm2 [748.93224ms]
Aug 17 04:03:43.965: INFO: Created: latency-svc-sxkcx
Aug 17 04:03:44.004: INFO: Got endpoints: latency-svc-m5mkd [747.014771ms]
Aug 17 04:03:44.014: INFO: Created: latency-svc-pxqxn
Aug 17 04:03:44.055: INFO: Got endpoints: latency-svc-cjqfj [748.904093ms]
Aug 17 04:03:44.064: INFO: Created: latency-svc-9nqfv
Aug 17 04:03:44.106: INFO: Got endpoints: latency-svc-bt5v9 [749.600223ms]
Aug 17 04:03:44.117: INFO: Created: latency-svc-dp28k
Aug 17 04:03:44.156: INFO: Got endpoints: latency-svc-zr8rb [750.581152ms]
Aug 17 04:03:44.167: INFO: Created: latency-svc-wm8xt
Aug 17 04:03:44.206: INFO: Got endpoints: latency-svc-6llcp [750.55801ms]
Aug 17 04:03:44.216: INFO: Created: latency-svc-t9flv
Aug 17 04:03:44.256: INFO: Got endpoints: latency-svc-5l8rg [749.963981ms]
Aug 17 04:03:44.267: INFO: Created: latency-svc-nzmw8
Aug 17 04:03:44.307: INFO: Got endpoints: latency-svc-bjxx9 [750.540828ms]
Aug 17 04:03:44.316: INFO: Created: latency-svc-krq56
Aug 17 04:03:44.356: INFO: Got endpoints: latency-svc-t5wz2 [750.193688ms]
Aug 17 04:03:44.368: INFO: Created: latency-svc-l68h9
Aug 17 04:03:44.406: INFO: Got endpoints: latency-svc-lnpdh [749.482951ms]
Aug 17 04:03:44.416: INFO: Created: latency-svc-96qks
Aug 17 04:03:44.456: INFO: Got endpoints: latency-svc-mc656 [749.804558ms]
Aug 17 04:03:44.466: INFO: Created: latency-svc-xst6j
Aug 17 04:03:44.507: INFO: Got endpoints: latency-svc-fq7p8 [751.223687ms]
Aug 17 04:03:44.518: INFO: Created: latency-svc-fq5bx
Aug 17 04:03:44.556: INFO: Got endpoints: latency-svc-44xvg [750.112917ms]
Aug 17 04:03:44.568: INFO: Created: latency-svc-pxwlc
Aug 17 04:03:44.606: INFO: Got endpoints: latency-svc-fsq2z [749.956839ms]
Aug 17 04:03:44.616: INFO: Created: latency-svc-mtgk4
Aug 17 04:03:44.655: INFO: Got endpoints: latency-svc-rj9bd [749.3333ms]
Aug 17 04:03:44.665: INFO: Created: latency-svc-dsbx4
Aug 17 04:03:44.705: INFO: Got endpoints: latency-svc-sxkcx [750.184275ms]
Aug 17 04:03:44.715: INFO: Created: latency-svc-8qsrw
Aug 17 04:03:44.756: INFO: Got endpoints: latency-svc-pxqxn [751.781442ms]
Aug 17 04:03:44.766: INFO: Created: latency-svc-xk7m5
Aug 17 04:03:44.807: INFO: Got endpoints: latency-svc-9nqfv [751.52436ms]
Aug 17 04:03:44.817: INFO: Created: latency-svc-xgcbl
Aug 17 04:03:44.858: INFO: Got endpoints: latency-svc-dp28k [752.437066ms]
Aug 17 04:03:44.870: INFO: Created: latency-svc-l4b7g
Aug 17 04:03:44.907: INFO: Got endpoints: latency-svc-wm8xt [750.270559ms]
Aug 17 04:03:44.918: INFO: Created: latency-svc-692c9
Aug 17 04:03:44.956: INFO: Got endpoints: latency-svc-t9flv [750.548385ms]
Aug 17 04:03:44.969: INFO: Created: latency-svc-9pxwd
Aug 17 04:03:45.005: INFO: Got endpoints: latency-svc-nzmw8 [749.731247ms]
Aug 17 04:03:45.018: INFO: Created: latency-svc-24v6g
Aug 17 04:03:45.056: INFO: Got endpoints: latency-svc-krq56 [749.007711ms]
Aug 17 04:03:45.066: INFO: Created: latency-svc-b9xgg
Aug 17 04:03:45.106: INFO: Got endpoints: latency-svc-l68h9 [749.515385ms]
Aug 17 04:03:45.116: INFO: Created: latency-svc-75hjt
Aug 17 04:03:45.155: INFO: Got endpoints: latency-svc-96qks [749.397818ms]
Aug 17 04:03:45.166: INFO: Created: latency-svc-2ctmk
Aug 17 04:03:45.205: INFO: Got endpoints: latency-svc-xst6j [749.779306ms]
Aug 17 04:03:45.216: INFO: Created: latency-svc-288jl
Aug 17 04:03:45.255: INFO: Got endpoints: latency-svc-fq5bx [747.983253ms]
Aug 17 04:03:45.265: INFO: Created: latency-svc-hsc24
Aug 17 04:03:45.306: INFO: Got endpoints: latency-svc-pxwlc [749.331369ms]
Aug 17 04:03:45.317: INFO: Created: latency-svc-gkgj2
Aug 17 04:03:45.356: INFO: Got endpoints: latency-svc-mtgk4 [749.976556ms]
Aug 17 04:03:45.367: INFO: Created: latency-svc-l72dw
Aug 17 04:03:45.405: INFO: Got endpoints: latency-svc-dsbx4 [750.263319ms]
Aug 17 04:03:45.417: INFO: Created: latency-svc-vw86c
Aug 17 04:03:45.455: INFO: Got endpoints: latency-svc-8qsrw [750.395665ms]
Aug 17 04:03:45.467: INFO: Created: latency-svc-bgmdn
Aug 17 04:03:45.506: INFO: Got endpoints: latency-svc-xk7m5 [749.884797ms]
Aug 17 04:03:45.518: INFO: Created: latency-svc-bhgd6
Aug 17 04:03:45.556: INFO: Got endpoints: latency-svc-xgcbl [749.016194ms]
Aug 17 04:03:45.566: INFO: Created: latency-svc-xm7w7
Aug 17 04:03:45.606: INFO: Got endpoints: latency-svc-l4b7g [747.214859ms]
Aug 17 04:03:45.617: INFO: Created: latency-svc-45dpb
Aug 17 04:03:45.656: INFO: Got endpoints: latency-svc-692c9 [748.954994ms]
Aug 17 04:03:45.668: INFO: Created: latency-svc-6vf7v
Aug 17 04:03:45.706: INFO: Got endpoints: latency-svc-9pxwd [749.555913ms]
Aug 17 04:03:45.717: INFO: Created: latency-svc-qtwfp
Aug 17 04:03:45.756: INFO: Got endpoints: latency-svc-24v6g [750.743316ms]
Aug 17 04:03:45.767: INFO: Created: latency-svc-mdzb7
Aug 17 04:03:45.806: INFO: Got endpoints: latency-svc-b9xgg [750.228028ms]
Aug 17 04:03:45.816: INFO: Created: latency-svc-gbqrp
Aug 17 04:03:45.856: INFO: Got endpoints: latency-svc-75hjt [750.071583ms]
Aug 17 04:03:45.869: INFO: Created: latency-svc-j9h44
Aug 17 04:03:45.906: INFO: Got endpoints: latency-svc-2ctmk [750.704508ms]
Aug 17 04:03:45.917: INFO: Created: latency-svc-7dqlp
Aug 17 04:03:45.956: INFO: Got endpoints: latency-svc-288jl [750.011461ms]
Aug 17 04:03:45.966: INFO: Created: latency-svc-jqbsd
Aug 17 04:03:46.005: INFO: Got endpoints: latency-svc-hsc24 [750.289545ms]
Aug 17 04:03:46.017: INFO: Created: latency-svc-lmrvh
Aug 17 04:03:46.056: INFO: Got endpoints: latency-svc-gkgj2 [750.408883ms]
Aug 17 04:03:46.067: INFO: Created: latency-svc-bd4zx
Aug 17 04:03:46.106: INFO: Got endpoints: latency-svc-l72dw [749.910626ms]
Aug 17 04:03:46.117: INFO: Created: latency-svc-fr82m
Aug 17 04:03:46.156: INFO: Got endpoints: latency-svc-vw86c [751.247738ms]
Aug 17 04:03:46.167: INFO: Created: latency-svc-26sb9
Aug 17 04:03:46.206: INFO: Got endpoints: latency-svc-bgmdn [750.67088ms]
Aug 17 04:03:46.217: INFO: Created: latency-svc-llk2c
Aug 17 04:03:46.255: INFO: Got endpoints: latency-svc-bhgd6 [749.231923ms]
Aug 17 04:03:46.266: INFO: Created: latency-svc-dxgwm
Aug 17 04:03:46.307: INFO: Got endpoints: latency-svc-xm7w7 [751.240042ms]
Aug 17 04:03:46.318: INFO: Created: latency-svc-hdlfv
Aug 17 04:03:46.355: INFO: Got endpoints: latency-svc-45dpb [749.373897ms]
Aug 17 04:03:46.368: INFO: Created: latency-svc-p42xj
Aug 17 04:03:46.406: INFO: Got endpoints: latency-svc-6vf7v [750.096352ms]
Aug 17 04:03:46.417: INFO: Created: latency-svc-rl9qp
Aug 17 04:03:46.456: INFO: Got endpoints: latency-svc-qtwfp [750.107392ms]
Aug 17 04:03:46.467: INFO: Created: latency-svc-tt8hp
Aug 17 04:03:46.506: INFO: Got endpoints: latency-svc-mdzb7 [750.183715ms]
Aug 17 04:03:46.518: INFO: Created: latency-svc-5gk7w
Aug 17 04:03:46.556: INFO: Got endpoints: latency-svc-gbqrp [749.563916ms]
Aug 17 04:03:46.566: INFO: Created: latency-svc-wnbqj
Aug 17 04:03:46.606: INFO: Got endpoints: latency-svc-j9h44 [749.789717ms]
Aug 17 04:03:46.617: INFO: Created: latency-svc-fhgks
Aug 17 04:03:46.656: INFO: Got endpoints: latency-svc-7dqlp [749.491248ms]
Aug 17 04:03:46.667: INFO: Created: latency-svc-pj9bb
Aug 17 04:03:46.706: INFO: Got endpoints: latency-svc-jqbsd [750.246529ms]
Aug 17 04:03:46.721: INFO: Created: latency-svc-kgrpg
Aug 17 04:03:46.756: INFO: Got endpoints: latency-svc-lmrvh [750.459231ms]
Aug 17 04:03:46.771: INFO: Created: latency-svc-rwtlg
Aug 17 04:03:46.805: INFO: Got endpoints: latency-svc-bd4zx [749.294289ms]
Aug 17 04:03:46.816: INFO: Created: latency-svc-45crb
Aug 17 04:03:46.857: INFO: Got endpoints: latency-svc-fr82m [750.708162ms]
Aug 17 04:03:46.867: INFO: Created: latency-svc-ps26x
Aug 17 04:03:46.907: INFO: Got endpoints: latency-svc-26sb9 [750.371967ms]
Aug 17 04:03:46.919: INFO: Created: latency-svc-pd6gl
Aug 17 04:03:46.956: INFO: Got endpoints: latency-svc-llk2c [749.413975ms]
Aug 17 04:03:46.967: INFO: Created: latency-svc-9bgv8
Aug 17 04:03:47.006: INFO: Got endpoints: latency-svc-dxgwm [750.287167ms]
Aug 17 04:03:47.016: INFO: Created: latency-svc-l5z7p
Aug 17 04:03:47.055: INFO: Got endpoints: latency-svc-hdlfv [748.069542ms]
Aug 17 04:03:47.067: INFO: Created: latency-svc-klvcp
Aug 17 04:03:47.106: INFO: Got endpoints: latency-svc-p42xj [750.358699ms]
Aug 17 04:03:47.117: INFO: Created: latency-svc-8pjt2
Aug 17 04:03:47.155: INFO: Got endpoints: latency-svc-rl9qp [749.49636ms]
Aug 17 04:03:47.167: INFO: Created: latency-svc-b254x
Aug 17 04:03:47.206: INFO: Got endpoints: latency-svc-tt8hp [749.565157ms]
Aug 17 04:03:47.216: INFO: Created: latency-svc-fsjxf
Aug 17 04:03:47.256: INFO: Got endpoints: latency-svc-5gk7w [749.900509ms]
Aug 17 04:03:47.267: INFO: Created: latency-svc-k2c29
Aug 17 04:03:47.306: INFO: Got endpoints: latency-svc-wnbqj [750.532269ms]
Aug 17 04:03:47.319: INFO: Created: latency-svc-w6blf
Aug 17 04:03:47.356: INFO: Got endpoints: latency-svc-fhgks [749.687811ms]
Aug 17 04:03:47.366: INFO: Created: latency-svc-gnlrf
Aug 17 04:03:47.406: INFO: Got endpoints: latency-svc-pj9bb [749.913308ms]
Aug 17 04:03:47.417: INFO: Created: latency-svc-cx8kz
Aug 17 04:03:47.457: INFO: Got endpoints: latency-svc-kgrpg [750.824131ms]
Aug 17 04:03:47.467: INFO: Created: latency-svc-5l4z9
Aug 17 04:03:47.506: INFO: Got endpoints: latency-svc-rwtlg [749.821067ms]
Aug 17 04:03:47.517: INFO: Created: latency-svc-tqpt8
Aug 17 04:03:47.555: INFO: Got endpoints: latency-svc-45crb [749.833286ms]
Aug 17 04:03:47.566: INFO: Created: latency-svc-dmqld
Aug 17 04:03:47.606: INFO: Got endpoints: latency-svc-ps26x [749.189686ms]
Aug 17 04:03:47.617: INFO: Created: latency-svc-q85jh
Aug 17 04:03:47.656: INFO: Got endpoints: latency-svc-pd6gl [749.665068ms]
Aug 17 04:03:47.668: INFO: Created: latency-svc-czcpq
Aug 17 04:03:47.706: INFO: Got endpoints: latency-svc-9bgv8 [750.612844ms]
Aug 17 04:03:47.717: INFO: Created: latency-svc-7dmq2
Aug 17 04:03:47.756: INFO: Got endpoints: latency-svc-l5z7p [749.882172ms]
Aug 17 04:03:47.766: INFO: Created: latency-svc-wtlf7
Aug 17 04:03:47.806: INFO: Got endpoints: latency-svc-klvcp [750.844336ms]
Aug 17 04:03:47.818: INFO: Created: latency-svc-l9cht
Aug 17 04:03:47.856: INFO: Got endpoints: latency-svc-8pjt2 [749.965753ms]
Aug 17 04:03:47.867: INFO: Created: latency-svc-s97nw
Aug 17 04:03:47.906: INFO: Got endpoints: latency-svc-b254x [750.798202ms]
Aug 17 04:03:47.918: INFO: Created: latency-svc-4nwcr
Aug 17 04:03:47.955: INFO: Got endpoints: latency-svc-fsjxf [749.365883ms]
Aug 17 04:03:47.966: INFO: Created: latency-svc-cwxp6
Aug 17 04:03:48.007: INFO: Got endpoints: latency-svc-k2c29 [750.204099ms]
Aug 17 04:03:48.018: INFO: Created: latency-svc-h8jbh
Aug 17 04:03:48.056: INFO: Got endpoints: latency-svc-w6blf [749.8019ms]
Aug 17 04:03:48.067: INFO: Created: latency-svc-lk5fp
Aug 17 04:03:48.106: INFO: Got endpoints: latency-svc-gnlrf [750.700789ms]
Aug 17 04:03:48.119: INFO: Created: latency-svc-2b2hw
Aug 17 04:03:48.156: INFO: Got endpoints: latency-svc-cx8kz [750.391006ms]
Aug 17 04:03:48.167: INFO: Created: latency-svc-pmklq
Aug 17 04:03:48.206: INFO: Got endpoints: latency-svc-5l4z9 [749.380025ms]
Aug 17 04:03:48.220: INFO: Created: latency-svc-l4vz5
Aug 17 04:03:48.255: INFO: Got endpoints: latency-svc-tqpt8 [749.513321ms]
Aug 17 04:03:48.266: INFO: Created: latency-svc-r2kr2
Aug 17 04:03:48.305: INFO: Got endpoints: latency-svc-dmqld [749.797603ms]
Aug 17 04:03:48.316: INFO: Created: latency-svc-vddrl
Aug 17 04:03:48.356: INFO: Got endpoints: latency-svc-q85jh [750.104488ms]
Aug 17 04:03:48.368: INFO: Created: latency-svc-d5n2v
Aug 17 04:03:48.405: INFO: Got endpoints: latency-svc-czcpq [748.816376ms]
Aug 17 04:03:48.416: INFO: Created: latency-svc-v8whx
Aug 17 04:03:48.456: INFO: Got endpoints: latency-svc-7dmq2 [749.07476ms]
Aug 17 04:03:48.467: INFO: Created: latency-svc-5vqp7
Aug 17 04:03:48.505: INFO: Got endpoints: latency-svc-wtlf7 [749.775885ms]
Aug 17 04:03:48.517: INFO: Created: latency-svc-9bz8d
Aug 17 04:03:48.556: INFO: Got endpoints: latency-svc-l9cht [749.552365ms]
Aug 17 04:03:48.566: INFO: Created: latency-svc-vqchm
Aug 17 04:03:48.606: INFO: Got endpoints: latency-svc-s97nw [749.957018ms]
Aug 17 04:03:48.617: INFO: Created: latency-svc-48j4k
Aug 17 04:03:48.655: INFO: Got endpoints: latency-svc-4nwcr [748.498518ms]
Aug 17 04:03:48.666: INFO: Created: latency-svc-wzj5g
Aug 17 04:03:48.706: INFO: Got endpoints: latency-svc-cwxp6 [750.471498ms]
Aug 17 04:03:48.717: INFO: Created: latency-svc-h226g
Aug 17 04:03:48.756: INFO: Got endpoints: latency-svc-h8jbh [749.494236ms]
Aug 17 04:03:48.766: INFO: Created: latency-svc-vgsv2
Aug 17 04:03:48.806: INFO: Got endpoints: latency-svc-lk5fp [750.184734ms]
Aug 17 04:03:48.817: INFO: Created: latency-svc-sxblp
Aug 17 04:03:48.857: INFO: Got endpoints: latency-svc-2b2hw [750.295621ms]
Aug 17 04:03:48.868: INFO: Created: latency-svc-nc489
Aug 17 04:03:48.905: INFO: Got endpoints: latency-svc-pmklq [749.313901ms]
Aug 17 04:03:48.955: INFO: Got endpoints: latency-svc-l4vz5 [748.685184ms]
Aug 17 04:03:49.005: INFO: Got endpoints: latency-svc-r2kr2 [749.785337ms]
Aug 17 04:03:49.056: INFO: Got endpoints: latency-svc-vddrl [750.418849ms]
Aug 17 04:03:49.106: INFO: Got endpoints: latency-svc-d5n2v [749.455064ms]
Aug 17 04:03:49.155: INFO: Got endpoints: latency-svc-v8whx [750.031878ms]
Aug 17 04:03:49.205: INFO: Got endpoints: latency-svc-5vqp7 [749.227523ms]
Aug 17 04:03:49.256: INFO: Got endpoints: latency-svc-9bz8d [750.303292ms]
Aug 17 04:03:49.306: INFO: Got endpoints: latency-svc-vqchm [750.381992ms]
Aug 17 04:03:49.356: INFO: Got endpoints: latency-svc-48j4k [750.108314ms]
Aug 17 04:03:49.405: INFO: Got endpoints: latency-svc-wzj5g [750.461332ms]
Aug 17 04:03:49.455: INFO: Got endpoints: latency-svc-h226g [749.46547ms]
Aug 17 04:03:49.507: INFO: Got endpoints: latency-svc-vgsv2 [750.779207ms]
Aug 17 04:03:49.556: INFO: Got endpoints: latency-svc-sxblp [749.492353ms]
Aug 17 04:03:49.606: INFO: Got endpoints: latency-svc-nc489 [749.079499ms]
Aug 17 04:03:49.606: INFO: Latencies: [13.489358ms 15.008964ms 17.121928ms 20.639878ms 26.246661ms 29.513453ms 33.357373ms 38.364341ms 43.892475ms 50.017614ms 55.508615ms 61.584026ms 68.458877ms 69.412086ms 70.629518ms 70.851659ms 71.035538ms 71.2148ms 71.319522ms 71.364601ms 71.58399ms 73.126421ms 73.398263ms 73.580478ms 73.84958ms 74.362809ms 74.725301ms 75.605156ms 75.632782ms 75.969729ms 79.539158ms 81.038435ms 104.025941ms 150.624962ms 196.863028ms 288.437292ms 300.301795ms 325.789632ms 375.080195ms 420.643279ms 467.262383ms 511.019055ms 556.593071ms 603.015693ms 646.745986ms 694.925938ms 740.13488ms 747.014771ms 747.214859ms 747.412787ms 747.983253ms 748.069542ms 748.366987ms 748.498518ms 748.542191ms 748.582972ms 748.685184ms 748.816376ms 748.904093ms 748.93224ms 748.954994ms 748.962921ms 749.007711ms 749.016194ms 749.07476ms 749.079499ms 749.189686ms 749.227523ms 749.231923ms 749.294289ms 749.313901ms 749.331369ms 749.3333ms 749.365883ms 749.373897ms 749.380025ms 749.397818ms 749.398192ms 749.413975ms 749.417634ms 749.440822ms 749.455064ms 749.46547ms 749.482951ms 749.491248ms 749.492353ms 749.494236ms 749.49636ms 749.498558ms 749.513321ms 749.515385ms 749.552365ms 749.555913ms 749.563916ms 749.565157ms 749.571986ms 749.574793ms 749.590231ms 749.600223ms 749.665068ms 749.687811ms 749.703211ms 749.715031ms 749.729728ms 749.731247ms 749.759486ms 749.775885ms 749.779306ms 749.785337ms 749.789717ms 749.797603ms 749.8019ms 749.804558ms 749.821067ms 749.821633ms 749.833286ms 749.853337ms 749.882172ms 749.884797ms 749.900509ms 749.910626ms 749.913308ms 749.926435ms 749.956272ms 749.956839ms 749.957018ms 749.963981ms 749.965753ms 749.97178ms 749.976556ms 749.99821ms 750.009014ms 750.011461ms 750.031878ms 750.071583ms 750.096352ms 750.104488ms 750.107392ms 750.108314ms 750.112917ms 750.183715ms 750.184275ms 750.184734ms 750.193688ms 750.204099ms 750.227228ms 750.228028ms 750.246529ms 750.248839ms 750.263319ms 750.270559ms 750.287167ms 750.289545ms 750.295621ms 750.303292ms 750.307933ms 750.321264ms 750.358699ms 750.371967ms 750.372425ms 750.374507ms 750.381992ms 750.391006ms 750.395665ms 750.408883ms 750.418849ms 750.441146ms 750.446023ms 750.459231ms 750.461332ms 750.471498ms 750.480108ms 750.532269ms 750.540828ms 750.548385ms 750.55801ms 750.581152ms 750.612844ms 750.61387ms 750.6345ms 750.669838ms 750.67088ms 750.700789ms 750.704508ms 750.708162ms 750.743316ms 750.779207ms 750.786704ms 750.798202ms 750.824131ms 750.844336ms 751.223687ms 751.225299ms 751.240042ms 751.247738ms 751.360372ms 751.52436ms 751.677507ms 751.781442ms 752.437066ms]
Aug 17 04:03:49.606: INFO: 50 %ile: 749.687811ms
Aug 17 04:03:49.606: INFO: 90 %ile: 750.669838ms
Aug 17 04:03:49.606: INFO: 99 %ile: 751.781442ms
Aug 17 04:03:49.606: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:49.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-4406" for this suite. 08/17/23 04:03:49.614
------------------------------
• [SLOW TEST] [10.766 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:38.856
    Aug 17 04:03:38.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svc-latency 08/17/23 04:03:38.857
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:38.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:38.874
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 17 04:03:38.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4406 08/17/23 04:03:38.879
    I0817 04:03:38.885388      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4406, replica count: 1
    I0817 04:03:39.937272      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 04:03:40.938076      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 04:03:41.050: INFO: Created: latency-svc-mfj6c
    Aug 17 04:03:41.054: INFO: Got endpoints: latency-svc-mfj6c [16.100936ms]
    Aug 17 04:03:41.065: INFO: Created: latency-svc-brs6z
    Aug 17 04:03:41.068: INFO: Got endpoints: latency-svc-brs6z [13.489358ms]
    Aug 17 04:03:41.069: INFO: Created: latency-svc-sv7zp
    Aug 17 04:03:41.072: INFO: Got endpoints: latency-svc-sv7zp [17.121928ms]
    Aug 17 04:03:41.073: INFO: Created: latency-svc-qzn26
    Aug 17 04:03:41.076: INFO: Got endpoints: latency-svc-qzn26 [20.639878ms]
    Aug 17 04:03:41.077: INFO: Created: latency-svc-844w8
    Aug 17 04:03:41.081: INFO: Got endpoints: latency-svc-844w8 [26.246661ms]
    Aug 17 04:03:41.082: INFO: Created: latency-svc-qqrkv
    Aug 17 04:03:41.085: INFO: Got endpoints: latency-svc-qqrkv [29.513453ms]
    Aug 17 04:03:41.086: INFO: Created: latency-svc-9fkdt
    Aug 17 04:03:41.089: INFO: Got endpoints: latency-svc-9fkdt [33.357373ms]
    Aug 17 04:03:41.091: INFO: Created: latency-svc-dj8jx
    Aug 17 04:03:41.094: INFO: Got endpoints: latency-svc-dj8jx [38.364341ms]
    Aug 17 04:03:41.096: INFO: Created: latency-svc-c9tq2
    Aug 17 04:03:41.099: INFO: Got endpoints: latency-svc-c9tq2 [43.892475ms]
    Aug 17 04:03:41.102: INFO: Created: latency-svc-ktjxr
    Aug 17 04:03:41.106: INFO: Got endpoints: latency-svc-ktjxr [50.017614ms]
    Aug 17 04:03:41.107: INFO: Created: latency-svc-v92vg
    Aug 17 04:03:41.111: INFO: Got endpoints: latency-svc-v92vg [55.508615ms]
    Aug 17 04:03:41.116: INFO: Created: latency-svc-kn8lq
    Aug 17 04:03:41.126: INFO: Created: latency-svc-pgw9h
    Aug 17 04:03:41.126: INFO: Got endpoints: latency-svc-pgw9h [70.851659ms]
    Aug 17 04:03:41.126: INFO: Created: latency-svc-gfktg
    Aug 17 04:03:41.126: INFO: Got endpoints: latency-svc-kn8lq [70.629518ms]
    Aug 17 04:03:41.127: INFO: Got endpoints: latency-svc-gfktg [71.58399ms]
    Aug 17 04:03:41.127: INFO: Created: latency-svc-c8x7b
    Aug 17 04:03:41.131: INFO: Got endpoints: latency-svc-c8x7b [75.605156ms]
    Aug 17 04:03:41.132: INFO: Created: latency-svc-v7qz5
    Aug 17 04:03:41.135: INFO: Got endpoints: latency-svc-v7qz5 [79.539158ms]
    Aug 17 04:03:41.137: INFO: Created: latency-svc-gqbf7
    Aug 17 04:03:41.140: INFO: Got endpoints: latency-svc-gqbf7 [71.319522ms]
    Aug 17 04:03:41.141: INFO: Created: latency-svc-h95dx
    Aug 17 04:03:41.146: INFO: Got endpoints: latency-svc-h95dx [73.84958ms]
    Aug 17 04:03:41.147: INFO: Created: latency-svc-glskw
    Aug 17 04:03:41.150: INFO: Got endpoints: latency-svc-glskw [74.362809ms]
    Aug 17 04:03:41.152: INFO: Created: latency-svc-kn9xx
    Aug 17 04:03:41.155: INFO: Got endpoints: latency-svc-kn9xx [73.580478ms]
    Aug 17 04:03:41.157: INFO: Created: latency-svc-wpd97
    Aug 17 04:03:41.159: INFO: Got endpoints: latency-svc-wpd97 [74.725301ms]
    Aug 17 04:03:41.161: INFO: Created: latency-svc-v9nm5
    Aug 17 04:03:41.165: INFO: Got endpoints: latency-svc-v9nm5 [75.969729ms]
    Aug 17 04:03:41.167: INFO: Created: latency-svc-gqnwn
    Aug 17 04:03:41.169: INFO: Got endpoints: latency-svc-gqnwn [75.632782ms]
    Aug 17 04:03:41.176: INFO: Created: latency-svc-chtj7
    Aug 17 04:03:41.177: INFO: Created: latency-svc-rlx7v
    Aug 17 04:03:41.180: INFO: Got endpoints: latency-svc-chtj7 [15.008964ms]
    Aug 17 04:03:41.180: INFO: Got endpoints: latency-svc-rlx7v [81.038435ms]
    Aug 17 04:03:41.181: INFO: Created: latency-svc-x7vlx
    Aug 17 04:03:41.184: INFO: Got endpoints: latency-svc-x7vlx [73.126421ms]
    Aug 17 04:03:41.185: INFO: Created: latency-svc-gpxzz
    Aug 17 04:03:41.188: INFO: Got endpoints: latency-svc-gpxzz [61.584026ms]
    Aug 17 04:03:41.191: INFO: Created: latency-svc-flnp2
    Aug 17 04:03:41.195: INFO: Created: latency-svc-8gwfw
    Aug 17 04:03:41.195: INFO: Got endpoints: latency-svc-flnp2 [68.458877ms]
    Aug 17 04:03:41.198: INFO: Got endpoints: latency-svc-8gwfw [71.035538ms]
    Aug 17 04:03:41.199: INFO: Created: latency-svc-9q482
    Aug 17 04:03:41.203: INFO: Got endpoints: latency-svc-9q482 [71.2148ms]
    Aug 17 04:03:41.205: INFO: Created: latency-svc-r6nxc
    Aug 17 04:03:41.208: INFO: Created: latency-svc-svm26
    Aug 17 04:03:41.209: INFO: Got endpoints: latency-svc-r6nxc [73.398263ms]
    Aug 17 04:03:41.211: INFO: Got endpoints: latency-svc-svm26 [71.364601ms]
    Aug 17 04:03:41.212: INFO: Created: latency-svc-m7p5t
    Aug 17 04:03:41.215: INFO: Got endpoints: latency-svc-m7p5t [69.412086ms]
    Aug 17 04:03:41.218: INFO: Created: latency-svc-bvg5f
    Aug 17 04:03:41.222: INFO: Created: latency-svc-52wpg
    Aug 17 04:03:41.227: INFO: Created: latency-svc-4zff5
    Aug 17 04:03:41.231: INFO: Created: latency-svc-6lzlp
    Aug 17 04:03:41.235: INFO: Created: latency-svc-6vv45
    Aug 17 04:03:41.240: INFO: Created: latency-svc-zdt9r
    Aug 17 04:03:41.244: INFO: Created: latency-svc-xg5w8
    Aug 17 04:03:41.248: INFO: Created: latency-svc-h5zrz
    Aug 17 04:03:41.253: INFO: Created: latency-svc-g7ph4
    Aug 17 04:03:41.254: INFO: Got endpoints: latency-svc-bvg5f [104.025941ms]
    Aug 17 04:03:41.257: INFO: Created: latency-svc-cftpk
    Aug 17 04:03:41.262: INFO: Created: latency-svc-qp2kt
    Aug 17 04:03:41.266: INFO: Created: latency-svc-fkmvm
    Aug 17 04:03:41.270: INFO: Created: latency-svc-vl8rm
    Aug 17 04:03:41.278: INFO: Created: latency-svc-5rql5
    Aug 17 04:03:41.283: INFO: Created: latency-svc-mkgnp
    Aug 17 04:03:41.286: INFO: Created: latency-svc-84wwd
    Aug 17 04:03:41.306: INFO: Got endpoints: latency-svc-52wpg [150.624962ms]
    Aug 17 04:03:41.317: INFO: Created: latency-svc-qhhx5
    Aug 17 04:03:41.356: INFO: Got endpoints: latency-svc-4zff5 [196.863028ms]
    Aug 17 04:03:41.367: INFO: Created: latency-svc-4fh86
    Aug 17 04:03:41.406: INFO: Got endpoints: latency-svc-6lzlp [300.301795ms]
    Aug 17 04:03:41.416: INFO: Created: latency-svc-92h2x
    Aug 17 04:03:41.458: INFO: Got endpoints: latency-svc-6vv45 [288.437292ms]
    Aug 17 04:03:41.469: INFO: Created: latency-svc-xj7m2
    Aug 17 04:03:41.506: INFO: Got endpoints: latency-svc-zdt9r [325.789632ms]
    Aug 17 04:03:41.517: INFO: Created: latency-svc-8nnpn
    Aug 17 04:03:41.556: INFO: Got endpoints: latency-svc-xg5w8 [375.080195ms]
    Aug 17 04:03:41.566: INFO: Created: latency-svc-xct2t
    Aug 17 04:03:41.605: INFO: Got endpoints: latency-svc-h5zrz [420.643279ms]
    Aug 17 04:03:41.616: INFO: Created: latency-svc-hzmgn
    Aug 17 04:03:41.655: INFO: Got endpoints: latency-svc-g7ph4 [467.262383ms]
    Aug 17 04:03:41.665: INFO: Created: latency-svc-j7h5v
    Aug 17 04:03:41.706: INFO: Got endpoints: latency-svc-cftpk [511.019055ms]
    Aug 17 04:03:41.718: INFO: Created: latency-svc-bhxwv
    Aug 17 04:03:41.755: INFO: Got endpoints: latency-svc-qp2kt [556.593071ms]
    Aug 17 04:03:41.766: INFO: Created: latency-svc-qnsjn
    Aug 17 04:03:41.806: INFO: Got endpoints: latency-svc-fkmvm [603.015693ms]
    Aug 17 04:03:41.817: INFO: Created: latency-svc-qlmc7
    Aug 17 04:03:41.855: INFO: Got endpoints: latency-svc-vl8rm [646.745986ms]
    Aug 17 04:03:41.867: INFO: Created: latency-svc-5rw94
    Aug 17 04:03:41.906: INFO: Got endpoints: latency-svc-5rql5 [694.925938ms]
    Aug 17 04:03:41.918: INFO: Created: latency-svc-7s9mh
    Aug 17 04:03:41.956: INFO: Got endpoints: latency-svc-mkgnp [740.13488ms]
    Aug 17 04:03:41.966: INFO: Created: latency-svc-8khmx
    Aug 17 04:03:42.005: INFO: Got endpoints: latency-svc-84wwd [750.786704ms]
    Aug 17 04:03:42.015: INFO: Created: latency-svc-b95xt
    Aug 17 04:03:42.056: INFO: Got endpoints: latency-svc-qhhx5 [749.729728ms]
    Aug 17 04:03:42.066: INFO: Created: latency-svc-66j5r
    Aug 17 04:03:42.105: INFO: Got endpoints: latency-svc-4fh86 [748.962921ms]
    Aug 17 04:03:42.116: INFO: Created: latency-svc-g7nkm
    Aug 17 04:03:42.154: INFO: Got endpoints: latency-svc-92h2x [748.366987ms]
    Aug 17 04:03:42.167: INFO: Created: latency-svc-krlxq
    Aug 17 04:03:42.206: INFO: Got endpoints: latency-svc-xj7m2 [747.412787ms]
    Aug 17 04:03:42.217: INFO: Created: latency-svc-kt97d
    Aug 17 04:03:42.255: INFO: Got endpoints: latency-svc-8nnpn [749.574793ms]
    Aug 17 04:03:42.265: INFO: Created: latency-svc-lk72j
    Aug 17 04:03:42.306: INFO: Got endpoints: latency-svc-xct2t [749.97178ms]
    Aug 17 04:03:42.317: INFO: Created: latency-svc-md2mb
    Aug 17 04:03:42.355: INFO: Got endpoints: latency-svc-hzmgn [749.99821ms]
    Aug 17 04:03:42.367: INFO: Created: latency-svc-8pxsd
    Aug 17 04:03:42.406: INFO: Got endpoints: latency-svc-j7h5v [750.446023ms]
    Aug 17 04:03:42.416: INFO: Created: latency-svc-zsgvj
    Aug 17 04:03:42.455: INFO: Got endpoints: latency-svc-bhxwv [748.542191ms]
    Aug 17 04:03:42.468: INFO: Created: latency-svc-4g48p
    Aug 17 04:03:42.505: INFO: Got endpoints: latency-svc-qnsjn [749.956272ms]
    Aug 17 04:03:42.515: INFO: Created: latency-svc-6dk8k
    Aug 17 04:03:42.556: INFO: Got endpoints: latency-svc-qlmc7 [750.307933ms]
    Aug 17 04:03:42.566: INFO: Created: latency-svc-7r4gl
    Aug 17 04:03:42.606: INFO: Got endpoints: latency-svc-5rw94 [750.248839ms]
    Aug 17 04:03:42.617: INFO: Created: latency-svc-nt5sl
    Aug 17 04:03:42.656: INFO: Got endpoints: latency-svc-7s9mh [750.009014ms]
    Aug 17 04:03:42.666: INFO: Created: latency-svc-v6pj2
    Aug 17 04:03:42.705: INFO: Got endpoints: latency-svc-8khmx [749.703211ms]
    Aug 17 04:03:42.716: INFO: Created: latency-svc-6qggf
    Aug 17 04:03:42.756: INFO: Got endpoints: latency-svc-b95xt [750.227228ms]
    Aug 17 04:03:42.767: INFO: Created: latency-svc-khbt5
    Aug 17 04:03:42.806: INFO: Got endpoints: latency-svc-66j5r [750.441146ms]
    Aug 17 04:03:42.817: INFO: Created: latency-svc-9szgt
    Aug 17 04:03:42.856: INFO: Got endpoints: latency-svc-g7nkm [750.372425ms]
    Aug 17 04:03:42.867: INFO: Created: latency-svc-jslt9
    Aug 17 04:03:42.906: INFO: Got endpoints: latency-svc-krlxq [751.225299ms]
    Aug 17 04:03:42.918: INFO: Created: latency-svc-2fd4h
    Aug 17 04:03:42.956: INFO: Got endpoints: latency-svc-kt97d [750.374507ms]
    Aug 17 04:03:42.967: INFO: Created: latency-svc-zfbhd
    Aug 17 04:03:43.007: INFO: Got endpoints: latency-svc-lk72j [751.360372ms]
    Aug 17 04:03:43.018: INFO: Created: latency-svc-vlhqb
    Aug 17 04:03:43.055: INFO: Got endpoints: latency-svc-md2mb [749.498558ms]
    Aug 17 04:03:43.066: INFO: Created: latency-svc-2mw5h
    Aug 17 04:03:43.106: INFO: Got endpoints: latency-svc-8pxsd [750.480108ms]
    Aug 17 04:03:43.117: INFO: Created: latency-svc-975zp
    Aug 17 04:03:43.155: INFO: Got endpoints: latency-svc-zsgvj [749.417634ms]
    Aug 17 04:03:43.166: INFO: Created: latency-svc-zmxwr
    Aug 17 04:03:43.205: INFO: Got endpoints: latency-svc-4g48p [750.61387ms]
    Aug 17 04:03:43.218: INFO: Created: latency-svc-wtkm2
    Aug 17 04:03:43.257: INFO: Got endpoints: latency-svc-6dk8k [751.677507ms]
    Aug 17 04:03:43.269: INFO: Created: latency-svc-m5mkd
    Aug 17 04:03:43.306: INFO: Got endpoints: latency-svc-7r4gl [749.926435ms]
    Aug 17 04:03:43.321: INFO: Created: latency-svc-cjqfj
    Aug 17 04:03:43.356: INFO: Got endpoints: latency-svc-nt5sl [750.321264ms]
    Aug 17 04:03:43.367: INFO: Created: latency-svc-bt5v9
    Aug 17 04:03:43.406: INFO: Got endpoints: latency-svc-v6pj2 [749.440822ms]
    Aug 17 04:03:43.417: INFO: Created: latency-svc-zr8rb
    Aug 17 04:03:43.455: INFO: Got endpoints: latency-svc-6qggf [749.571986ms]
    Aug 17 04:03:43.466: INFO: Created: latency-svc-6llcp
    Aug 17 04:03:43.505: INFO: Got endpoints: latency-svc-khbt5 [749.853337ms]
    Aug 17 04:03:43.517: INFO: Created: latency-svc-5l8rg
    Aug 17 04:03:43.556: INFO: Got endpoints: latency-svc-9szgt [749.398192ms]
    Aug 17 04:03:43.567: INFO: Created: latency-svc-bjxx9
    Aug 17 04:03:43.606: INFO: Got endpoints: latency-svc-jslt9 [749.821633ms]
    Aug 17 04:03:43.617: INFO: Created: latency-svc-t5wz2
    Aug 17 04:03:43.656: INFO: Got endpoints: latency-svc-2fd4h [750.669838ms]
    Aug 17 04:03:43.667: INFO: Created: latency-svc-lnpdh
    Aug 17 04:03:43.706: INFO: Got endpoints: latency-svc-zfbhd [749.715031ms]
    Aug 17 04:03:43.720: INFO: Created: latency-svc-mc656
    Aug 17 04:03:43.756: INFO: Got endpoints: latency-svc-vlhqb [748.582972ms]
    Aug 17 04:03:43.766: INFO: Created: latency-svc-fq7p8
    Aug 17 04:03:43.806: INFO: Got endpoints: latency-svc-2mw5h [750.6345ms]
    Aug 17 04:03:43.818: INFO: Created: latency-svc-44xvg
    Aug 17 04:03:43.856: INFO: Got endpoints: latency-svc-975zp [749.759486ms]
    Aug 17 04:03:43.867: INFO: Created: latency-svc-fsq2z
    Aug 17 04:03:43.905: INFO: Got endpoints: latency-svc-zmxwr [749.590231ms]
    Aug 17 04:03:43.916: INFO: Created: latency-svc-rj9bd
    Aug 17 04:03:43.955: INFO: Got endpoints: latency-svc-wtkm2 [748.93224ms]
    Aug 17 04:03:43.965: INFO: Created: latency-svc-sxkcx
    Aug 17 04:03:44.004: INFO: Got endpoints: latency-svc-m5mkd [747.014771ms]
    Aug 17 04:03:44.014: INFO: Created: latency-svc-pxqxn
    Aug 17 04:03:44.055: INFO: Got endpoints: latency-svc-cjqfj [748.904093ms]
    Aug 17 04:03:44.064: INFO: Created: latency-svc-9nqfv
    Aug 17 04:03:44.106: INFO: Got endpoints: latency-svc-bt5v9 [749.600223ms]
    Aug 17 04:03:44.117: INFO: Created: latency-svc-dp28k
    Aug 17 04:03:44.156: INFO: Got endpoints: latency-svc-zr8rb [750.581152ms]
    Aug 17 04:03:44.167: INFO: Created: latency-svc-wm8xt
    Aug 17 04:03:44.206: INFO: Got endpoints: latency-svc-6llcp [750.55801ms]
    Aug 17 04:03:44.216: INFO: Created: latency-svc-t9flv
    Aug 17 04:03:44.256: INFO: Got endpoints: latency-svc-5l8rg [749.963981ms]
    Aug 17 04:03:44.267: INFO: Created: latency-svc-nzmw8
    Aug 17 04:03:44.307: INFO: Got endpoints: latency-svc-bjxx9 [750.540828ms]
    Aug 17 04:03:44.316: INFO: Created: latency-svc-krq56
    Aug 17 04:03:44.356: INFO: Got endpoints: latency-svc-t5wz2 [750.193688ms]
    Aug 17 04:03:44.368: INFO: Created: latency-svc-l68h9
    Aug 17 04:03:44.406: INFO: Got endpoints: latency-svc-lnpdh [749.482951ms]
    Aug 17 04:03:44.416: INFO: Created: latency-svc-96qks
    Aug 17 04:03:44.456: INFO: Got endpoints: latency-svc-mc656 [749.804558ms]
    Aug 17 04:03:44.466: INFO: Created: latency-svc-xst6j
    Aug 17 04:03:44.507: INFO: Got endpoints: latency-svc-fq7p8 [751.223687ms]
    Aug 17 04:03:44.518: INFO: Created: latency-svc-fq5bx
    Aug 17 04:03:44.556: INFO: Got endpoints: latency-svc-44xvg [750.112917ms]
    Aug 17 04:03:44.568: INFO: Created: latency-svc-pxwlc
    Aug 17 04:03:44.606: INFO: Got endpoints: latency-svc-fsq2z [749.956839ms]
    Aug 17 04:03:44.616: INFO: Created: latency-svc-mtgk4
    Aug 17 04:03:44.655: INFO: Got endpoints: latency-svc-rj9bd [749.3333ms]
    Aug 17 04:03:44.665: INFO: Created: latency-svc-dsbx4
    Aug 17 04:03:44.705: INFO: Got endpoints: latency-svc-sxkcx [750.184275ms]
    Aug 17 04:03:44.715: INFO: Created: latency-svc-8qsrw
    Aug 17 04:03:44.756: INFO: Got endpoints: latency-svc-pxqxn [751.781442ms]
    Aug 17 04:03:44.766: INFO: Created: latency-svc-xk7m5
    Aug 17 04:03:44.807: INFO: Got endpoints: latency-svc-9nqfv [751.52436ms]
    Aug 17 04:03:44.817: INFO: Created: latency-svc-xgcbl
    Aug 17 04:03:44.858: INFO: Got endpoints: latency-svc-dp28k [752.437066ms]
    Aug 17 04:03:44.870: INFO: Created: latency-svc-l4b7g
    Aug 17 04:03:44.907: INFO: Got endpoints: latency-svc-wm8xt [750.270559ms]
    Aug 17 04:03:44.918: INFO: Created: latency-svc-692c9
    Aug 17 04:03:44.956: INFO: Got endpoints: latency-svc-t9flv [750.548385ms]
    Aug 17 04:03:44.969: INFO: Created: latency-svc-9pxwd
    Aug 17 04:03:45.005: INFO: Got endpoints: latency-svc-nzmw8 [749.731247ms]
    Aug 17 04:03:45.018: INFO: Created: latency-svc-24v6g
    Aug 17 04:03:45.056: INFO: Got endpoints: latency-svc-krq56 [749.007711ms]
    Aug 17 04:03:45.066: INFO: Created: latency-svc-b9xgg
    Aug 17 04:03:45.106: INFO: Got endpoints: latency-svc-l68h9 [749.515385ms]
    Aug 17 04:03:45.116: INFO: Created: latency-svc-75hjt
    Aug 17 04:03:45.155: INFO: Got endpoints: latency-svc-96qks [749.397818ms]
    Aug 17 04:03:45.166: INFO: Created: latency-svc-2ctmk
    Aug 17 04:03:45.205: INFO: Got endpoints: latency-svc-xst6j [749.779306ms]
    Aug 17 04:03:45.216: INFO: Created: latency-svc-288jl
    Aug 17 04:03:45.255: INFO: Got endpoints: latency-svc-fq5bx [747.983253ms]
    Aug 17 04:03:45.265: INFO: Created: latency-svc-hsc24
    Aug 17 04:03:45.306: INFO: Got endpoints: latency-svc-pxwlc [749.331369ms]
    Aug 17 04:03:45.317: INFO: Created: latency-svc-gkgj2
    Aug 17 04:03:45.356: INFO: Got endpoints: latency-svc-mtgk4 [749.976556ms]
    Aug 17 04:03:45.367: INFO: Created: latency-svc-l72dw
    Aug 17 04:03:45.405: INFO: Got endpoints: latency-svc-dsbx4 [750.263319ms]
    Aug 17 04:03:45.417: INFO: Created: latency-svc-vw86c
    Aug 17 04:03:45.455: INFO: Got endpoints: latency-svc-8qsrw [750.395665ms]
    Aug 17 04:03:45.467: INFO: Created: latency-svc-bgmdn
    Aug 17 04:03:45.506: INFO: Got endpoints: latency-svc-xk7m5 [749.884797ms]
    Aug 17 04:03:45.518: INFO: Created: latency-svc-bhgd6
    Aug 17 04:03:45.556: INFO: Got endpoints: latency-svc-xgcbl [749.016194ms]
    Aug 17 04:03:45.566: INFO: Created: latency-svc-xm7w7
    Aug 17 04:03:45.606: INFO: Got endpoints: latency-svc-l4b7g [747.214859ms]
    Aug 17 04:03:45.617: INFO: Created: latency-svc-45dpb
    Aug 17 04:03:45.656: INFO: Got endpoints: latency-svc-692c9 [748.954994ms]
    Aug 17 04:03:45.668: INFO: Created: latency-svc-6vf7v
    Aug 17 04:03:45.706: INFO: Got endpoints: latency-svc-9pxwd [749.555913ms]
    Aug 17 04:03:45.717: INFO: Created: latency-svc-qtwfp
    Aug 17 04:03:45.756: INFO: Got endpoints: latency-svc-24v6g [750.743316ms]
    Aug 17 04:03:45.767: INFO: Created: latency-svc-mdzb7
    Aug 17 04:03:45.806: INFO: Got endpoints: latency-svc-b9xgg [750.228028ms]
    Aug 17 04:03:45.816: INFO: Created: latency-svc-gbqrp
    Aug 17 04:03:45.856: INFO: Got endpoints: latency-svc-75hjt [750.071583ms]
    Aug 17 04:03:45.869: INFO: Created: latency-svc-j9h44
    Aug 17 04:03:45.906: INFO: Got endpoints: latency-svc-2ctmk [750.704508ms]
    Aug 17 04:03:45.917: INFO: Created: latency-svc-7dqlp
    Aug 17 04:03:45.956: INFO: Got endpoints: latency-svc-288jl [750.011461ms]
    Aug 17 04:03:45.966: INFO: Created: latency-svc-jqbsd
    Aug 17 04:03:46.005: INFO: Got endpoints: latency-svc-hsc24 [750.289545ms]
    Aug 17 04:03:46.017: INFO: Created: latency-svc-lmrvh
    Aug 17 04:03:46.056: INFO: Got endpoints: latency-svc-gkgj2 [750.408883ms]
    Aug 17 04:03:46.067: INFO: Created: latency-svc-bd4zx
    Aug 17 04:03:46.106: INFO: Got endpoints: latency-svc-l72dw [749.910626ms]
    Aug 17 04:03:46.117: INFO: Created: latency-svc-fr82m
    Aug 17 04:03:46.156: INFO: Got endpoints: latency-svc-vw86c [751.247738ms]
    Aug 17 04:03:46.167: INFO: Created: latency-svc-26sb9
    Aug 17 04:03:46.206: INFO: Got endpoints: latency-svc-bgmdn [750.67088ms]
    Aug 17 04:03:46.217: INFO: Created: latency-svc-llk2c
    Aug 17 04:03:46.255: INFO: Got endpoints: latency-svc-bhgd6 [749.231923ms]
    Aug 17 04:03:46.266: INFO: Created: latency-svc-dxgwm
    Aug 17 04:03:46.307: INFO: Got endpoints: latency-svc-xm7w7 [751.240042ms]
    Aug 17 04:03:46.318: INFO: Created: latency-svc-hdlfv
    Aug 17 04:03:46.355: INFO: Got endpoints: latency-svc-45dpb [749.373897ms]
    Aug 17 04:03:46.368: INFO: Created: latency-svc-p42xj
    Aug 17 04:03:46.406: INFO: Got endpoints: latency-svc-6vf7v [750.096352ms]
    Aug 17 04:03:46.417: INFO: Created: latency-svc-rl9qp
    Aug 17 04:03:46.456: INFO: Got endpoints: latency-svc-qtwfp [750.107392ms]
    Aug 17 04:03:46.467: INFO: Created: latency-svc-tt8hp
    Aug 17 04:03:46.506: INFO: Got endpoints: latency-svc-mdzb7 [750.183715ms]
    Aug 17 04:03:46.518: INFO: Created: latency-svc-5gk7w
    Aug 17 04:03:46.556: INFO: Got endpoints: latency-svc-gbqrp [749.563916ms]
    Aug 17 04:03:46.566: INFO: Created: latency-svc-wnbqj
    Aug 17 04:03:46.606: INFO: Got endpoints: latency-svc-j9h44 [749.789717ms]
    Aug 17 04:03:46.617: INFO: Created: latency-svc-fhgks
    Aug 17 04:03:46.656: INFO: Got endpoints: latency-svc-7dqlp [749.491248ms]
    Aug 17 04:03:46.667: INFO: Created: latency-svc-pj9bb
    Aug 17 04:03:46.706: INFO: Got endpoints: latency-svc-jqbsd [750.246529ms]
    Aug 17 04:03:46.721: INFO: Created: latency-svc-kgrpg
    Aug 17 04:03:46.756: INFO: Got endpoints: latency-svc-lmrvh [750.459231ms]
    Aug 17 04:03:46.771: INFO: Created: latency-svc-rwtlg
    Aug 17 04:03:46.805: INFO: Got endpoints: latency-svc-bd4zx [749.294289ms]
    Aug 17 04:03:46.816: INFO: Created: latency-svc-45crb
    Aug 17 04:03:46.857: INFO: Got endpoints: latency-svc-fr82m [750.708162ms]
    Aug 17 04:03:46.867: INFO: Created: latency-svc-ps26x
    Aug 17 04:03:46.907: INFO: Got endpoints: latency-svc-26sb9 [750.371967ms]
    Aug 17 04:03:46.919: INFO: Created: latency-svc-pd6gl
    Aug 17 04:03:46.956: INFO: Got endpoints: latency-svc-llk2c [749.413975ms]
    Aug 17 04:03:46.967: INFO: Created: latency-svc-9bgv8
    Aug 17 04:03:47.006: INFO: Got endpoints: latency-svc-dxgwm [750.287167ms]
    Aug 17 04:03:47.016: INFO: Created: latency-svc-l5z7p
    Aug 17 04:03:47.055: INFO: Got endpoints: latency-svc-hdlfv [748.069542ms]
    Aug 17 04:03:47.067: INFO: Created: latency-svc-klvcp
    Aug 17 04:03:47.106: INFO: Got endpoints: latency-svc-p42xj [750.358699ms]
    Aug 17 04:03:47.117: INFO: Created: latency-svc-8pjt2
    Aug 17 04:03:47.155: INFO: Got endpoints: latency-svc-rl9qp [749.49636ms]
    Aug 17 04:03:47.167: INFO: Created: latency-svc-b254x
    Aug 17 04:03:47.206: INFO: Got endpoints: latency-svc-tt8hp [749.565157ms]
    Aug 17 04:03:47.216: INFO: Created: latency-svc-fsjxf
    Aug 17 04:03:47.256: INFO: Got endpoints: latency-svc-5gk7w [749.900509ms]
    Aug 17 04:03:47.267: INFO: Created: latency-svc-k2c29
    Aug 17 04:03:47.306: INFO: Got endpoints: latency-svc-wnbqj [750.532269ms]
    Aug 17 04:03:47.319: INFO: Created: latency-svc-w6blf
    Aug 17 04:03:47.356: INFO: Got endpoints: latency-svc-fhgks [749.687811ms]
    Aug 17 04:03:47.366: INFO: Created: latency-svc-gnlrf
    Aug 17 04:03:47.406: INFO: Got endpoints: latency-svc-pj9bb [749.913308ms]
    Aug 17 04:03:47.417: INFO: Created: latency-svc-cx8kz
    Aug 17 04:03:47.457: INFO: Got endpoints: latency-svc-kgrpg [750.824131ms]
    Aug 17 04:03:47.467: INFO: Created: latency-svc-5l4z9
    Aug 17 04:03:47.506: INFO: Got endpoints: latency-svc-rwtlg [749.821067ms]
    Aug 17 04:03:47.517: INFO: Created: latency-svc-tqpt8
    Aug 17 04:03:47.555: INFO: Got endpoints: latency-svc-45crb [749.833286ms]
    Aug 17 04:03:47.566: INFO: Created: latency-svc-dmqld
    Aug 17 04:03:47.606: INFO: Got endpoints: latency-svc-ps26x [749.189686ms]
    Aug 17 04:03:47.617: INFO: Created: latency-svc-q85jh
    Aug 17 04:03:47.656: INFO: Got endpoints: latency-svc-pd6gl [749.665068ms]
    Aug 17 04:03:47.668: INFO: Created: latency-svc-czcpq
    Aug 17 04:03:47.706: INFO: Got endpoints: latency-svc-9bgv8 [750.612844ms]
    Aug 17 04:03:47.717: INFO: Created: latency-svc-7dmq2
    Aug 17 04:03:47.756: INFO: Got endpoints: latency-svc-l5z7p [749.882172ms]
    Aug 17 04:03:47.766: INFO: Created: latency-svc-wtlf7
    Aug 17 04:03:47.806: INFO: Got endpoints: latency-svc-klvcp [750.844336ms]
    Aug 17 04:03:47.818: INFO: Created: latency-svc-l9cht
    Aug 17 04:03:47.856: INFO: Got endpoints: latency-svc-8pjt2 [749.965753ms]
    Aug 17 04:03:47.867: INFO: Created: latency-svc-s97nw
    Aug 17 04:03:47.906: INFO: Got endpoints: latency-svc-b254x [750.798202ms]
    Aug 17 04:03:47.918: INFO: Created: latency-svc-4nwcr
    Aug 17 04:03:47.955: INFO: Got endpoints: latency-svc-fsjxf [749.365883ms]
    Aug 17 04:03:47.966: INFO: Created: latency-svc-cwxp6
    Aug 17 04:03:48.007: INFO: Got endpoints: latency-svc-k2c29 [750.204099ms]
    Aug 17 04:03:48.018: INFO: Created: latency-svc-h8jbh
    Aug 17 04:03:48.056: INFO: Got endpoints: latency-svc-w6blf [749.8019ms]
    Aug 17 04:03:48.067: INFO: Created: latency-svc-lk5fp
    Aug 17 04:03:48.106: INFO: Got endpoints: latency-svc-gnlrf [750.700789ms]
    Aug 17 04:03:48.119: INFO: Created: latency-svc-2b2hw
    Aug 17 04:03:48.156: INFO: Got endpoints: latency-svc-cx8kz [750.391006ms]
    Aug 17 04:03:48.167: INFO: Created: latency-svc-pmklq
    Aug 17 04:03:48.206: INFO: Got endpoints: latency-svc-5l4z9 [749.380025ms]
    Aug 17 04:03:48.220: INFO: Created: latency-svc-l4vz5
    Aug 17 04:03:48.255: INFO: Got endpoints: latency-svc-tqpt8 [749.513321ms]
    Aug 17 04:03:48.266: INFO: Created: latency-svc-r2kr2
    Aug 17 04:03:48.305: INFO: Got endpoints: latency-svc-dmqld [749.797603ms]
    Aug 17 04:03:48.316: INFO: Created: latency-svc-vddrl
    Aug 17 04:03:48.356: INFO: Got endpoints: latency-svc-q85jh [750.104488ms]
    Aug 17 04:03:48.368: INFO: Created: latency-svc-d5n2v
    Aug 17 04:03:48.405: INFO: Got endpoints: latency-svc-czcpq [748.816376ms]
    Aug 17 04:03:48.416: INFO: Created: latency-svc-v8whx
    Aug 17 04:03:48.456: INFO: Got endpoints: latency-svc-7dmq2 [749.07476ms]
    Aug 17 04:03:48.467: INFO: Created: latency-svc-5vqp7
    Aug 17 04:03:48.505: INFO: Got endpoints: latency-svc-wtlf7 [749.775885ms]
    Aug 17 04:03:48.517: INFO: Created: latency-svc-9bz8d
    Aug 17 04:03:48.556: INFO: Got endpoints: latency-svc-l9cht [749.552365ms]
    Aug 17 04:03:48.566: INFO: Created: latency-svc-vqchm
    Aug 17 04:03:48.606: INFO: Got endpoints: latency-svc-s97nw [749.957018ms]
    Aug 17 04:03:48.617: INFO: Created: latency-svc-48j4k
    Aug 17 04:03:48.655: INFO: Got endpoints: latency-svc-4nwcr [748.498518ms]
    Aug 17 04:03:48.666: INFO: Created: latency-svc-wzj5g
    Aug 17 04:03:48.706: INFO: Got endpoints: latency-svc-cwxp6 [750.471498ms]
    Aug 17 04:03:48.717: INFO: Created: latency-svc-h226g
    Aug 17 04:03:48.756: INFO: Got endpoints: latency-svc-h8jbh [749.494236ms]
    Aug 17 04:03:48.766: INFO: Created: latency-svc-vgsv2
    Aug 17 04:03:48.806: INFO: Got endpoints: latency-svc-lk5fp [750.184734ms]
    Aug 17 04:03:48.817: INFO: Created: latency-svc-sxblp
    Aug 17 04:03:48.857: INFO: Got endpoints: latency-svc-2b2hw [750.295621ms]
    Aug 17 04:03:48.868: INFO: Created: latency-svc-nc489
    Aug 17 04:03:48.905: INFO: Got endpoints: latency-svc-pmklq [749.313901ms]
    Aug 17 04:03:48.955: INFO: Got endpoints: latency-svc-l4vz5 [748.685184ms]
    Aug 17 04:03:49.005: INFO: Got endpoints: latency-svc-r2kr2 [749.785337ms]
    Aug 17 04:03:49.056: INFO: Got endpoints: latency-svc-vddrl [750.418849ms]
    Aug 17 04:03:49.106: INFO: Got endpoints: latency-svc-d5n2v [749.455064ms]
    Aug 17 04:03:49.155: INFO: Got endpoints: latency-svc-v8whx [750.031878ms]
    Aug 17 04:03:49.205: INFO: Got endpoints: latency-svc-5vqp7 [749.227523ms]
    Aug 17 04:03:49.256: INFO: Got endpoints: latency-svc-9bz8d [750.303292ms]
    Aug 17 04:03:49.306: INFO: Got endpoints: latency-svc-vqchm [750.381992ms]
    Aug 17 04:03:49.356: INFO: Got endpoints: latency-svc-48j4k [750.108314ms]
    Aug 17 04:03:49.405: INFO: Got endpoints: latency-svc-wzj5g [750.461332ms]
    Aug 17 04:03:49.455: INFO: Got endpoints: latency-svc-h226g [749.46547ms]
    Aug 17 04:03:49.507: INFO: Got endpoints: latency-svc-vgsv2 [750.779207ms]
    Aug 17 04:03:49.556: INFO: Got endpoints: latency-svc-sxblp [749.492353ms]
    Aug 17 04:03:49.606: INFO: Got endpoints: latency-svc-nc489 [749.079499ms]
    Aug 17 04:03:49.606: INFO: Latencies: [13.489358ms 15.008964ms 17.121928ms 20.639878ms 26.246661ms 29.513453ms 33.357373ms 38.364341ms 43.892475ms 50.017614ms 55.508615ms 61.584026ms 68.458877ms 69.412086ms 70.629518ms 70.851659ms 71.035538ms 71.2148ms 71.319522ms 71.364601ms 71.58399ms 73.126421ms 73.398263ms 73.580478ms 73.84958ms 74.362809ms 74.725301ms 75.605156ms 75.632782ms 75.969729ms 79.539158ms 81.038435ms 104.025941ms 150.624962ms 196.863028ms 288.437292ms 300.301795ms 325.789632ms 375.080195ms 420.643279ms 467.262383ms 511.019055ms 556.593071ms 603.015693ms 646.745986ms 694.925938ms 740.13488ms 747.014771ms 747.214859ms 747.412787ms 747.983253ms 748.069542ms 748.366987ms 748.498518ms 748.542191ms 748.582972ms 748.685184ms 748.816376ms 748.904093ms 748.93224ms 748.954994ms 748.962921ms 749.007711ms 749.016194ms 749.07476ms 749.079499ms 749.189686ms 749.227523ms 749.231923ms 749.294289ms 749.313901ms 749.331369ms 749.3333ms 749.365883ms 749.373897ms 749.380025ms 749.397818ms 749.398192ms 749.413975ms 749.417634ms 749.440822ms 749.455064ms 749.46547ms 749.482951ms 749.491248ms 749.492353ms 749.494236ms 749.49636ms 749.498558ms 749.513321ms 749.515385ms 749.552365ms 749.555913ms 749.563916ms 749.565157ms 749.571986ms 749.574793ms 749.590231ms 749.600223ms 749.665068ms 749.687811ms 749.703211ms 749.715031ms 749.729728ms 749.731247ms 749.759486ms 749.775885ms 749.779306ms 749.785337ms 749.789717ms 749.797603ms 749.8019ms 749.804558ms 749.821067ms 749.821633ms 749.833286ms 749.853337ms 749.882172ms 749.884797ms 749.900509ms 749.910626ms 749.913308ms 749.926435ms 749.956272ms 749.956839ms 749.957018ms 749.963981ms 749.965753ms 749.97178ms 749.976556ms 749.99821ms 750.009014ms 750.011461ms 750.031878ms 750.071583ms 750.096352ms 750.104488ms 750.107392ms 750.108314ms 750.112917ms 750.183715ms 750.184275ms 750.184734ms 750.193688ms 750.204099ms 750.227228ms 750.228028ms 750.246529ms 750.248839ms 750.263319ms 750.270559ms 750.287167ms 750.289545ms 750.295621ms 750.303292ms 750.307933ms 750.321264ms 750.358699ms 750.371967ms 750.372425ms 750.374507ms 750.381992ms 750.391006ms 750.395665ms 750.408883ms 750.418849ms 750.441146ms 750.446023ms 750.459231ms 750.461332ms 750.471498ms 750.480108ms 750.532269ms 750.540828ms 750.548385ms 750.55801ms 750.581152ms 750.612844ms 750.61387ms 750.6345ms 750.669838ms 750.67088ms 750.700789ms 750.704508ms 750.708162ms 750.743316ms 750.779207ms 750.786704ms 750.798202ms 750.824131ms 750.844336ms 751.223687ms 751.225299ms 751.240042ms 751.247738ms 751.360372ms 751.52436ms 751.677507ms 751.781442ms 752.437066ms]
    Aug 17 04:03:49.606: INFO: 50 %ile: 749.687811ms
    Aug 17 04:03:49.606: INFO: 90 %ile: 750.669838ms
    Aug 17 04:03:49.606: INFO: 99 %ile: 751.781442ms
    Aug 17 04:03:49.606: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:49.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-4406" for this suite. 08/17/23 04:03:49.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:49.623
Aug 17 04:03:49.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:03:49.623
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:49.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:49.641
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/17/23 04:03:49.645
Aug 17 04:03:49.653: INFO: Waiting up to 5m0s for pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b" in namespace "svcaccounts-8711" to be "Succeeded or Failed"
Aug 17 04:03:49.657: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664822ms
Aug 17 04:03:51.662: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008933645s
Aug 17 04:03:53.662: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Running", Reason="", readiness=false. Elapsed: 4.008761975s
Aug 17 04:03:55.663: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010005625s
STEP: Saw pod success 08/17/23 04:03:55.663
Aug 17 04:03:55.663: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b" satisfied condition "Succeeded or Failed"
Aug 17 04:03:55.667: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b container agnhost-container: <nil>
STEP: delete the pod 08/17/23 04:03:55.676
Aug 17 04:03:55.686: INFO: Waiting for pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b to disappear
Aug 17 04:03:55.690: INFO: Pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:55.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8711" for this suite. 08/17/23 04:03:55.699
------------------------------
• [SLOW TEST] [6.083 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:49.623
    Aug 17 04:03:49.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:03:49.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:49.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:49.641
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/17/23 04:03:49.645
    Aug 17 04:03:49.653: INFO: Waiting up to 5m0s for pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b" in namespace "svcaccounts-8711" to be "Succeeded or Failed"
    Aug 17 04:03:49.657: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664822ms
    Aug 17 04:03:51.662: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008933645s
    Aug 17 04:03:53.662: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Running", Reason="", readiness=false. Elapsed: 4.008761975s
    Aug 17 04:03:55.663: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010005625s
    STEP: Saw pod success 08/17/23 04:03:55.663
    Aug 17 04:03:55.663: INFO: Pod "test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b" satisfied condition "Succeeded or Failed"
    Aug 17 04:03:55.667: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 04:03:55.676
    Aug 17 04:03:55.686: INFO: Waiting for pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b to disappear
    Aug 17 04:03:55.690: INFO: Pod test-pod-90cf549e-2875-4169-a4b2-553aa23a5d5b no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:55.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8711" for this suite. 08/17/23 04:03:55.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:55.707
Aug 17 04:03:55.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubelet-test 08/17/23 04:03:55.708
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:55.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:55.729
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:03:59.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8313" for this suite. 08/17/23 04:03:59.766
------------------------------
• [4.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:55.707
    Aug 17 04:03:55.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 04:03:55.708
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:55.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:55.729
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:03:59.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8313" for this suite. 08/17/23 04:03:59.766
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:03:59.777
Aug 17 04:03:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-pred 08/17/23 04:03:59.778
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:59.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:59.798
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 04:03:59.803: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 04:03:59.816: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 04:03:59.821: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
Aug 17 04:03:59.834: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 04:03:59.834: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:03:59.834: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container coredns ready: true, restart count 0
Aug 17 04:03:59.834: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container coredns ready: true, restart count 0
Aug 17 04:03:59.834: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:03:59.834: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container metrics-server ready: true, restart count 0
Aug 17 04:03:59.834: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Aug 17 04:03:59.834: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 04:03:59.834: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container vpn-target ready: true, restart count 0
Aug 17 04:03:59.834: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:03:59.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:03:59.834: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:03:59.834: INFO: 
Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
Aug 17 04:03:59.845: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:03:59.845: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:03:59.845: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 04:03:59.845: INFO: bin-false49e2810e-e077-46b9-8eba-865d0058b2cb from kubelet-test-8313 started at 2023-08-17 04:03:55 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container bin-false49e2810e-e077-46b9-8eba-865d0058b2cb ready: false, restart count 0
Aug 17 04:03:59.845: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 04:03:59.845: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container e2e ready: true, restart count 0
Aug 17 04:03:59.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:03:59.845: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:03:59.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:03:59.845: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/17/23 04:03:59.845
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177c0fe5db723e2d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 08/17/23 04:03:59.885
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9506" for this suite. 08/17/23 04:04:00.891
------------------------------
• [1.121 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:03:59.777
    Aug 17 04:03:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-pred 08/17/23 04:03:59.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:03:59.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:03:59.798
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 04:03:59.803: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 04:03:59.816: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 04:03:59.821: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
    Aug 17 04:03:59.834: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Aug 17 04:03:59.834: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container vpn-target ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:03:59.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 04:03:59.834: INFO: 
    Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
    Aug 17 04:03:59.845: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: bin-false49e2810e-e077-46b9-8eba-865d0058b2cb from kubelet-test-8313 started at 2023-08-17 04:03:55 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container bin-false49e2810e-e077-46b9-8eba-865d0058b2cb ready: false, restart count 0
    Aug 17 04:03:59.845: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:03:59.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:03:59.845: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/17/23 04:03:59.845
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177c0fe5db723e2d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 08/17/23 04:03:59.885
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9506" for this suite. 08/17/23 04:04:00.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:00.902
Aug 17 04:04:00.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:04:00.902
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:00.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:00.921
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/17/23 04:04:00.926
Aug 17 04:04:00.926: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1170 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/17/23 04:04:00.979
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:00.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1170" for this suite. 08/17/23 04:04:00.999
------------------------------
• [0.106 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:00.902
    Aug 17 04:04:00.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:04:00.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:00.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:00.921
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/17/23 04:04:00.926
    Aug 17 04:04:00.926: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-1170 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/17/23 04:04:00.979
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:00.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1170" for this suite. 08/17/23 04:04:00.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:01.009
Aug 17 04:04:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 04:04:01.01
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:01.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:01.036
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/17/23 04:04:01.041
STEP: waiting for pod running 08/17/23 04:04:01.053
Aug 17 04:04:01.053: INFO: Waiting up to 2m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908" to be "running"
Aug 17 04:04:01.058: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778645ms
Aug 17 04:04:03.064: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010930249s
Aug 17 04:04:03.064: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" satisfied condition "running"
STEP: creating a file in subpath 08/17/23 04:04:03.064
Aug 17 04:04:03.069: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5908 PodName:var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 04:04:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 04:04:03.070: INFO: ExecWithOptions: Clientset creation
Aug 17 04:04:03.070: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-5908/pods/var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/17/23 04:04:03.297
Aug 17 04:04:03.302: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5908 PodName:var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 17 04:04:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
Aug 17 04:04:03.303: INFO: ExecWithOptions: Clientset creation
Aug 17 04:04:03.303: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-5908/pods/var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/17/23 04:04:03.693
Aug 17 04:04:04.209: INFO: Successfully updated pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2"
STEP: waiting for annotated pod running 08/17/23 04:04:04.209
Aug 17 04:04:04.209: INFO: Waiting up to 2m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908" to be "running"
Aug 17 04:04:04.214: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.988776ms
Aug 17 04:04:04.214: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" satisfied condition "running"
STEP: deleting the pod gracefully 08/17/23 04:04:04.214
Aug 17 04:04:04.214: INFO: Deleting pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908"
Aug 17 04:04:04.225: INFO: Wait up to 5m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:38.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5908" for this suite. 08/17/23 04:04:38.243
------------------------------
• [SLOW TEST] [37.241 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:01.009
    Aug 17 04:04:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 04:04:01.01
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:01.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:01.036
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/17/23 04:04:01.041
    STEP: waiting for pod running 08/17/23 04:04:01.053
    Aug 17 04:04:01.053: INFO: Waiting up to 2m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908" to be "running"
    Aug 17 04:04:01.058: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778645ms
    Aug 17 04:04:03.064: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010930249s
    Aug 17 04:04:03.064: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" satisfied condition "running"
    STEP: creating a file in subpath 08/17/23 04:04:03.064
    Aug 17 04:04:03.069: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5908 PodName:var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 04:04:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 04:04:03.070: INFO: ExecWithOptions: Clientset creation
    Aug 17 04:04:03.070: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-5908/pods/var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/17/23 04:04:03.297
    Aug 17 04:04:03.302: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5908 PodName:var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 17 04:04:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    Aug 17 04:04:03.303: INFO: ExecWithOptions: Clientset creation
    Aug 17 04:04:03.303: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-5908/pods/var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/17/23 04:04:03.693
    Aug 17 04:04:04.209: INFO: Successfully updated pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2"
    STEP: waiting for annotated pod running 08/17/23 04:04:04.209
    Aug 17 04:04:04.209: INFO: Waiting up to 2m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908" to be "running"
    Aug 17 04:04:04.214: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.988776ms
    Aug 17 04:04:04.214: INFO: Pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" satisfied condition "running"
    STEP: deleting the pod gracefully 08/17/23 04:04:04.214
    Aug 17 04:04:04.214: INFO: Deleting pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" in namespace "var-expansion-5908"
    Aug 17 04:04:04.225: INFO: Wait up to 5m0s for pod "var-expansion-9b2229ea-828f-4499-acb1-fb8a7ac655e2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:38.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5908" for this suite. 08/17/23 04:04:38.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:38.253
Aug 17 04:04:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 04:04:38.254
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:38.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:38.274
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3812 08/17/23 04:04:38.279
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 04:04:38.29
STEP: creating service externalsvc in namespace services-3812 08/17/23 04:04:38.29
STEP: creating replication controller externalsvc in namespace services-3812 08/17/23 04:04:38.3
I0817 04:04:38.305432      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3812, replica count: 2
I0817 04:04:41.357178      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/17/23 04:04:41.361
Aug 17 04:04:41.378: INFO: Creating new exec pod
Aug 17 04:04:41.384: INFO: Waiting up to 5m0s for pod "execpodl2wd2" in namespace "services-3812" to be "running"
Aug 17 04:04:41.388: INFO: Pod "execpodl2wd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473162ms
Aug 17 04:04:43.393: INFO: Pod "execpodl2wd2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009460493s
Aug 17 04:04:43.393: INFO: Pod "execpodl2wd2" satisfied condition "running"
Aug 17 04:04:43.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3812 exec execpodl2wd2 -- /bin/sh -x -c nslookup clusterip-service.services-3812.svc.cluster.local'
Aug 17 04:04:43.879: INFO: stderr: "+ nslookup clusterip-service.services-3812.svc.cluster.local\n"
Aug 17 04:04:43.879: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nclusterip-service.services-3812.svc.cluster.local\tcanonical name = externalsvc.services-3812.svc.cluster.local.\nName:\texternalsvc.services-3812.svc.cluster.local\nAddress: 172.20.56.7\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3812, will wait for the garbage collector to delete the pods 08/17/23 04:04:43.879
Aug 17 04:04:43.942: INFO: Deleting ReplicationController externalsvc took: 6.954397ms
Aug 17 04:04:44.042: INFO: Terminating ReplicationController externalsvc pods took: 100.753824ms
Aug 17 04:04:45.960: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:45.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3812" for this suite. 08/17/23 04:04:45.979
------------------------------
• [SLOW TEST] [7.734 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:38.253
    Aug 17 04:04:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 04:04:38.254
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:38.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:38.274
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3812 08/17/23 04:04:38.279
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/17/23 04:04:38.29
    STEP: creating service externalsvc in namespace services-3812 08/17/23 04:04:38.29
    STEP: creating replication controller externalsvc in namespace services-3812 08/17/23 04:04:38.3
    I0817 04:04:38.305432      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3812, replica count: 2
    I0817 04:04:41.357178      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/17/23 04:04:41.361
    Aug 17 04:04:41.378: INFO: Creating new exec pod
    Aug 17 04:04:41.384: INFO: Waiting up to 5m0s for pod "execpodl2wd2" in namespace "services-3812" to be "running"
    Aug 17 04:04:41.388: INFO: Pod "execpodl2wd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473162ms
    Aug 17 04:04:43.393: INFO: Pod "execpodl2wd2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009460493s
    Aug 17 04:04:43.393: INFO: Pod "execpodl2wd2" satisfied condition "running"
    Aug 17 04:04:43.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-3812 exec execpodl2wd2 -- /bin/sh -x -c nslookup clusterip-service.services-3812.svc.cluster.local'
    Aug 17 04:04:43.879: INFO: stderr: "+ nslookup clusterip-service.services-3812.svc.cluster.local\n"
    Aug 17 04:04:43.879: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nclusterip-service.services-3812.svc.cluster.local\tcanonical name = externalsvc.services-3812.svc.cluster.local.\nName:\texternalsvc.services-3812.svc.cluster.local\nAddress: 172.20.56.7\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3812, will wait for the garbage collector to delete the pods 08/17/23 04:04:43.879
    Aug 17 04:04:43.942: INFO: Deleting ReplicationController externalsvc took: 6.954397ms
    Aug 17 04:04:44.042: INFO: Terminating ReplicationController externalsvc pods took: 100.753824ms
    Aug 17 04:04:45.960: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:45.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3812" for this suite. 08/17/23 04:04:45.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:45.988
Aug 17 04:04:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 04:04:45.989
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:46.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:46.011
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 17 04:04:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:47.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5648" for this suite. 08/17/23 04:04:47.055
------------------------------
• [1.074 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:45.988
    Aug 17 04:04:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename custom-resource-definition 08/17/23 04:04:45.989
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:46.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:46.011
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 17 04:04:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:47.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5648" for this suite. 08/17/23 04:04:47.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:47.064
Aug 17 04:04:47.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 04:04:47.064
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:47.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:47.082
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 04:04:47.087
Aug 17 04:04:47.097: INFO: Waiting up to 5m0s for pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6" in namespace "emptydir-7723" to be "Succeeded or Failed"
Aug 17 04:04:47.102: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516319ms
Aug 17 04:04:49.108: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010808529s
Aug 17 04:04:51.107: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009551672s
STEP: Saw pod success 08/17/23 04:04:51.107
Aug 17 04:04:51.107: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6" satisfied condition "Succeeded or Failed"
Aug 17 04:04:51.111: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 container test-container: <nil>
STEP: delete the pod 08/17/23 04:04:51.158
Aug 17 04:04:51.169: INFO: Waiting for pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 to disappear
Aug 17 04:04:51.173: INFO: Pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:51.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7723" for this suite. 08/17/23 04:04:51.179
------------------------------
• [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:47.064
    Aug 17 04:04:47.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 04:04:47.064
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:47.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:47.082
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/17/23 04:04:47.087
    Aug 17 04:04:47.097: INFO: Waiting up to 5m0s for pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6" in namespace "emptydir-7723" to be "Succeeded or Failed"
    Aug 17 04:04:47.102: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516319ms
    Aug 17 04:04:49.108: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010808529s
    Aug 17 04:04:51.107: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009551672s
    STEP: Saw pod success 08/17/23 04:04:51.107
    Aug 17 04:04:51.107: INFO: Pod "pod-2ae9301a-d072-4a78-9429-4bf406b35fc6" satisfied condition "Succeeded or Failed"
    Aug 17 04:04:51.111: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 container test-container: <nil>
    STEP: delete the pod 08/17/23 04:04:51.158
    Aug 17 04:04:51.169: INFO: Waiting for pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 to disappear
    Aug 17 04:04:51.173: INFO: Pod pod-2ae9301a-d072-4a78-9429-4bf406b35fc6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:51.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7723" for this suite. 08/17/23 04:04:51.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:51.187
Aug 17 04:04:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption 08/17/23 04:04:51.188
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:51.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:51.207
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/17/23 04:04:51.212
STEP: Waiting for the pdb to be processed 08/17/23 04:04:51.218
STEP: updating the pdb 08/17/23 04:04:53.23
STEP: Waiting for the pdb to be processed 08/17/23 04:04:53.24
STEP: patching the pdb 08/17/23 04:04:55.252
STEP: Waiting for the pdb to be processed 08/17/23 04:04:55.265
STEP: Waiting for the pdb to be deleted 08/17/23 04:04:57.281
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:57.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3752" for this suite. 08/17/23 04:04:57.293
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:51.187
    Aug 17 04:04:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption 08/17/23 04:04:51.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:51.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:51.207
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/17/23 04:04:51.212
    STEP: Waiting for the pdb to be processed 08/17/23 04:04:51.218
    STEP: updating the pdb 08/17/23 04:04:53.23
    STEP: Waiting for the pdb to be processed 08/17/23 04:04:53.24
    STEP: patching the pdb 08/17/23 04:04:55.252
    STEP: Waiting for the pdb to be processed 08/17/23 04:04:55.265
    STEP: Waiting for the pdb to be deleted 08/17/23 04:04:57.281
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:57.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3752" for this suite. 08/17/23 04:04:57.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:57.304
Aug 17 04:04:57.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename podtemplate 08/17/23 04:04:57.305
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:57.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:57.327
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/17/23 04:04:57.331
Aug 17 04:04:57.338: INFO: created test-podtemplate-1
Aug 17 04:04:57.344: INFO: created test-podtemplate-2
Aug 17 04:04:57.350: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/17/23 04:04:57.35
STEP: delete collection of pod templates 08/17/23 04:04:57.355
Aug 17 04:04:57.355: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/17/23 04:04:57.373
Aug 17 04:04:57.373: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 04:04:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2516" for this suite. 08/17/23 04:04:57.385
------------------------------
• [0.088 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:57.304
    Aug 17 04:04:57.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename podtemplate 08/17/23 04:04:57.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:57.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:57.327
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/17/23 04:04:57.331
    Aug 17 04:04:57.338: INFO: created test-podtemplate-1
    Aug 17 04:04:57.344: INFO: created test-podtemplate-2
    Aug 17 04:04:57.350: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/17/23 04:04:57.35
    STEP: delete collection of pod templates 08/17/23 04:04:57.355
    Aug 17 04:04:57.355: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/17/23 04:04:57.373
    Aug 17 04:04:57.373: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:04:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2516" for this suite. 08/17/23 04:04:57.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:04:57.393
Aug 17 04:04:57.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:04:57.394
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:57.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:57.419
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/17/23 04:04:57.424
Aug 17 04:04:57.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 create -f -'
Aug 17 04:04:57.614: INFO: stderr: ""
Aug 17 04:04:57.614: INFO: stdout: "pod/pause created\n"
Aug 17 04:04:57.614: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 17 04:04:57.614: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2398" to be "running and ready"
Aug 17 04:04:57.618: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353132ms
Aug 17 04:04:57.618: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ske-ubuntu-79fff84d86x69988-vjwlx' to be 'Running' but was 'Pending'
Aug 17 04:04:59.624: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010133427s
Aug 17 04:04:59.624: INFO: Pod "pause" satisfied condition "running and ready"
Aug 17 04:04:59.624: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/17/23 04:04:59.624
Aug 17 04:04:59.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 label pods pause testing-label=testing-label-value'
Aug 17 04:04:59.705: INFO: stderr: ""
Aug 17 04:04:59.705: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/17/23 04:04:59.705
Aug 17 04:04:59.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pod pause -L testing-label'
Aug 17 04:04:59.771: INFO: stderr: ""
Aug 17 04:04:59.771: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/17/23 04:04:59.771
Aug 17 04:04:59.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 label pods pause testing-label-'
Aug 17 04:04:59.849: INFO: stderr: ""
Aug 17 04:04:59.849: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/17/23 04:04:59.849
Aug 17 04:04:59.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pod pause -L testing-label'
Aug 17 04:04:59.922: INFO: stderr: ""
Aug 17 04:04:59.922: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/17/23 04:04:59.922
Aug 17 04:04:59.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 delete --grace-period=0 --force -f -'
Aug 17 04:05:00.035: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 17 04:05:00.035: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 17 04:05:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get rc,svc -l name=pause --no-headers'
Aug 17 04:05:00.134: INFO: stderr: "No resources found in kubectl-2398 namespace.\n"
Aug 17 04:05:00.134: INFO: stdout: ""
Aug 17 04:05:00.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 17 04:05:00.218: INFO: stderr: ""
Aug 17 04:05:00.218: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2398" for this suite. 08/17/23 04:05:00.226
------------------------------
• [2.844 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:04:57.393
    Aug 17 04:04:57.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:04:57.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:04:57.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:04:57.419
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/17/23 04:04:57.424
    Aug 17 04:04:57.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 create -f -'
    Aug 17 04:04:57.614: INFO: stderr: ""
    Aug 17 04:04:57.614: INFO: stdout: "pod/pause created\n"
    Aug 17 04:04:57.614: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 17 04:04:57.614: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2398" to be "running and ready"
    Aug 17 04:04:57.618: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353132ms
    Aug 17 04:04:57.618: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ske-ubuntu-79fff84d86x69988-vjwlx' to be 'Running' but was 'Pending'
    Aug 17 04:04:59.624: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010133427s
    Aug 17 04:04:59.624: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 17 04:04:59.624: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/17/23 04:04:59.624
    Aug 17 04:04:59.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 label pods pause testing-label=testing-label-value'
    Aug 17 04:04:59.705: INFO: stderr: ""
    Aug 17 04:04:59.705: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/17/23 04:04:59.705
    Aug 17 04:04:59.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pod pause -L testing-label'
    Aug 17 04:04:59.771: INFO: stderr: ""
    Aug 17 04:04:59.771: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/17/23 04:04:59.771
    Aug 17 04:04:59.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 label pods pause testing-label-'
    Aug 17 04:04:59.849: INFO: stderr: ""
    Aug 17 04:04:59.849: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/17/23 04:04:59.849
    Aug 17 04:04:59.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pod pause -L testing-label'
    Aug 17 04:04:59.922: INFO: stderr: ""
    Aug 17 04:04:59.922: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/17/23 04:04:59.922
    Aug 17 04:04:59.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 delete --grace-period=0 --force -f -'
    Aug 17 04:05:00.035: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 17 04:05:00.035: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 17 04:05:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get rc,svc -l name=pause --no-headers'
    Aug 17 04:05:00.134: INFO: stderr: "No resources found in kubectl-2398 namespace.\n"
    Aug 17 04:05:00.134: INFO: stdout: ""
    Aug 17 04:05:00.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-2398 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 17 04:05:00.218: INFO: stderr: ""
    Aug 17 04:05:00.218: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2398" for this suite. 08/17/23 04:05:00.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:00.238
Aug 17 04:05:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:05:00.238
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:00.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:00.261
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-b72e2cff-0c4c-4c89-994d-9fa6b8013274 08/17/23 04:05:00.273
STEP: Creating configMap with name cm-test-opt-upd-48aab653-ec23-4135-9dc3-594b73dcc1c4 08/17/23 04:05:00.279
STEP: Creating the pod 08/17/23 04:05:00.286
Aug 17 04:05:00.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2" in namespace "configmap-2226" to be "running and ready"
Aug 17 04:05:00.302: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.205313ms
Aug 17 04:05:00.302: INFO: The phase of Pod pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:02.308: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010631334s
Aug 17 04:05:02.308: INFO: The phase of Pod pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2 is Running (Ready = true)
Aug 17 04:05:02.308: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b72e2cff-0c4c-4c89-994d-9fa6b8013274 08/17/23 04:05:02.466
STEP: Updating configmap cm-test-opt-upd-48aab653-ec23-4135-9dc3-594b73dcc1c4 08/17/23 04:05:02.474
STEP: Creating configMap with name cm-test-opt-create-27c1308d-2360-4feb-b6bd-96adfc9cf021 08/17/23 04:05:02.48
STEP: waiting to observe update in volume 08/17/23 04:05:02.485
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:04.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2226" for this suite. 08/17/23 04:05:04.699
------------------------------
• [4.469 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:00.238
    Aug 17 04:05:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:05:00.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:00.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:00.261
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-b72e2cff-0c4c-4c89-994d-9fa6b8013274 08/17/23 04:05:00.273
    STEP: Creating configMap with name cm-test-opt-upd-48aab653-ec23-4135-9dc3-594b73dcc1c4 08/17/23 04:05:00.279
    STEP: Creating the pod 08/17/23 04:05:00.286
    Aug 17 04:05:00.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2" in namespace "configmap-2226" to be "running and ready"
    Aug 17 04:05:00.302: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.205313ms
    Aug 17 04:05:00.302: INFO: The phase of Pod pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:02.308: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010631334s
    Aug 17 04:05:02.308: INFO: The phase of Pod pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2 is Running (Ready = true)
    Aug 17 04:05:02.308: INFO: Pod "pod-configmaps-8dc08ce6-106e-4929-ac25-5bce845b43a2" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b72e2cff-0c4c-4c89-994d-9fa6b8013274 08/17/23 04:05:02.466
    STEP: Updating configmap cm-test-opt-upd-48aab653-ec23-4135-9dc3-594b73dcc1c4 08/17/23 04:05:02.474
    STEP: Creating configMap with name cm-test-opt-create-27c1308d-2360-4feb-b6bd-96adfc9cf021 08/17/23 04:05:02.48
    STEP: waiting to observe update in volume 08/17/23 04:05:02.485
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:04.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2226" for this suite. 08/17/23 04:05:04.699
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:04.708
Aug 17 04:05:04.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption 08/17/23 04:05:04.708
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:04.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:04.728
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:04.732
Aug 17 04:05:04.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption-2 08/17/23 04:05:04.733
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:04.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:04.751
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/17/23 04:05:04.763
STEP: Waiting for the pdb to be processed 08/17/23 04:05:04.773
STEP: Waiting for the pdb to be processed 08/17/23 04:05:06.79
STEP: listing a collection of PDBs across all namespaces 08/17/23 04:05:08.802
STEP: listing a collection of PDBs in namespace disruption-1215 08/17/23 04:05:08.807
STEP: deleting a collection of PDBs 08/17/23 04:05:08.812
STEP: Waiting for the PDB collection to be deleted 08/17/23 04:05:08.827
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:08.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-5161" for this suite. 08/17/23 04:05:08.845
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1215" for this suite. 08/17/23 04:05:08.853
------------------------------
• [4.152 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:04.708
    Aug 17 04:05:04.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption 08/17/23 04:05:04.708
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:04.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:04.728
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:04.732
    Aug 17 04:05:04.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption-2 08/17/23 04:05:04.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:04.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:04.751
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/17/23 04:05:04.763
    STEP: Waiting for the pdb to be processed 08/17/23 04:05:04.773
    STEP: Waiting for the pdb to be processed 08/17/23 04:05:06.79
    STEP: listing a collection of PDBs across all namespaces 08/17/23 04:05:08.802
    STEP: listing a collection of PDBs in namespace disruption-1215 08/17/23 04:05:08.807
    STEP: deleting a collection of PDBs 08/17/23 04:05:08.812
    STEP: Waiting for the PDB collection to be deleted 08/17/23 04:05:08.827
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:08.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-5161" for this suite. 08/17/23 04:05:08.845
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1215" for this suite. 08/17/23 04:05:08.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:08.861
Aug 17 04:05:08.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:05:08.862
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:08.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:08.88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-67be2775-79b0-4f9b-a985-d0aa7f777669 08/17/23 04:05:08.884
STEP: Creating a pod to test consume configMaps 08/17/23 04:05:08.89
Aug 17 04:05:08.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e" in namespace "projected-2764" to be "Succeeded or Failed"
Aug 17 04:05:08.904: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097549ms
Aug 17 04:05:10.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010135752s
Aug 17 04:05:12.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009704789s
STEP: Saw pod success 08/17/23 04:05:12.91
Aug 17 04:05:12.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e" satisfied condition "Succeeded or Failed"
Aug 17 04:05:12.914: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e container agnhost-container: <nil>
STEP: delete the pod 08/17/23 04:05:12.926
Aug 17 04:05:12.937: INFO: Waiting for pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e to disappear
Aug 17 04:05:12.942: INFO: Pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:12.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2764" for this suite. 08/17/23 04:05:12.948
------------------------------
• [4.096 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:08.861
    Aug 17 04:05:08.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:05:08.862
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:08.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:08.88
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-67be2775-79b0-4f9b-a985-d0aa7f777669 08/17/23 04:05:08.884
    STEP: Creating a pod to test consume configMaps 08/17/23 04:05:08.89
    Aug 17 04:05:08.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e" in namespace "projected-2764" to be "Succeeded or Failed"
    Aug 17 04:05:08.904: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097549ms
    Aug 17 04:05:10.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010135752s
    Aug 17 04:05:12.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009704789s
    STEP: Saw pod success 08/17/23 04:05:12.91
    Aug 17 04:05:12.910: INFO: Pod "pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e" satisfied condition "Succeeded or Failed"
    Aug 17 04:05:12.914: INFO: Trying to get logs from node ske-rhel-6c9d465fc4xbjl6l-wz7jz pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 04:05:12.926
    Aug 17 04:05:12.937: INFO: Waiting for pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e to disappear
    Aug 17 04:05:12.942: INFO: Pod pod-projected-configmaps-b31f14b1-411f-4dc5-b70b-d8e295d5493e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:12.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2764" for this suite. 08/17/23 04:05:12.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:12.959
Aug 17 04:05:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:05:12.959
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:12.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:12.977
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:05:12.998
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:05:13.311
STEP: Deploying the webhook pod 08/17/23 04:05:13.32
STEP: Wait for the deployment to be ready 08/17/23 04:05:13.334
Aug 17 04:05:13.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 04:05:15.357
STEP: Verifying the service has paired with the endpoint 08/17/23 04:05:15.368
Aug 17 04:05:16.368: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 17 04:05:16.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8870-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 04:05:16.887
STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 04:05:16.991
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:19.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8410" for this suite. 08/17/23 04:05:19.729
STEP: Destroying namespace "webhook-8410-markers" for this suite. 08/17/23 04:05:19.739
------------------------------
• [SLOW TEST] [6.787 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:12.959
    Aug 17 04:05:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:05:12.959
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:12.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:12.977
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:05:12.998
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:05:13.311
    STEP: Deploying the webhook pod 08/17/23 04:05:13.32
    STEP: Wait for the deployment to be ready 08/17/23 04:05:13.334
    Aug 17 04:05:13.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 04:05:15.357
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:05:15.368
    Aug 17 04:05:16.368: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 17 04:05:16.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8870-crds.webhook.example.com via the AdmissionRegistration API 08/17/23 04:05:16.887
    STEP: Creating a custom resource that should be mutated by the webhook 08/17/23 04:05:16.991
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:19.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8410" for this suite. 08/17/23 04:05:19.729
    STEP: Destroying namespace "webhook-8410-markers" for this suite. 08/17/23 04:05:19.739
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:19.746
Aug 17 04:05:19.746: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sched-pred 08/17/23 04:05:19.746
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:19.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:19.78
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 17 04:05:19.787: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 17 04:05:19.799: INFO: Waiting for terminating namespaces to be deleted...
Aug 17 04:05:19.804: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
Aug 17 04:05:19.817: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 17 04:05:19.817: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:05:19.817: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container coredns ready: true, restart count 0
Aug 17 04:05:19.817: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container coredns ready: true, restart count 0
Aug 17 04:05:19.817: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:05:19.817: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container metrics-server ready: true, restart count 0
Aug 17 04:05:19.817: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Aug 17 04:05:19.817: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 04:05:19.817: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container vpn-target ready: true, restart count 0
Aug 17 04:05:19.817: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:05:19.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:05:19.817: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 17 04:05:19.817: INFO: 
Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
Aug 17 04:05:19.828: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container calico-node ready: true, restart count 0
Aug 17 04:05:19.828: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 17 04:05:19.828: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Aug 17 04:05:19.828: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 17 04:05:19.828: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container e2e ready: true, restart count 0
Aug 17 04:05:19.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:05:19.828: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
Aug 17 04:05:19.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 17 04:05:19.828: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ske-rhel-6c9d465fc4xbjl6l-wz7jz 08/17/23 04:05:19.857
STEP: verifying the node has the label node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 04:05:19.873
Aug 17 04:05:19.896: INFO: Pod calico-kube-controllers-77cc457ff7-x9r5c requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod calico-node-lxgkj requesting resource cpu=250m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod calico-node-pbjkm requesting resource cpu=250m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod coredns-7b4f76cbb6-9cggt requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod coredns-7b4f76cbb6-kvnv6 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod kube-proxy-snplq requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod kube-proxy-zfx4n requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod metrics-server-54cd6dc7f5-jkfs9 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod nfs-subdir-external-provisioner-68cdd7797d-466ng requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod node-exporter-6bjp9 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod node-exporter-9kjt5 requesting resource cpu=100m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod vpn-target-bcf545797-885zq requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.896: INFO: Pod sonobuoy requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod sonobuoy-e2e-job-5d7551a4c3ac4b95 requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.896: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
STEP: Starting Pods to consume most of the cluster CPU. 08/17/23 04:05:19.896
Aug 17 04:05:19.896: INFO: Creating a pod which consumes cpu=826m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
Aug 17 04:05:19.906: INFO: Creating a pod which consumes cpu=1106m on Node ske-ubuntu-79fff84d86x69988-vjwlx
Aug 17 04:05:19.912: INFO: Waiting up to 5m0s for pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178" in namespace "sched-pred-6342" to be "running"
Aug 17 04:05:19.916: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03084ms
Aug 17 04:05:21.923: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178": Phase="Running", Reason="", readiness=true. Elapsed: 2.011224661s
Aug 17 04:05:21.923: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178" satisfied condition "running"
Aug 17 04:05:21.923: INFO: Waiting up to 5m0s for pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb" in namespace "sched-pred-6342" to be "running"
Aug 17 04:05:21.928: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333802ms
Aug 17 04:05:23.933: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009912184s
Aug 17 04:05:23.933: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/17/23 04:05:23.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff87db9711c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6342/filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178 to ske-rhel-6c9d465fc4xbjl6l-wz7jz] 08/17/23 04:05:23.939
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a20f7e29], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 04:05:23.939
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a351885d], Reason = [Created], Message = [Created container filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178] 08/17/23 04:05:23.939
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a8838636], Reason = [Started], Message = [Started container filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178] 08/17/23 04:05:23.939
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff87e1e4bca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6342/filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb to ske-ubuntu-79fff84d86x69988-vjwlx] 08/17/23 04:05:23.939
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8b4e10c9a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 04:05:23.94
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8b84f2f83], Reason = [Created], Message = [Created container filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb] 08/17/23 04:05:23.94
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8c1afabb7], Reason = [Started], Message = [Started container filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb] 08/17/23 04:05:23.94
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177c0ff96e532e71], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 08/17/23 04:05:23.954
STEP: removing the label node off the node ske-rhel-6c9d465fc4xbjl6l-wz7jz 08/17/23 04:05:24.953
STEP: verifying the node doesn't have the label node 08/17/23 04:05:24.968
STEP: removing the label node off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 04:05:24.973
STEP: verifying the node doesn't have the label node 08/17/23 04:05:24.987
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:24.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6342" for this suite. 08/17/23 04:05:25.003
------------------------------
• [SLOW TEST] [5.265 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:19.746
    Aug 17 04:05:19.746: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sched-pred 08/17/23 04:05:19.746
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:19.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:19.78
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 17 04:05:19.787: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 17 04:05:19.799: INFO: Waiting for terminating namespaces to be deleted...
    Aug 17 04:05:19.804: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-6c9d465fc4xbjl6l-wz7jz before test
    Aug 17 04:05:19.817: INFO: calico-kube-controllers-77cc457ff7-x9r5c from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: calico-node-pbjkm from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: coredns-7b4f76cbb6-9cggt from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: coredns-7b4f76cbb6-kvnv6 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container coredns ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: kube-proxy-snplq from kube-system started at 2023-08-17 01:55:00 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: metrics-server-54cd6dc7f5-jkfs9 from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: nfs-subdir-external-provisioner-68cdd7797d-466ng from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Aug 17 04:05:19.817: INFO: node-exporter-6bjp9 from kube-system started at 2023-08-16 08:28:44 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: vpn-target-bcf545797-885zq from kube-system started at 2023-08-16 08:29:22 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container vpn-target ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:05:19.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 17 04:05:19.817: INFO: 
    Logging pods the apiserver thinks is on node ske-ubuntu-79fff84d86x69988-vjwlx before test
    Aug 17 04:05:19.828: INFO: calico-node-lxgkj from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container calico-node ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: kube-proxy-zfx4n from kube-system started at 2023-08-17 01:54:56 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: node-exporter-9kjt5 from kube-system started at 2023-08-16 08:28:07 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: sonobuoy from sonobuoy started at 2023-08-17 02:59:54 +0000 UTC (1 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: sonobuoy-e2e-job-5d7551a4c3ac4b95 from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container e2e ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws from sonobuoy started at 2023-08-17 02:59:55 +0000 UTC (2 container statuses recorded)
    Aug 17 04:05:19.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 17 04:05:19.828: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ske-rhel-6c9d465fc4xbjl6l-wz7jz 08/17/23 04:05:19.857
    STEP: verifying the node has the label node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 04:05:19.873
    Aug 17 04:05:19.896: INFO: Pod calico-kube-controllers-77cc457ff7-x9r5c requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod calico-node-lxgkj requesting resource cpu=250m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod calico-node-pbjkm requesting resource cpu=250m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod coredns-7b4f76cbb6-9cggt requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod coredns-7b4f76cbb6-kvnv6 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod kube-proxy-snplq requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod kube-proxy-zfx4n requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod metrics-server-54cd6dc7f5-jkfs9 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod nfs-subdir-external-provisioner-68cdd7797d-466ng requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod node-exporter-6bjp9 requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod node-exporter-9kjt5 requesting resource cpu=100m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod vpn-target-bcf545797-885zq requesting resource cpu=100m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.896: INFO: Pod sonobuoy requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod sonobuoy-e2e-job-5d7551a4c3ac4b95 requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-5b2ws requesting resource cpu=0m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.896: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fb98f6682a24f35-l6bq8 requesting resource cpu=0m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    STEP: Starting Pods to consume most of the cluster CPU. 08/17/23 04:05:19.896
    Aug 17 04:05:19.896: INFO: Creating a pod which consumes cpu=826m on Node ske-rhel-6c9d465fc4xbjl6l-wz7jz
    Aug 17 04:05:19.906: INFO: Creating a pod which consumes cpu=1106m on Node ske-ubuntu-79fff84d86x69988-vjwlx
    Aug 17 04:05:19.912: INFO: Waiting up to 5m0s for pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178" in namespace "sched-pred-6342" to be "running"
    Aug 17 04:05:19.916: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03084ms
    Aug 17 04:05:21.923: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178": Phase="Running", Reason="", readiness=true. Elapsed: 2.011224661s
    Aug 17 04:05:21.923: INFO: Pod "filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178" satisfied condition "running"
    Aug 17 04:05:21.923: INFO: Waiting up to 5m0s for pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb" in namespace "sched-pred-6342" to be "running"
    Aug 17 04:05:21.928: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333802ms
    Aug 17 04:05:23.933: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009912184s
    Aug 17 04:05:23.933: INFO: Pod "filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/17/23 04:05:23.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff87db9711c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6342/filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178 to ske-rhel-6c9d465fc4xbjl6l-wz7jz] 08/17/23 04:05:23.939
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a20f7e29], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 04:05:23.939
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a351885d], Reason = [Created], Message = [Created container filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178] 08/17/23 04:05:23.939
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178.177c0ff8a8838636], Reason = [Started], Message = [Started container filler-pod-0e7f6036-5e63-42a8-a324-429bc183c178] 08/17/23 04:05:23.939
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff87e1e4bca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6342/filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb to ske-ubuntu-79fff84d86x69988-vjwlx] 08/17/23 04:05:23.939
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8b4e10c9a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/17/23 04:05:23.94
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8b84f2f83], Reason = [Created], Message = [Created container filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb] 08/17/23 04:05:23.94
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb.177c0ff8c1afabb7], Reason = [Started], Message = [Started container filler-pod-8c637771-3691-4ed0-ab3e-b45301c1cfcb] 08/17/23 04:05:23.94
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177c0ff96e532e71], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 08/17/23 04:05:23.954
    STEP: removing the label node off the node ske-rhel-6c9d465fc4xbjl6l-wz7jz 08/17/23 04:05:24.953
    STEP: verifying the node doesn't have the label node 08/17/23 04:05:24.968
    STEP: removing the label node off the node ske-ubuntu-79fff84d86x69988-vjwlx 08/17/23 04:05:24.973
    STEP: verifying the node doesn't have the label node 08/17/23 04:05:24.987
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:24.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6342" for this suite. 08/17/23 04:05:25.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:25.012
Aug 17 04:05:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:05:25.014
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:25.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:25.044
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 17 04:05:25.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/17/23 04:05:26.544
Aug 17 04:05:26.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
Aug 17 04:05:27.201: INFO: stderr: ""
Aug 17 04:05:27.201: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 04:05:27.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 delete e2e-test-crd-publish-openapi-8866-crds test-foo'
Aug 17 04:05:27.270: INFO: stderr: ""
Aug 17 04:05:27.270: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 17 04:05:27.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
Aug 17 04:05:27.477: INFO: stderr: ""
Aug 17 04:05:27.477: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 17 04:05:27.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 delete e2e-test-crd-publish-openapi-8866-crds test-foo'
Aug 17 04:05:27.544: INFO: stderr: ""
Aug 17 04:05:27.544: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/17/23 04:05:27.544
Aug 17 04:05:27.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
Aug 17 04:05:27.723: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/17/23 04:05:27.723
Aug 17 04:05:27.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
Aug 17 04:05:27.932: INFO: rc: 1
Aug 17 04:05:27.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
Aug 17 04:05:28.161: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/17/23 04:05:28.161
Aug 17 04:05:28.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
Aug 17 04:05:28.340: INFO: rc: 1
Aug 17 04:05:28.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
Aug 17 04:05:28.529: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/17/23 04:05:28.529
Aug 17 04:05:28.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds'
Aug 17 04:05:28.711: INFO: stderr: ""
Aug 17 04:05:28.711: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/17/23 04:05:28.712
Aug 17 04:05:28.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.metadata'
Aug 17 04:05:28.881: INFO: stderr: ""
Aug 17 04:05:28.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 17 04:05:28.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec'
Aug 17 04:05:29.058: INFO: stderr: ""
Aug 17 04:05:29.058: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 17 04:05:29.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec.bars'
Aug 17 04:05:29.235: INFO: stderr: ""
Aug 17 04:05:29.235: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/17/23 04:05:29.235
Aug 17 04:05:29.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec.bars2'
Aug 17 04:05:29.418: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1801" for this suite. 08/17/23 04:05:30.876
------------------------------
• [SLOW TEST] [5.873 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:25.012
    Aug 17 04:05:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename crd-publish-openapi 08/17/23 04:05:25.014
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:25.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:25.044
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 17 04:05:25.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/17/23 04:05:26.544
    Aug 17 04:05:26.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
    Aug 17 04:05:27.201: INFO: stderr: ""
    Aug 17 04:05:27.201: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 17 04:05:27.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 delete e2e-test-crd-publish-openapi-8866-crds test-foo'
    Aug 17 04:05:27.270: INFO: stderr: ""
    Aug 17 04:05:27.270: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 17 04:05:27.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
    Aug 17 04:05:27.477: INFO: stderr: ""
    Aug 17 04:05:27.477: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 17 04:05:27.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 delete e2e-test-crd-publish-openapi-8866-crds test-foo'
    Aug 17 04:05:27.544: INFO: stderr: ""
    Aug 17 04:05:27.544: INFO: stdout: "e2e-test-crd-publish-openapi-8866-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/17/23 04:05:27.544
    Aug 17 04:05:27.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
    Aug 17 04:05:27.723: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/17/23 04:05:27.723
    Aug 17 04:05:27.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
    Aug 17 04:05:27.932: INFO: rc: 1
    Aug 17 04:05:27.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
    Aug 17 04:05:28.161: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/17/23 04:05:28.161
    Aug 17 04:05:28.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 create -f -'
    Aug 17 04:05:28.340: INFO: rc: 1
    Aug 17 04:05:28.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 --namespace=crd-publish-openapi-1801 apply -f -'
    Aug 17 04:05:28.529: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/17/23 04:05:28.529
    Aug 17 04:05:28.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds'
    Aug 17 04:05:28.711: INFO: stderr: ""
    Aug 17 04:05:28.711: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/17/23 04:05:28.712
    Aug 17 04:05:28.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.metadata'
    Aug 17 04:05:28.881: INFO: stderr: ""
    Aug 17 04:05:28.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 17 04:05:28.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec'
    Aug 17 04:05:29.058: INFO: stderr: ""
    Aug 17 04:05:29.058: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 17 04:05:29.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec.bars'
    Aug 17 04:05:29.235: INFO: stderr: ""
    Aug 17 04:05:29.235: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8866-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/17/23 04:05:29.235
    Aug 17 04:05:29.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=crd-publish-openapi-1801 explain e2e-test-crd-publish-openapi-8866-crds.spec.bars2'
    Aug 17 04:05:29.418: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1801" for this suite. 08/17/23 04:05:30.876
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:30.886
Aug 17 04:05:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 04:05:30.891
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:30.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:30.906
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/17/23 04:05:30.916
STEP: delete the rc 08/17/23 04:05:35.926
STEP: wait for the rc to be deleted 08/17/23 04:05:35.931
Aug 17 04:05:36.950: INFO: 80 pods remaining
Aug 17 04:05:36.950: INFO: 80 pods has nil DeletionTimestamp
Aug 17 04:05:36.950: INFO: 
Aug 17 04:05:37.964: INFO: 71 pods remaining
Aug 17 04:05:37.964: INFO: 70 pods has nil DeletionTimestamp
Aug 17 04:05:37.964: INFO: 
Aug 17 04:05:38.989: INFO: 60 pods remaining
Aug 17 04:05:38.989: INFO: 60 pods has nil DeletionTimestamp
Aug 17 04:05:38.989: INFO: 
Aug 17 04:05:39.975: INFO: 40 pods remaining
Aug 17 04:05:39.975: INFO: 40 pods has nil DeletionTimestamp
Aug 17 04:05:39.975: INFO: 
Aug 17 04:05:40.974: INFO: 30 pods remaining
Aug 17 04:05:40.974: INFO: 30 pods has nil DeletionTimestamp
Aug 17 04:05:40.974: INFO: 
Aug 17 04:05:41.946: INFO: 20 pods remaining
Aug 17 04:05:41.946: INFO: 20 pods has nil DeletionTimestamp
Aug 17 04:05:41.946: INFO: 
STEP: Gathering metrics 08/17/23 04:05:42.942
W0817 04:05:42.957035      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 04:05:42.957: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:42.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-264" for this suite. 08/17/23 04:05:42.963
------------------------------
• [SLOW TEST] [12.085 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:30.886
    Aug 17 04:05:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 04:05:30.891
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:30.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:30.906
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/17/23 04:05:30.916
    STEP: delete the rc 08/17/23 04:05:35.926
    STEP: wait for the rc to be deleted 08/17/23 04:05:35.931
    Aug 17 04:05:36.950: INFO: 80 pods remaining
    Aug 17 04:05:36.950: INFO: 80 pods has nil DeletionTimestamp
    Aug 17 04:05:36.950: INFO: 
    Aug 17 04:05:37.964: INFO: 71 pods remaining
    Aug 17 04:05:37.964: INFO: 70 pods has nil DeletionTimestamp
    Aug 17 04:05:37.964: INFO: 
    Aug 17 04:05:38.989: INFO: 60 pods remaining
    Aug 17 04:05:38.989: INFO: 60 pods has nil DeletionTimestamp
    Aug 17 04:05:38.989: INFO: 
    Aug 17 04:05:39.975: INFO: 40 pods remaining
    Aug 17 04:05:39.975: INFO: 40 pods has nil DeletionTimestamp
    Aug 17 04:05:39.975: INFO: 
    Aug 17 04:05:40.974: INFO: 30 pods remaining
    Aug 17 04:05:40.974: INFO: 30 pods has nil DeletionTimestamp
    Aug 17 04:05:40.974: INFO: 
    Aug 17 04:05:41.946: INFO: 20 pods remaining
    Aug 17 04:05:41.946: INFO: 20 pods has nil DeletionTimestamp
    Aug 17 04:05:41.946: INFO: 
    STEP: Gathering metrics 08/17/23 04:05:42.942
    W0817 04:05:42.957035      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 04:05:42.957: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:42.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-264" for this suite. 08/17/23 04:05:42.963
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:42.975
Aug 17 04:05:42.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 04:05:42.977
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:42.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:42.996
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/17/23 04:05:43.001
STEP: submitting the pod to kubernetes 08/17/23 04:05:43.001
STEP: verifying QOS class is set on the pod 08/17/23 04:05:43.011
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 17 04:05:43.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6265" for this suite. 08/17/23 04:05:43.029
------------------------------
• [0.064 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:42.975
    Aug 17 04:05:42.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 04:05:42.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:42.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:42.996
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/17/23 04:05:43.001
    STEP: submitting the pod to kubernetes 08/17/23 04:05:43.001
    STEP: verifying QOS class is set on the pod 08/17/23 04:05:43.011
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:05:43.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6265" for this suite. 08/17/23 04:05:43.029
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:05:43.045
Aug 17 04:05:43.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 04:05:43.055
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:43.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:43.136
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/17/23 04:05:43.209
Aug 17 04:05:43.227: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7452" to be "running and ready"
Aug 17 04:05:43.236: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.55974ms
Aug 17 04:05:43.240: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:45.244: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015963618s
Aug 17 04:05:45.244: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:47.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017706995s
Aug 17 04:05:47.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:49.249: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02047902s
Aug 17 04:05:49.249: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:51.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017727986s
Aug 17 04:05:51.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:53.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017558337s
Aug 17 04:05:53.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:55.245: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016232243s
Aug 17 04:05:55.245: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:57.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017326533s
Aug 17 04:05:57.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:05:59.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017757689s
Aug 17 04:05:59.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:06:01.245: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 18.01673612s
Aug 17 04:06:01.245: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 17 04:06:01.245: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/17/23 04:06:01.249
STEP: Then the orphan pod is adopted 08/17/23 04:06:01.256
STEP: When the matched label of one of its pods change 08/17/23 04:06:02.266
Aug 17 04:06:02.271: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/17/23 04:06:02.29
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:06:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7452" for this suite. 08/17/23 04:06:02.304
------------------------------
• [SLOW TEST] [19.267 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:05:43.045
    Aug 17 04:05:43.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 04:05:43.055
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:05:43.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:05:43.136
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/17/23 04:05:43.209
    Aug 17 04:05:43.227: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7452" to be "running and ready"
    Aug 17 04:05:43.236: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.55974ms
    Aug 17 04:05:43.240: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:45.244: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015963618s
    Aug 17 04:05:45.244: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:47.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017706995s
    Aug 17 04:05:47.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:49.249: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02047902s
    Aug 17 04:05:49.249: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:51.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017727986s
    Aug 17 04:05:51.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:53.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.017558337s
    Aug 17 04:05:53.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:55.245: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016232243s
    Aug 17 04:05:55.245: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:57.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017326533s
    Aug 17 04:05:57.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:05:59.246: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017757689s
    Aug 17 04:05:59.246: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:06:01.245: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 18.01673612s
    Aug 17 04:06:01.245: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 17 04:06:01.245: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/17/23 04:06:01.249
    STEP: Then the orphan pod is adopted 08/17/23 04:06:01.256
    STEP: When the matched label of one of its pods change 08/17/23 04:06:02.266
    Aug 17 04:06:02.271: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/17/23 04:06:02.29
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:06:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7452" for this suite. 08/17/23 04:06:02.304
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:06:02.313
Aug 17 04:06:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 04:06:02.325
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:06:02.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:06:02.347
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 17 04:06:02.356: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 17 04:06:02.366: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 17 04:06:07.371: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/17/23 04:06:07.371
Aug 17 04:06:07.371: INFO: Creating deployment "test-rolling-update-deployment"
Aug 17 04:06:07.377: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 17 04:06:07.387: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 17 04:06:09.397: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 17 04:06:09.401: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 04:06:09.415: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4753  09de622a-4c2a-451f-b6ce-17b450f43c59 243927 1 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d0ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 04:06:07 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-17 04:06:08 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 17 04:06:09.419: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4753  abc25822-ffac-4bd5-b0ff-694a9e3958b1 243918 1 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 09de622a-4c2a-451f-b6ce-17b450f43c59 0xc0049d13c7 0xc0049d13c8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09de622a-4c2a-451f-b6ce-17b450f43c59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d1478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:06:09.420: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 17 04:06:09.420: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4753  6188859e-4c21-4e32-826f-5087b07bd5fb 243926 2 2023-08-17 04:06:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 09de622a-4c2a-451f-b6ce-17b450f43c59 0xc0049d1297 0xc0049d1298}] [] [{e2e.test Update apps/v1 2023-08-17 04:06:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09de622a-4c2a-451f-b6ce-17b450f43c59\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049d1358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:06:09.425: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8th7z" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8th7z test-rolling-update-deployment-7549d9f46d- deployment-4753  5517fbaf-22f8-4978-86ef-d1d14d9bbe66 243917 0 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5e787369fe21a4ffe0433b361f18a3e48cb39e8be4dadc080d10246ebccc2a71 cni.projectcalico.org/podIP:172.21.86.140/32 cni.projectcalico.org/podIPs:172.21.86.140/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d abc25822-ffac-4bd5-b0ff-694a9e3958b1 0xc0008e4767 0xc0008e4768}] [] [{calico Update v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abc25822-ffac-4bd5-b0ff-694a9e3958b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdrn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdrn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.140,StartTime:2023-08-17 04:06:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:06:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://961b8f078d66ee67509bf373d4a4c5d5d1cef795c41f90bd5fd2136f4c52eec5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 04:06:09.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4753" for this suite. 08/17/23 04:06:09.432
------------------------------
• [SLOW TEST] [7.127 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:06:02.313
    Aug 17 04:06:02.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 04:06:02.325
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:06:02.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:06:02.347
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 17 04:06:02.356: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 17 04:06:02.366: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 17 04:06:07.371: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/17/23 04:06:07.371
    Aug 17 04:06:07.371: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 17 04:06:07.377: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 17 04:06:07.387: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 17 04:06:09.397: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 17 04:06:09.401: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 04:06:09.415: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4753  09de622a-4c2a-451f-b6ce-17b450f43c59 243927 1 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d0ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-17 04:06:07 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-17 04:06:08 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 17 04:06:09.419: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4753  abc25822-ffac-4bd5-b0ff-694a9e3958b1 243918 1 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 09de622a-4c2a-451f-b6ce-17b450f43c59 0xc0049d13c7 0xc0049d13c8}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09de622a-4c2a-451f-b6ce-17b450f43c59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d1478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:06:09.420: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 17 04:06:09.420: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4753  6188859e-4c21-4e32-826f-5087b07bd5fb 243926 2 2023-08-17 04:06:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 09de622a-4c2a-451f-b6ce-17b450f43c59 0xc0049d1297 0xc0049d1298}] [] [{e2e.test Update apps/v1 2023-08-17 04:06:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09de622a-4c2a-451f-b6ce-17b450f43c59\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049d1358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:06:09.425: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8th7z" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8th7z test-rolling-update-deployment-7549d9f46d- deployment-4753  5517fbaf-22f8-4978-86ef-d1d14d9bbe66 243917 0 2023-08-17 04:06:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5e787369fe21a4ffe0433b361f18a3e48cb39e8be4dadc080d10246ebccc2a71 cni.projectcalico.org/podIP:172.21.86.140/32 cni.projectcalico.org/podIPs:172.21.86.140/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d abc25822-ffac-4bd5-b0ff-694a9e3958b1 0xc0008e4767 0xc0008e4768}] [] [{calico Update v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-17 04:06:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abc25822-ffac-4bd5-b0ff-694a9e3958b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 04:06:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdrn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdrn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:06:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.140,StartTime:2023-08-17 04:06:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:06:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://961b8f078d66ee67509bf373d4a4c5d5d1cef795c41f90bd5fd2136f4c52eec5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:06:09.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4753" for this suite. 08/17/23 04:06:09.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:06:09.441
Aug 17 04:06:09.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename cronjob 08/17/23 04:06:09.442
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:06:09.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:06:09.461
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/17/23 04:06:09.466
STEP: Ensuring a job is scheduled 08/17/23 04:06:09.473
STEP: Ensuring exactly one is scheduled 08/17/23 04:07:01.48
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 04:07:01.489
STEP: Ensuring no more jobs are scheduled 08/17/23 04:07:01.495
STEP: Removing cronjob 08/17/23 04:12:01.51
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 04:12:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7935" for this suite. 08/17/23 04:12:01.53
------------------------------
• [SLOW TEST] [352.100 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:06:09.441
    Aug 17 04:06:09.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename cronjob 08/17/23 04:06:09.442
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:06:09.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:06:09.461
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/17/23 04:06:09.466
    STEP: Ensuring a job is scheduled 08/17/23 04:06:09.473
    STEP: Ensuring exactly one is scheduled 08/17/23 04:07:01.48
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/17/23 04:07:01.489
    STEP: Ensuring no more jobs are scheduled 08/17/23 04:07:01.495
    STEP: Removing cronjob 08/17/23 04:12:01.51
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:12:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7935" for this suite. 08/17/23 04:12:01.53
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:12:01.542
Aug 17 04:12:01.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 04:12:01.543
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:01.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:01.579
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/17/23 04:12:01.588
Aug 17 04:12:01.602: INFO: Waiting up to 5m0s for pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79" in namespace "pods-5913" to be "running and ready"
Aug 17 04:12:01.607: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452432ms
Aug 17 04:12:01.607: INFO: The phase of Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:12:03.614: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79": Phase="Running", Reason="", readiness=true. Elapsed: 2.012201129s
Aug 17 04:12:03.614: INFO: The phase of Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 is Running (Ready = true)
Aug 17 04:12:03.614: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79" satisfied condition "running and ready"
Aug 17 04:12:03.623: INFO: Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 has hostIP: 192.168.11.4
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 04:12:03.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5913" for this suite. 08/17/23 04:12:03.631
------------------------------
• [2.097 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:12:01.542
    Aug 17 04:12:01.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 04:12:01.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:01.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:01.579
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/17/23 04:12:01.588
    Aug 17 04:12:01.602: INFO: Waiting up to 5m0s for pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79" in namespace "pods-5913" to be "running and ready"
    Aug 17 04:12:01.607: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452432ms
    Aug 17 04:12:01.607: INFO: The phase of Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:12:03.614: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79": Phase="Running", Reason="", readiness=true. Elapsed: 2.012201129s
    Aug 17 04:12:03.614: INFO: The phase of Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 is Running (Ready = true)
    Aug 17 04:12:03.614: INFO: Pod "pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79" satisfied condition "running and ready"
    Aug 17 04:12:03.623: INFO: Pod pod-hostip-4a0d8605-55f8-4d7d-ad61-3c8074e31c79 has hostIP: 192.168.11.4
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:12:03.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5913" for this suite. 08/17/23 04:12:03.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:12:03.641
Aug 17 04:12:03.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:12:03.642
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:03.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:03.662
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:12:03.682
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:12:04.306
STEP: Deploying the webhook pod 08/17/23 04:12:04.316
STEP: Wait for the deployment to be ready 08/17/23 04:12:04.33
Aug 17 04:12:04.340: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 04:12:06.357
STEP: Verifying the service has paired with the endpoint 08/17/23 04:12:06.37
Aug 17 04:12:07.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/17/23 04:12:07.377
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.486
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/17/23 04:12:07.585
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.599
STEP: Patching a validating webhook configuration's rules to include the create operation 08/17/23 04:12:07.612
STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.621
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:12:07.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1294" for this suite. 08/17/23 04:12:07.684
STEP: Destroying namespace "webhook-1294-markers" for this suite. 08/17/23 04:12:07.698
------------------------------
• [4.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:12:03.641
    Aug 17 04:12:03.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:12:03.642
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:03.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:03.662
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:12:03.682
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:12:04.306
    STEP: Deploying the webhook pod 08/17/23 04:12:04.316
    STEP: Wait for the deployment to be ready 08/17/23 04:12:04.33
    Aug 17 04:12:04.340: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 04:12:06.357
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:12:06.37
    Aug 17 04:12:07.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/17/23 04:12:07.377
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.486
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/17/23 04:12:07.585
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.599
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/17/23 04:12:07.612
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/17/23 04:12:07.621
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:12:07.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1294" for this suite. 08/17/23 04:12:07.684
    STEP: Destroying namespace "webhook-1294-markers" for this suite. 08/17/23 04:12:07.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:12:07.716
Aug 17 04:12:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename limitrange 08/17/23 04:12:07.716
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:07.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:07.737
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-shh6r" in namespace "limitrange-4713" 08/17/23 04:12:07.742
STEP: Creating another limitRange in another namespace 08/17/23 04:12:07.749
Aug 17 04:12:07.768: INFO: Namespace "e2e-limitrange-shh6r-1897" created
Aug 17 04:12:07.768: INFO: Creating LimitRange "e2e-limitrange-shh6r" in namespace "e2e-limitrange-shh6r-1897"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-shh6r" 08/17/23 04:12:07.775
Aug 17 04:12:07.784: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-shh6r" in "limitrange-4713" namespace 08/17/23 04:12:07.784
Aug 17 04:12:07.793: INFO: LimitRange "e2e-limitrange-shh6r" has been patched
STEP: Delete LimitRange "e2e-limitrange-shh6r" by Collection with labelSelector: "e2e-limitrange-shh6r=patched" 08/17/23 04:12:07.793
STEP: Confirm that the limitRange "e2e-limitrange-shh6r" has been deleted 08/17/23 04:12:07.802
Aug 17 04:12:07.803: INFO: Requesting list of LimitRange to confirm quantity
Aug 17 04:12:07.807: INFO: Found 0 LimitRange with label "e2e-limitrange-shh6r=patched"
Aug 17 04:12:07.807: INFO: LimitRange "e2e-limitrange-shh6r" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-shh6r" 08/17/23 04:12:07.807
Aug 17 04:12:07.814: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 17 04:12:07.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-4713" for this suite. 08/17/23 04:12:07.824
STEP: Destroying namespace "e2e-limitrange-shh6r-1897" for this suite. 08/17/23 04:12:07.833
------------------------------
• [0.127 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:12:07.716
    Aug 17 04:12:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename limitrange 08/17/23 04:12:07.716
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:07.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:07.737
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-shh6r" in namespace "limitrange-4713" 08/17/23 04:12:07.742
    STEP: Creating another limitRange in another namespace 08/17/23 04:12:07.749
    Aug 17 04:12:07.768: INFO: Namespace "e2e-limitrange-shh6r-1897" created
    Aug 17 04:12:07.768: INFO: Creating LimitRange "e2e-limitrange-shh6r" in namespace "e2e-limitrange-shh6r-1897"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-shh6r" 08/17/23 04:12:07.775
    Aug 17 04:12:07.784: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-shh6r" in "limitrange-4713" namespace 08/17/23 04:12:07.784
    Aug 17 04:12:07.793: INFO: LimitRange "e2e-limitrange-shh6r" has been patched
    STEP: Delete LimitRange "e2e-limitrange-shh6r" by Collection with labelSelector: "e2e-limitrange-shh6r=patched" 08/17/23 04:12:07.793
    STEP: Confirm that the limitRange "e2e-limitrange-shh6r" has been deleted 08/17/23 04:12:07.802
    Aug 17 04:12:07.803: INFO: Requesting list of LimitRange to confirm quantity
    Aug 17 04:12:07.807: INFO: Found 0 LimitRange with label "e2e-limitrange-shh6r=patched"
    Aug 17 04:12:07.807: INFO: LimitRange "e2e-limitrange-shh6r" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-shh6r" 08/17/23 04:12:07.807
    Aug 17 04:12:07.814: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:12:07.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-4713" for this suite. 08/17/23 04:12:07.824
    STEP: Destroying namespace "e2e-limitrange-shh6r-1897" for this suite. 08/17/23 04:12:07.833
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:12:07.843
Aug 17 04:12:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption 08/17/23 04:12:07.844
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:07.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:07.873
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/17/23 04:12:07.885
STEP: Waiting for all pods to be running 08/17/23 04:12:09.92
Aug 17 04:12:09.927: INFO: running pods: 0 < 3
Aug 17 04:12:11.936: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:12:13.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4694" for this suite. 08/17/23 04:12:13.945
------------------------------
• [SLOW TEST] [6.109 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:12:07.843
    Aug 17 04:12:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption 08/17/23 04:12:07.844
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:07.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:07.873
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/17/23 04:12:07.885
    STEP: Waiting for all pods to be running 08/17/23 04:12:09.92
    Aug 17 04:12:09.927: INFO: running pods: 0 < 3
    Aug 17 04:12:11.936: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:12:13.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4694" for this suite. 08/17/23 04:12:13.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:12:13.959
Aug 17 04:12:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:12:13.96
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:13.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:13.978
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6420 08/17/23 04:12:13.983
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-6420 08/17/23 04:12:13.99
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6420 08/17/23 04:12:13.996
Aug 17 04:12:14.001: INFO: Found 0 stateful pods, waiting for 1
Aug 17 04:12:24.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/17/23 04:12:24.006
Aug 17 04:12:24.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:12:24.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:12:24.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:12:24.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:12:24.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 04:12:34.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:12:34.455: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:12:34.472: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Aug 17 04:12:34.472: INFO: ss-0  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  }]
Aug 17 04:12:34.472: INFO: 
Aug 17 04:12:34.472: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 17 04:12:35.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995338556s
Aug 17 04:12:36.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988373477s
Aug 17 04:12:37.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981950738s
Aug 17 04:12:38.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976154807s
Aug 17 04:12:39.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968519751s
Aug 17 04:12:40.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962896968s
Aug 17 04:12:41.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957360695s
Aug 17 04:12:42.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951133201s
Aug 17 04:12:43.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.153553ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6420 08/17/23 04:12:44.529
Aug 17 04:12:44.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:12:44.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:12:44.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:12:44.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:12:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:12:45.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 04:12:45.388: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:12:45.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:12:45.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:12:45.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 17 04:12:45.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:12:45.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:12:45.897: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:12:45.897: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:12:45.897: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/17/23 04:12:45.897
Aug 17 04:12:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:12:46.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:12:46.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:12:46.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:12:46.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:12:46.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:12:46.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:12:46.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:12:46.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:12:47.294: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:12:47.294: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:12:47.294: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:12:47.294: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:12:47.299: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 17 04:12:57.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:12:57.309: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:12:57.309: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:12:57.323: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Aug 17 04:12:57.323: INFO: ss-0  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  }]
Aug 17 04:12:57.323: INFO: ss-1  ske-rhel-6c9d465fc4xbjl6l-wz7jz    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
Aug 17 04:12:57.323: INFO: ss-2  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
Aug 17 04:12:57.323: INFO: 
Aug 17 04:12:57.323: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 17 04:12:58.328: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Aug 17 04:12:58.328: INFO: ss-1  ske-rhel-6c9d465fc4xbjl6l-wz7jz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
Aug 17 04:12:58.328: INFO: 
Aug 17 04:12:58.328: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 17 04:12:59.332: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990292782s
Aug 17 04:13:00.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986000544s
Aug 17 04:13:01.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981638425s
Aug 17 04:13:02.347: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975554671s
Aug 17 04:13:03.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970961657s
Aug 17 04:13:04.359: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.964936221s
Aug 17 04:13:05.363: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.958936197s
Aug 17 04:13:06.368: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.691391ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6420 08/17/23 04:13:07.369
Aug 17 04:13:07.374: INFO: Scaling statefulset ss to 0
Aug 17 04:13:07.387: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:13:07.390: INFO: Deleting all statefulset in ns statefulset-6420
Aug 17 04:13:07.393: INFO: Scaling statefulset ss to 0
Aug 17 04:13:07.405: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:13:07.408: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:13:07.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6420" for this suite. 08/17/23 04:13:07.428
------------------------------
• [SLOW TEST] [53.477 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:12:13.959
    Aug 17 04:12:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:12:13.96
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:12:13.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:12:13.978
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6420 08/17/23 04:12:13.983
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-6420 08/17/23 04:12:13.99
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6420 08/17/23 04:12:13.996
    Aug 17 04:12:14.001: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 04:12:24.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/17/23 04:12:24.006
    Aug 17 04:12:24.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:12:24.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:12:24.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:12:24.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:12:24.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 17 04:12:34.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:12:34.455: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:12:34.472: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
    Aug 17 04:12:34.472: INFO: ss-0  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  }]
    Aug 17 04:12:34.472: INFO: 
    Aug 17 04:12:34.472: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 17 04:12:35.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995338556s
    Aug 17 04:12:36.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988373477s
    Aug 17 04:12:37.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981950738s
    Aug 17 04:12:38.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976154807s
    Aug 17 04:12:39.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968519751s
    Aug 17 04:12:40.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962896968s
    Aug 17 04:12:41.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957360695s
    Aug 17 04:12:42.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951133201s
    Aug 17 04:12:43.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.153553ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6420 08/17/23 04:12:44.529
    Aug 17 04:12:44.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:12:44.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 04:12:44.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:12:44.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:12:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:12:45.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 17 04:12:45.388: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:12:45.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:12:45.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:12:45.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 17 04:12:45.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:12:45.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:12:45.897: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:12:45.897: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:12:45.897: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/17/23 04:12:45.897
    Aug 17 04:12:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:12:46.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:12:46.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:12:46.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:12:46.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:12:46.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:12:46.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:12:46.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:12:46.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-6420 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:12:47.294: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:12:47.294: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:12:47.294: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:12:47.294: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:12:47.299: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 17 04:12:57.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:12:57.309: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:12:57.309: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:12:57.323: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
    Aug 17 04:12:57.323: INFO: ss-0  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:14 +0000 UTC  }]
    Aug 17 04:12:57.323: INFO: ss-1  ske-rhel-6c9d465fc4xbjl6l-wz7jz    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
    Aug 17 04:12:57.323: INFO: ss-2  ske-ubuntu-79fff84d86x69988-vjwlx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
    Aug 17 04:12:57.323: INFO: 
    Aug 17 04:12:57.323: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 17 04:12:58.328: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
    Aug 17 04:12:58.328: INFO: ss-1  ske-rhel-6c9d465fc4xbjl6l-wz7jz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:12:34 +0000 UTC  }]
    Aug 17 04:12:58.328: INFO: 
    Aug 17 04:12:58.328: INFO: StatefulSet ss has not reached scale 0, at 1
    Aug 17 04:12:59.332: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990292782s
    Aug 17 04:13:00.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986000544s
    Aug 17 04:13:01.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981638425s
    Aug 17 04:13:02.347: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975554671s
    Aug 17 04:13:03.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970961657s
    Aug 17 04:13:04.359: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.964936221s
    Aug 17 04:13:05.363: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.958936197s
    Aug 17 04:13:06.368: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.691391ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6420 08/17/23 04:13:07.369
    Aug 17 04:13:07.374: INFO: Scaling statefulset ss to 0
    Aug 17 04:13:07.387: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:13:07.390: INFO: Deleting all statefulset in ns statefulset-6420
    Aug 17 04:13:07.393: INFO: Scaling statefulset ss to 0
    Aug 17 04:13:07.405: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:13:07.408: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:13:07.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6420" for this suite. 08/17/23 04:13:07.428
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:13:07.437
Aug 17 04:13:07.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 04:13:07.438
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:13:07.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:13:07.455
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb in namespace container-probe-638 08/17/23 04:13:07.46
Aug 17 04:13:07.469: INFO: Waiting up to 5m0s for pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb" in namespace "container-probe-638" to be "not pending"
Aug 17 04:13:07.474: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.759404ms
Aug 17 04:13:09.479: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599263s
Aug 17 04:13:09.479: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb" satisfied condition "not pending"
Aug 17 04:13:09.479: INFO: Started pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb in namespace container-probe-638
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:13:09.479
Aug 17 04:13:09.483: INFO: Initial restart count of pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is 0
Aug 17 04:13:29.540: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 1 (20.05734539s elapsed)
Aug 17 04:13:49.600: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 2 (40.116793932s elapsed)
Aug 17 04:14:09.656: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 3 (1m0.172853217s elapsed)
Aug 17 04:14:29.717: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 4 (1m20.234276754s elapsed)
Aug 17 04:15:39.940: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 5 (2m30.457124573s elapsed)
STEP: deleting the pod 08/17/23 04:15:39.94
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-638" for this suite. 08/17/23 04:15:39.96
------------------------------
• [SLOW TEST] [152.530 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:13:07.437
    Aug 17 04:13:07.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 04:13:07.438
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:13:07.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:13:07.455
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb in namespace container-probe-638 08/17/23 04:13:07.46
    Aug 17 04:13:07.469: INFO: Waiting up to 5m0s for pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb" in namespace "container-probe-638" to be "not pending"
    Aug 17 04:13:07.474: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.759404ms
    Aug 17 04:13:09.479: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599263s
    Aug 17 04:13:09.479: INFO: Pod "liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb" satisfied condition "not pending"
    Aug 17 04:13:09.479: INFO: Started pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb in namespace container-probe-638
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:13:09.479
    Aug 17 04:13:09.483: INFO: Initial restart count of pod liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is 0
    Aug 17 04:13:29.540: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 1 (20.05734539s elapsed)
    Aug 17 04:13:49.600: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 2 (40.116793932s elapsed)
    Aug 17 04:14:09.656: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 3 (1m0.172853217s elapsed)
    Aug 17 04:14:29.717: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 4 (1m20.234276754s elapsed)
    Aug 17 04:15:39.940: INFO: Restart count of pod container-probe-638/liveness-b66f0746-aa06-4482-b23a-5e1b8df73bcb is now 5 (2m30.457124573s elapsed)
    STEP: deleting the pod 08/17/23 04:15:39.94
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-638" for this suite. 08/17/23 04:15:39.96
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:39.968
Aug 17 04:15:39.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 04:15:39.968
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:39.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:39.99
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 17 04:15:39.996: INFO: Creating ReplicaSet my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42
Aug 17 04:15:40.006: INFO: Pod name my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Found 0 pods out of 1
Aug 17 04:15:45.013: INFO: Pod name my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Found 1 pods out of 1
Aug 17 04:15:45.013: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42" is running
Aug 17 04:15:45.013: INFO: Waiting up to 5m0s for pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" in namespace "replicaset-6588" to be "running"
Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg": Phase="Running", Reason="", readiness=true. Elapsed: 3.744262ms
Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" satisfied condition "running"
Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:40 +0000 UTC Reason: Message:}])
Aug 17 04:15:45.017: INFO: Trying to dial the pod
Aug 17 04:15:50.117: INFO: Controller my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Got expected result from replica 1 [my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg]: "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6588" for this suite. 08/17/23 04:15:50.125
------------------------------
• [SLOW TEST] [10.164 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:39.968
    Aug 17 04:15:39.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 04:15:39.968
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:39.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:39.99
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 17 04:15:39.996: INFO: Creating ReplicaSet my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42
    Aug 17 04:15:40.006: INFO: Pod name my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Found 0 pods out of 1
    Aug 17 04:15:45.013: INFO: Pod name my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Found 1 pods out of 1
    Aug 17 04:15:45.013: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42" is running
    Aug 17 04:15:45.013: INFO: Waiting up to 5m0s for pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" in namespace "replicaset-6588" to be "running"
    Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg": Phase="Running", Reason="", readiness=true. Elapsed: 3.744262ms
    Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" satisfied condition "running"
    Aug 17 04:15:45.017: INFO: Pod "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-17 04:15:40 +0000 UTC Reason: Message:}])
    Aug 17 04:15:45.017: INFO: Trying to dial the pod
    Aug 17 04:15:50.117: INFO: Controller my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42: Got expected result from replica 1 [my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg]: "my-hostname-basic-81c0765e-f186-404a-a101-a57c92b4fa42-j87hg", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6588" for this suite. 08/17/23 04:15:50.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:50.133
Aug 17 04:15:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 04:15:50.134
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:50.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:50.151
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:50.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3706" for this suite. 08/17/23 04:15:50.166
------------------------------
• [0.039 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:50.133
    Aug 17 04:15:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 04:15:50.134
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:50.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:50.151
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:50.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3706" for this suite. 08/17/23 04:15:50.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:50.174
Aug 17 04:15:50.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:15:50.175
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:50.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:50.191
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b77fd4de-83b5-4228-b57a-9ef179dce8ee 08/17/23 04:15:50.196
STEP: Creating secret with name secret-projected-all-test-volume-ac0d06c1-74b3-46d2-a6a7-46a965bafc1d 08/17/23 04:15:50.201
STEP: Creating a pod to test Check all projections for projected volume plugin 08/17/23 04:15:50.207
Aug 17 04:15:50.216: INFO: Waiting up to 5m0s for pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984" in namespace "projected-4689" to be "Succeeded or Failed"
Aug 17 04:15:50.221: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503189ms
Aug 17 04:15:52.226: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009475533s
Aug 17 04:15:54.227: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010051113s
STEP: Saw pod success 08/17/23 04:15:54.227
Aug 17 04:15:54.227: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984" satisfied condition "Succeeded or Failed"
Aug 17 04:15:54.232: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 container projected-all-volume-test: <nil>
STEP: delete the pod 08/17/23 04:15:54.244
Aug 17 04:15:54.258: INFO: Waiting for pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 to disappear
Aug 17 04:15:54.263: INFO: Pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:54.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4689" for this suite. 08/17/23 04:15:54.27
------------------------------
• [4.104 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:50.174
    Aug 17 04:15:50.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:15:50.175
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:50.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:50.191
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b77fd4de-83b5-4228-b57a-9ef179dce8ee 08/17/23 04:15:50.196
    STEP: Creating secret with name secret-projected-all-test-volume-ac0d06c1-74b3-46d2-a6a7-46a965bafc1d 08/17/23 04:15:50.201
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/17/23 04:15:50.207
    Aug 17 04:15:50.216: INFO: Waiting up to 5m0s for pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984" in namespace "projected-4689" to be "Succeeded or Failed"
    Aug 17 04:15:50.221: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503189ms
    Aug 17 04:15:52.226: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009475533s
    Aug 17 04:15:54.227: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010051113s
    STEP: Saw pod success 08/17/23 04:15:54.227
    Aug 17 04:15:54.227: INFO: Pod "projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984" satisfied condition "Succeeded or Failed"
    Aug 17 04:15:54.232: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 container projected-all-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:15:54.244
    Aug 17 04:15:54.258: INFO: Waiting for pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 to disappear
    Aug 17 04:15:54.263: INFO: Pod projected-volume-eb9f4fe7-c3f5-4b36-9c58-86b234f95984 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:54.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4689" for this suite. 08/17/23 04:15:54.27
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:54.28
Aug 17 04:15:54.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename certificates 08/17/23 04:15:54.281
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:54.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:54.302
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/17/23 04:15:54.591
STEP: getting /apis/certificates.k8s.io 08/17/23 04:15:54.596
STEP: getting /apis/certificates.k8s.io/v1 08/17/23 04:15:54.598
STEP: creating 08/17/23 04:15:54.601
STEP: getting 08/17/23 04:15:54.62
STEP: listing 08/17/23 04:15:54.624
STEP: watching 08/17/23 04:15:54.629
Aug 17 04:15:54.629: INFO: starting watch
STEP: patching 08/17/23 04:15:54.631
STEP: updating 08/17/23 04:15:54.638
Aug 17 04:15:54.645: INFO: waiting for watch events with expected annotations
Aug 17 04:15:54.645: INFO: saw patched and updated annotations
STEP: getting /approval 08/17/23 04:15:54.645
STEP: patching /approval 08/17/23 04:15:54.65
STEP: updating /approval 08/17/23 04:15:54.658
STEP: getting /status 08/17/23 04:15:54.666
STEP: patching /status 08/17/23 04:15:54.67
STEP: updating /status 08/17/23 04:15:54.678
STEP: deleting 08/17/23 04:15:54.686
STEP: deleting a collection 08/17/23 04:15:54.7
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:54.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-2618" for this suite. 08/17/23 04:15:54.725
------------------------------
• [0.453 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:54.28
    Aug 17 04:15:54.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename certificates 08/17/23 04:15:54.281
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:54.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:54.302
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/17/23 04:15:54.591
    STEP: getting /apis/certificates.k8s.io 08/17/23 04:15:54.596
    STEP: getting /apis/certificates.k8s.io/v1 08/17/23 04:15:54.598
    STEP: creating 08/17/23 04:15:54.601
    STEP: getting 08/17/23 04:15:54.62
    STEP: listing 08/17/23 04:15:54.624
    STEP: watching 08/17/23 04:15:54.629
    Aug 17 04:15:54.629: INFO: starting watch
    STEP: patching 08/17/23 04:15:54.631
    STEP: updating 08/17/23 04:15:54.638
    Aug 17 04:15:54.645: INFO: waiting for watch events with expected annotations
    Aug 17 04:15:54.645: INFO: saw patched and updated annotations
    STEP: getting /approval 08/17/23 04:15:54.645
    STEP: patching /approval 08/17/23 04:15:54.65
    STEP: updating /approval 08/17/23 04:15:54.658
    STEP: getting /status 08/17/23 04:15:54.666
    STEP: patching /status 08/17/23 04:15:54.67
    STEP: updating /status 08/17/23 04:15:54.678
    STEP: deleting 08/17/23 04:15:54.686
    STEP: deleting a collection 08/17/23 04:15:54.7
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:54.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-2618" for this suite. 08/17/23 04:15:54.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:54.734
Aug 17 04:15:54.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:15:54.736
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:54.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:54.755
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/17/23 04:15:54.759
Aug 17 04:15:54.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc" in namespace "downward-api-4703" to be "Succeeded or Failed"
Aug 17 04:15:54.774: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933494ms
Aug 17 04:15:56.779: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010075619s
Aug 17 04:15:58.780: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011243923s
STEP: Saw pod success 08/17/23 04:15:58.78
Aug 17 04:15:58.781: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc" satisfied condition "Succeeded or Failed"
Aug 17 04:15:58.785: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc container client-container: <nil>
STEP: delete the pod 08/17/23 04:15:58.795
Aug 17 04:15:58.806: INFO: Waiting for pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc to disappear
Aug 17 04:15:58.810: INFO: Pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 04:15:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4703" for this suite. 08/17/23 04:15:58.817
------------------------------
• [4.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:54.734
    Aug 17 04:15:54.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:15:54.736
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:54.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:54.755
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/17/23 04:15:54.759
    Aug 17 04:15:54.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc" in namespace "downward-api-4703" to be "Succeeded or Failed"
    Aug 17 04:15:54.774: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.933494ms
    Aug 17 04:15:56.779: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010075619s
    Aug 17 04:15:58.780: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011243923s
    STEP: Saw pod success 08/17/23 04:15:58.78
    Aug 17 04:15:58.781: INFO: Pod "downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc" satisfied condition "Succeeded or Failed"
    Aug 17 04:15:58.785: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc container client-container: <nil>
    STEP: delete the pod 08/17/23 04:15:58.795
    Aug 17 04:15:58.806: INFO: Waiting for pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc to disappear
    Aug 17 04:15:58.810: INFO: Pod downwardapi-volume-7bc72cd3-22ef-4307-b3b1-edcd281fe0cc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:15:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4703" for this suite. 08/17/23 04:15:58.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:15:58.826
Aug 17 04:15:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename job 08/17/23 04:15:58.827
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:58.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:58.845
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/17/23 04:15:58.849
STEP: Ensuring job reaches completions 08/17/23 04:15:58.855
STEP: Ensuring pods with index for job exist 08/17/23 04:16:06.861
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:06.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7355" for this suite. 08/17/23 04:16:06.876
------------------------------
• [SLOW TEST] [8.059 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:15:58.826
    Aug 17 04:15:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename job 08/17/23 04:15:58.827
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:15:58.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:15:58.845
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/17/23 04:15:58.849
    STEP: Ensuring job reaches completions 08/17/23 04:15:58.855
    STEP: Ensuring pods with index for job exist 08/17/23 04:16:06.861
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:06.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7355" for this suite. 08/17/23 04:16:06.876
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:06.886
Aug 17 04:16:06.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 04:16:06.886
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:06.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:06.907
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-tlhvb" 08/17/23 04:16:06.912
Aug 17 04:16:06.920: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
Aug 17 04:16:07.925: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
Aug 17 04:16:07.930: INFO: Found 1 replicas for "e2e-rc-tlhvb" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-tlhvb" 08/17/23 04:16:07.93
STEP: Updating a scale subresource 08/17/23 04:16:07.934
STEP: Verifying replicas where modified for replication controller "e2e-rc-tlhvb" 08/17/23 04:16:07.94
Aug 17 04:16:07.940: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
Aug 17 04:16:08.944: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
Aug 17 04:16:08.949: INFO: Found 2 replicas for "e2e-rc-tlhvb" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:08.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4484" for this suite. 08/17/23 04:16:08.957
------------------------------
• [2.081 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:06.886
    Aug 17 04:16:06.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 04:16:06.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:06.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:06.907
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-tlhvb" 08/17/23 04:16:06.912
    Aug 17 04:16:06.920: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
    Aug 17 04:16:07.925: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
    Aug 17 04:16:07.930: INFO: Found 1 replicas for "e2e-rc-tlhvb" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-tlhvb" 08/17/23 04:16:07.93
    STEP: Updating a scale subresource 08/17/23 04:16:07.934
    STEP: Verifying replicas where modified for replication controller "e2e-rc-tlhvb" 08/17/23 04:16:07.94
    Aug 17 04:16:07.940: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
    Aug 17 04:16:08.944: INFO: Get Replication Controller "e2e-rc-tlhvb" to confirm replicas
    Aug 17 04:16:08.949: INFO: Found 2 replicas for "e2e-rc-tlhvb" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:08.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4484" for this suite. 08/17/23 04:16:08.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:08.975
Aug 17 04:16:08.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename init-container 08/17/23 04:16:08.976
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:08.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:08.993
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/17/23 04:16:08.998
Aug 17 04:16:08.999: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:14.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8972" for this suite. 08/17/23 04:16:14.5
------------------------------
• [SLOW TEST] [5.538 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:08.975
    Aug 17 04:16:08.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename init-container 08/17/23 04:16:08.976
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:08.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:08.993
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/17/23 04:16:08.998
    Aug 17 04:16:08.999: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:14.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8972" for this suite. 08/17/23 04:16:14.5
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:14.513
Aug 17 04:16:14.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename endpointslice 08/17/23 04:16:14.514
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:14.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:14.532
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1141" for this suite. 08/17/23 04:16:14.584
------------------------------
• [0.078 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:14.513
    Aug 17 04:16:14.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename endpointslice 08/17/23 04:16:14.514
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:14.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:14.532
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1141" for this suite. 08/17/23 04:16:14.584
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:14.591
Aug 17 04:16:14.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename init-container 08/17/23 04:16:14.592
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:14.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:14.614
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/17/23 04:16:14.62
Aug 17 04:16:14.620: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:19.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4639" for this suite. 08/17/23 04:16:19.518
------------------------------
• [4.934 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:14.591
    Aug 17 04:16:14.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename init-container 08/17/23 04:16:14.592
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:14.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:14.614
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/17/23 04:16:14.62
    Aug 17 04:16:14.620: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:19.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4639" for this suite. 08/17/23 04:16:19.518
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:19.526
Aug 17 04:16:19.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:16:19.527
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:19.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:19.542
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/17/23 04:16:19.546
Aug 17 04:16:19.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078" in namespace "downward-api-1516" to be "Succeeded or Failed"
Aug 17 04:16:19.560: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Pending", Reason="", readiness=false. Elapsed: 4.602775ms
Aug 17 04:16:21.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010297996s
Aug 17 04:16:23.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010893008s
STEP: Saw pod success 08/17/23 04:16:23.566
Aug 17 04:16:23.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078" satisfied condition "Succeeded or Failed"
Aug 17 04:16:23.571: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 container client-container: <nil>
STEP: delete the pod 08/17/23 04:16:23.62
Aug 17 04:16:23.631: INFO: Waiting for pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 to disappear
Aug 17 04:16:23.637: INFO: Pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 04:16:23.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1516" for this suite. 08/17/23 04:16:23.644
------------------------------
• [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:19.526
    Aug 17 04:16:19.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:16:19.527
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:19.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:19.542
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/17/23 04:16:19.546
    Aug 17 04:16:19.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078" in namespace "downward-api-1516" to be "Succeeded or Failed"
    Aug 17 04:16:19.560: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Pending", Reason="", readiness=false. Elapsed: 4.602775ms
    Aug 17 04:16:21.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010297996s
    Aug 17 04:16:23.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010893008s
    STEP: Saw pod success 08/17/23 04:16:23.566
    Aug 17 04:16:23.566: INFO: Pod "downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078" satisfied condition "Succeeded or Failed"
    Aug 17 04:16:23.571: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 container client-container: <nil>
    STEP: delete the pod 08/17/23 04:16:23.62
    Aug 17 04:16:23.631: INFO: Waiting for pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 to disappear
    Aug 17 04:16:23.637: INFO: Pod downwardapi-volume-1e3104e2-7e07-4cb7-9b4b-4a4863935078 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:16:23.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1516" for this suite. 08/17/23 04:16:23.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:16:23.654
Aug 17 04:16:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 04:16:23.655
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:23.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:23.671
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 in namespace container-probe-4598 08/17/23 04:16:23.675
Aug 17 04:16:23.683: INFO: Waiting up to 5m0s for pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604" in namespace "container-probe-4598" to be "not pending"
Aug 17 04:16:23.687: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396679ms
Aug 17 04:16:25.693: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604": Phase="Running", Reason="", readiness=true. Elapsed: 2.009403269s
Aug 17 04:16:25.693: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604" satisfied condition "not pending"
Aug 17 04:16:25.693: INFO: Started pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 in namespace container-probe-4598
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:16:25.693
Aug 17 04:16:25.697: INFO: Initial restart count of pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 is 0
STEP: deleting the pod 08/17/23 04:20:26.403
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 04:20:26.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4598" for this suite. 08/17/23 04:20:26.436
------------------------------
• [SLOW TEST] [242.790 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:16:23.654
    Aug 17 04:16:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 04:16:23.655
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:16:23.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:16:23.671
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 in namespace container-probe-4598 08/17/23 04:16:23.675
    Aug 17 04:16:23.683: INFO: Waiting up to 5m0s for pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604" in namespace "container-probe-4598" to be "not pending"
    Aug 17 04:16:23.687: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396679ms
    Aug 17 04:16:25.693: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604": Phase="Running", Reason="", readiness=true. Elapsed: 2.009403269s
    Aug 17 04:16:25.693: INFO: Pod "busybox-ee314c30-18c3-47bf-8376-bd85a278a604" satisfied condition "not pending"
    Aug 17 04:16:25.693: INFO: Started pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 in namespace container-probe-4598
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:16:25.693
    Aug 17 04:16:25.697: INFO: Initial restart count of pod busybox-ee314c30-18c3-47bf-8376-bd85a278a604 is 0
    STEP: deleting the pod 08/17/23 04:20:26.403
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:20:26.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4598" for this suite. 08/17/23 04:20:26.436
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:20:26.446
Aug 17 04:20:26.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-runtime 08/17/23 04:20:26.447
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:20:26.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:20:26.467
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/17/23 04:20:26.472
STEP: wait for the container to reach Failed 08/17/23 04:20:26.481
STEP: get the container status 08/17/23 04:20:30.512
STEP: the container should be terminated 08/17/23 04:20:30.517
STEP: the termination message should be set 08/17/23 04:20:30.517
Aug 17 04:20:30.517: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/17/23 04:20:30.517
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 04:20:30.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9571" for this suite. 08/17/23 04:20:30.542
------------------------------
• [4.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:20:26.446
    Aug 17 04:20:26.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-runtime 08/17/23 04:20:26.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:20:26.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:20:26.467
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/17/23 04:20:26.472
    STEP: wait for the container to reach Failed 08/17/23 04:20:26.481
    STEP: get the container status 08/17/23 04:20:30.512
    STEP: the container should be terminated 08/17/23 04:20:30.517
    STEP: the termination message should be set 08/17/23 04:20:30.517
    Aug 17 04:20:30.517: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/17/23 04:20:30.517
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:20:30.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9571" for this suite. 08/17/23 04:20:30.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:20:30.551
Aug 17 04:20:30.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:20:30.551
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:20:30.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:20:30.571
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3116 08/17/23 04:20:30.576
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/17/23 04:20:30.583
STEP: Creating stateful set ss in namespace statefulset-3116 08/17/23 04:20:30.587
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3116 08/17/23 04:20:30.592
Aug 17 04:20:30.597: INFO: Found 0 stateful pods, waiting for 1
Aug 17 04:20:40.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/17/23 04:20:40.603
Aug 17 04:20:40.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:20:40.955: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:20:40.955: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:20:40.955: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:20:40.960: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 17 04:20:50.966: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:20:50.966: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:20:50.983: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999809s
Aug 17 04:20:51.988: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995954896s
Aug 17 04:20:52.994: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990367666s
Aug 17 04:20:53.999: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985688s
Aug 17 04:20:55.005: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979864947s
Aug 17 04:20:56.010: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974322148s
Aug 17 04:20:57.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968588586s
Aug 17 04:20:58.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96373229s
Aug 17 04:20:59.026: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958627484s
Aug 17 04:21:00.033: INFO: Verifying statefulset ss doesn't scale past 1 for another 953.794093ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3116 08/17/23 04:21:01.034
Aug 17 04:21:01.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:01.564: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:21:01.564: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:21:01.564: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:21:01.568: INFO: Found 1 stateful pods, waiting for 3
Aug 17 04:21:11.578: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:21:11.578: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:21:11.578: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/17/23 04:21:11.578
STEP: Scale down will halt with unhealthy stateful pod 08/17/23 04:21:11.578
Aug 17 04:21:11.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:21:12.029: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:21:12.029: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:21:12.029: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:21:12.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:21:12.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:21:12.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:21:12.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:21:12.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 17 04:21:13.001: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 17 04:21:13.001: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 17 04:21:13.001: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 17 04:21:13.001: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:21:13.006: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 17 04:21:23.016: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:21:23.016: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:21:23.016: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 17 04:21:23.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999304s
Aug 17 04:21:24.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995440356s
Aug 17 04:21:25.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989164238s
Aug 17 04:21:26.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98361313s
Aug 17 04:21:27.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977628675s
Aug 17 04:21:28.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96648858s
Aug 17 04:21:29.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960731566s
Aug 17 04:21:30.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955418894s
Aug 17 04:21:31.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949591083s
Aug 17 04:21:32.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.656326ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3116 08/17/23 04:21:33.088
Aug 17 04:21:33.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:33.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:21:33.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:21:33.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:21:33.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:33.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 17 04:21:33.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 17 04:21:33.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 17 04:21:33.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:34.470: INFO: rc: 1
Aug 17 04:21:34.470: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: container is in CONTAINER_EXITED state

error:
exit status 1
Aug 17 04:21:44.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:44.534: INFO: rc: 1
Aug 17 04:21:44.534: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:21:54.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:21:54.606: INFO: rc: 1
Aug 17 04:21:54.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:04.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:04.684: INFO: rc: 1
Aug 17 04:22:04.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:14.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:14.758: INFO: rc: 1
Aug 17 04:22:14.758: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:24.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:24.826: INFO: rc: 1
Aug 17 04:22:24.826: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:34.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:34.894: INFO: rc: 1
Aug 17 04:22:34.894: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:44.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:44.962: INFO: rc: 1
Aug 17 04:22:44.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:22:54.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:22:55.033: INFO: rc: 1
Aug 17 04:22:55.033: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:05.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:05.116: INFO: rc: 1
Aug 17 04:23:05.116: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:15.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:15.196: INFO: rc: 1
Aug 17 04:23:15.196: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:25.270: INFO: rc: 1
Aug 17 04:23:25.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:35.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:35.347: INFO: rc: 1
Aug 17 04:23:35.347: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:45.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:45.414: INFO: rc: 1
Aug 17 04:23:45.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:23:55.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:23:55.485: INFO: rc: 1
Aug 17 04:23:55.485: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:05.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:05.563: INFO: rc: 1
Aug 17 04:24:05.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:15.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:15.635: INFO: rc: 1
Aug 17 04:24:15.635: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:25.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:25.704: INFO: rc: 1
Aug 17 04:24:25.705: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:35.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:35.773: INFO: rc: 1
Aug 17 04:24:35.773: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:45.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:45.842: INFO: rc: 1
Aug 17 04:24:45.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:24:55.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:24:55.914: INFO: rc: 1
Aug 17 04:24:55.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:05.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:05.983: INFO: rc: 1
Aug 17 04:25:05.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:15.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:16.058: INFO: rc: 1
Aug 17 04:25:16.058: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:26.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:26.127: INFO: rc: 1
Aug 17 04:25:26.127: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:36.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:36.194: INFO: rc: 1
Aug 17 04:25:36.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:46.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:46.261: INFO: rc: 1
Aug 17 04:25:46.261: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:25:56.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:25:56.333: INFO: rc: 1
Aug 17 04:25:56.333: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:26:06.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:26:06.411: INFO: rc: 1
Aug 17 04:26:06.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:26:16.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:26:16.481: INFO: rc: 1
Aug 17 04:26:16.481: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:26:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:26:26.549: INFO: rc: 1
Aug 17 04:26:26.549: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 17 04:26:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 17 04:26:36.623: INFO: rc: 1
Aug 17 04:26:36.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Aug 17 04:26:36.623: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/17/23 04:26:36.639
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:26:36.640: INFO: Deleting all statefulset in ns statefulset-3116
Aug 17 04:26:36.644: INFO: Scaling statefulset ss to 0
Aug 17 04:26:36.659: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:26:36.663: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:26:36.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3116" for this suite. 08/17/23 04:26:36.685
------------------------------
• [SLOW TEST] [366.143 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:20:30.551
    Aug 17 04:20:30.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:20:30.551
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:20:30.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:20:30.571
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3116 08/17/23 04:20:30.576
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/17/23 04:20:30.583
    STEP: Creating stateful set ss in namespace statefulset-3116 08/17/23 04:20:30.587
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3116 08/17/23 04:20:30.592
    Aug 17 04:20:30.597: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 04:20:40.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/17/23 04:20:40.603
    Aug 17 04:20:40.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:20:40.955: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:20:40.955: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:20:40.955: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:20:40.960: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 17 04:20:50.966: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:20:50.966: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:20:50.983: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999809s
    Aug 17 04:20:51.988: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995954896s
    Aug 17 04:20:52.994: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990367666s
    Aug 17 04:20:53.999: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985688s
    Aug 17 04:20:55.005: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979864947s
    Aug 17 04:20:56.010: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974322148s
    Aug 17 04:20:57.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968588586s
    Aug 17 04:20:58.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96373229s
    Aug 17 04:20:59.026: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958627484s
    Aug 17 04:21:00.033: INFO: Verifying statefulset ss doesn't scale past 1 for another 953.794093ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3116 08/17/23 04:21:01.034
    Aug 17 04:21:01.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:01.564: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 04:21:01.564: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:21:01.564: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:21:01.568: INFO: Found 1 stateful pods, waiting for 3
    Aug 17 04:21:11.578: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:21:11.578: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:21:11.578: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/17/23 04:21:11.578
    STEP: Scale down will halt with unhealthy stateful pod 08/17/23 04:21:11.578
    Aug 17 04:21:11.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:21:12.029: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:21:12.029: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:21:12.029: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:21:12.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:21:12.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:21:12.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:21:12.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:21:12.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 17 04:21:13.001: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 17 04:21:13.001: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 17 04:21:13.001: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 17 04:21:13.001: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:21:13.006: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 17 04:21:23.016: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:21:23.016: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:21:23.016: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 17 04:21:23.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999304s
    Aug 17 04:21:24.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995440356s
    Aug 17 04:21:25.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989164238s
    Aug 17 04:21:26.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98361313s
    Aug 17 04:21:27.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977628675s
    Aug 17 04:21:28.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96648858s
    Aug 17 04:21:29.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960731566s
    Aug 17 04:21:30.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955418894s
    Aug 17 04:21:31.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949591083s
    Aug 17 04:21:32.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.656326ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3116 08/17/23 04:21:33.088
    Aug 17 04:21:33.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:33.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 04:21:33.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:21:33.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:21:33.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:33.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 17 04:21:33.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 17 04:21:33.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 17 04:21:33.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:34.470: INFO: rc: 1
    Aug 17 04:21:34.470: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: failed to exec in container: container is in CONTAINER_EXITED state

    error:
    exit status 1
    Aug 17 04:21:44.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:44.534: INFO: rc: 1
    Aug 17 04:21:44.534: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:21:54.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:21:54.606: INFO: rc: 1
    Aug 17 04:21:54.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:04.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:04.684: INFO: rc: 1
    Aug 17 04:22:04.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:14.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:14.758: INFO: rc: 1
    Aug 17 04:22:14.758: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:24.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:24.826: INFO: rc: 1
    Aug 17 04:22:24.826: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:34.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:34.894: INFO: rc: 1
    Aug 17 04:22:34.894: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:44.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:44.962: INFO: rc: 1
    Aug 17 04:22:44.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:22:54.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:22:55.033: INFO: rc: 1
    Aug 17 04:22:55.033: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:05.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:05.116: INFO: rc: 1
    Aug 17 04:23:05.116: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:15.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:15.196: INFO: rc: 1
    Aug 17 04:23:15.196: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:25.270: INFO: rc: 1
    Aug 17 04:23:25.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:35.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:35.347: INFO: rc: 1
    Aug 17 04:23:35.347: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:45.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:45.414: INFO: rc: 1
    Aug 17 04:23:45.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:23:55.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:23:55.485: INFO: rc: 1
    Aug 17 04:23:55.485: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:05.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:05.563: INFO: rc: 1
    Aug 17 04:24:05.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:15.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:15.635: INFO: rc: 1
    Aug 17 04:24:15.635: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:25.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:25.704: INFO: rc: 1
    Aug 17 04:24:25.705: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:35.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:35.773: INFO: rc: 1
    Aug 17 04:24:35.773: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:45.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:45.842: INFO: rc: 1
    Aug 17 04:24:45.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:24:55.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:24:55.914: INFO: rc: 1
    Aug 17 04:24:55.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:05.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:05.983: INFO: rc: 1
    Aug 17 04:25:05.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:15.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:16.058: INFO: rc: 1
    Aug 17 04:25:16.058: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:26.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:26.127: INFO: rc: 1
    Aug 17 04:25:26.127: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:36.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:36.194: INFO: rc: 1
    Aug 17 04:25:36.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:46.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:46.261: INFO: rc: 1
    Aug 17 04:25:46.261: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:25:56.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:25:56.333: INFO: rc: 1
    Aug 17 04:25:56.333: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:26:06.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:26:06.411: INFO: rc: 1
    Aug 17 04:26:06.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:26:16.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:26:16.481: INFO: rc: 1
    Aug 17 04:26:16.481: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:26:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:26:26.549: INFO: rc: 1
    Aug 17 04:26:26.549: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Aug 17 04:26:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=statefulset-3116 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 17 04:26:36.623: INFO: rc: 1
    Aug 17 04:26:36.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Aug 17 04:26:36.623: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/17/23 04:26:36.639
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:26:36.640: INFO: Deleting all statefulset in ns statefulset-3116
    Aug 17 04:26:36.644: INFO: Scaling statefulset ss to 0
    Aug 17 04:26:36.659: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:26:36.663: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:26:36.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3116" for this suite. 08/17/23 04:26:36.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:26:36.695
Aug 17 04:26:36.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 04:26:36.695
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:36.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:36.716
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 04:26:36.745
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 04:26:36.751
Aug 17 04:26:36.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:26:36.761: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 04:26:37.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:26:37.773: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 04:26:38.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 04:26:38.774: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 08/17/23 04:26:38.778
STEP: DeleteCollection of the DaemonSets 08/17/23 04:26:38.784
STEP: Verify that ReplicaSets have been deleted 08/17/23 04:26:38.793
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Aug 17 04:26:38.806: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"248930"},"items":null}

Aug 17 04:26:38.811: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"248930"},"items":[{"metadata":{"name":"daemon-set-ch6s8","generateName":"daemon-set-","namespace":"daemonsets-7838","uid":"236df5b2-c7aa-4754-a9d7-f594b9dc8662","resourceVersion":"248929","creationTimestamp":"2023-08-17T04:26:36Z","deletionTimestamp":"2023-08-17T04:27:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3a982ad78919b60f0db0ac2393d578a509b257dd29ed19746dde79091b605c9d","cni.projectcalico.org/podIP":"172.21.86.151/32","cni.projectcalico.org/podIPs":"172.21.86.151/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4470ca9a-7a44-4f78-9e4a-5a265406d669","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4470ca9a-7a44-4f78-9e4a-5a265406d669\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7mtcb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7mtcb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-6c9d465fc4xbjl6l-wz7jz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-6c9d465fc4xbjl6l-wz7jz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"}],"hostIP":"192.168.11.3","podIP":"172.21.86.151","podIPs":[{"ip":"172.21.86.151"}],"startTime":"2023-08-17T04:26:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T04:26:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3252f8666737f7493b5b9f4f1f8988620e6e9e445ecbcbc7e14441458eafeafd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fxqmp","generateName":"daemon-set-","namespace":"daemonsets-7838","uid":"309b6ebc-465e-42d6-95a7-d12288d0438f","resourceVersion":"248928","creationTimestamp":"2023-08-17T04:26:36Z","deletionTimestamp":"2023-08-17T04:27:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1d4f3f8b6876442383fb41021d520d6f7e743354e326447e6f4774a73d809eec","cni.projectcalico.org/podIP":"172.21.15.111/32","cni.projectcalico.org/podIPs":"172.21.15.111/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4470ca9a-7a44-4f78-9e4a-5a265406d669","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4470ca9a-7a44-4f78-9e4a-5a265406d669\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-67w64","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-67w64","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-ubuntu-79fff84d86x69988-vjwlx","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-ubuntu-79fff84d86x69988-vjwlx"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"}],"hostIP":"192.168.11.4","podIP":"172.21.15.111","podIPs":[{"ip":"172.21.15.111"}],"startTime":"2023-08-17T04:26:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T04:26:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://abcb30f17972603a3e847e0dbb181fa4f3aa2a3d6722de352883ffa8604c5e2f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:26:38.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7838" for this suite. 08/17/23 04:26:38.837
------------------------------
• [2.150 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:26:36.695
    Aug 17 04:26:36.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 04:26:36.695
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:36.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:36.716
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 04:26:36.745
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 04:26:36.751
    Aug 17 04:26:36.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:26:36.761: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 04:26:37.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:26:37.773: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 04:26:38.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 04:26:38.774: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 08/17/23 04:26:38.778
    STEP: DeleteCollection of the DaemonSets 08/17/23 04:26:38.784
    STEP: Verify that ReplicaSets have been deleted 08/17/23 04:26:38.793
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Aug 17 04:26:38.806: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"248930"},"items":null}

    Aug 17 04:26:38.811: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"248930"},"items":[{"metadata":{"name":"daemon-set-ch6s8","generateName":"daemon-set-","namespace":"daemonsets-7838","uid":"236df5b2-c7aa-4754-a9d7-f594b9dc8662","resourceVersion":"248929","creationTimestamp":"2023-08-17T04:26:36Z","deletionTimestamp":"2023-08-17T04:27:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3a982ad78919b60f0db0ac2393d578a509b257dd29ed19746dde79091b605c9d","cni.projectcalico.org/podIP":"172.21.86.151/32","cni.projectcalico.org/podIPs":"172.21.86.151/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4470ca9a-7a44-4f78-9e4a-5a265406d669","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4470ca9a-7a44-4f78-9e4a-5a265406d669\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7mtcb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7mtcb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-6c9d465fc4xbjl6l-wz7jz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-6c9d465fc4xbjl6l-wz7jz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"}],"hostIP":"192.168.11.3","podIP":"172.21.86.151","podIPs":[{"ip":"172.21.86.151"}],"startTime":"2023-08-17T04:26:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T04:26:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3252f8666737f7493b5b9f4f1f8988620e6e9e445ecbcbc7e14441458eafeafd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fxqmp","generateName":"daemon-set-","namespace":"daemonsets-7838","uid":"309b6ebc-465e-42d6-95a7-d12288d0438f","resourceVersion":"248928","creationTimestamp":"2023-08-17T04:26:36Z","deletionTimestamp":"2023-08-17T04:27:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1d4f3f8b6876442383fb41021d520d6f7e743354e326447e6f4774a73d809eec","cni.projectcalico.org/podIP":"172.21.15.111/32","cni.projectcalico.org/podIPs":"172.21.15.111/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4470ca9a-7a44-4f78-9e4a-5a265406d669","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4470ca9a-7a44-4f78-9e4a-5a265406d669\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-17T04:26:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-67w64","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-67w64","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-ubuntu-79fff84d86x69988-vjwlx","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-ubuntu-79fff84d86x69988-vjwlx"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-17T04:26:36Z"}],"hostIP":"192.168.11.4","podIP":"172.21.15.111","podIPs":[{"ip":"172.21.15.111"}],"startTime":"2023-08-17T04:26:36Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-17T04:26:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://abcb30f17972603a3e847e0dbb181fa4f3aa2a3d6722de352883ffa8604c5e2f","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:26:38.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7838" for this suite. 08/17/23 04:26:38.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:26:38.846
Aug 17 04:26:38.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 04:26:38.847
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:38.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:38.867
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/17/23 04:26:38.872
Aug 17 04:26:38.885: INFO: Waiting up to 5m0s for pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5" in namespace "emptydir-9021" to be "Succeeded or Failed"
Aug 17 04:26:38.889: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.60747ms
Aug 17 04:26:40.895: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010108096s
Aug 17 04:26:42.896: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01089556s
STEP: Saw pod success 08/17/23 04:26:42.896
Aug 17 04:26:42.896: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5" satisfied condition "Succeeded or Failed"
Aug 17 04:26:42.901: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 container test-container: <nil>
STEP: delete the pod 08/17/23 04:26:42.914
Aug 17 04:26:42.926: INFO: Waiting for pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 to disappear
Aug 17 04:26:42.931: INFO: Pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 04:26:42.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9021" for this suite. 08/17/23 04:26:42.938
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:26:38.846
    Aug 17 04:26:38.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 04:26:38.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:38.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:38.867
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/17/23 04:26:38.872
    Aug 17 04:26:38.885: INFO: Waiting up to 5m0s for pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5" in namespace "emptydir-9021" to be "Succeeded or Failed"
    Aug 17 04:26:38.889: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.60747ms
    Aug 17 04:26:40.895: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010108096s
    Aug 17 04:26:42.896: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01089556s
    STEP: Saw pod success 08/17/23 04:26:42.896
    Aug 17 04:26:42.896: INFO: Pod "pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5" satisfied condition "Succeeded or Failed"
    Aug 17 04:26:42.901: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 container test-container: <nil>
    STEP: delete the pod 08/17/23 04:26:42.914
    Aug 17 04:26:42.926: INFO: Waiting for pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 to disappear
    Aug 17 04:26:42.931: INFO: Pod pod-091b5126-44bd-4cfc-b9cd-6067e1e68ab5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:26:42.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9021" for this suite. 08/17/23 04:26:42.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:26:42.95
Aug 17 04:26:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:26:42.95
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:42.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:42.967
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-6f069fcc-a48a-4743-b549-61f59bfb4923 08/17/23 04:26:42.971
STEP: Creating a pod to test consume secrets 08/17/23 04:26:42.977
Aug 17 04:26:42.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80" in namespace "projected-3166" to be "Succeeded or Failed"
Aug 17 04:26:42.995: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14791ms
Aug 17 04:26:45.000: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009027185s
Aug 17 04:26:47.002: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011205424s
STEP: Saw pod success 08/17/23 04:26:47.002
Aug 17 04:26:47.002: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80" satisfied condition "Succeeded or Failed"
Aug 17 04:26:47.007: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:26:47.018
Aug 17 04:26:47.031: INFO: Waiting for pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 to disappear
Aug 17 04:26:47.035: INFO: Pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 04:26:47.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3166" for this suite. 08/17/23 04:26:47.042
------------------------------
• [4.101 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:26:42.95
    Aug 17 04:26:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:26:42.95
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:42.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:42.967
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-6f069fcc-a48a-4743-b549-61f59bfb4923 08/17/23 04:26:42.971
    STEP: Creating a pod to test consume secrets 08/17/23 04:26:42.977
    Aug 17 04:26:42.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80" in namespace "projected-3166" to be "Succeeded or Failed"
    Aug 17 04:26:42.995: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14791ms
    Aug 17 04:26:45.000: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009027185s
    Aug 17 04:26:47.002: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011205424s
    STEP: Saw pod success 08/17/23 04:26:47.002
    Aug 17 04:26:47.002: INFO: Pod "pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80" satisfied condition "Succeeded or Failed"
    Aug 17 04:26:47.007: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:26:47.018
    Aug 17 04:26:47.031: INFO: Waiting for pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 to disappear
    Aug 17 04:26:47.035: INFO: Pod pod-projected-secrets-49fef140-c22a-4f1b-a4f3-1b9ef3925b80 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:26:47.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3166" for this suite. 08/17/23 04:26:47.042
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:26:47.052
Aug 17 04:26:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename watch 08/17/23 04:26:47.053
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:47.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:47.078
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/17/23 04:26:47.084
STEP: starting a background goroutine to produce watch events 08/17/23 04:26:47.089
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/17/23 04:26:47.089
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 04:26:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9504" for this suite. 08/17/23 04:26:49.91
------------------------------
• [2.909 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:26:47.052
    Aug 17 04:26:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename watch 08/17/23 04:26:47.053
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:47.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:47.078
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/17/23 04:26:47.084
    STEP: starting a background goroutine to produce watch events 08/17/23 04:26:47.089
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/17/23 04:26:47.089
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:26:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9504" for this suite. 08/17/23 04:26:49.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:26:49.963
Aug 17 04:26:49.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:26:49.965
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:49.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:49.984
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1889 08/17/23 04:26:49.989
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1889 08/17/23 04:26:49.996
Aug 17 04:26:50.004: INFO: Found 0 stateful pods, waiting for 1
Aug 17 04:27:00.016: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/17/23 04:27:00.03
STEP: updating a scale subresource 08/17/23 04:27:00.036
STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 04:27:00.047
STEP: Patch a scale subresource 08/17/23 04:27:00.058
STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 04:27:00.07
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:27:00.077: INFO: Deleting all statefulset in ns statefulset-1889
Aug 17 04:27:00.084: INFO: Scaling statefulset ss to 0
Aug 17 04:27:10.138: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:27:10.143: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:10.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1889" for this suite. 08/17/23 04:27:10.166
------------------------------
• [SLOW TEST] [20.210 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:26:49.963
    Aug 17 04:26:49.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:26:49.965
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:26:49.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:26:49.984
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1889 08/17/23 04:26:49.989
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1889 08/17/23 04:26:49.996
    Aug 17 04:26:50.004: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 04:27:00.016: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/17/23 04:27:00.03
    STEP: updating a scale subresource 08/17/23 04:27:00.036
    STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 04:27:00.047
    STEP: Patch a scale subresource 08/17/23 04:27:00.058
    STEP: verifying the statefulset Spec.Replicas was modified 08/17/23 04:27:00.07
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:27:00.077: INFO: Deleting all statefulset in ns statefulset-1889
    Aug 17 04:27:00.084: INFO: Scaling statefulset ss to 0
    Aug 17 04:27:10.138: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:27:10.143: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:10.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1889" for this suite. 08/17/23 04:27:10.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:10.175
Aug 17 04:27:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 04:27:10.176
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:10.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:10.197
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/17/23 04:27:10.202
STEP: Counting existing ResourceQuota 08/17/23 04:27:15.207
STEP: Creating a ResourceQuota 08/17/23 04:27:20.214
STEP: Ensuring resource quota status is calculated 08/17/23 04:27:20.222
STEP: Creating a Secret 08/17/23 04:27:22.228
STEP: Ensuring resource quota status captures secret creation 08/17/23 04:27:22.241
STEP: Deleting a secret 08/17/23 04:27:24.247
STEP: Ensuring resource quota status released usage 08/17/23 04:27:24.256
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:26.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-958" for this suite. 08/17/23 04:27:26.271
------------------------------
• [SLOW TEST] [16.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:10.175
    Aug 17 04:27:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 04:27:10.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:10.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:10.197
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/17/23 04:27:10.202
    STEP: Counting existing ResourceQuota 08/17/23 04:27:15.207
    STEP: Creating a ResourceQuota 08/17/23 04:27:20.214
    STEP: Ensuring resource quota status is calculated 08/17/23 04:27:20.222
    STEP: Creating a Secret 08/17/23 04:27:22.228
    STEP: Ensuring resource quota status captures secret creation 08/17/23 04:27:22.241
    STEP: Deleting a secret 08/17/23 04:27:24.247
    STEP: Ensuring resource quota status released usage 08/17/23 04:27:24.256
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:26.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-958" for this suite. 08/17/23 04:27:26.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:26.282
Aug 17 04:27:26.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename events 08/17/23 04:27:26.283
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:26.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:26.305
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/17/23 04:27:26.31
STEP: listing events in all namespaces 08/17/23 04:27:26.317
STEP: listing events in test namespace 08/17/23 04:27:26.322
STEP: listing events with field selection filtering on source 08/17/23 04:27:26.328
STEP: listing events with field selection filtering on reportingController 08/17/23 04:27:26.334
STEP: getting the test event 08/17/23 04:27:26.338
STEP: patching the test event 08/17/23 04:27:26.344
STEP: getting the test event 08/17/23 04:27:26.355
STEP: updating the test event 08/17/23 04:27:26.363
STEP: getting the test event 08/17/23 04:27:26.375
STEP: deleting the test event 08/17/23 04:27:26.38
STEP: listing events in all namespaces 08/17/23 04:27:26.392
STEP: listing events in test namespace 08/17/23 04:27:26.397
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:26.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6827" for this suite. 08/17/23 04:27:26.411
------------------------------
• [0.137 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:26.282
    Aug 17 04:27:26.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename events 08/17/23 04:27:26.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:26.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:26.305
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/17/23 04:27:26.31
    STEP: listing events in all namespaces 08/17/23 04:27:26.317
    STEP: listing events in test namespace 08/17/23 04:27:26.322
    STEP: listing events with field selection filtering on source 08/17/23 04:27:26.328
    STEP: listing events with field selection filtering on reportingController 08/17/23 04:27:26.334
    STEP: getting the test event 08/17/23 04:27:26.338
    STEP: patching the test event 08/17/23 04:27:26.344
    STEP: getting the test event 08/17/23 04:27:26.355
    STEP: updating the test event 08/17/23 04:27:26.363
    STEP: getting the test event 08/17/23 04:27:26.375
    STEP: deleting the test event 08/17/23 04:27:26.38
    STEP: listing events in all namespaces 08/17/23 04:27:26.392
    STEP: listing events in test namespace 08/17/23 04:27:26.397
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:26.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6827" for this suite. 08/17/23 04:27:26.411
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:26.421
Aug 17 04:27:26.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 04:27:26.422
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:26.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:26.445
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-8732 08/17/23 04:27:26.451
STEP: creating service affinity-nodeport in namespace services-8732 08/17/23 04:27:26.451
STEP: creating replication controller affinity-nodeport in namespace services-8732 08/17/23 04:27:26.472
I0817 04:27:26.479917      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8732, replica count: 3
I0817 04:27:29.530656      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:27:29.546: INFO: Creating new exec pod
Aug 17 04:27:29.553: INFO: Waiting up to 5m0s for pod "execpod-affinityfmr5t" in namespace "services-8732" to be "running"
Aug 17 04:27:29.556: INFO: Pod "execpod-affinityfmr5t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709597ms
Aug 17 04:27:31.562: INFO: Pod "execpod-affinityfmr5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009499842s
Aug 17 04:27:31.562: INFO: Pod "execpod-affinityfmr5t" satisfied condition "running"
Aug 17 04:27:32.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 17 04:27:32.988: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 17 04:27:32.988: INFO: stdout: ""
Aug 17 04:27:32.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 172.20.95.237 80'
Aug 17 04:27:33.363: INFO: stderr: "+ nc -v -z -w 2 172.20.95.237 80\nConnection to 172.20.95.237 80 port [tcp/http] succeeded!\n"
Aug 17 04:27:33.363: INFO: stdout: ""
Aug 17 04:27:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 30867'
Aug 17 04:27:33.876: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 30867\nConnection to 192.168.11.3 30867 port [tcp/*] succeeded!\n"
Aug 17 04:27:33.876: INFO: stdout: ""
Aug 17 04:27:33.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 30867'
Aug 17 04:27:34.304: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 30867\nConnection to 192.168.11.4 30867 port [tcp/*] succeeded!\n"
Aug 17 04:27:34.304: INFO: stdout: ""
Aug 17 04:27:34.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:30867/ ; done'
Aug 17 04:27:34.834: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n"
Aug 17 04:27:34.834: INFO: stdout: "\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j"
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
Aug 17 04:27:34.834: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8732, will wait for the garbage collector to delete the pods 08/17/23 04:27:34.849
Aug 17 04:27:34.912: INFO: Deleting ReplicationController affinity-nodeport took: 7.672901ms
Aug 17 04:27:35.013: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.852128ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:37.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8732" for this suite. 08/17/23 04:27:37.243
------------------------------
• [SLOW TEST] [10.830 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:26.421
    Aug 17 04:27:26.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 04:27:26.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:26.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:26.445
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-8732 08/17/23 04:27:26.451
    STEP: creating service affinity-nodeport in namespace services-8732 08/17/23 04:27:26.451
    STEP: creating replication controller affinity-nodeport in namespace services-8732 08/17/23 04:27:26.472
    I0817 04:27:26.479917      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8732, replica count: 3
    I0817 04:27:29.530656      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 04:27:29.546: INFO: Creating new exec pod
    Aug 17 04:27:29.553: INFO: Waiting up to 5m0s for pod "execpod-affinityfmr5t" in namespace "services-8732" to be "running"
    Aug 17 04:27:29.556: INFO: Pod "execpod-affinityfmr5t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709597ms
    Aug 17 04:27:31.562: INFO: Pod "execpod-affinityfmr5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009499842s
    Aug 17 04:27:31.562: INFO: Pod "execpod-affinityfmr5t" satisfied condition "running"
    Aug 17 04:27:32.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 17 04:27:32.988: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 17 04:27:32.988: INFO: stdout: ""
    Aug 17 04:27:32.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 172.20.95.237 80'
    Aug 17 04:27:33.363: INFO: stderr: "+ nc -v -z -w 2 172.20.95.237 80\nConnection to 172.20.95.237 80 port [tcp/http] succeeded!\n"
    Aug 17 04:27:33.363: INFO: stdout: ""
    Aug 17 04:27:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 192.168.11.3 30867'
    Aug 17 04:27:33.876: INFO: stderr: "+ nc -v -z -w 2 192.168.11.3 30867\nConnection to 192.168.11.3 30867 port [tcp/*] succeeded!\n"
    Aug 17 04:27:33.876: INFO: stdout: ""
    Aug 17 04:27:33.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c nc -v -z -w 2 192.168.11.4 30867'
    Aug 17 04:27:34.304: INFO: stderr: "+ nc -v -z -w 2 192.168.11.4 30867\nConnection to 192.168.11.4 30867 port [tcp/*] succeeded!\n"
    Aug 17 04:27:34.304: INFO: stdout: ""
    Aug 17 04:27:34.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=services-8732 exec execpod-affinityfmr5t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:30867/ ; done'
    Aug 17 04:27:34.834: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:30867/\n"
    Aug 17 04:27:34.834: INFO: stdout: "\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j\naffinity-nodeport-kq64j"
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Received response from host: affinity-nodeport-kq64j
    Aug 17 04:27:34.834: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-8732, will wait for the garbage collector to delete the pods 08/17/23 04:27:34.849
    Aug 17 04:27:34.912: INFO: Deleting ReplicationController affinity-nodeport took: 7.672901ms
    Aug 17 04:27:35.013: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.852128ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:37.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8732" for this suite. 08/17/23 04:27:37.243
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:37.253
Aug 17 04:27:37.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 04:27:37.254
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:37.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:37.275
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 17 04:27:37.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: creating the pod 08/17/23 04:27:37.282
STEP: submitting the pod to kubernetes 08/17/23 04:27:37.282
Aug 17 04:27:37.294: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a" in namespace "pods-3385" to be "running and ready"
Aug 17 04:27:37.299: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044395ms
Aug 17 04:27:37.299: INFO: The phase of Pod pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:27:39.306: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011438853s
Aug 17 04:27:39.306: INFO: The phase of Pod pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a is Running (Ready = true)
Aug 17 04:27:39.306: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:39.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3385" for this suite. 08/17/23 04:27:39.378
------------------------------
• [2.136 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:37.253
    Aug 17 04:27:37.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 04:27:37.254
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:37.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:37.275
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 17 04:27:37.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: creating the pod 08/17/23 04:27:37.282
    STEP: submitting the pod to kubernetes 08/17/23 04:27:37.282
    Aug 17 04:27:37.294: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a" in namespace "pods-3385" to be "running and ready"
    Aug 17 04:27:37.299: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044395ms
    Aug 17 04:27:37.299: INFO: The phase of Pod pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:27:39.306: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011438853s
    Aug 17 04:27:39.306: INFO: The phase of Pod pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a is Running (Ready = true)
    Aug 17 04:27:39.306: INFO: Pod "pod-logs-websocket-ac0b356d-9ff3-40e7-ad82-7812f447816a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:39.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3385" for this suite. 08/17/23 04:27:39.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:39.389
Aug 17 04:27:39.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename podtemplate 08/17/23 04:27:39.39
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:39.41
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/17/23 04:27:39.415
STEP: Replace a pod template 08/17/23 04:27:39.421
Aug 17 04:27:39.432: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:39.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9460" for this suite. 08/17/23 04:27:39.439
------------------------------
• [0.058 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:39.389
    Aug 17 04:27:39.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename podtemplate 08/17/23 04:27:39.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:39.41
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/17/23 04:27:39.415
    STEP: Replace a pod template 08/17/23 04:27:39.421
    Aug 17 04:27:39.432: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:39.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9460" for this suite. 08/17/23 04:27:39.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:39.449
Aug 17 04:27:39.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 04:27:39.45
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:39.468
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/17/23 04:27:39.472
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.484
STEP: Creating a service in the namespace 08/17/23 04:27:39.488
STEP: Deleting the namespace 08/17/23 04:27:39.5
STEP: Waiting for the namespace to be removed. 08/17/23 04:27:39.509
STEP: Recreating the namespace 08/17/23 04:27:45.514
STEP: Verifying there is no service in the namespace 08/17/23 04:27:45.527
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:45.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4463" for this suite. 08/17/23 04:27:45.539
STEP: Destroying namespace "nsdeletetest-2956" for this suite. 08/17/23 04:27:45.547
Aug 17 04:27:45.552: INFO: Namespace nsdeletetest-2956 was already deleted
STEP: Destroying namespace "nsdeletetest-563" for this suite. 08/17/23 04:27:45.552
------------------------------
• [SLOW TEST] [6.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:39.449
    Aug 17 04:27:39.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 04:27:39.45
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:39.468
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/17/23 04:27:39.472
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:39.484
    STEP: Creating a service in the namespace 08/17/23 04:27:39.488
    STEP: Deleting the namespace 08/17/23 04:27:39.5
    STEP: Waiting for the namespace to be removed. 08/17/23 04:27:39.509
    STEP: Recreating the namespace 08/17/23 04:27:45.514
    STEP: Verifying there is no service in the namespace 08/17/23 04:27:45.527
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:45.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4463" for this suite. 08/17/23 04:27:45.539
    STEP: Destroying namespace "nsdeletetest-2956" for this suite. 08/17/23 04:27:45.547
    Aug 17 04:27:45.552: INFO: Namespace nsdeletetest-2956 was already deleted
    STEP: Destroying namespace "nsdeletetest-563" for this suite. 08/17/23 04:27:45.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:45.566
Aug 17 04:27:45.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:27:45.567
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:45.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:45.586
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/17/23 04:27:45.591
Aug 17 04:27:45.602: INFO: Waiting up to 5m0s for pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb" in namespace "downward-api-1021" to be "Succeeded or Failed"
Aug 17 04:27:45.609: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.200452ms
Aug 17 04:27:47.618: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015496331s
Aug 17 04:27:49.615: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013262727s
STEP: Saw pod success 08/17/23 04:27:49.615
Aug 17 04:27:49.615: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb" satisfied condition "Succeeded or Failed"
Aug 17 04:27:49.620: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb container dapi-container: <nil>
STEP: delete the pod 08/17/23 04:27:49.704
Aug 17 04:27:49.717: INFO: Waiting for pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb to disappear
Aug 17 04:27:49.720: INFO: Pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:49.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1021" for this suite. 08/17/23 04:27:49.727
------------------------------
• [4.169 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:45.566
    Aug 17 04:27:45.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:27:45.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:45.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:45.586
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/17/23 04:27:45.591
    Aug 17 04:27:45.602: INFO: Waiting up to 5m0s for pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb" in namespace "downward-api-1021" to be "Succeeded or Failed"
    Aug 17 04:27:45.609: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.200452ms
    Aug 17 04:27:47.618: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015496331s
    Aug 17 04:27:49.615: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013262727s
    STEP: Saw pod success 08/17/23 04:27:49.615
    Aug 17 04:27:49.615: INFO: Pod "downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb" satisfied condition "Succeeded or Failed"
    Aug 17 04:27:49.620: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb container dapi-container: <nil>
    STEP: delete the pod 08/17/23 04:27:49.704
    Aug 17 04:27:49.717: INFO: Waiting for pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb to disappear
    Aug 17 04:27:49.720: INFO: Pod downward-api-2cab3881-73b4-444d-be8c-30ba9d6ee5eb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:49.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1021" for this suite. 08/17/23 04:27:49.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:49.736
Aug 17 04:27:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 04:27:49.737
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:49.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:49.757
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/17/23 04:27:49.762
Aug 17 04:27:49.771: INFO: Waiting up to 5m0s for pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b" in namespace "var-expansion-400" to be "Succeeded or Failed"
Aug 17 04:27:49.776: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91373ms
Aug 17 04:27:51.782: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011000088s
Aug 17 04:27:53.783: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012131542s
STEP: Saw pod success 08/17/23 04:27:53.783
Aug 17 04:27:53.784: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b" satisfied condition "Succeeded or Failed"
Aug 17 04:27:53.789: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b container dapi-container: <nil>
STEP: delete the pod 08/17/23 04:27:53.8
Aug 17 04:27:53.814: INFO: Waiting for pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b to disappear
Aug 17 04:27:53.820: INFO: Pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:53.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-400" for this suite. 08/17/23 04:27:53.83
------------------------------
• [4.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:49.736
    Aug 17 04:27:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 04:27:49.737
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:49.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:49.757
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/17/23 04:27:49.762
    Aug 17 04:27:49.771: INFO: Waiting up to 5m0s for pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b" in namespace "var-expansion-400" to be "Succeeded or Failed"
    Aug 17 04:27:49.776: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91373ms
    Aug 17 04:27:51.782: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011000088s
    Aug 17 04:27:53.783: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012131542s
    STEP: Saw pod success 08/17/23 04:27:53.783
    Aug 17 04:27:53.784: INFO: Pod "var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b" satisfied condition "Succeeded or Failed"
    Aug 17 04:27:53.789: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b container dapi-container: <nil>
    STEP: delete the pod 08/17/23 04:27:53.8
    Aug 17 04:27:53.814: INFO: Waiting for pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b to disappear
    Aug 17 04:27:53.820: INFO: Pod var-expansion-fa5c8f6f-92ff-4d1d-970c-980c261b9a8b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:53.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-400" for this suite. 08/17/23 04:27:53.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:53.846
Aug 17 04:27:53.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:27:53.847
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:53.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:53.87
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/17/23 04:27:53.875
Aug 17 04:27:53.891: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd" in namespace "downward-api-697" to be "Succeeded or Failed"
Aug 17 04:27:53.896: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351203ms
Aug 17 04:27:55.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011234246s
Aug 17 04:27:57.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010403888s
Aug 17 04:27:59.901: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010223228s
STEP: Saw pod success 08/17/23 04:27:59.901
Aug 17 04:27:59.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd" satisfied condition "Succeeded or Failed"
Aug 17 04:27:59.906: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd container client-container: <nil>
STEP: delete the pod 08/17/23 04:27:59.917
Aug 17 04:27:59.930: INFO: Waiting for pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd to disappear
Aug 17 04:27:59.934: INFO: Pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 04:27:59.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-697" for this suite. 08/17/23 04:27:59.941
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:53.846
    Aug 17 04:27:53.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:27:53.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:53.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:53.87
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/17/23 04:27:53.875
    Aug 17 04:27:53.891: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd" in namespace "downward-api-697" to be "Succeeded or Failed"
    Aug 17 04:27:53.896: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351203ms
    Aug 17 04:27:55.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011234246s
    Aug 17 04:27:57.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010403888s
    Aug 17 04:27:59.901: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010223228s
    STEP: Saw pod success 08/17/23 04:27:59.901
    Aug 17 04:27:59.902: INFO: Pod "downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd" satisfied condition "Succeeded or Failed"
    Aug 17 04:27:59.906: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd container client-container: <nil>
    STEP: delete the pod 08/17/23 04:27:59.917
    Aug 17 04:27:59.930: INFO: Waiting for pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd to disappear
    Aug 17 04:27:59.934: INFO: Pod downwardapi-volume-0c8b56f3-884c-465b-a9d1-e87868741cdd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:27:59.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-697" for this suite. 08/17/23 04:27:59.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:27:59.95
Aug 17 04:27:59.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:27:59.951
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:59.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:59.973
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 17 04:27:59.982: INFO: Got root ca configmap in namespace "svcaccounts-8228"
Aug 17 04:27:59.989: INFO: Deleted root ca configmap in namespace "svcaccounts-8228"
STEP: waiting for a new root ca configmap created 08/17/23 04:28:00.49
Aug 17 04:28:00.499: INFO: Recreated root ca configmap in namespace "svcaccounts-8228"
Aug 17 04:28:00.506: INFO: Updated root ca configmap in namespace "svcaccounts-8228"
STEP: waiting for the root ca configmap reconciled 08/17/23 04:28:01.007
Aug 17 04:28:01.013: INFO: Reconciled root ca configmap in namespace "svcaccounts-8228"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8228" for this suite. 08/17/23 04:28:01.021
------------------------------
• [1.080 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:27:59.95
    Aug 17 04:27:59.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:27:59.951
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:27:59.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:27:59.973
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 17 04:27:59.982: INFO: Got root ca configmap in namespace "svcaccounts-8228"
    Aug 17 04:27:59.989: INFO: Deleted root ca configmap in namespace "svcaccounts-8228"
    STEP: waiting for a new root ca configmap created 08/17/23 04:28:00.49
    Aug 17 04:28:00.499: INFO: Recreated root ca configmap in namespace "svcaccounts-8228"
    Aug 17 04:28:00.506: INFO: Updated root ca configmap in namespace "svcaccounts-8228"
    STEP: waiting for the root ca configmap reconciled 08/17/23 04:28:01.007
    Aug 17 04:28:01.013: INFO: Reconciled root ca configmap in namespace "svcaccounts-8228"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8228" for this suite. 08/17/23 04:28:01.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:01.033
Aug 17 04:28:01.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:28:01.034
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:01.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:01.059
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:28:01.088
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:28:01.48
STEP: Deploying the webhook pod 08/17/23 04:28:01.494
STEP: Wait for the deployment to be ready 08/17/23 04:28:01.511
Aug 17 04:28:01.522: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/17/23 04:28:03.541
STEP: Verifying the service has paired with the endpoint 08/17/23 04:28:03.556
Aug 17 04:28:04.556: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/17/23 04:28:04.629
STEP: Creating a configMap that should be mutated 08/17/23 04:28:04.739
STEP: Deleting the collection of validation webhooks 08/17/23 04:28:05.249
STEP: Creating a configMap that should not be mutated 08/17/23 04:28:05.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6995" for this suite. 08/17/23 04:28:05.359
STEP: Destroying namespace "webhook-6995-markers" for this suite. 08/17/23 04:28:05.381
------------------------------
• [4.362 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:01.033
    Aug 17 04:28:01.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:28:01.034
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:01.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:01.059
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:28:01.088
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:28:01.48
    STEP: Deploying the webhook pod 08/17/23 04:28:01.494
    STEP: Wait for the deployment to be ready 08/17/23 04:28:01.511
    Aug 17 04:28:01.522: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/17/23 04:28:03.541
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:28:03.556
    Aug 17 04:28:04.556: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/17/23 04:28:04.629
    STEP: Creating a configMap that should be mutated 08/17/23 04:28:04.739
    STEP: Deleting the collection of validation webhooks 08/17/23 04:28:05.249
    STEP: Creating a configMap that should not be mutated 08/17/23 04:28:05.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6995" for this suite. 08/17/23 04:28:05.359
    STEP: Destroying namespace "webhook-6995-markers" for this suite. 08/17/23 04:28:05.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:05.397
Aug 17 04:28:05.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:28:05.397
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:05.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:05.419
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-d96ca6cc-d2a0-4790-aeb7-a8c0913ee680 08/17/23 04:28:05.424
STEP: Creating a pod to test consume configMaps 08/17/23 04:28:05.43
Aug 17 04:28:05.442: INFO: Waiting up to 5m0s for pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b" in namespace "configmap-86" to be "Succeeded or Failed"
Aug 17 04:28:05.448: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985184ms
Aug 17 04:28:07.455: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012823192s
Aug 17 04:28:09.454: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012205161s
STEP: Saw pod success 08/17/23 04:28:09.454
Aug 17 04:28:09.454: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b" satisfied condition "Succeeded or Failed"
Aug 17 04:28:09.460: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b container agnhost-container: <nil>
STEP: delete the pod 08/17/23 04:28:09.51
Aug 17 04:28:09.521: INFO: Waiting for pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b to disappear
Aug 17 04:28:09.525: INFO: Pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:09.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-86" for this suite. 08/17/23 04:28:09.533
------------------------------
• [4.144 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:05.397
    Aug 17 04:28:05.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:28:05.397
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:05.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:05.419
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-d96ca6cc-d2a0-4790-aeb7-a8c0913ee680 08/17/23 04:28:05.424
    STEP: Creating a pod to test consume configMaps 08/17/23 04:28:05.43
    Aug 17 04:28:05.442: INFO: Waiting up to 5m0s for pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b" in namespace "configmap-86" to be "Succeeded or Failed"
    Aug 17 04:28:05.448: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985184ms
    Aug 17 04:28:07.455: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012823192s
    Aug 17 04:28:09.454: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012205161s
    STEP: Saw pod success 08/17/23 04:28:09.454
    Aug 17 04:28:09.454: INFO: Pod "pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b" satisfied condition "Succeeded or Failed"
    Aug 17 04:28:09.460: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 04:28:09.51
    Aug 17 04:28:09.521: INFO: Waiting for pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b to disappear
    Aug 17 04:28:09.525: INFO: Pod pod-configmaps-b246bb96-a1d1-44e8-8cb0-b524237b6b1b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:09.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-86" for this suite. 08/17/23 04:28:09.533
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:09.542
Aug 17 04:28:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replicaset 08/17/23 04:28:09.543
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:09.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:09.562
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/17/23 04:28:09.567
STEP: Verify that the required pods have come up 08/17/23 04:28:09.574
Aug 17 04:28:09.579: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 17 04:28:14.585: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/17/23 04:28:14.585
Aug 17 04:28:14.589: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/17/23 04:28:14.589
STEP: DeleteCollection of the ReplicaSets 08/17/23 04:28:14.596
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/17/23 04:28:14.605
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:14.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2160" for this suite. 08/17/23 04:28:14.626
------------------------------
• [SLOW TEST] [5.092 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:09.542
    Aug 17 04:28:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replicaset 08/17/23 04:28:09.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:09.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:09.562
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/17/23 04:28:09.567
    STEP: Verify that the required pods have come up 08/17/23 04:28:09.574
    Aug 17 04:28:09.579: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 17 04:28:14.585: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/17/23 04:28:14.585
    Aug 17 04:28:14.589: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/17/23 04:28:14.589
    STEP: DeleteCollection of the ReplicaSets 08/17/23 04:28:14.596
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/17/23 04:28:14.605
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:14.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2160" for this suite. 08/17/23 04:28:14.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:14.634
Aug 17 04:28:14.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename configmap 08/17/23 04:28:14.635
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:14.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:14.655
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-52cc53f8-0070-4000-a29b-99faf653bc96 08/17/23 04:28:14.66
STEP: Creating a pod to test consume configMaps 08/17/23 04:28:14.667
Aug 17 04:28:14.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752" in namespace "configmap-9324" to be "Succeeded or Failed"
Aug 17 04:28:14.694: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Pending", Reason="", readiness=false. Elapsed: 5.656047ms
Aug 17 04:28:16.700: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011755828s
Aug 17 04:28:18.701: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012606228s
STEP: Saw pod success 08/17/23 04:28:18.701
Aug 17 04:28:18.701: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752" satisfied condition "Succeeded or Failed"
Aug 17 04:28:18.706: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 container agnhost-container: <nil>
STEP: delete the pod 08/17/23 04:28:18.988
Aug 17 04:28:19.000: INFO: Waiting for pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 to disappear
Aug 17 04:28:19.004: INFO: Pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:19.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9324" for this suite. 08/17/23 04:28:19.011
------------------------------
• [4.384 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:14.634
    Aug 17 04:28:14.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename configmap 08/17/23 04:28:14.635
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:14.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:14.655
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-52cc53f8-0070-4000-a29b-99faf653bc96 08/17/23 04:28:14.66
    STEP: Creating a pod to test consume configMaps 08/17/23 04:28:14.667
    Aug 17 04:28:14.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752" in namespace "configmap-9324" to be "Succeeded or Failed"
    Aug 17 04:28:14.694: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Pending", Reason="", readiness=false. Elapsed: 5.656047ms
    Aug 17 04:28:16.700: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011755828s
    Aug 17 04:28:18.701: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012606228s
    STEP: Saw pod success 08/17/23 04:28:18.701
    Aug 17 04:28:18.701: INFO: Pod "pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752" satisfied condition "Succeeded or Failed"
    Aug 17 04:28:18.706: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 container agnhost-container: <nil>
    STEP: delete the pod 08/17/23 04:28:18.988
    Aug 17 04:28:19.000: INFO: Waiting for pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 to disappear
    Aug 17 04:28:19.004: INFO: Pod pod-configmaps-c39c9a19-f5a7-4cea-bd58-930e89942752 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:19.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9324" for this suite. 08/17/23 04:28:19.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:19.02
Aug 17 04:28:19.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:28:19.021
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:19.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:19.039
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/17/23 04:28:19.044
Aug 17 04:28:19.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3119 api-versions'
Aug 17 04:28:19.124: INFO: stderr: ""
Aug 17 04:28:19.124: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3119" for this suite. 08/17/23 04:28:19.131
------------------------------
• [0.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:19.02
    Aug 17 04:28:19.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:28:19.021
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:19.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:19.039
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/17/23 04:28:19.044
    Aug 17 04:28:19.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-3119 api-versions'
    Aug 17 04:28:19.124: INFO: stderr: ""
    Aug 17 04:28:19.124: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3119" for this suite. 08/17/23 04:28:19.131
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:19.139
Aug 17 04:28:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 04:28:19.142
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:19.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:19.162
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/17/23 04:28:19.166
STEP: Ensuring ResourceQuota status is calculated 08/17/23 04:28:19.171
STEP: Creating a ResourceQuota with not best effort scope 08/17/23 04:28:21.177
STEP: Ensuring ResourceQuota status is calculated 08/17/23 04:28:21.182
STEP: Creating a best-effort pod 08/17/23 04:28:23.188
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/17/23 04:28:23.202
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/17/23 04:28:25.214
STEP: Deleting the pod 08/17/23 04:28:27.219
STEP: Ensuring resource quota status released the pod usage 08/17/23 04:28:27.231
STEP: Creating a not best-effort pod 08/17/23 04:28:29.237
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/17/23 04:28:29.247
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/17/23 04:28:31.253
STEP: Deleting the pod 08/17/23 04:28:33.259
STEP: Ensuring resource quota status released the pod usage 08/17/23 04:28:33.272
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:35.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1590" for this suite. 08/17/23 04:28:35.285
------------------------------
• [SLOW TEST] [16.155 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:19.139
    Aug 17 04:28:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 04:28:19.142
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:19.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:19.162
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/17/23 04:28:19.166
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 04:28:19.171
    STEP: Creating a ResourceQuota with not best effort scope 08/17/23 04:28:21.177
    STEP: Ensuring ResourceQuota status is calculated 08/17/23 04:28:21.182
    STEP: Creating a best-effort pod 08/17/23 04:28:23.188
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/17/23 04:28:23.202
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/17/23 04:28:25.214
    STEP: Deleting the pod 08/17/23 04:28:27.219
    STEP: Ensuring resource quota status released the pod usage 08/17/23 04:28:27.231
    STEP: Creating a not best-effort pod 08/17/23 04:28:29.237
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/17/23 04:28:29.247
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/17/23 04:28:31.253
    STEP: Deleting the pod 08/17/23 04:28:33.259
    STEP: Ensuring resource quota status released the pod usage 08/17/23 04:28:33.272
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:35.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1590" for this suite. 08/17/23 04:28:35.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:35.295
Aug 17 04:28:35.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename cronjob 08/17/23 04:28:35.296
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:35.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:35.315
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/17/23 04:28:35.32
STEP: creating 08/17/23 04:28:35.32
STEP: getting 08/17/23 04:28:35.326
STEP: listing 08/17/23 04:28:35.33
STEP: watching 08/17/23 04:28:35.335
Aug 17 04:28:35.335: INFO: starting watch
STEP: cluster-wide listing 08/17/23 04:28:35.338
STEP: cluster-wide watching 08/17/23 04:28:35.342
Aug 17 04:28:35.343: INFO: starting watch
STEP: patching 08/17/23 04:28:35.345
STEP: updating 08/17/23 04:28:35.353
Aug 17 04:28:35.368: INFO: waiting for watch events with expected annotations
Aug 17 04:28:35.368: INFO: saw patched and updated annotations
STEP: patching /status 08/17/23 04:28:35.368
STEP: updating /status 08/17/23 04:28:35.377
STEP: get /status 08/17/23 04:28:35.389
STEP: deleting 08/17/23 04:28:35.394
STEP: deleting a collection 08/17/23 04:28:35.413
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:35.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8687" for this suite. 08/17/23 04:28:35.433
------------------------------
• [0.146 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:35.295
    Aug 17 04:28:35.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename cronjob 08/17/23 04:28:35.296
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:35.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:35.315
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/17/23 04:28:35.32
    STEP: creating 08/17/23 04:28:35.32
    STEP: getting 08/17/23 04:28:35.326
    STEP: listing 08/17/23 04:28:35.33
    STEP: watching 08/17/23 04:28:35.335
    Aug 17 04:28:35.335: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 04:28:35.338
    STEP: cluster-wide watching 08/17/23 04:28:35.342
    Aug 17 04:28:35.343: INFO: starting watch
    STEP: patching 08/17/23 04:28:35.345
    STEP: updating 08/17/23 04:28:35.353
    Aug 17 04:28:35.368: INFO: waiting for watch events with expected annotations
    Aug 17 04:28:35.368: INFO: saw patched and updated annotations
    STEP: patching /status 08/17/23 04:28:35.368
    STEP: updating /status 08/17/23 04:28:35.377
    STEP: get /status 08/17/23 04:28:35.389
    STEP: deleting 08/17/23 04:28:35.394
    STEP: deleting a collection 08/17/23 04:28:35.413
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:35.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8687" for this suite. 08/17/23 04:28:35.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:35.445
Aug 17 04:28:35.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:28:35.446
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:35.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:35.464
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/17/23 04:28:35.469
Aug 17 04:28:35.469: INFO: namespace kubectl-9258
Aug 17 04:28:35.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 create -f -'
Aug 17 04:28:36.067: INFO: stderr: ""
Aug 17 04:28:36.067: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/17/23 04:28:36.067
Aug 17 04:28:37.074: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:28:37.074: INFO: Found 0 / 1
Aug 17 04:28:38.075: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:28:38.075: INFO: Found 1 / 1
Aug 17 04:28:38.075: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 17 04:28:38.080: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 17 04:28:38.080: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 17 04:28:38.081: INFO: wait on agnhost-primary startup in kubectl-9258 
Aug 17 04:28:38.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 logs agnhost-primary-hbmgh agnhost-primary'
Aug 17 04:28:38.156: INFO: stderr: ""
Aug 17 04:28:38.156: INFO: stdout: "Paused\n"
STEP: exposing RC 08/17/23 04:28:38.156
Aug 17 04:28:38.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 17 04:28:38.242: INFO: stderr: ""
Aug 17 04:28:38.242: INFO: stdout: "service/rm2 exposed\n"
Aug 17 04:28:38.247: INFO: Service rm2 in namespace kubectl-9258 found.
STEP: exposing service 08/17/23 04:28:40.259
Aug 17 04:28:40.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 17 04:28:40.337: INFO: stderr: ""
Aug 17 04:28:40.337: INFO: stdout: "service/rm3 exposed\n"
Aug 17 04:28:40.341: INFO: Service rm3 in namespace kubectl-9258 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:42.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9258" for this suite. 08/17/23 04:28:42.357
------------------------------
• [SLOW TEST] [6.920 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:35.445
    Aug 17 04:28:35.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:28:35.446
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:35.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:35.464
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/17/23 04:28:35.469
    Aug 17 04:28:35.469: INFO: namespace kubectl-9258
    Aug 17 04:28:35.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 create -f -'
    Aug 17 04:28:36.067: INFO: stderr: ""
    Aug 17 04:28:36.067: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/17/23 04:28:36.067
    Aug 17 04:28:37.074: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:28:37.074: INFO: Found 0 / 1
    Aug 17 04:28:38.075: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:28:38.075: INFO: Found 1 / 1
    Aug 17 04:28:38.075: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 17 04:28:38.080: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 17 04:28:38.080: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 17 04:28:38.081: INFO: wait on agnhost-primary startup in kubectl-9258 
    Aug 17 04:28:38.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 logs agnhost-primary-hbmgh agnhost-primary'
    Aug 17 04:28:38.156: INFO: stderr: ""
    Aug 17 04:28:38.156: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/17/23 04:28:38.156
    Aug 17 04:28:38.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 17 04:28:38.242: INFO: stderr: ""
    Aug 17 04:28:38.242: INFO: stdout: "service/rm2 exposed\n"
    Aug 17 04:28:38.247: INFO: Service rm2 in namespace kubectl-9258 found.
    STEP: exposing service 08/17/23 04:28:40.259
    Aug 17 04:28:40.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-9258 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 17 04:28:40.337: INFO: stderr: ""
    Aug 17 04:28:40.337: INFO: stdout: "service/rm3 exposed\n"
    Aug 17 04:28:40.341: INFO: Service rm3 in namespace kubectl-9258 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:42.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9258" for this suite. 08/17/23 04:28:42.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:42.367
Aug 17 04:28:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubelet-test 08/17/23 04:28:42.368
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:42.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:42.385
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 17 04:28:42.397: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c" in namespace "kubelet-test-5112" to be "running and ready"
Aug 17 04:28:42.400: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.436934ms
Aug 17 04:28:42.400: INFO: The phase of Pod busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:28:44.408: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011082799s
Aug 17 04:28:44.408: INFO: The phase of Pod busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c is Running (Ready = true)
Aug 17 04:28:44.408: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:44.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5112" for this suite. 08/17/23 04:28:44.435
------------------------------
• [2.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:42.367
    Aug 17 04:28:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubelet-test 08/17/23 04:28:42.368
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:42.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:42.385
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 17 04:28:42.397: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c" in namespace "kubelet-test-5112" to be "running and ready"
    Aug 17 04:28:42.400: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.436934ms
    Aug 17 04:28:42.400: INFO: The phase of Pod busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:28:44.408: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011082799s
    Aug 17 04:28:44.408: INFO: The phase of Pod busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c is Running (Ready = true)
    Aug 17 04:28:44.408: INFO: Pod "busybox-readonly-fs37fefe53-1cd8-4064-bb9f-dd0935f8830c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:44.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5112" for this suite. 08/17/23 04:28:44.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:44.446
Aug 17 04:28:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename events 08/17/23 04:28:44.448
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.47
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/17/23 04:28:44.474
Aug 17 04:28:44.480: INFO: created test-event-1
Aug 17 04:28:44.484: INFO: created test-event-2
Aug 17 04:28:44.489: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/17/23 04:28:44.489
STEP: delete collection of events 08/17/23 04:28:44.495
Aug 17 04:28:44.495: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/17/23 04:28:44.51
Aug 17 04:28:44.510: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:44.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7759" for this suite. 08/17/23 04:28:44.521
------------------------------
• [0.084 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:44.446
    Aug 17 04:28:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename events 08/17/23 04:28:44.448
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.47
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/17/23 04:28:44.474
    Aug 17 04:28:44.480: INFO: created test-event-1
    Aug 17 04:28:44.484: INFO: created test-event-2
    Aug 17 04:28:44.489: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/17/23 04:28:44.489
    STEP: delete collection of events 08/17/23 04:28:44.495
    Aug 17 04:28:44.495: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/17/23 04:28:44.51
    Aug 17 04:28:44.510: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:44.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7759" for this suite. 08/17/23 04:28:44.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:44.532
Aug 17 04:28:44.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:28:44.533
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.551
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-t8gn5"  08/17/23 04:28:44.556
Aug 17 04:28:44.562: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-t8gn5"  08/17/23 04:28:44.562
Aug 17 04:28:44.572: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3172" for this suite. 08/17/23 04:28:44.579
------------------------------
• [0.055 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:44.532
    Aug 17 04:28:44.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:28:44.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.551
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-t8gn5"  08/17/23 04:28:44.556
    Aug 17 04:28:44.562: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-t8gn5"  08/17/23 04:28:44.562
    Aug 17 04:28:44.572: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3172" for this suite. 08/17/23 04:28:44.579
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:44.588
Aug 17 04:28:44.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:28:44.589
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.608
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 17 04:28:44.630: INFO: Waiting up to 5m0s for pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63" in namespace "svcaccounts-3407" to be "running"
Aug 17 04:28:44.635: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055031ms
Aug 17 04:28:46.643: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63": Phase="Running", Reason="", readiness=true. Elapsed: 2.012570953s
Aug 17 04:28:46.643: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63" satisfied condition "running"
STEP: reading a file in the container 08/17/23 04:28:46.643
Aug 17 04:28:46.643: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/17/23 04:28:47.076
Aug 17 04:28:47.076: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/17/23 04:28:47.555
Aug 17 04:28:47.555: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 17 04:28:48.085: INFO: Got root ca configmap in namespace "svcaccounts-3407"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:48.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3407" for this suite. 08/17/23 04:28:48.096
------------------------------
• [3.516 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:44.588
    Aug 17 04:28:44.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:28:44.589
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:44.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:44.608
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 17 04:28:44.630: INFO: Waiting up to 5m0s for pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63" in namespace "svcaccounts-3407" to be "running"
    Aug 17 04:28:44.635: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055031ms
    Aug 17 04:28:46.643: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63": Phase="Running", Reason="", readiness=true. Elapsed: 2.012570953s
    Aug 17 04:28:46.643: INFO: Pod "pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63" satisfied condition "running"
    STEP: reading a file in the container 08/17/23 04:28:46.643
    Aug 17 04:28:46.643: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/17/23 04:28:47.076
    Aug 17 04:28:47.076: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/17/23 04:28:47.555
    Aug 17 04:28:47.555: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3407 pod-service-account-b73f2b44-75f8-47c0-8e66-8546185d9c63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 17 04:28:48.085: INFO: Got root ca configmap in namespace "svcaccounts-3407"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:48.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3407" for this suite. 08/17/23 04:28:48.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:48.106
Aug 17 04:28:48.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 04:28:48.106
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:48.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:48.127
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/17/23 04:28:48.141
STEP: watching for Pod to be ready 08/17/23 04:28:48.152
Aug 17 04:28:48.154: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 17 04:28:48.158: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
Aug 17 04:28:48.170: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
Aug 17 04:28:48.883: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
Aug 17 04:28:50.096: INFO: Found Pod pod-test in namespace pods-864 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/17/23 04:28:50.106
STEP: getting the Pod and ensuring that it's patched 08/17/23 04:28:50.131
STEP: replacing the Pod's status Ready condition to False 08/17/23 04:28:50.136
STEP: check the Pod again to ensure its Ready conditions are False 08/17/23 04:28:50.151
STEP: deleting the Pod via a Collection with a LabelSelector 08/17/23 04:28:50.151
STEP: watching for the Pod to be deleted 08/17/23 04:28:50.161
Aug 17 04:28:50.164: INFO: observed event type MODIFIED
Aug 17 04:28:52.051: INFO: observed event type MODIFIED
Aug 17 04:28:52.327: INFO: observed event type MODIFIED
Aug 17 04:28:53.100: INFO: observed event type MODIFIED
Aug 17 04:28:53.109: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:53.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-864" for this suite. 08/17/23 04:28:53.124
------------------------------
• [SLOW TEST] [5.026 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:48.106
    Aug 17 04:28:48.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 04:28:48.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:48.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:48.127
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/17/23 04:28:48.141
    STEP: watching for Pod to be ready 08/17/23 04:28:48.152
    Aug 17 04:28:48.154: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 17 04:28:48.158: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
    Aug 17 04:28:48.170: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
    Aug 17 04:28:48.883: INFO: observed Pod pod-test in namespace pods-864 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
    Aug 17 04:28:50.096: INFO: Found Pod pod-test in namespace pods-864 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-17 04:28:48 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/17/23 04:28:50.106
    STEP: getting the Pod and ensuring that it's patched 08/17/23 04:28:50.131
    STEP: replacing the Pod's status Ready condition to False 08/17/23 04:28:50.136
    STEP: check the Pod again to ensure its Ready conditions are False 08/17/23 04:28:50.151
    STEP: deleting the Pod via a Collection with a LabelSelector 08/17/23 04:28:50.151
    STEP: watching for the Pod to be deleted 08/17/23 04:28:50.161
    Aug 17 04:28:50.164: INFO: observed event type MODIFIED
    Aug 17 04:28:52.051: INFO: observed event type MODIFIED
    Aug 17 04:28:52.327: INFO: observed event type MODIFIED
    Aug 17 04:28:53.100: INFO: observed event type MODIFIED
    Aug 17 04:28:53.109: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:53.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-864" for this suite. 08/17/23 04:28:53.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:53.136
Aug 17 04:28:53.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-runtime 08/17/23 04:28:53.137
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:53.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:53.166
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/17/23 04:28:53.173
STEP: wait for the container to reach Succeeded 08/17/23 04:28:53.182
STEP: get the container status 08/17/23 04:28:57.21
STEP: the container should be terminated 08/17/23 04:28:57.216
STEP: the termination message should be set 08/17/23 04:28:57.216
Aug 17 04:28:57.216: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/17/23 04:28:57.216
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:57.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-308" for this suite. 08/17/23 04:28:57.242
------------------------------
• [4.115 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:53.136
    Aug 17 04:28:53.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-runtime 08/17/23 04:28:53.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:53.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:53.166
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/17/23 04:28:53.173
    STEP: wait for the container to reach Succeeded 08/17/23 04:28:53.182
    STEP: get the container status 08/17/23 04:28:57.21
    STEP: the container should be terminated 08/17/23 04:28:57.216
    STEP: the termination message should be set 08/17/23 04:28:57.216
    Aug 17 04:28:57.216: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/17/23 04:28:57.216
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:57.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-308" for this suite. 08/17/23 04:28:57.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:57.252
Aug 17 04:28:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 04:28:57.253
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:57.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:57.273
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 17 04:28:57.278: INFO: Creating deployment "test-recreate-deployment"
Aug 17 04:28:57.284: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 17 04:28:57.292: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 17 04:28:59.303: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 17 04:28:59.311: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 17 04:28:59.324: INFO: Updating deployment test-recreate-deployment
Aug 17 04:28:59.324: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 04:28:59.391: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5730  07b35f3a-5907-418e-9763-23b77c3b06b4 250597 2 2023-08-17 04:28:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0078bf848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 04:28:59 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-17 04:28:59 +0000 UTC,LastTransitionTime:2023-08-17 04:28:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 17 04:28:59.398: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5730  fdf806f7-65f9-4068-9f1e-50c53c8e045a 250595 1 2023-08-17 04:28:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 07b35f3a-5907-418e-9763-23b77c3b06b4 0xc004266880 0xc004266881}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b35f3a-5907-418e-9763-23b77c3b06b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004266918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:28:59.398: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 17 04:28:59.398: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5730  77bc36d0-fcf0-4e9b-a509-c1f140f4fc55 250584 2 2023-08-17 04:28:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 07b35f3a-5907-418e-9763-23b77c3b06b4 0xc004266767 0xc004266768}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b35f3a-5907-418e-9763-23b77c3b06b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004266818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 17 04:28:59.404: INFO: Pod "test-recreate-deployment-cff6dc657-5rbhc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-5rbhc test-recreate-deployment-cff6dc657- deployment-5730  2b3dfec5-6484-45cc-989f-ede135955ece 250596 0 2023-08-17 04:28:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 fdf806f7-65f9-4068-9f1e-50c53c8e045a 0xc0078bfbc0 0xc0078bfbc1}] [] [{kube-controller-manager Update v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdf806f7-65f9-4068-9f1e-50c53c8e045a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr7k7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr7k7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 04:28:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 04:28:59.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5730" for this suite. 08/17/23 04:28:59.411
------------------------------
• [2.168 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:57.252
    Aug 17 04:28:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 04:28:57.253
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:57.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:57.273
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 17 04:28:57.278: INFO: Creating deployment "test-recreate-deployment"
    Aug 17 04:28:57.284: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 17 04:28:57.292: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 17 04:28:59.303: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 17 04:28:59.311: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 17 04:28:59.324: INFO: Updating deployment test-recreate-deployment
    Aug 17 04:28:59.324: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 04:28:59.391: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5730  07b35f3a-5907-418e-9763-23b77c3b06b4 250597 2 2023-08-17 04:28:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0078bf848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-17 04:28:59 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-17 04:28:59 +0000 UTC,LastTransitionTime:2023-08-17 04:28:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 17 04:28:59.398: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5730  fdf806f7-65f9-4068-9f1e-50c53c8e045a 250595 1 2023-08-17 04:28:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 07b35f3a-5907-418e-9763-23b77c3b06b4 0xc004266880 0xc004266881}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b35f3a-5907-418e-9763-23b77c3b06b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004266918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:28:59.398: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 17 04:28:59.398: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5730  77bc36d0-fcf0-4e9b-a509-c1f140f4fc55 250584 2 2023-08-17 04:28:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 07b35f3a-5907-418e-9763-23b77c3b06b4 0xc004266767 0xc004266768}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07b35f3a-5907-418e-9763-23b77c3b06b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004266818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 17 04:28:59.404: INFO: Pod "test-recreate-deployment-cff6dc657-5rbhc" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-5rbhc test-recreate-deployment-cff6dc657- deployment-5730  2b3dfec5-6484-45cc-989f-ede135955ece 250596 0 2023-08-17 04:28:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 fdf806f7-65f9-4068-9f1e-50c53c8e045a 0xc0078bfbc0 0xc0078bfbc1}] [] [{kube-controller-manager Update v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdf806f7-65f9-4068-9f1e-50c53c8e045a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-17 04:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr7k7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr7k7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:28:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-08-17 04:28:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:28:59.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5730" for this suite. 08/17/23 04:28:59.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:28:59.421
Aug 17 04:28:59.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename dns 08/17/23 04:28:59.422
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:59.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:59.44
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/17/23 04:28:59.444
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:28:59.45
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:28:59.45
STEP: creating a pod to probe DNS 08/17/23 04:28:59.45
STEP: submitting the pod to kubernetes 08/17/23 04:28:59.451
Aug 17 04:28:59.460: INFO: Waiting up to 15m0s for pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb" in namespace "dns-9579" to be "running"
Aug 17 04:28:59.464: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.212478ms
Aug 17 04:29:01.470: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009537143s
Aug 17 04:29:01.470: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb" satisfied condition "running"
STEP: retrieving the pod 08/17/23 04:29:01.47
STEP: looking for the results for each expected name from probers 08/17/23 04:29:01.475
Aug 17 04:29:01.611: INFO: DNS probes using dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb succeeded

STEP: deleting the pod 08/17/23 04:29:01.611
STEP: changing the externalName to bar.example.com 08/17/23 04:29:01.627
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:29:01.649
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:29:01.65
STEP: creating a second pod to probe DNS 08/17/23 04:29:01.65
STEP: submitting the pod to kubernetes 08/17/23 04:29:01.65
Aug 17 04:29:01.660: INFO: Waiting up to 15m0s for pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7" in namespace "dns-9579" to be "running"
Aug 17 04:29:01.665: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176427ms
Aug 17 04:29:03.672: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012007896s
Aug 17 04:29:03.672: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7" satisfied condition "running"
STEP: retrieving the pod 08/17/23 04:29:03.672
STEP: looking for the results for each expected name from probers 08/17/23 04:29:03.677
Aug 17 04:29:03.767: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:03.812: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:03.812: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:08.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:08.863: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:08.863: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:13.820: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:13.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:13.865: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:18.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:18.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:18.864: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:23.820: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:23.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:23.864: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:28.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:28.863: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 17 04:29:28.863: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

Aug 17 04:29:33.864: INFO: DNS probes using dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 succeeded

STEP: deleting the pod 08/17/23 04:29:33.864
STEP: changing the service to type=ClusterIP 08/17/23 04:29:33.88
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:29:33.906
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
 08/17/23 04:29:33.906
STEP: creating a third pod to probe DNS 08/17/23 04:29:33.906
STEP: submitting the pod to kubernetes 08/17/23 04:29:33.911
Aug 17 04:29:33.923: INFO: Waiting up to 15m0s for pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f" in namespace "dns-9579" to be "running"
Aug 17 04:29:33.929: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.051544ms
Aug 17 04:29:35.937: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012280478s
Aug 17 04:29:37.938: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Running", Reason="", readiness=true. Elapsed: 4.01405229s
Aug 17 04:29:37.939: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f" satisfied condition "running"
STEP: retrieving the pod 08/17/23 04:29:37.939
STEP: looking for the results for each expected name from probers 08/17/23 04:29:37.945
Aug 17 04:29:38.081: INFO: DNS probes using dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f succeeded

STEP: deleting the pod 08/17/23 04:29:38.081
STEP: deleting the test externalName service 08/17/23 04:29:38.096
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:38.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9579" for this suite. 08/17/23 04:29:38.118
------------------------------
• [SLOW TEST] [38.704 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:28:59.421
    Aug 17 04:28:59.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename dns 08/17/23 04:28:59.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:28:59.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:28:59.44
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/17/23 04:28:59.444
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:28:59.45
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:28:59.45
    STEP: creating a pod to probe DNS 08/17/23 04:28:59.45
    STEP: submitting the pod to kubernetes 08/17/23 04:28:59.451
    Aug 17 04:28:59.460: INFO: Waiting up to 15m0s for pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb" in namespace "dns-9579" to be "running"
    Aug 17 04:28:59.464: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.212478ms
    Aug 17 04:29:01.470: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009537143s
    Aug 17 04:29:01.470: INFO: Pod "dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 04:29:01.47
    STEP: looking for the results for each expected name from probers 08/17/23 04:29:01.475
    Aug 17 04:29:01.611: INFO: DNS probes using dns-test-23bedcad-eaf7-453e-a2ed-411b96af8dfb succeeded

    STEP: deleting the pod 08/17/23 04:29:01.611
    STEP: changing the externalName to bar.example.com 08/17/23 04:29:01.627
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:29:01.649
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:29:01.65
    STEP: creating a second pod to probe DNS 08/17/23 04:29:01.65
    STEP: submitting the pod to kubernetes 08/17/23 04:29:01.65
    Aug 17 04:29:01.660: INFO: Waiting up to 15m0s for pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7" in namespace "dns-9579" to be "running"
    Aug 17 04:29:01.665: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176427ms
    Aug 17 04:29:03.672: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012007896s
    Aug 17 04:29:03.672: INFO: Pod "dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 04:29:03.672
    STEP: looking for the results for each expected name from probers 08/17/23 04:29:03.677
    Aug 17 04:29:03.767: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:03.812: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:03.812: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:08.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:08.863: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:08.863: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:13.820: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:13.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:13.865: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:18.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:18.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:18.864: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:23.820: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:23.864: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:23.864: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:28.819: INFO: File wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:28.863: INFO: File jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local from pod  dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 17 04:29:28.863: INFO: Lookups using dns-9579/dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 failed for: [wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local]

    Aug 17 04:29:33.864: INFO: DNS probes using dns-test-1ab1e6b6-1d98-4bd6-9dbb-e0ff23bc74f7 succeeded

    STEP: deleting the pod 08/17/23 04:29:33.864
    STEP: changing the service to type=ClusterIP 08/17/23 04:29:33.88
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:29:33.906
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9579.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9579.svc.cluster.local; sleep 1; done
     08/17/23 04:29:33.906
    STEP: creating a third pod to probe DNS 08/17/23 04:29:33.906
    STEP: submitting the pod to kubernetes 08/17/23 04:29:33.911
    Aug 17 04:29:33.923: INFO: Waiting up to 15m0s for pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f" in namespace "dns-9579" to be "running"
    Aug 17 04:29:33.929: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.051544ms
    Aug 17 04:29:35.937: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012280478s
    Aug 17 04:29:37.938: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f": Phase="Running", Reason="", readiness=true. Elapsed: 4.01405229s
    Aug 17 04:29:37.939: INFO: Pod "dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f" satisfied condition "running"
    STEP: retrieving the pod 08/17/23 04:29:37.939
    STEP: looking for the results for each expected name from probers 08/17/23 04:29:37.945
    Aug 17 04:29:38.081: INFO: DNS probes using dns-test-3b50d4c7-d5f9-4ddf-80a6-8723e39c1b9f succeeded

    STEP: deleting the pod 08/17/23 04:29:38.081
    STEP: deleting the test externalName service 08/17/23 04:29:38.096
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:38.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9579" for this suite. 08/17/23 04:29:38.118
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:38.127
Aug 17 04:29:38.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sysctl 08/17/23 04:29:38.128
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:38.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:38.148
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/17/23 04:29:38.153
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3564" for this suite. 08/17/23 04:29:38.167
------------------------------
• [0.048 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:38.127
    Aug 17 04:29:38.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sysctl 08/17/23 04:29:38.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:38.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:38.148
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/17/23 04:29:38.153
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3564" for this suite. 08/17/23 04:29:38.167
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:38.178
Aug 17 04:29:38.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename prestop 08/17/23 04:29:38.179
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:38.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:38.198
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2660 08/17/23 04:29:38.203
STEP: Waiting for pods to come up. 08/17/23 04:29:38.214
Aug 17 04:29:38.214: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2660" to be "running"
Aug 17 04:29:38.218: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927326ms
Aug 17 04:29:40.223: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009317461s
Aug 17 04:29:42.224: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.010485318s
Aug 17 04:29:42.224: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2660 08/17/23 04:29:42.228
Aug 17 04:29:42.236: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2660" to be "running"
Aug 17 04:29:42.243: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891238ms
Aug 17 04:29:44.248: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01184073s
Aug 17 04:29:46.251: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.015067096s
Aug 17 04:29:46.251: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/17/23 04:29:46.251
Aug 17 04:29:51.352: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/17/23 04:29:51.352
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:51.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2660" for this suite. 08/17/23 04:29:51.382
------------------------------
• [SLOW TEST] [13.212 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:38.178
    Aug 17 04:29:38.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename prestop 08/17/23 04:29:38.179
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:38.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:38.198
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2660 08/17/23 04:29:38.203
    STEP: Waiting for pods to come up. 08/17/23 04:29:38.214
    Aug 17 04:29:38.214: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2660" to be "running"
    Aug 17 04:29:38.218: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927326ms
    Aug 17 04:29:40.223: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009317461s
    Aug 17 04:29:42.224: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.010485318s
    Aug 17 04:29:42.224: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2660 08/17/23 04:29:42.228
    Aug 17 04:29:42.236: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2660" to be "running"
    Aug 17 04:29:42.243: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891238ms
    Aug 17 04:29:44.248: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01184073s
    Aug 17 04:29:46.251: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.015067096s
    Aug 17 04:29:46.251: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/17/23 04:29:46.251
    Aug 17 04:29:51.352: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/17/23 04:29:51.352
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:51.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2660" for this suite. 08/17/23 04:29:51.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:51.393
Aug 17 04:29:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 04:29:51.394
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:51.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:51.416
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 08/17/23 04:29:51.454
STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 04:29:51.46
Aug 17 04:29:51.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:29:51.471: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 04:29:52.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 04:29:52.485: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 04:29:53.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 04:29:53.485: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 08/17/23 04:29:53.489
Aug 17 04:29:53.494: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/17/23 04:29:53.494
Aug 17 04:29:53.506: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/17/23 04:29:53.506
Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: ADDED
Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.510: INFO: Found daemon set daemon-set in namespace daemonsets-3088 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 04:29:53.510: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/17/23 04:29:53.51
STEP: watching for the daemon set status to be patched 08/17/23 04:29:53.519
Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: ADDED
Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.522: INFO: Observed daemon set daemon-set in namespace daemonsets-3088 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
Aug 17 04:29:53.522: INFO: Found daemon set daemon-set in namespace daemonsets-3088 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 17 04:29:53.522: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 04:29:53.527
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3088, will wait for the garbage collector to delete the pods 08/17/23 04:29:53.527
Aug 17 04:29:53.589: INFO: Deleting DaemonSet.extensions daemon-set took: 7.872495ms
Aug 17 04:29:53.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.374661ms
Aug 17 04:29:56.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:29:56.396: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 04:29:56.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"251032"},"items":null}

Aug 17 04:29:56.405: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"251032"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:56.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3088" for this suite. 08/17/23 04:29:56.43
------------------------------
• [SLOW TEST] [5.046 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:51.393
    Aug 17 04:29:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 04:29:51.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:51.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:51.416
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 08/17/23 04:29:51.454
    STEP: Check that daemon pods launch on every node of the cluster. 08/17/23 04:29:51.46
    Aug 17 04:29:51.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:29:51.471: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 04:29:52.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 04:29:52.485: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 04:29:53.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 04:29:53.485: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 08/17/23 04:29:53.489
    Aug 17 04:29:53.494: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/17/23 04:29:53.494
    Aug 17 04:29:53.506: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/17/23 04:29:53.506
    Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: ADDED
    Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.510: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.510: INFO: Found daemon set daemon-set in namespace daemonsets-3088 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 04:29:53.510: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/17/23 04:29:53.51
    STEP: watching for the daemon set status to be patched 08/17/23 04:29:53.519
    Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: ADDED
    Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.522: INFO: Observed daemon set daemon-set in namespace daemonsets-3088 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 17 04:29:53.522: INFO: Observed &DaemonSet event: MODIFIED
    Aug 17 04:29:53.522: INFO: Found daemon set daemon-set in namespace daemonsets-3088 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 17 04:29:53.522: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 04:29:53.527
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3088, will wait for the garbage collector to delete the pods 08/17/23 04:29:53.527
    Aug 17 04:29:53.589: INFO: Deleting DaemonSet.extensions daemon-set took: 7.872495ms
    Aug 17 04:29:53.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.374661ms
    Aug 17 04:29:56.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:29:56.396: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 04:29:56.401: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"251032"},"items":null}

    Aug 17 04:29:56.405: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"251032"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:56.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3088" for this suite. 08/17/23 04:29:56.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:56.441
Aug 17 04:29:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 04:29:56.441
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:56.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:56.463
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/17/23 04:29:56.469
STEP: getting 08/17/23 04:29:56.49
STEP: listing 08/17/23 04:29:56.5
STEP: deleting 08/17/23 04:29:56.505
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:56.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4308" for this suite. 08/17/23 04:29:56.535
------------------------------
• [0.102 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:56.441
    Aug 17 04:29:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename csiinlinevolumes 08/17/23 04:29:56.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:56.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:56.463
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/17/23 04:29:56.469
    STEP: getting 08/17/23 04:29:56.49
    STEP: listing 08/17/23 04:29:56.5
    STEP: deleting 08/17/23 04:29:56.505
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:56.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4308" for this suite. 08/17/23 04:29:56.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:56.545
Aug 17 04:29:56.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename replication-controller 08/17/23 04:29:56.546
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:56.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:56.563
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/17/23 04:29:56.571
STEP: waiting for RC to be added 08/17/23 04:29:56.577
STEP: waiting for available Replicas 08/17/23 04:29:56.578
STEP: patching ReplicationController 08/17/23 04:29:58.341
STEP: waiting for RC to be modified 08/17/23 04:29:58.351
STEP: patching ReplicationController status 08/17/23 04:29:58.351
STEP: waiting for RC to be modified 08/17/23 04:29:58.358
STEP: waiting for available Replicas 08/17/23 04:29:58.358
STEP: fetching ReplicationController status 08/17/23 04:29:58.367
STEP: patching ReplicationController scale 08/17/23 04:29:58.372
STEP: waiting for RC to be modified 08/17/23 04:29:58.381
STEP: waiting for ReplicationController's scale to be the max amount 08/17/23 04:29:58.381
STEP: fetching ReplicationController; ensuring that it's patched 08/17/23 04:29:59.48
STEP: updating ReplicationController status 08/17/23 04:29:59.484
STEP: waiting for RC to be modified 08/17/23 04:29:59.491
STEP: listing all ReplicationControllers 08/17/23 04:29:59.491
STEP: checking that ReplicationController has expected values 08/17/23 04:29:59.495
STEP: deleting ReplicationControllers by collection 08/17/23 04:29:59.495
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/17/23 04:29:59.503
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:29:59.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7618" for this suite. 08/17/23 04:29:59.557
------------------------------
• [3.021 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:56.545
    Aug 17 04:29:56.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename replication-controller 08/17/23 04:29:56.546
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:56.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:56.563
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/17/23 04:29:56.571
    STEP: waiting for RC to be added 08/17/23 04:29:56.577
    STEP: waiting for available Replicas 08/17/23 04:29:56.578
    STEP: patching ReplicationController 08/17/23 04:29:58.341
    STEP: waiting for RC to be modified 08/17/23 04:29:58.351
    STEP: patching ReplicationController status 08/17/23 04:29:58.351
    STEP: waiting for RC to be modified 08/17/23 04:29:58.358
    STEP: waiting for available Replicas 08/17/23 04:29:58.358
    STEP: fetching ReplicationController status 08/17/23 04:29:58.367
    STEP: patching ReplicationController scale 08/17/23 04:29:58.372
    STEP: waiting for RC to be modified 08/17/23 04:29:58.381
    STEP: waiting for ReplicationController's scale to be the max amount 08/17/23 04:29:58.381
    STEP: fetching ReplicationController; ensuring that it's patched 08/17/23 04:29:59.48
    STEP: updating ReplicationController status 08/17/23 04:29:59.484
    STEP: waiting for RC to be modified 08/17/23 04:29:59.491
    STEP: listing all ReplicationControllers 08/17/23 04:29:59.491
    STEP: checking that ReplicationController has expected values 08/17/23 04:29:59.495
    STEP: deleting ReplicationControllers by collection 08/17/23 04:29:59.495
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/17/23 04:29:59.503
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:29:59.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7618" for this suite. 08/17/23 04:29:59.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:29:59.567
Aug 17 04:29:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:29:59.568
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:59.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:59.589
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-c8bf6a2a-9668-42b2-9ae8-82be65b5b59b 08/17/23 04:29:59.593
STEP: Creating a pod to test consume secrets 08/17/23 04:29:59.599
Aug 17 04:29:59.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5" in namespace "projected-5666" to be "Succeeded or Failed"
Aug 17 04:29:59.613: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40041ms
Aug 17 04:30:01.619: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010075338s
Aug 17 04:30:03.620: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011787764s
STEP: Saw pod success 08/17/23 04:30:03.62
Aug 17 04:30:03.621: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5" satisfied condition "Succeeded or Failed"
Aug 17 04:30:03.627: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:30:03.684
Aug 17 04:30:03.702: INFO: Waiting for pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 to disappear
Aug 17 04:30:03.709: INFO: Pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5666" for this suite. 08/17/23 04:30:03.72
------------------------------
• [4.164 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:29:59.567
    Aug 17 04:29:59.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:29:59.568
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:29:59.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:29:59.589
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-c8bf6a2a-9668-42b2-9ae8-82be65b5b59b 08/17/23 04:29:59.593
    STEP: Creating a pod to test consume secrets 08/17/23 04:29:59.599
    Aug 17 04:29:59.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5" in namespace "projected-5666" to be "Succeeded or Failed"
    Aug 17 04:29:59.613: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40041ms
    Aug 17 04:30:01.619: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010075338s
    Aug 17 04:30:03.620: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011787764s
    STEP: Saw pod success 08/17/23 04:30:03.62
    Aug 17 04:30:03.621: INFO: Pod "pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5" satisfied condition "Succeeded or Failed"
    Aug 17 04:30:03.627: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:30:03.684
    Aug 17 04:30:03.702: INFO: Waiting for pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 to disappear
    Aug 17 04:30:03.709: INFO: Pod pod-projected-secrets-5dc05e2a-c112-4a9f-8d9d-6b01e555cde5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5666" for this suite. 08/17/23 04:30:03.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:03.734
Aug 17 04:30:03.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename webhook 08/17/23 04:30:03.735
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:03.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:03.766
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/17/23 04:30:03.795
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:30:04.594
STEP: Deploying the webhook pod 08/17/23 04:30:04.603
STEP: Wait for the deployment to be ready 08/17/23 04:30:04.619
Aug 17 04:30:04.631: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/17/23 04:30:06.647
STEP: Verifying the service has paired with the endpoint 08/17/23 04:30:06.659
Aug 17 04:30:07.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 17 04:30:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/17/23 04:30:08.18
STEP: Creating a custom resource that should be denied by the webhook 08/17/23 04:30:08.286
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/17/23 04:30:10.413
STEP: Updating the custom resource with disallowed data should be denied 08/17/23 04:30:10.466
STEP: Deleting the custom resource should be denied 08/17/23 04:30:10.523
STEP: Remove the offending key and value from the custom resource data 08/17/23 04:30:10.577
STEP: Deleting the updated custom resource should be successful 08/17/23 04:30:10.636
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8993" for this suite. 08/17/23 04:30:11.264
STEP: Destroying namespace "webhook-8993-markers" for this suite. 08/17/23 04:30:11.273
------------------------------
• [SLOW TEST] [7.564 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:03.734
    Aug 17 04:30:03.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename webhook 08/17/23 04:30:03.735
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:03.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:03.766
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/17/23 04:30:03.795
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/17/23 04:30:04.594
    STEP: Deploying the webhook pod 08/17/23 04:30:04.603
    STEP: Wait for the deployment to be ready 08/17/23 04:30:04.619
    Aug 17 04:30:04.631: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/17/23 04:30:06.647
    STEP: Verifying the service has paired with the endpoint 08/17/23 04:30:06.659
    Aug 17 04:30:07.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 17 04:30:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/17/23 04:30:08.18
    STEP: Creating a custom resource that should be denied by the webhook 08/17/23 04:30:08.286
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/17/23 04:30:10.413
    STEP: Updating the custom resource with disallowed data should be denied 08/17/23 04:30:10.466
    STEP: Deleting the custom resource should be denied 08/17/23 04:30:10.523
    STEP: Remove the offending key and value from the custom resource data 08/17/23 04:30:10.577
    STEP: Deleting the updated custom resource should be successful 08/17/23 04:30:10.636
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8993" for this suite. 08/17/23 04:30:11.264
    STEP: Destroying namespace "webhook-8993-markers" for this suite. 08/17/23 04:30:11.273
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:11.304
Aug 17 04:30:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename subpath 08/17/23 04:30:11.311
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:11.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:11.343
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/17/23 04:30:11.349
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-8288 08/17/23 04:30:11.363
STEP: Creating a pod to test atomic-volume-subpath 08/17/23 04:30:11.363
Aug 17 04:30:11.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8288" in namespace "subpath-565" to be "Succeeded or Failed"
Aug 17 04:30:11.380: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717615ms
Aug 17 04:30:13.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012769469s
Aug 17 04:30:15.388: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 4.014770972s
Aug 17 04:30:17.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 6.012825171s
Aug 17 04:30:19.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 8.012667793s
Aug 17 04:30:21.385: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 10.012056735s
Aug 17 04:30:23.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 12.012783048s
Aug 17 04:30:25.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 14.012586537s
Aug 17 04:30:27.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 16.012597022s
Aug 17 04:30:29.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 18.012797794s
Aug 17 04:30:31.387: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 20.014034908s
Aug 17 04:30:33.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 22.012991394s
Aug 17 04:30:35.385: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=false. Elapsed: 24.012023007s
Aug 17 04:30:37.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012861338s
STEP: Saw pod success 08/17/23 04:30:37.386
Aug 17 04:30:37.386: INFO: Pod "pod-subpath-test-downwardapi-8288" satisfied condition "Succeeded or Failed"
Aug 17 04:30:37.391: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-downwardapi-8288 container test-container-subpath-downwardapi-8288: <nil>
STEP: delete the pod 08/17/23 04:30:37.402
Aug 17 04:30:37.412: INFO: Waiting for pod pod-subpath-test-downwardapi-8288 to disappear
Aug 17 04:30:37.416: INFO: Pod pod-subpath-test-downwardapi-8288 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8288 08/17/23 04:30:37.416
Aug 17 04:30:37.416: INFO: Deleting pod "pod-subpath-test-downwardapi-8288" in namespace "subpath-565"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:37.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-565" for this suite. 08/17/23 04:30:37.428
------------------------------
• [SLOW TEST] [26.131 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:11.304
    Aug 17 04:30:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename subpath 08/17/23 04:30:11.311
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:11.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:11.343
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/17/23 04:30:11.349
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-8288 08/17/23 04:30:11.363
    STEP: Creating a pod to test atomic-volume-subpath 08/17/23 04:30:11.363
    Aug 17 04:30:11.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8288" in namespace "subpath-565" to be "Succeeded or Failed"
    Aug 17 04:30:11.380: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717615ms
    Aug 17 04:30:13.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012769469s
    Aug 17 04:30:15.388: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 4.014770972s
    Aug 17 04:30:17.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 6.012825171s
    Aug 17 04:30:19.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 8.012667793s
    Aug 17 04:30:21.385: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 10.012056735s
    Aug 17 04:30:23.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 12.012783048s
    Aug 17 04:30:25.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 14.012586537s
    Aug 17 04:30:27.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 16.012597022s
    Aug 17 04:30:29.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 18.012797794s
    Aug 17 04:30:31.387: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 20.014034908s
    Aug 17 04:30:33.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=true. Elapsed: 22.012991394s
    Aug 17 04:30:35.385: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Running", Reason="", readiness=false. Elapsed: 24.012023007s
    Aug 17 04:30:37.386: INFO: Pod "pod-subpath-test-downwardapi-8288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012861338s
    STEP: Saw pod success 08/17/23 04:30:37.386
    Aug 17 04:30:37.386: INFO: Pod "pod-subpath-test-downwardapi-8288" satisfied condition "Succeeded or Failed"
    Aug 17 04:30:37.391: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-subpath-test-downwardapi-8288 container test-container-subpath-downwardapi-8288: <nil>
    STEP: delete the pod 08/17/23 04:30:37.402
    Aug 17 04:30:37.412: INFO: Waiting for pod pod-subpath-test-downwardapi-8288 to disappear
    Aug 17 04:30:37.416: INFO: Pod pod-subpath-test-downwardapi-8288 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-8288 08/17/23 04:30:37.416
    Aug 17 04:30:37.416: INFO: Deleting pod "pod-subpath-test-downwardapi-8288" in namespace "subpath-565"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:37.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-565" for this suite. 08/17/23 04:30:37.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:37.437
Aug 17 04:30:37.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-runtime 08/17/23 04:30:37.437
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:37.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:37.456
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/17/23 04:30:37.46
STEP: wait for the container to reach Succeeded 08/17/23 04:30:37.469
STEP: get the container status 08/17/23 04:30:41.497
STEP: the container should be terminated 08/17/23 04:30:41.505
STEP: the termination message should be set 08/17/23 04:30:41.506
Aug 17 04:30:41.506: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/17/23 04:30:41.506
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:41.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1455" for this suite. 08/17/23 04:30:41.531
------------------------------
• [4.103 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:37.437
    Aug 17 04:30:37.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-runtime 08/17/23 04:30:37.437
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:37.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:37.456
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/17/23 04:30:37.46
    STEP: wait for the container to reach Succeeded 08/17/23 04:30:37.469
    STEP: get the container status 08/17/23 04:30:41.497
    STEP: the container should be terminated 08/17/23 04:30:41.505
    STEP: the termination message should be set 08/17/23 04:30:41.506
    Aug 17 04:30:41.506: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/17/23 04:30:41.506
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:41.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1455" for this suite. 08/17/23 04:30:41.531
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:41.54
Aug 17 04:30:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename watch 08/17/23 04:30:41.541
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:41.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:41.559
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/17/23 04:30:41.563
STEP: modifying the configmap once 08/17/23 04:30:41.568
STEP: modifying the configmap a second time 08/17/23 04:30:41.579
STEP: deleting the configmap 08/17/23 04:30:41.59
STEP: creating a watch on configmaps from the resource version returned by the first update 08/17/23 04:30:41.598
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/17/23 04:30:41.601
Aug 17 04:30:41.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2350  e2641989-3740-4869-94d7-579136484cae 251462 0 2023-08-17 04:30:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 04:30:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 17 04:30:41.601: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2350  e2641989-3740-4869-94d7-579136484cae 251463 0 2023-08-17 04:30:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 04:30:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:41.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2350" for this suite. 08/17/23 04:30:41.608
------------------------------
• [0.076 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:41.54
    Aug 17 04:30:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename watch 08/17/23 04:30:41.541
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:41.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:41.559
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/17/23 04:30:41.563
    STEP: modifying the configmap once 08/17/23 04:30:41.568
    STEP: modifying the configmap a second time 08/17/23 04:30:41.579
    STEP: deleting the configmap 08/17/23 04:30:41.59
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/17/23 04:30:41.598
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/17/23 04:30:41.601
    Aug 17 04:30:41.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2350  e2641989-3740-4869-94d7-579136484cae 251462 0 2023-08-17 04:30:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 04:30:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 17 04:30:41.601: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2350  e2641989-3740-4869-94d7-579136484cae 251463 0 2023-08-17 04:30:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-17 04:30:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:41.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2350" for this suite. 08/17/23 04:30:41.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:41.619
Aug 17 04:30:41.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:30:41.62
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:41.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:41.64
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-c41717bd-262b-47bc-8824-b90cdb4b5f42 08/17/23 04:30:41.644
STEP: Creating a pod to test consume secrets 08/17/23 04:30:41.651
Aug 17 04:30:41.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f" in namespace "projected-6075" to be "Succeeded or Failed"
Aug 17 04:30:41.667: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.162101ms
Aug 17 04:30:43.672: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012417487s
Aug 17 04:30:45.674: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01408573s
STEP: Saw pod success 08/17/23 04:30:45.674
Aug 17 04:30:45.674: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f" satisfied condition "Succeeded or Failed"
Aug 17 04:30:45.679: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:30:45.688
Aug 17 04:30:45.702: INFO: Waiting for pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f to disappear
Aug 17 04:30:45.706: INFO: Pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6075" for this suite. 08/17/23 04:30:45.714
------------------------------
• [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:41.619
    Aug 17 04:30:41.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:30:41.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:41.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:41.64
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-c41717bd-262b-47bc-8824-b90cdb4b5f42 08/17/23 04:30:41.644
    STEP: Creating a pod to test consume secrets 08/17/23 04:30:41.651
    Aug 17 04:30:41.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f" in namespace "projected-6075" to be "Succeeded or Failed"
    Aug 17 04:30:41.667: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.162101ms
    Aug 17 04:30:43.672: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012417487s
    Aug 17 04:30:45.674: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01408573s
    STEP: Saw pod success 08/17/23 04:30:45.674
    Aug 17 04:30:45.674: INFO: Pod "pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f" satisfied condition "Succeeded or Failed"
    Aug 17 04:30:45.679: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:30:45.688
    Aug 17 04:30:45.702: INFO: Waiting for pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f to disappear
    Aug 17 04:30:45.706: INFO: Pod pod-projected-secrets-12d49b37-127d-4143-89e2-34ef42fbbb6f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6075" for this suite. 08/17/23 04:30:45.714
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:45.726
Aug 17 04:30:45.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename pods 08/17/23 04:30:45.727
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:45.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:45.746
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/17/23 04:30:45.75
STEP: submitting the pod to kubernetes 08/17/23 04:30:45.751
Aug 17 04:30:45.760: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" in namespace "pods-6091" to be "running and ready"
Aug 17 04:30:45.765: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.608675ms
Aug 17 04:30:45.765: INFO: The phase of Pod pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:30:47.772: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011620386s
Aug 17 04:30:47.772: INFO: The phase of Pod pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d is Running (Ready = true)
Aug 17 04:30:47.772: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/17/23 04:30:47.777
STEP: updating the pod 08/17/23 04:30:47.782
Aug 17 04:30:48.299: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d"
Aug 17 04:30:48.299: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" in namespace "pods-6091" to be "terminated with reason DeadlineExceeded"
Aug 17 04:30:48.304: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 4.869427ms
Aug 17 04:30:50.311: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012054802s
Aug 17 04:30:52.310: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011126436s
Aug 17 04:30:52.310: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:52.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6091" for this suite. 08/17/23 04:30:52.319
------------------------------
• [SLOW TEST] [6.601 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:45.726
    Aug 17 04:30:45.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename pods 08/17/23 04:30:45.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:45.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:45.746
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/17/23 04:30:45.75
    STEP: submitting the pod to kubernetes 08/17/23 04:30:45.751
    Aug 17 04:30:45.760: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" in namespace "pods-6091" to be "running and ready"
    Aug 17 04:30:45.765: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.608675ms
    Aug 17 04:30:45.765: INFO: The phase of Pod pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:30:47.772: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011620386s
    Aug 17 04:30:47.772: INFO: The phase of Pod pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d is Running (Ready = true)
    Aug 17 04:30:47.772: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/17/23 04:30:47.777
    STEP: updating the pod 08/17/23 04:30:47.782
    Aug 17 04:30:48.299: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d"
    Aug 17 04:30:48.299: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" in namespace "pods-6091" to be "terminated with reason DeadlineExceeded"
    Aug 17 04:30:48.304: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 4.869427ms
    Aug 17 04:30:50.311: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012054802s
    Aug 17 04:30:52.310: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011126436s
    Aug 17 04:30:52.310: INFO: Pod "pod-update-activedeadlineseconds-5be6f6bd-ff07-4c86-8a3d-878d2e87471d" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:52.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6091" for this suite. 08/17/23 04:30:52.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:52.329
Aug 17 04:30:52.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:30:52.33
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:52.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:52.349
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/17/23 04:30:52.354
Aug 17 04:30:52.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7" in namespace "downward-api-5822" to be "Succeeded or Failed"
Aug 17 04:30:52.370: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314325ms
Aug 17 04:30:54.375: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009611601s
Aug 17 04:30:56.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Running", Reason="", readiness=false. Elapsed: 4.011228181s
Aug 17 04:30:58.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011695908s
STEP: Saw pod success 08/17/23 04:30:58.377
Aug 17 04:30:58.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7" satisfied condition "Succeeded or Failed"
Aug 17 04:30:58.382: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 container client-container: <nil>
STEP: delete the pod 08/17/23 04:30:58.393
Aug 17 04:30:58.406: INFO: Waiting for pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 to disappear
Aug 17 04:30:58.410: INFO: Pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 17 04:30:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5822" for this suite. 08/17/23 04:30:58.417
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:52.329
    Aug 17 04:30:52.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:30:52.33
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:52.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:52.349
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/17/23 04:30:52.354
    Aug 17 04:30:52.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7" in namespace "downward-api-5822" to be "Succeeded or Failed"
    Aug 17 04:30:52.370: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314325ms
    Aug 17 04:30:54.375: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009611601s
    Aug 17 04:30:56.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Running", Reason="", readiness=false. Elapsed: 4.011228181s
    Aug 17 04:30:58.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011695908s
    STEP: Saw pod success 08/17/23 04:30:58.377
    Aug 17 04:30:58.377: INFO: Pod "downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7" satisfied condition "Succeeded or Failed"
    Aug 17 04:30:58.382: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 container client-container: <nil>
    STEP: delete the pod 08/17/23 04:30:58.393
    Aug 17 04:30:58.406: INFO: Waiting for pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 to disappear
    Aug 17 04:30:58.410: INFO: Pod downwardapi-volume-6451525e-8167-400f-b3be-d943d7c04cd7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:30:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5822" for this suite. 08/17/23 04:30:58.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:30:58.426
Aug 17 04:30:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 04:30:58.427
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:58.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:58.446
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 17 04:30:58.460: INFO: Waiting up to 2m0s for pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" in namespace "var-expansion-3615" to be "container 0 failed with reason CreateContainerConfigError"
Aug 17 04:30:58.464: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204938ms
Aug 17 04:31:00.470: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010138819s
Aug 17 04:31:00.470: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 17 04:31:00.470: INFO: Deleting pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" in namespace "var-expansion-3615"
Aug 17 04:31:00.479: INFO: Wait up to 5m0s for pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:04.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3615" for this suite. 08/17/23 04:31:04.5
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:30:58.426
    Aug 17 04:30:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 04:30:58.427
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:30:58.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:30:58.446
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 17 04:30:58.460: INFO: Waiting up to 2m0s for pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" in namespace "var-expansion-3615" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 17 04:30:58.464: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204938ms
    Aug 17 04:31:00.470: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010138819s
    Aug 17 04:31:00.470: INFO: Pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 17 04:31:00.470: INFO: Deleting pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" in namespace "var-expansion-3615"
    Aug 17 04:31:00.479: INFO: Wait up to 5m0s for pod "var-expansion-2dc4015b-0d81-45d4-afec-5ed0ab0ddec4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:04.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3615" for this suite. 08/17/23 04:31:04.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:04.515
Aug 17 04:31:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename deployment 08/17/23 04:31:04.516
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:04.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:04.542
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/17/23 04:31:04.554
STEP: waiting for Deployment to be created 08/17/23 04:31:04.562
STEP: waiting for all Replicas to be Ready 08/17/23 04:31:04.565
Aug 17 04:31:04.568: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.568: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.590: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.590: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.607: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:04.607: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 17 04:31:05.633: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 17 04:31:05.633: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 17 04:31:06.560: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/17/23 04:31:06.56
W0817 04:31:06.570120      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 04:31:06.573: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/17/23 04:31:06.573
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.584: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.584: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.596: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.596: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:06.610: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:06.610: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:06.621: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:06.621: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:08.578: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:08.578: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:08.604: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
STEP: listing Deployments 08/17/23 04:31:08.604
Aug 17 04:31:08.610: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/17/23 04:31:08.61
Aug 17 04:31:08.625: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/17/23 04:31:08.625
Aug 17 04:31:08.635: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:08.635: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:08.650: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:08.662: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:09.679: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:10.597: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:10.616: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:10.629: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 17 04:31:11.661: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/17/23 04:31:11.682
STEP: fetching the DeploymentStatus 08/17/23 04:31:11.691
Aug 17 04:31:11.699: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3
Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:11.701: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
Aug 17 04:31:11.701: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3
STEP: deleting the Deployment 08/17/23 04:31:11.701
Aug 17 04:31:11.713: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.714: INFO: observed event type MODIFIED
Aug 17 04:31:11.715: INFO: observed event type MODIFIED
Aug 17 04:31:11.715: INFO: observed event type MODIFIED
Aug 17 04:31:11.715: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 17 04:31:11.719: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 17 04:31:11.724: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6002  e2605152-6ad9-4f29-a463-1b04477a80b5 251881 2 2023-08-17 04:31:08 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f287 0xc004d8f288}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f310 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 17 04:31:11.729: INFO: pod: "test-deployment-7b7876f9d6-b4fdj":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-b4fdj test-deployment-7b7876f9d6- deployment-6002  8d266a82-44f8-4c65-a0c9-0a49510d9e65 251842 0 2023-08-17 04:31:08 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:7a5ee059fc1befc7409845816885128981a2602bbe1b9b1431d538948aa12b14 cni.projectcalico.org/podIP:172.21.15.76/32 cni.projectcalico.org/podIPs:172.21.15.76/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e2605152-6ad9-4f29-a463-1b04477a80b5 0xc004d8f7a7 0xc004d8f7a8}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2605152-6ad9-4f29-a463-1b04477a80b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hscnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hscnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.76,StartTime:2023-08-17 04:31:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45519260af6a84420740c9ee9223ad7214e933e80159d9989c7af47f26d8b04d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 04:31:11.730: INFO: pod: "test-deployment-7b7876f9d6-pnqs6":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-pnqs6 test-deployment-7b7876f9d6- deployment-6002  1d0e4679-f55e-4829-b5ec-3261cdc8aa41 251880 0 2023-08-17 04:31:10 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2889e364c09b608494b7075cae59725654313d174d546ef83af823975ab37381 cni.projectcalico.org/podIP:172.21.86.179/32 cni.projectcalico.org/podIPs:172.21.86.179/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e2605152-6ad9-4f29-a463-1b04477a80b5 0xc004d8f9d7 0xc004d8f9d8}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2605152-6ad9-4f29-a463-1b04477a80b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssgqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssgqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.179,StartTime:2023-08-17 04:31:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2dcf34c63e6f3a346e9840ab5d9485729399dbf662eb2f7ec2b738dd247b6f94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 04:31:11.730: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6002  841b6166-a573-4bdf-acbd-36a1a47a3947 251889 4 2023-08-17 04:31:06 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f377 0xc004d8f378}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f400 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 17 04:31:11.734: INFO: pod: "test-deployment-7df74c55ff-l9rfb":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-l9rfb test-deployment-7df74c55ff- deployment-6002  b6b98b57-5f5a-49bc-908f-069cb377d1a3 251884 0 2023-08-17 04:31:08 +0000 UTC 2023-08-17 04:31:12 +0000 UTC 0xc007938c20 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:3f78a6a47e10064810fff3ebd8e7790520044bf2a8ecd5228bd1637a973229a2 cni.projectcalico.org/podIP:172.21.86.157/32 cni.projectcalico.org/podIPs:172.21.86.157/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 841b6166-a573-4bdf-acbd-36a1a47a3947 0xc007938c77 0xc007938c78}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"841b6166-a573-4bdf-acbd-36a1a47a3947\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gpcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gpcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.157,StartTime:2023-08-17 04:31:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2cc58a313470a6ec0c08323484e8abd1a6898d02b99de43d12a10fd3f83ba62b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 17 04:31:11.734: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6002  3c1516cb-87d1-489c-842d-3ec4e6369858 251774 3 2023-08-17 04:31:04 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f467 0xc004d8f468}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f4f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:11.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6002" for this suite. 08/17/23 04:31:11.748
------------------------------
• [SLOW TEST] [7.241 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:04.515
    Aug 17 04:31:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename deployment 08/17/23 04:31:04.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:04.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:04.542
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/17/23 04:31:04.554
    STEP: waiting for Deployment to be created 08/17/23 04:31:04.562
    STEP: waiting for all Replicas to be Ready 08/17/23 04:31:04.565
    Aug 17 04:31:04.568: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.568: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.590: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.590: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.607: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:04.607: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 17 04:31:05.633: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 17 04:31:05.633: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 17 04:31:06.560: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/17/23 04:31:06.56
    W0817 04:31:06.570120      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 04:31:06.573: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/17/23 04:31:06.573
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.576: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 0
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.577: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.584: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.584: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.596: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.596: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:06.610: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:06.610: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:06.621: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:06.621: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:08.578: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:08.578: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:08.604: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    STEP: listing Deployments 08/17/23 04:31:08.604
    Aug 17 04:31:08.610: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/17/23 04:31:08.61
    Aug 17 04:31:08.625: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/17/23 04:31:08.625
    Aug 17 04:31:08.635: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:08.635: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:08.650: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:08.662: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:09.679: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:10.597: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:10.616: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:10.629: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 17 04:31:11.661: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/17/23 04:31:11.682
    STEP: fetching the DeploymentStatus 08/17/23 04:31:11.691
    Aug 17 04:31:11.699: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 1
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3
    Aug 17 04:31:11.700: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:11.701: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 2
    Aug 17 04:31:11.701: INFO: observed Deployment test-deployment in namespace deployment-6002 with ReadyReplicas 3
    STEP: deleting the Deployment 08/17/23 04:31:11.701
    Aug 17 04:31:11.713: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.714: INFO: observed event type MODIFIED
    Aug 17 04:31:11.715: INFO: observed event type MODIFIED
    Aug 17 04:31:11.715: INFO: observed event type MODIFIED
    Aug 17 04:31:11.715: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 17 04:31:11.719: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 17 04:31:11.724: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6002  e2605152-6ad9-4f29-a463-1b04477a80b5 251881 2 2023-08-17 04:31:08 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f287 0xc004d8f288}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f310 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 17 04:31:11.729: INFO: pod: "test-deployment-7b7876f9d6-b4fdj":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-b4fdj test-deployment-7b7876f9d6- deployment-6002  8d266a82-44f8-4c65-a0c9-0a49510d9e65 251842 0 2023-08-17 04:31:08 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:7a5ee059fc1befc7409845816885128981a2602bbe1b9b1431d538948aa12b14 cni.projectcalico.org/podIP:172.21.15.76/32 cni.projectcalico.org/podIPs:172.21.15.76/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e2605152-6ad9-4f29-a463-1b04477a80b5 0xc004d8f7a7 0xc004d8f7a8}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2605152-6ad9-4f29-a463-1b04477a80b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.15.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hscnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hscnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-ubuntu-79fff84d86x69988-vjwlx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.15.76,StartTime:2023-08-17 04:31:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45519260af6a84420740c9ee9223ad7214e933e80159d9989c7af47f26d8b04d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.15.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 04:31:11.730: INFO: pod: "test-deployment-7b7876f9d6-pnqs6":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-pnqs6 test-deployment-7b7876f9d6- deployment-6002  1d0e4679-f55e-4829-b5ec-3261cdc8aa41 251880 0 2023-08-17 04:31:10 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2889e364c09b608494b7075cae59725654313d174d546ef83af823975ab37381 cni.projectcalico.org/podIP:172.21.86.179/32 cni.projectcalico.org/podIPs:172.21.86.179/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e2605152-6ad9-4f29-a463-1b04477a80b5 0xc004d8f9d7 0xc004d8f9d8}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2605152-6ad9-4f29-a463-1b04477a80b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssgqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssgqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.179,StartTime:2023-08-17 04:31:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2dcf34c63e6f3a346e9840ab5d9485729399dbf662eb2f7ec2b738dd247b6f94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 04:31:11.730: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6002  841b6166-a573-4bdf-acbd-36a1a47a3947 251889 4 2023-08-17 04:31:06 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f377 0xc004d8f378}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f400 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 17 04:31:11.734: INFO: pod: "test-deployment-7df74c55ff-l9rfb":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-l9rfb test-deployment-7df74c55ff- deployment-6002  b6b98b57-5f5a-49bc-908f-069cb377d1a3 251884 0 2023-08-17 04:31:08 +0000 UTC 2023-08-17 04:31:12 +0000 UTC 0xc007938c20 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:3f78a6a47e10064810fff3ebd8e7790520044bf2a8ecd5228bd1637a973229a2 cni.projectcalico.org/podIP:172.21.86.157/32 cni.projectcalico.org/podIPs:172.21.86.157/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 841b6166-a573-4bdf-acbd-36a1a47a3947 0xc007938c77 0xc007938c78}] [] [{kube-controller-manager Update v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"841b6166-a573-4bdf-acbd-36a1a47a3947\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-17 04:31:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.86.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gpcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gpcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-6c9d465fc4xbjl6l-wz7jz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-17 04:31:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.86.157,StartTime:2023-08-17 04:31:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-17 04:31:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://2cc58a313470a6ec0c08323484e8abd1a6898d02b99de43d12a10fd3f83ba62b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.86.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 17 04:31:11.734: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6002  3c1516cb-87d1-489c-842d-3ec4e6369858 251774 3 2023-08-17 04:31:04 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 14eba34c-d936-495d-a3ca-d623e3533d90 0xc004d8f467 0xc004d8f468}] [] [{kube-controller-manager Update apps/v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14eba34c-d936-495d-a3ca-d623e3533d90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-17 04:31:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8f4f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:11.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6002" for this suite. 08/17/23 04:31:11.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:11.758
Aug 17 04:31:11.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename services 08/17/23 04:31:11.759
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:11.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:11.779
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/17/23 04:31:11.784
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:11.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8169" for this suite. 08/17/23 04:31:11.796
------------------------------
• [0.048 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:11.758
    Aug 17 04:31:11.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename services 08/17/23 04:31:11.759
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:11.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:11.779
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/17/23 04:31:11.784
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:11.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8169" for this suite. 08/17/23 04:31:11.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:11.81
Aug 17 04:31:11.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:31:11.811
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:11.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:11.832
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-20c7a64f-54eb-427b-85c6-3e2cb4c16af6 08/17/23 04:31:11.837
STEP: Creating a pod to test consume secrets 08/17/23 04:31:11.843
Aug 17 04:31:11.853: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2" in namespace "projected-8251" to be "Succeeded or Failed"
Aug 17 04:31:11.863: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.180647ms
Aug 17 04:31:13.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015709534s
Aug 17 04:31:15.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015033407s
STEP: Saw pod success 08/17/23 04:31:15.869
Aug 17 04:31:15.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2" satisfied condition "Succeeded or Failed"
Aug 17 04:31:15.873: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:31:15.882
Aug 17 04:31:15.893: INFO: Waiting for pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 to disappear
Aug 17 04:31:15.897: INFO: Pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:15.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8251" for this suite. 08/17/23 04:31:15.905
------------------------------
• [4.103 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:11.81
    Aug 17 04:31:11.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:31:11.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:11.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:11.832
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-20c7a64f-54eb-427b-85c6-3e2cb4c16af6 08/17/23 04:31:11.837
    STEP: Creating a pod to test consume secrets 08/17/23 04:31:11.843
    Aug 17 04:31:11.853: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2" in namespace "projected-8251" to be "Succeeded or Failed"
    Aug 17 04:31:11.863: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.180647ms
    Aug 17 04:31:13.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015709534s
    Aug 17 04:31:15.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015033407s
    STEP: Saw pod success 08/17/23 04:31:15.869
    Aug 17 04:31:15.869: INFO: Pod "pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2" satisfied condition "Succeeded or Failed"
    Aug 17 04:31:15.873: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:31:15.882
    Aug 17 04:31:15.893: INFO: Waiting for pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 to disappear
    Aug 17 04:31:15.897: INFO: Pod pod-projected-secrets-5dfd6f94-85b2-4469-a388-e0ff472040a2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:15.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8251" for this suite. 08/17/23 04:31:15.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:15.913
Aug 17 04:31:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename kubectl 08/17/23 04:31:15.914
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:15.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:15.933
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/17/23 04:31:15.938
Aug 17 04:31:15.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-640 cluster-info'
Aug 17 04:31:16.011: INFO: stderr: ""
Aug 17 04:31:16.011: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:16.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-640" for this suite. 08/17/23 04:31:16.02
------------------------------
• [0.115 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:15.913
    Aug 17 04:31:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename kubectl 08/17/23 04:31:15.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:15.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:15.933
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/17/23 04:31:15.938
    Aug 17 04:31:15.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2331764595 --namespace=kubectl-640 cluster-info'
    Aug 17 04:31:16.011: INFO: stderr: ""
    Aug 17 04:31:16.011: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:16.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-640" for this suite. 08/17/23 04:31:16.02
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:16.029
Aug 17 04:31:16.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 04:31:16.03
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:16.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:16.05
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-3139/secret-test-fd024ab1-26b1-4814-af80-6fbfdc830bce 08/17/23 04:31:16.056
STEP: Creating a pod to test consume secrets 08/17/23 04:31:16.062
Aug 17 04:31:16.073: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a" in namespace "secrets-3139" to be "Succeeded or Failed"
Aug 17 04:31:16.078: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399847ms
Aug 17 04:31:18.084: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246107s
Aug 17 04:31:20.085: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011475871s
STEP: Saw pod success 08/17/23 04:31:20.085
Aug 17 04:31:20.085: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a" satisfied condition "Succeeded or Failed"
Aug 17 04:31:20.090: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a container env-test: <nil>
STEP: delete the pod 08/17/23 04:31:20.14
Aug 17 04:31:20.153: INFO: Waiting for pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a to disappear
Aug 17 04:31:20.156: INFO: Pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:20.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3139" for this suite. 08/17/23 04:31:20.165
------------------------------
• [4.144 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:16.029
    Aug 17 04:31:16.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 04:31:16.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:16.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:16.05
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-3139/secret-test-fd024ab1-26b1-4814-af80-6fbfdc830bce 08/17/23 04:31:16.056
    STEP: Creating a pod to test consume secrets 08/17/23 04:31:16.062
    Aug 17 04:31:16.073: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a" in namespace "secrets-3139" to be "Succeeded or Failed"
    Aug 17 04:31:16.078: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399847ms
    Aug 17 04:31:18.084: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246107s
    Aug 17 04:31:20.085: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011475871s
    STEP: Saw pod success 08/17/23 04:31:20.085
    Aug 17 04:31:20.085: INFO: Pod "pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a" satisfied condition "Succeeded or Failed"
    Aug 17 04:31:20.090: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a container env-test: <nil>
    STEP: delete the pod 08/17/23 04:31:20.14
    Aug 17 04:31:20.153: INFO: Waiting for pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a to disappear
    Aug 17 04:31:20.156: INFO: Pod pod-configmaps-e9b3bbc0-e39a-4043-b536-cad04074be4a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:20.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3139" for this suite. 08/17/23 04:31:20.165
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:20.173
Aug 17 04:31:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 04:31:20.175
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:20.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:20.193
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/17/23 04:31:20.206
Aug 17 04:31:20.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8591" to be "running and ready"
Aug 17 04:31:20.223: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.614284ms
Aug 17 04:31:20.223: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:31:22.228: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011422627s
Aug 17 04:31:22.228: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 17 04:31:22.228: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/17/23 04:31:22.233
Aug 17 04:31:22.240: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8591" to be "running and ready"
Aug 17 04:31:22.246: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566908ms
Aug 17 04:31:22.246: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:31:24.251: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010591243s
Aug 17 04:31:24.251: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 17 04:31:24.251: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/17/23 04:31:24.255
STEP: delete the pod with lifecycle hook 08/17/23 04:31:24.304
Aug 17 04:31:24.312: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 04:31:24.317: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 04:31:26.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 04:31:26.322: INFO: Pod pod-with-poststart-http-hook still exists
Aug 17 04:31:28.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 17 04:31:28.323: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:28.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8591" for this suite. 08/17/23 04:31:28.332
------------------------------
• [SLOW TEST] [8.168 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:20.173
    Aug 17 04:31:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/17/23 04:31:20.175
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:20.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:20.193
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/17/23 04:31:20.206
    Aug 17 04:31:20.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8591" to be "running and ready"
    Aug 17 04:31:20.223: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.614284ms
    Aug 17 04:31:20.223: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:31:22.228: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011422627s
    Aug 17 04:31:22.228: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 17 04:31:22.228: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/17/23 04:31:22.233
    Aug 17 04:31:22.240: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8591" to be "running and ready"
    Aug 17 04:31:22.246: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566908ms
    Aug 17 04:31:22.246: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:31:24.251: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010591243s
    Aug 17 04:31:24.251: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 17 04:31:24.251: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/17/23 04:31:24.255
    STEP: delete the pod with lifecycle hook 08/17/23 04:31:24.304
    Aug 17 04:31:24.312: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 04:31:24.317: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 17 04:31:26.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 04:31:26.322: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 17 04:31:28.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 17 04:31:28.323: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:28.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8591" for this suite. 08/17/23 04:31:28.332
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:28.344
Aug 17 04:31:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename var-expansion 08/17/23 04:31:28.345
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:28.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:28.365
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 17 04:31:28.377: INFO: Waiting up to 2m0s for pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" in namespace "var-expansion-1024" to be "container 0 failed with reason CreateContainerConfigError"
Aug 17 04:31:28.381: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610627ms
Aug 17 04:31:30.387: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009974358s
Aug 17 04:31:30.387: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 17 04:31:30.387: INFO: Deleting pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" in namespace "var-expansion-1024"
Aug 17 04:31:30.396: INFO: Wait up to 5m0s for pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 17 04:31:34.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1024" for this suite. 08/17/23 04:31:34.416
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:28.344
    Aug 17 04:31:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename var-expansion 08/17/23 04:31:28.345
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:28.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:28.365
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 17 04:31:28.377: INFO: Waiting up to 2m0s for pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" in namespace "var-expansion-1024" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 17 04:31:28.381: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610627ms
    Aug 17 04:31:30.387: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009974358s
    Aug 17 04:31:30.387: INFO: Pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 17 04:31:30.387: INFO: Deleting pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" in namespace "var-expansion-1024"
    Aug 17 04:31:30.396: INFO: Wait up to 5m0s for pod "var-expansion-243c04e8-7a5a-4fed-8549-915a3d7cc4fd" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:31:34.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1024" for this suite. 08/17/23 04:31:34.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:31:34.425
Aug 17 04:31:34.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 04:31:34.425
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:34.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:34.444
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-e9131947-3847-47e9-917b-4dc15be94206 in namespace container-probe-4083 08/17/23 04:31:34.449
Aug 17 04:31:34.459: INFO: Waiting up to 5m0s for pod "busybox-e9131947-3847-47e9-917b-4dc15be94206" in namespace "container-probe-4083" to be "not pending"
Aug 17 04:31:34.463: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191418ms
Aug 17 04:31:36.469: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206": Phase="Running", Reason="", readiness=true. Elapsed: 2.00986644s
Aug 17 04:31:36.469: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206" satisfied condition "not pending"
Aug 17 04:31:36.469: INFO: Started pod busybox-e9131947-3847-47e9-917b-4dc15be94206 in namespace container-probe-4083
STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:31:36.469
Aug 17 04:31:36.474: INFO: Initial restart count of pod busybox-e9131947-3847-47e9-917b-4dc15be94206 is 0
Aug 17 04:32:26.634: INFO: Restart count of pod container-probe-4083/busybox-e9131947-3847-47e9-917b-4dc15be94206 is now 1 (50.160339161s elapsed)
STEP: deleting the pod 08/17/23 04:32:26.634
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 04:32:26.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4083" for this suite. 08/17/23 04:32:26.658
------------------------------
• [SLOW TEST] [52.242 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:31:34.425
    Aug 17 04:31:34.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 04:31:34.425
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:31:34.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:31:34.444
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-e9131947-3847-47e9-917b-4dc15be94206 in namespace container-probe-4083 08/17/23 04:31:34.449
    Aug 17 04:31:34.459: INFO: Waiting up to 5m0s for pod "busybox-e9131947-3847-47e9-917b-4dc15be94206" in namespace "container-probe-4083" to be "not pending"
    Aug 17 04:31:34.463: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191418ms
    Aug 17 04:31:36.469: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206": Phase="Running", Reason="", readiness=true. Elapsed: 2.00986644s
    Aug 17 04:31:36.469: INFO: Pod "busybox-e9131947-3847-47e9-917b-4dc15be94206" satisfied condition "not pending"
    Aug 17 04:31:36.469: INFO: Started pod busybox-e9131947-3847-47e9-917b-4dc15be94206 in namespace container-probe-4083
    STEP: checking the pod's current state and verifying that restartCount is present 08/17/23 04:31:36.469
    Aug 17 04:31:36.474: INFO: Initial restart count of pod busybox-e9131947-3847-47e9-917b-4dc15be94206 is 0
    Aug 17 04:32:26.634: INFO: Restart count of pod container-probe-4083/busybox-e9131947-3847-47e9-917b-4dc15be94206 is now 1 (50.160339161s elapsed)
    STEP: deleting the pod 08/17/23 04:32:26.634
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:32:26.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4083" for this suite. 08/17/23 04:32:26.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:32:26.668
Aug 17 04:32:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:32:26.668
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:26.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:26.69
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/17/23 04:32:26.695
Aug 17 04:32:26.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc" in namespace "projected-5710" to be "Succeeded or Failed"
Aug 17 04:32:26.713: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146437ms
Aug 17 04:32:28.720: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012882702s
Aug 17 04:32:30.721: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014050821s
Aug 17 04:32:32.719: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012024139s
STEP: Saw pod success 08/17/23 04:32:32.719
Aug 17 04:32:32.719: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc" satisfied condition "Succeeded or Failed"
Aug 17 04:32:32.723: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc container client-container: <nil>
STEP: delete the pod 08/17/23 04:32:32.774
Aug 17 04:32:32.787: INFO: Waiting for pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc to disappear
Aug 17 04:32:32.791: INFO: Pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 17 04:32:32.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5710" for this suite. 08/17/23 04:32:32.8
------------------------------
• [SLOW TEST] [6.144 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:32:26.668
    Aug 17 04:32:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:32:26.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:26.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:26.69
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/17/23 04:32:26.695
    Aug 17 04:32:26.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc" in namespace "projected-5710" to be "Succeeded or Failed"
    Aug 17 04:32:26.713: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146437ms
    Aug 17 04:32:28.720: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012882702s
    Aug 17 04:32:30.721: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014050821s
    Aug 17 04:32:32.719: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012024139s
    STEP: Saw pod success 08/17/23 04:32:32.719
    Aug 17 04:32:32.719: INFO: Pod "downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc" satisfied condition "Succeeded or Failed"
    Aug 17 04:32:32.723: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc container client-container: <nil>
    STEP: delete the pod 08/17/23 04:32:32.774
    Aug 17 04:32:32.787: INFO: Waiting for pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc to disappear
    Aug 17 04:32:32.791: INFO: Pod downwardapi-volume-801c2b33-9621-4811-b591-0db8735d9abc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:32:32.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5710" for this suite. 08/17/23 04:32:32.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:32:32.814
Aug 17 04:32:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename sysctl 08/17/23 04:32:32.815
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:32.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:32.834
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/17/23 04:32:32.839
STEP: Watching for error events or started pod 08/17/23 04:32:32.847
STEP: Waiting for pod completion 08/17/23 04:32:34.853
Aug 17 04:32:34.853: INFO: Waiting up to 3m0s for pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56" in namespace "sysctl-7010" to be "completed"
Aug 17 04:32:34.858: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418666ms
Aug 17 04:32:36.864: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011153431s
Aug 17 04:32:36.864: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/17/23 04:32:36.869
STEP: Getting logs from the pod 08/17/23 04:32:36.87
STEP: Checking that the sysctl is actually updated 08/17/23 04:32:36.92
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:32:36.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7010" for this suite. 08/17/23 04:32:36.929
------------------------------
• [4.123 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:32:32.814
    Aug 17 04:32:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename sysctl 08/17/23 04:32:32.815
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:32.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:32.834
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/17/23 04:32:32.839
    STEP: Watching for error events or started pod 08/17/23 04:32:32.847
    STEP: Waiting for pod completion 08/17/23 04:32:34.853
    Aug 17 04:32:34.853: INFO: Waiting up to 3m0s for pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56" in namespace "sysctl-7010" to be "completed"
    Aug 17 04:32:34.858: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418666ms
    Aug 17 04:32:36.864: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011153431s
    Aug 17 04:32:36.864: INFO: Pod "sysctl-72580aac-b704-46c7-b3d5-4d66cff96a56" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/17/23 04:32:36.869
    STEP: Getting logs from the pod 08/17/23 04:32:36.87
    STEP: Checking that the sysctl is actually updated 08/17/23 04:32:36.92
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:32:36.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7010" for this suite. 08/17/23 04:32:36.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:32:36.941
Aug 17 04:32:36.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 04:32:36.941
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:36.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:36.96
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-ng6g5" 08/17/23 04:32:36.97
Aug 17 04:32:36.980: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard cpu limit of 500m
Aug 17 04:32:36.980: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-ng6g5" /status 08/17/23 04:32:36.98
STEP: Confirm /status for "e2e-rq-status-ng6g5" resourceQuota via watch 08/17/23 04:32:36.99
Aug 17 04:32:36.992: INFO: observed resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList(nil)
Aug 17 04:32:36.992: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 17 04:32:36.992: INFO: ResourceQuota "e2e-rq-status-ng6g5" /status was updated
STEP: Patching hard spec values for cpu & memory 08/17/23 04:32:36.996
Aug 17 04:32:37.003: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard cpu limit of 1
Aug 17 04:32:37.003: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-ng6g5" /status 08/17/23 04:32:37.003
STEP: Confirm /status for "e2e-rq-status-ng6g5" resourceQuota via watch 08/17/23 04:32:37.009
Aug 17 04:32:37.012: INFO: observed resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 17 04:32:37.012: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 17 04:32:37.012: INFO: ResourceQuota "e2e-rq-status-ng6g5" /status was patched
STEP: Get "e2e-rq-status-ng6g5" /status 08/17/23 04:32:37.012
Aug 17 04:32:37.018: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard cpu of 1
Aug 17 04:32:37.018: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-ng6g5" /status before checking Spec is unchanged 08/17/23 04:32:37.022
Aug 17 04:32:37.029: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard cpu of 2
Aug 17 04:32:37.029: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard memory of 2Gi
Aug 17 04:32:37.032: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 17 04:36:02.046: INFO: ResourceQuota "e2e-rq-status-ng6g5" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 04:36:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3956" for this suite. 08/17/23 04:36:02.054
------------------------------
• [SLOW TEST] [205.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:32:36.941
    Aug 17 04:32:36.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 04:32:36.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:32:36.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:32:36.96
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-ng6g5" 08/17/23 04:32:36.97
    Aug 17 04:32:36.980: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard cpu limit of 500m
    Aug 17 04:32:36.980: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-ng6g5" /status 08/17/23 04:32:36.98
    STEP: Confirm /status for "e2e-rq-status-ng6g5" resourceQuota via watch 08/17/23 04:32:36.99
    Aug 17 04:32:36.992: INFO: observed resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList(nil)
    Aug 17 04:32:36.992: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 17 04:32:36.992: INFO: ResourceQuota "e2e-rq-status-ng6g5" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/17/23 04:32:36.996
    Aug 17 04:32:37.003: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard cpu limit of 1
    Aug 17 04:32:37.003: INFO: Resource quota "e2e-rq-status-ng6g5" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-ng6g5" /status 08/17/23 04:32:37.003
    STEP: Confirm /status for "e2e-rq-status-ng6g5" resourceQuota via watch 08/17/23 04:32:37.009
    Aug 17 04:32:37.012: INFO: observed resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 17 04:32:37.012: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 17 04:32:37.012: INFO: ResourceQuota "e2e-rq-status-ng6g5" /status was patched
    STEP: Get "e2e-rq-status-ng6g5" /status 08/17/23 04:32:37.012
    Aug 17 04:32:37.018: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard cpu of 1
    Aug 17 04:32:37.018: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-ng6g5" /status before checking Spec is unchanged 08/17/23 04:32:37.022
    Aug 17 04:32:37.029: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard cpu of 2
    Aug 17 04:32:37.029: INFO: Resourcequota "e2e-rq-status-ng6g5" reports status: hard memory of 2Gi
    Aug 17 04:32:37.032: INFO: Found resourceQuota "e2e-rq-status-ng6g5" in namespace "resourcequota-3956" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 17 04:36:02.046: INFO: ResourceQuota "e2e-rq-status-ng6g5" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:36:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3956" for this suite. 08/17/23 04:36:02.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:36:02.064
Aug 17 04:36:02.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 04:36:02.067
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:36:02.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:36:02.089
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-94521f1d-4cfd-46d5-ab3d-e8519e4fdbf0 08/17/23 04:36:02.105
STEP: Creating secret with name s-test-opt-upd-685011c6-4314-4bfb-91a4-768c007d8684 08/17/23 04:36:02.113
STEP: Creating the pod 08/17/23 04:36:02.119
Aug 17 04:36:02.129: INFO: Waiting up to 5m0s for pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09" in namespace "secrets-1072" to be "running and ready"
Aug 17 04:36:02.134: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960441ms
Aug 17 04:36:02.134: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:36:04.141: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011697362s
Aug 17 04:36:04.141: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:36:06.143: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Running", Reason="", readiness=true. Elapsed: 4.013910194s
Aug 17 04:36:06.143: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Running (Ready = true)
Aug 17 04:36:06.143: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-94521f1d-4cfd-46d5-ab3d-e8519e4fdbf0 08/17/23 04:36:06.335
STEP: Updating secret s-test-opt-upd-685011c6-4314-4bfb-91a4-768c007d8684 08/17/23 04:36:06.347
STEP: Creating secret with name s-test-opt-create-14d69123-5545-42c3-8154-b90ad34a841f 08/17/23 04:36:06.355
STEP: waiting to observe update in volume 08/17/23 04:36:06.363
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1072" for this suite. 08/17/23 04:37:17.116
------------------------------
• [SLOW TEST] [75.059 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:36:02.064
    Aug 17 04:36:02.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 04:36:02.067
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:36:02.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:36:02.089
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-94521f1d-4cfd-46d5-ab3d-e8519e4fdbf0 08/17/23 04:36:02.105
    STEP: Creating secret with name s-test-opt-upd-685011c6-4314-4bfb-91a4-768c007d8684 08/17/23 04:36:02.113
    STEP: Creating the pod 08/17/23 04:36:02.119
    Aug 17 04:36:02.129: INFO: Waiting up to 5m0s for pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09" in namespace "secrets-1072" to be "running and ready"
    Aug 17 04:36:02.134: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960441ms
    Aug 17 04:36:02.134: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:36:04.141: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011697362s
    Aug 17 04:36:04.141: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:36:06.143: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09": Phase="Running", Reason="", readiness=true. Elapsed: 4.013910194s
    Aug 17 04:36:06.143: INFO: The phase of Pod pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09 is Running (Ready = true)
    Aug 17 04:36:06.143: INFO: Pod "pod-secrets-9c9353cb-2222-41d2-a3d9-892b863b5e09" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-94521f1d-4cfd-46d5-ab3d-e8519e4fdbf0 08/17/23 04:36:06.335
    STEP: Updating secret s-test-opt-upd-685011c6-4314-4bfb-91a4-768c007d8684 08/17/23 04:36:06.347
    STEP: Creating secret with name s-test-opt-create-14d69123-5545-42c3-8154-b90ad34a841f 08/17/23 04:36:06.355
    STEP: waiting to observe update in volume 08/17/23 04:36:06.363
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1072" for this suite. 08/17/23 04:37:17.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:17.127
Aug 17 04:37:17.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:37:17.128
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:17.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:17.145
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5833 08/17/23 04:37:17.149
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/17/23 04:37:17.154
STEP: Creating pod with conflicting port in namespace statefulset-5833 08/17/23 04:37:17.16
STEP: Waiting until pod test-pod will start running in namespace statefulset-5833 08/17/23 04:37:17.169
Aug 17 04:37:17.169: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5833" to be "running"
Aug 17 04:37:17.173: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666055ms
Aug 17 04:37:19.179: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009607418s
Aug 17 04:37:19.179: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5833 08/17/23 04:37:19.179
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5833 08/17/23 04:37:19.184
Aug 17 04:37:19.195: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Pending. Waiting for statefulset controller to delete.
Aug 17 04:37:19.204: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 04:37:19.210: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Failed. Waiting for statefulset controller to delete.
Aug 17 04:37:19.212: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5833
STEP: Removing pod with conflicting port in namespace statefulset-5833 08/17/23 04:37:19.212
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5833 and will be in running state 08/17/23 04:37:19.222
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:37:21.231: INFO: Deleting all statefulset in ns statefulset-5833
Aug 17 04:37:21.235: INFO: Scaling statefulset ss to 0
Aug 17 04:37:31.258: INFO: Waiting for statefulset status.replicas updated to 0
Aug 17 04:37:31.262: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:31.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5833" for this suite. 08/17/23 04:37:31.283
------------------------------
• [SLOW TEST] [14.163 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:17.127
    Aug 17 04:37:17.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:37:17.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:17.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:17.145
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5833 08/17/23 04:37:17.149
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/17/23 04:37:17.154
    STEP: Creating pod with conflicting port in namespace statefulset-5833 08/17/23 04:37:17.16
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5833 08/17/23 04:37:17.169
    Aug 17 04:37:17.169: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5833" to be "running"
    Aug 17 04:37:17.173: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666055ms
    Aug 17 04:37:19.179: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009607418s
    Aug 17 04:37:19.179: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5833 08/17/23 04:37:19.179
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5833 08/17/23 04:37:19.184
    Aug 17 04:37:19.195: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 17 04:37:19.204: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 17 04:37:19.210: INFO: Observed stateful pod in namespace: statefulset-5833, name: ss-0, uid: 8efb0649-7644-4218-9bb5-3cd6eed088de, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 17 04:37:19.212: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5833
    STEP: Removing pod with conflicting port in namespace statefulset-5833 08/17/23 04:37:19.212
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5833 and will be in running state 08/17/23 04:37:19.222
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:37:21.231: INFO: Deleting all statefulset in ns statefulset-5833
    Aug 17 04:37:21.235: INFO: Scaling statefulset ss to 0
    Aug 17 04:37:31.258: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 17 04:37:31.262: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:31.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5833" for this suite. 08/17/23 04:37:31.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:31.291
Aug 17 04:37:31.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename conformance-tests 08/17/23 04:37:31.292
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:31.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:31.309
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/17/23 04:37:31.313
Aug 17 04:37:31.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:31.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4375" for this suite. 08/17/23 04:37:31.328
------------------------------
• [0.043 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:31.291
    Aug 17 04:37:31.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename conformance-tests 08/17/23 04:37:31.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:31.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:31.309
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/17/23 04:37:31.313
    Aug 17 04:37:31.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:31.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4375" for this suite. 08/17/23 04:37:31.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:31.337
Aug 17 04:37:31.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename emptydir 08/17/23 04:37:31.337
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:31.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:31.354
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 04:37:31.358
Aug 17 04:37:31.367: INFO: Waiting up to 5m0s for pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4" in namespace "emptydir-5140" to be "Succeeded or Failed"
Aug 17 04:37:31.371: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75619ms
Aug 17 04:37:33.375: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632591s
Aug 17 04:37:35.376: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009609609s
STEP: Saw pod success 08/17/23 04:37:35.376
Aug 17 04:37:35.377: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4" satisfied condition "Succeeded or Failed"
Aug 17 04:37:35.382: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 container test-container: <nil>
STEP: delete the pod 08/17/23 04:37:35.395
Aug 17 04:37:35.408: INFO: Waiting for pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 to disappear
Aug 17 04:37:35.412: INFO: Pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:35.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5140" for this suite. 08/17/23 04:37:35.42
------------------------------
• [4.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:31.337
    Aug 17 04:37:31.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename emptydir 08/17/23 04:37:31.337
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:31.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:31.354
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/17/23 04:37:31.358
    Aug 17 04:37:31.367: INFO: Waiting up to 5m0s for pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4" in namespace "emptydir-5140" to be "Succeeded or Failed"
    Aug 17 04:37:31.371: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75619ms
    Aug 17 04:37:33.375: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632591s
    Aug 17 04:37:35.376: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009609609s
    STEP: Saw pod success 08/17/23 04:37:35.376
    Aug 17 04:37:35.377: INFO: Pod "pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4" satisfied condition "Succeeded or Failed"
    Aug 17 04:37:35.382: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 container test-container: <nil>
    STEP: delete the pod 08/17/23 04:37:35.395
    Aug 17 04:37:35.408: INFO: Waiting for pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 to disappear
    Aug 17 04:37:35.412: INFO: Pod pod-65a71cfa-9dd6-459c-b73a-10e7f6c444d4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:35.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5140" for this suite. 08/17/23 04:37:35.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:35.428
Aug 17 04:37:35.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename server-version 08/17/23 04:37:35.429
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.449
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/17/23 04:37:35.453
STEP: Confirm major version 08/17/23 04:37:35.455
Aug 17 04:37:35.455: INFO: Major version: 1
STEP: Confirm minor version 08/17/23 04:37:35.455
Aug 17 04:37:35.455: INFO: cleanMinorVersion: 26
Aug 17 04:37:35.455: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:35.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-4168" for this suite. 08/17/23 04:37:35.463
------------------------------
• [0.042 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:35.428
    Aug 17 04:37:35.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename server-version 08/17/23 04:37:35.429
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.449
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/17/23 04:37:35.453
    STEP: Confirm major version 08/17/23 04:37:35.455
    Aug 17 04:37:35.455: INFO: Major version: 1
    STEP: Confirm minor version 08/17/23 04:37:35.455
    Aug 17 04:37:35.455: INFO: cleanMinorVersion: 26
    Aug 17 04:37:35.455: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:35.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-4168" for this suite. 08/17/23 04:37:35.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:35.473
Aug 17 04:37:35.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename namespaces 08/17/23 04:37:35.474
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.49
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/17/23 04:37:35.494
STEP: patching the Namespace 08/17/23 04:37:35.505
STEP: get the Namespace and ensuring it has the label 08/17/23 04:37:35.51
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7093" for this suite. 08/17/23 04:37:35.52
STEP: Destroying namespace "nspatchtest-546402f1-1c4b-4c92-aadd-eb0d5b4af853-8200" for this suite. 08/17/23 04:37:35.527
------------------------------
• [0.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:35.473
    Aug 17 04:37:35.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename namespaces 08/17/23 04:37:35.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.49
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/17/23 04:37:35.494
    STEP: patching the Namespace 08/17/23 04:37:35.505
    STEP: get the Namespace and ensuring it has the label 08/17/23 04:37:35.51
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7093" for this suite. 08/17/23 04:37:35.52
    STEP: Destroying namespace "nspatchtest-546402f1-1c4b-4c92-aadd-eb0d5b4af853-8200" for this suite. 08/17/23 04:37:35.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:35.534
Aug 17 04:37:35.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename security-context-test 08/17/23 04:37:35.535
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.549
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 17 04:37:35.563: INFO: Waiting up to 5m0s for pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586" in namespace "security-context-test-7645" to be "Succeeded or Failed"
Aug 17 04:37:35.567: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159941ms
Aug 17 04:37:37.573: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554641s
Aug 17 04:37:39.572: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177403s
Aug 17 04:37:39.572: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:39.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7645" for this suite. 08/17/23 04:37:39.579
------------------------------
• [4.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:35.534
    Aug 17 04:37:35.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename security-context-test 08/17/23 04:37:35.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:35.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:35.549
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 17 04:37:35.563: INFO: Waiting up to 5m0s for pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586" in namespace "security-context-test-7645" to be "Succeeded or Failed"
    Aug 17 04:37:35.567: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Pending", Reason="", readiness=false. Elapsed: 4.159941ms
    Aug 17 04:37:37.573: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009554641s
    Aug 17 04:37:39.572: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177403s
    Aug 17 04:37:39.572: INFO: Pod "busybox-user-65534-17b32642-8a91-4a04-a357-8149d9d3d586" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:39.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7645" for this suite. 08/17/23 04:37:39.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:39.586
Aug 17 04:37:39.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename secrets 08/17/23 04:37:39.587
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:39.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:39.602
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-8e09fc48-6e55-4b41-aa69-f67e6c86e33e 08/17/23 04:37:39.605
STEP: Creating a pod to test consume secrets 08/17/23 04:37:39.61
Aug 17 04:37:39.619: INFO: Waiting up to 5m0s for pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948" in namespace "secrets-824" to be "Succeeded or Failed"
Aug 17 04:37:39.623: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890315ms
Aug 17 04:37:41.629: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010437104s
Aug 17 04:37:43.629: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010537057s
STEP: Saw pod success 08/17/23 04:37:43.629
Aug 17 04:37:43.630: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948" satisfied condition "Succeeded or Failed"
Aug 17 04:37:43.635: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 container secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:37:43.724
Aug 17 04:37:43.736: INFO: Waiting for pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 to disappear
Aug 17 04:37:43.739: INFO: Pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-824" for this suite. 08/17/23 04:37:43.746
------------------------------
• [4.169 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:39.586
    Aug 17 04:37:39.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename secrets 08/17/23 04:37:39.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:39.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:39.602
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-8e09fc48-6e55-4b41-aa69-f67e6c86e33e 08/17/23 04:37:39.605
    STEP: Creating a pod to test consume secrets 08/17/23 04:37:39.61
    Aug 17 04:37:39.619: INFO: Waiting up to 5m0s for pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948" in namespace "secrets-824" to be "Succeeded or Failed"
    Aug 17 04:37:39.623: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890315ms
    Aug 17 04:37:41.629: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010437104s
    Aug 17 04:37:43.629: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010537057s
    STEP: Saw pod success 08/17/23 04:37:43.629
    Aug 17 04:37:43.630: INFO: Pod "pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948" satisfied condition "Succeeded or Failed"
    Aug 17 04:37:43.635: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 container secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:37:43.724
    Aug 17 04:37:43.736: INFO: Waiting for pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 to disappear
    Aug 17 04:37:43.739: INFO: Pod pod-secrets-efb4efd9-d020-4b3f-bb3a-c9eb00a1b948 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-824" for this suite. 08/17/23 04:37:43.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:43.755
Aug 17 04:37:43.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:37:43.756
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:43.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:43.774
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 17 04:37:43.799: INFO: created pod pod-service-account-defaultsa
Aug 17 04:37:43.799: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 17 04:37:43.807: INFO: created pod pod-service-account-mountsa
Aug 17 04:37:43.807: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 17 04:37:43.813: INFO: created pod pod-service-account-nomountsa
Aug 17 04:37:43.813: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 17 04:37:43.819: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 17 04:37:43.819: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 17 04:37:43.824: INFO: created pod pod-service-account-mountsa-mountspec
Aug 17 04:37:43.824: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 17 04:37:43.828: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 17 04:37:43.828: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 17 04:37:43.834: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 17 04:37:43.834: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 17 04:37:43.840: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 17 04:37:43.840: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 17 04:37:43.844: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 17 04:37:43.844: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:43.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2915" for this suite. 08/17/23 04:37:43.853
------------------------------
• [0.105 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:43.755
    Aug 17 04:37:43.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:37:43.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:43.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:43.774
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 17 04:37:43.799: INFO: created pod pod-service-account-defaultsa
    Aug 17 04:37:43.799: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 17 04:37:43.807: INFO: created pod pod-service-account-mountsa
    Aug 17 04:37:43.807: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 17 04:37:43.813: INFO: created pod pod-service-account-nomountsa
    Aug 17 04:37:43.813: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 17 04:37:43.819: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 17 04:37:43.819: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 17 04:37:43.824: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 17 04:37:43.824: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 17 04:37:43.828: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 17 04:37:43.828: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 17 04:37:43.834: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 17 04:37:43.834: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 17 04:37:43.840: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 17 04:37:43.840: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 17 04:37:43.844: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 17 04:37:43.844: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:43.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2915" for this suite. 08/17/23 04:37:43.853
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:43.861
Aug 17 04:37:43.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename projected 08/17/23 04:37:43.863
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:43.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:43.88
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-8fd3e197-e417-4426-be2a-4433944cd419 08/17/23 04:37:43.885
STEP: Creating a pod to test consume secrets 08/17/23 04:37:43.891
Aug 17 04:37:43.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632" in namespace "projected-4375" to be "Succeeded or Failed"
Aug 17 04:37:43.903: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784148ms
Aug 17 04:37:45.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008861938s
Aug 17 04:37:47.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009331049s
Aug 17 04:37:49.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009106059s
Aug 17 04:37:51.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009271841s
STEP: Saw pod success 08/17/23 04:37:51.908
Aug 17 04:37:51.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632" satisfied condition "Succeeded or Failed"
Aug 17 04:37:51.912: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/17/23 04:37:51.999
Aug 17 04:37:52.012: INFO: Waiting for pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 to disappear
Aug 17 04:37:52.015: INFO: Pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:52.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4375" for this suite. 08/17/23 04:37:52.022
------------------------------
• [SLOW TEST] [8.169 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:43.861
    Aug 17 04:37:43.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename projected 08/17/23 04:37:43.863
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:43.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:43.88
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-8fd3e197-e417-4426-be2a-4433944cd419 08/17/23 04:37:43.885
    STEP: Creating a pod to test consume secrets 08/17/23 04:37:43.891
    Aug 17 04:37:43.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632" in namespace "projected-4375" to be "Succeeded or Failed"
    Aug 17 04:37:43.903: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784148ms
    Aug 17 04:37:45.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008861938s
    Aug 17 04:37:47.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009331049s
    Aug 17 04:37:49.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009106059s
    Aug 17 04:37:51.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009271841s
    STEP: Saw pod success 08/17/23 04:37:51.908
    Aug 17 04:37:51.908: INFO: Pod "pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632" satisfied condition "Succeeded or Failed"
    Aug 17 04:37:51.912: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/17/23 04:37:51.999
    Aug 17 04:37:52.012: INFO: Waiting for pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 to disappear
    Aug 17 04:37:52.015: INFO: Pod pod-projected-secrets-6b99c6ad-210b-40a7-a119-92a51e3a0632 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:52.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4375" for this suite. 08/17/23 04:37:52.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:52.032
Aug 17 04:37:52.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:37:52.033
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:52.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:52.051
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/17/23 04:37:52.055
Aug 17 04:37:52.066: INFO: Waiting up to 5m0s for pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9" in namespace "downward-api-9495" to be "Succeeded or Failed"
Aug 17 04:37:52.070: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.591812ms
Aug 17 04:37:54.074: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008479059s
Aug 17 04:37:56.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00857734s
Aug 17 04:37:58.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009303637s
STEP: Saw pod success 08/17/23 04:37:58.075
Aug 17 04:37:58.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9" satisfied condition "Succeeded or Failed"
Aug 17 04:37:58.079: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 container dapi-container: <nil>
STEP: delete the pod 08/17/23 04:37:58.168
Aug 17 04:37:58.180: INFO: Waiting for pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 to disappear
Aug 17 04:37:58.185: INFO: Pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 04:37:58.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9495" for this suite. 08/17/23 04:37:58.192
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:52.032
    Aug 17 04:37:52.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:37:52.033
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:52.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:52.051
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/17/23 04:37:52.055
    Aug 17 04:37:52.066: INFO: Waiting up to 5m0s for pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9" in namespace "downward-api-9495" to be "Succeeded or Failed"
    Aug 17 04:37:52.070: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.591812ms
    Aug 17 04:37:54.074: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008479059s
    Aug 17 04:37:56.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00857734s
    Aug 17 04:37:58.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009303637s
    STEP: Saw pod success 08/17/23 04:37:58.075
    Aug 17 04:37:58.075: INFO: Pod "downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9" satisfied condition "Succeeded or Failed"
    Aug 17 04:37:58.079: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 04:37:58.168
    Aug 17 04:37:58.180: INFO: Waiting for pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 to disappear
    Aug 17 04:37:58.185: INFO: Pod downward-api-8b6e0f4c-756d-4714-b17a-bbb314add4c9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:37:58.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9495" for this suite. 08/17/23 04:37:58.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:37:58.2
Aug 17 04:37:58.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename downward-api 08/17/23 04:37:58.201
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:58.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:58.221
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/17/23 04:37:58.225
Aug 17 04:37:58.235: INFO: Waiting up to 5m0s for pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90" in namespace "downward-api-7005" to be "Succeeded or Failed"
Aug 17 04:37:58.240: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807446ms
Aug 17 04:38:00.245: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010287488s
Aug 17 04:38:02.246: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011341774s
STEP: Saw pod success 08/17/23 04:38:02.246
Aug 17 04:38:02.246: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90" satisfied condition "Succeeded or Failed"
Aug 17 04:38:02.250: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 container dapi-container: <nil>
STEP: delete the pod 08/17/23 04:38:02.34
Aug 17 04:38:02.354: INFO: Waiting for pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 to disappear
Aug 17 04:38:02.358: INFO: Pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:02.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7005" for this suite. 08/17/23 04:38:02.366
------------------------------
• [4.174 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:37:58.2
    Aug 17 04:37:58.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename downward-api 08/17/23 04:37:58.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:37:58.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:37:58.221
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/17/23 04:37:58.225
    Aug 17 04:37:58.235: INFO: Waiting up to 5m0s for pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90" in namespace "downward-api-7005" to be "Succeeded or Failed"
    Aug 17 04:37:58.240: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807446ms
    Aug 17 04:38:00.245: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010287488s
    Aug 17 04:38:02.246: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011341774s
    STEP: Saw pod success 08/17/23 04:38:02.246
    Aug 17 04:38:02.246: INFO: Pod "downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90" satisfied condition "Succeeded or Failed"
    Aug 17 04:38:02.250: INFO: Trying to get logs from node ske-ubuntu-79fff84d86x69988-vjwlx pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 container dapi-container: <nil>
    STEP: delete the pod 08/17/23 04:38:02.34
    Aug 17 04:38:02.354: INFO: Waiting for pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 to disappear
    Aug 17 04:38:02.358: INFO: Pod downward-api-9c4387fa-c994-41fe-bf72-9d8ed59f1d90 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:02.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7005" for this suite. 08/17/23 04:38:02.366
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:02.375
Aug 17 04:38:02.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 04:38:02.376
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:02.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:02.396
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/17/23 04:38:02.401
STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 04:38:02.408
STEP: delete the deployment 08/17/23 04:38:02.918
STEP: wait for all rs to be garbage collected 08/17/23 04:38:02.925
STEP: expected 0 rs, got 1 rs 08/17/23 04:38:02.933
STEP: expected 0 pods, got 2 pods 08/17/23 04:38:02.938
STEP: Gathering metrics 08/17/23 04:38:03.451
W0817 04:38:03.464536      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 04:38:03.464: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:03.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3104" for this suite. 08/17/23 04:38:03.472
------------------------------
• [1.110 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:02.375
    Aug 17 04:38:02.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 04:38:02.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:02.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:02.396
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/17/23 04:38:02.401
    STEP: Wait for the Deployment to create new ReplicaSet 08/17/23 04:38:02.408
    STEP: delete the deployment 08/17/23 04:38:02.918
    STEP: wait for all rs to be garbage collected 08/17/23 04:38:02.925
    STEP: expected 0 rs, got 1 rs 08/17/23 04:38:02.933
    STEP: expected 0 pods, got 2 pods 08/17/23 04:38:02.938
    STEP: Gathering metrics 08/17/23 04:38:03.451
    W0817 04:38:03.464536      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 04:38:03.464: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:03.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3104" for this suite. 08/17/23 04:38:03.472
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:03.486
Aug 17 04:38:03.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename proxy 08/17/23 04:38:03.489
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:03.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:03.51
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/17/23 04:38:03.528
STEP: creating replication controller proxy-service-hn2pb in namespace proxy-2634 08/17/23 04:38:03.529
I0817 04:38:03.535800      18 runners.go:193] Created replication controller with name: proxy-service-hn2pb, namespace: proxy-2634, replica count: 1
I0817 04:38:04.588352      18 runners.go:193] proxy-service-hn2pb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0817 04:38:05.588862      18 runners.go:193] proxy-service-hn2pb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 17 04:38:05.594: INFO: setup took 2.078785132s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/17/23 04:38:05.594
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 51.80483ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 52.696515ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 53.032323ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 52.949937ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 53.27554ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 53.31537ms)
Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 53.229921ms)
Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 53.466344ms)
Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 53.631496ms)
Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 53.546137ms)
Aug 17 04:38:05.653: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 57.526652ms)
Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 59.373679ms)
Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 59.845145ms)
Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 60.169777ms)
Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 59.445613ms)
Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 60.686575ms)
Aug 17 04:38:05.661: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 5.668126ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 6.414838ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.810145ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.248146ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.0093ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.309913ms)
Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.310415ms)
Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.559288ms)
Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.721129ms)
Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.533902ms)
Aug 17 04:38:05.665: INFO: (1) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.763194ms)
Aug 17 04:38:05.665: INFO: (1) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 9.255224ms)
Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.358111ms)
Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.916859ms)
Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.162631ms)
Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.340281ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.524043ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.923565ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.750312ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.161803ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.207373ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.771212ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 8.415489ms)
Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.317017ms)
Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.077215ms)
Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 9.236236ms)
Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.12382ms)
Aug 17 04:38:05.679: INFO: (2) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.329419ms)
Aug 17 04:38:05.679: INFO: (2) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.471244ms)
Aug 17 04:38:05.680: INFO: (2) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.816482ms)
Aug 17 04:38:05.681: INFO: (2) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.147669ms)
Aug 17 04:38:05.681: INFO: (2) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 13.234229ms)
Aug 17 04:38:05.688: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 5.466983ms)
Aug 17 04:38:05.689: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.94487ms)
Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.008364ms)
Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.10063ms)
Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.450855ms)
Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.517531ms)
Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.898571ms)
Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.899874ms)
Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.305571ms)
Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.419159ms)
Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.286042ms)
Aug 17 04:38:05.692: INFO: (3) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.206172ms)
Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.203068ms)
Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 9.98507ms)
Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.213564ms)
Aug 17 04:38:05.694: INFO: (3) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.968505ms)
Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 6.947073ms)
Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.072167ms)
Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.096629ms)
Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.205477ms)
Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.55962ms)
Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.120697ms)
Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.077949ms)
Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.59058ms)
Aug 17 04:38:05.704: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.05524ms)
Aug 17 04:38:05.704: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.718318ms)
Aug 17 04:38:05.705: INFO: (4) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.369565ms)
Aug 17 04:38:05.706: INFO: (4) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.090101ms)
Aug 17 04:38:05.706: INFO: (4) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.783764ms)
Aug 17 04:38:05.707: INFO: (4) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.384856ms)
Aug 17 04:38:05.707: INFO: (4) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.510329ms)
Aug 17 04:38:05.708: INFO: (4) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 13.897312ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.485733ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.690824ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.866575ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.301299ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.033976ms)
Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.193401ms)
Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.324075ms)
Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.003915ms)
Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.624422ms)
Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.71295ms)
Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.51557ms)
Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.579198ms)
Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 9.985606ms)
Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.861762ms)
Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.834689ms)
Aug 17 04:38:05.721: INFO: (5) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.256635ms)
Aug 17 04:38:05.729: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.991331ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.652652ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.7309ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.785994ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.016829ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.59139ms)
Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.164989ms)
Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.749497ms)
Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.780039ms)
Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.019309ms)
Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.825744ms)
Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.820194ms)
Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.53959ms)
Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.273024ms)
Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.517387ms)
Aug 17 04:38:05.734: INFO: (6) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.725329ms)
Aug 17 04:38:05.740: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 6.464885ms)
Aug 17 04:38:05.741: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 6.883137ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.56864ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.680393ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.72491ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.553879ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.587335ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.734867ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.737245ms)
Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.667521ms)
Aug 17 04:38:05.743: INFO: (7) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 8.899707ms)
Aug 17 04:38:05.743: INFO: (7) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.734335ms)
Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.869826ms)
Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.125895ms)
Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 9.925747ms)
Aug 17 04:38:05.745: INFO: (7) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.204596ms)
Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.311241ms)
Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.646714ms)
Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.588235ms)
Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.944214ms)
Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.643897ms)
Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.655589ms)
Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.499042ms)
Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.572549ms)
Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.824956ms)
Aug 17 04:38:05.755: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.300576ms)
Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.304811ms)
Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.627414ms)
Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.381386ms)
Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.577481ms)
Aug 17 04:38:05.757: INFO: (8) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.973064ms)
Aug 17 04:38:05.758: INFO: (8) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.728163ms)
Aug 17 04:38:05.765: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.306105ms)
Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.903744ms)
Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.467378ms)
Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.691904ms)
Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.583872ms)
Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.801636ms)
Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.754603ms)
Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.849852ms)
Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.459373ms)
Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.874517ms)
Aug 17 04:38:05.769: INFO: (9) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.195914ms)
Aug 17 04:38:05.769: INFO: (9) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.321364ms)
Aug 17 04:38:05.770: INFO: (9) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.35281ms)
Aug 17 04:38:05.770: INFO: (9) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.309494ms)
Aug 17 04:38:05.771: INFO: (9) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.809237ms)
Aug 17 04:38:05.771: INFO: (9) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.032307ms)
Aug 17 04:38:05.778: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 6.512333ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.489944ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.185129ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.477138ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.475864ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.171673ms)
Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.720435ms)
Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.069525ms)
Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.6738ms)
Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.871469ms)
Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 9.325394ms)
Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 9.999185ms)
Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.710985ms)
Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.715384ms)
Aug 17 04:38:05.782: INFO: (10) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.205356ms)
Aug 17 04:38:05.784: INFO: (10) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.819511ms)
Aug 17 04:38:05.790: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 6.591523ms)
Aug 17 04:38:05.792: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.322272ms)
Aug 17 04:38:05.792: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.84063ms)
Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.035152ms)
Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.283069ms)
Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.495447ms)
Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 9.543471ms)
Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.476471ms)
Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 10.881584ms)
Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 10.719385ms)
Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.410772ms)
Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.496255ms)
Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.718771ms)
Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.678123ms)
Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 13.796811ms)
Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 13.729959ms)
Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.436379ms)
Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.466964ms)
Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.939419ms)
Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.882775ms)
Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.736245ms)
Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.743349ms)
Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.305655ms)
Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.467326ms)
Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.560148ms)
Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.524572ms)
Aug 17 04:38:05.809: INFO: (12) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.743812ms)
Aug 17 04:38:05.809: INFO: (12) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.783918ms)
Aug 17 04:38:05.810: INFO: (12) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.139446ms)
Aug 17 04:38:05.810: INFO: (12) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.600713ms)
Aug 17 04:38:05.811: INFO: (12) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.702432ms)
Aug 17 04:38:05.811: INFO: (12) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.003612ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.746775ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.014159ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.549414ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.287603ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.700877ms)
Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.895337ms)
Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.857667ms)
Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.064654ms)
Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 9.361495ms)
Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.299817ms)
Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.227587ms)
Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.30827ms)
Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.136599ms)
Aug 17 04:38:05.823: INFO: (13) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.53489ms)
Aug 17 04:38:05.823: INFO: (13) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.227736ms)
Aug 17 04:38:05.825: INFO: (13) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.81365ms)
Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.389553ms)
Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.704912ms)
Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 8.846672ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.595412ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.735723ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.803455ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.521188ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.389946ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.529674ms)
Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.528285ms)
Aug 17 04:38:05.835: INFO: (14) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.583961ms)
Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.050259ms)
Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.883342ms)
Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.996412ms)
Aug 17 04:38:05.838: INFO: (14) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.960692ms)
Aug 17 04:38:05.838: INFO: (14) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.726144ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.977441ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.173475ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.912668ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.161469ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.920657ms)
Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.072095ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 10.842366ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 10.651486ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.6295ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 10.674798ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.75462ms)
Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 10.568268ms)
Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.283452ms)
Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.988389ms)
Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.13976ms)
Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.206563ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 10.232466ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 10.06814ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 10.037489ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.919936ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.979755ms)
Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 10.080554ms)
Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 10.960144ms)
Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 10.856526ms)
Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 11.272913ms)
Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 11.595957ms)
Aug 17 04:38:05.863: INFO: (16) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.650975ms)
Aug 17 04:38:05.863: INFO: (16) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.80675ms)
Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 12.698486ms)
Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 12.778179ms)
Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.875942ms)
Aug 17 04:38:05.865: INFO: (16) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 14.55952ms)
Aug 17 04:38:05.871: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 5.948554ms)
Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 6.529934ms)
Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 6.488768ms)
Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.946186ms)
Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.200621ms)
Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 6.920571ms)
Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.470688ms)
Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.369916ms)
Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.760734ms)
Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.254553ms)
Aug 17 04:38:05.876: INFO: (17) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.599696ms)
Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.71859ms)
Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.333536ms)
Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.896626ms)
Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.199961ms)
Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.263887ms)
Aug 17 04:38:05.885: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.150113ms)
Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.185164ms)
Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.158274ms)
Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.687009ms)
Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 9.010055ms)
Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.848459ms)
Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.523633ms)
Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.101337ms)
Aug 17 04:38:05.888: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.264147ms)
Aug 17 04:38:05.888: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.920213ms)
Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.568458ms)
Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.024477ms)
Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.974698ms)
Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 12.025713ms)
Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.208362ms)
Aug 17 04:38:05.892: INFO: (18) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 13.375959ms)
Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.04204ms)
Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.677893ms)
Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.05358ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.009966ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.330609ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 8.68823ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.173468ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.753468ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.611843ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.674606ms)
Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 8.887171ms)
Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.573416ms)
Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.887696ms)
Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.4502ms)
Aug 17 04:38:05.909: INFO: (19) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 15.891506ms)
Aug 17 04:38:05.909: INFO: (19) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 16.990606ms)
STEP: deleting ReplicationController proxy-service-hn2pb in namespace proxy-2634, will wait for the garbage collector to delete the pods 08/17/23 04:38:05.909
Aug 17 04:38:05.971: INFO: Deleting ReplicationController proxy-service-hn2pb took: 7.50181ms
Aug 17 04:38:06.072: INFO: Terminating ReplicationController proxy-service-hn2pb pods took: 100.277582ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:08.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2634" for this suite. 08/17/23 04:38:08.58
------------------------------
• [SLOW TEST] [5.101 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:03.486
    Aug 17 04:38:03.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename proxy 08/17/23 04:38:03.489
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:03.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:03.51
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/17/23 04:38:03.528
    STEP: creating replication controller proxy-service-hn2pb in namespace proxy-2634 08/17/23 04:38:03.529
    I0817 04:38:03.535800      18 runners.go:193] Created replication controller with name: proxy-service-hn2pb, namespace: proxy-2634, replica count: 1
    I0817 04:38:04.588352      18 runners.go:193] proxy-service-hn2pb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0817 04:38:05.588862      18 runners.go:193] proxy-service-hn2pb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 17 04:38:05.594: INFO: setup took 2.078785132s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/17/23 04:38:05.594
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 51.80483ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 52.696515ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 53.032323ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 52.949937ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 53.27554ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 53.31537ms)
    Aug 17 04:38:05.648: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 53.229921ms)
    Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 53.466344ms)
    Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 53.631496ms)
    Aug 17 04:38:05.649: INFO: (0) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 53.546137ms)
    Aug 17 04:38:05.653: INFO: (0) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 57.526652ms)
    Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 59.373679ms)
    Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 59.845145ms)
    Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 60.169777ms)
    Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 59.445613ms)
    Aug 17 04:38:05.655: INFO: (0) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 60.686575ms)
    Aug 17 04:38:05.661: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 5.668126ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 6.414838ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.810145ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.248146ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.0093ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.309913ms)
    Aug 17 04:38:05.663: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.310415ms)
    Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.559288ms)
    Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.721129ms)
    Aug 17 04:38:05.664: INFO: (1) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.533902ms)
    Aug 17 04:38:05.665: INFO: (1) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.763194ms)
    Aug 17 04:38:05.665: INFO: (1) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 9.255224ms)
    Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.358111ms)
    Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.916859ms)
    Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.162631ms)
    Aug 17 04:38:05.667: INFO: (1) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.340281ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.524043ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.923565ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.750312ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.161803ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.207373ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.771212ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 8.415489ms)
    Aug 17 04:38:05.676: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.317017ms)
    Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.077215ms)
    Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 9.236236ms)
    Aug 17 04:38:05.677: INFO: (2) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.12382ms)
    Aug 17 04:38:05.679: INFO: (2) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.329419ms)
    Aug 17 04:38:05.679: INFO: (2) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.471244ms)
    Aug 17 04:38:05.680: INFO: (2) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.816482ms)
    Aug 17 04:38:05.681: INFO: (2) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.147669ms)
    Aug 17 04:38:05.681: INFO: (2) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 13.234229ms)
    Aug 17 04:38:05.688: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 5.466983ms)
    Aug 17 04:38:05.689: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.94487ms)
    Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.008364ms)
    Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.10063ms)
    Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.450855ms)
    Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.517531ms)
    Aug 17 04:38:05.690: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.898571ms)
    Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.899874ms)
    Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.305571ms)
    Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.419159ms)
    Aug 17 04:38:05.691: INFO: (3) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.286042ms)
    Aug 17 04:38:05.692: INFO: (3) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.206172ms)
    Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.203068ms)
    Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 9.98507ms)
    Aug 17 04:38:05.693: INFO: (3) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.213564ms)
    Aug 17 04:38:05.694: INFO: (3) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.968505ms)
    Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 6.947073ms)
    Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.072167ms)
    Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.096629ms)
    Aug 17 04:38:05.701: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.205477ms)
    Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.55962ms)
    Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.120697ms)
    Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.077949ms)
    Aug 17 04:38:05.702: INFO: (4) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 7.59058ms)
    Aug 17 04:38:05.704: INFO: (4) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.05524ms)
    Aug 17 04:38:05.704: INFO: (4) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.718318ms)
    Aug 17 04:38:05.705: INFO: (4) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.369565ms)
    Aug 17 04:38:05.706: INFO: (4) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.090101ms)
    Aug 17 04:38:05.706: INFO: (4) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.783764ms)
    Aug 17 04:38:05.707: INFO: (4) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.384856ms)
    Aug 17 04:38:05.707: INFO: (4) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.510329ms)
    Aug 17 04:38:05.708: INFO: (4) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 13.897312ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.485733ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.690824ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.866575ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.301299ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.033976ms)
    Aug 17 04:38:05.716: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.193401ms)
    Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.324075ms)
    Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.003915ms)
    Aug 17 04:38:05.717: INFO: (5) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.624422ms)
    Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.71295ms)
    Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.51557ms)
    Aug 17 04:38:05.718: INFO: (5) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.579198ms)
    Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 9.985606ms)
    Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.861762ms)
    Aug 17 04:38:05.719: INFO: (5) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.834689ms)
    Aug 17 04:38:05.721: INFO: (5) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.256635ms)
    Aug 17 04:38:05.729: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.991331ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.652652ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.7309ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.785994ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.016829ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.59139ms)
    Aug 17 04:38:05.730: INFO: (6) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.164989ms)
    Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.749497ms)
    Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.780039ms)
    Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.019309ms)
    Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.825744ms)
    Aug 17 04:38:05.731: INFO: (6) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.820194ms)
    Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.53959ms)
    Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.273024ms)
    Aug 17 04:38:05.732: INFO: (6) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.517387ms)
    Aug 17 04:38:05.734: INFO: (6) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.725329ms)
    Aug 17 04:38:05.740: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 6.464885ms)
    Aug 17 04:38:05.741: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 6.883137ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.56864ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.680393ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.72491ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.553879ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.587335ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.734867ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.737245ms)
    Aug 17 04:38:05.742: INFO: (7) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.667521ms)
    Aug 17 04:38:05.743: INFO: (7) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 8.899707ms)
    Aug 17 04:38:05.743: INFO: (7) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 8.734335ms)
    Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.869826ms)
    Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.125895ms)
    Aug 17 04:38:05.744: INFO: (7) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 9.925747ms)
    Aug 17 04:38:05.745: INFO: (7) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.204596ms)
    Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.311241ms)
    Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.646714ms)
    Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.588235ms)
    Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.944214ms)
    Aug 17 04:38:05.753: INFO: (8) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.643897ms)
    Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.655589ms)
    Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.499042ms)
    Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.572549ms)
    Aug 17 04:38:05.754: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.824956ms)
    Aug 17 04:38:05.755: INFO: (8) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.300576ms)
    Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.304811ms)
    Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.627414ms)
    Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.381386ms)
    Aug 17 04:38:05.756: INFO: (8) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.577481ms)
    Aug 17 04:38:05.757: INFO: (8) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.973064ms)
    Aug 17 04:38:05.758: INFO: (8) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.728163ms)
    Aug 17 04:38:05.765: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.306105ms)
    Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.903744ms)
    Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.467378ms)
    Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.691904ms)
    Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.583872ms)
    Aug 17 04:38:05.766: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.801636ms)
    Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.754603ms)
    Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.849852ms)
    Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.459373ms)
    Aug 17 04:38:05.768: INFO: (9) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.874517ms)
    Aug 17 04:38:05.769: INFO: (9) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.195914ms)
    Aug 17 04:38:05.769: INFO: (9) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.321364ms)
    Aug 17 04:38:05.770: INFO: (9) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.35281ms)
    Aug 17 04:38:05.770: INFO: (9) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.309494ms)
    Aug 17 04:38:05.771: INFO: (9) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 10.809237ms)
    Aug 17 04:38:05.771: INFO: (9) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.032307ms)
    Aug 17 04:38:05.778: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 6.512333ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.489944ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.185129ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.477138ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.475864ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.171673ms)
    Aug 17 04:38:05.779: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.720435ms)
    Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.069525ms)
    Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.6738ms)
    Aug 17 04:38:05.780: INFO: (10) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.871469ms)
    Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 9.325394ms)
    Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 9.999185ms)
    Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 9.710985ms)
    Aug 17 04:38:05.781: INFO: (10) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 9.715384ms)
    Aug 17 04:38:05.782: INFO: (10) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.205356ms)
    Aug 17 04:38:05.784: INFO: (10) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.819511ms)
    Aug 17 04:38:05.790: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 6.591523ms)
    Aug 17 04:38:05.792: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.322272ms)
    Aug 17 04:38:05.792: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.84063ms)
    Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.035152ms)
    Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.283069ms)
    Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.495447ms)
    Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 9.543471ms)
    Aug 17 04:38:05.793: INFO: (11) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.476471ms)
    Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 10.881584ms)
    Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 10.719385ms)
    Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.410772ms)
    Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.496255ms)
    Aug 17 04:38:05.795: INFO: (11) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.718771ms)
    Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.678123ms)
    Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 13.796811ms)
    Aug 17 04:38:05.798: INFO: (11) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 13.729959ms)
    Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.436379ms)
    Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.466964ms)
    Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.939419ms)
    Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.882775ms)
    Aug 17 04:38:05.806: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 7.736245ms)
    Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.743349ms)
    Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.305655ms)
    Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.467326ms)
    Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.560148ms)
    Aug 17 04:38:05.807: INFO: (12) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.524572ms)
    Aug 17 04:38:05.809: INFO: (12) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.743812ms)
    Aug 17 04:38:05.809: INFO: (12) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.783918ms)
    Aug 17 04:38:05.810: INFO: (12) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.139446ms)
    Aug 17 04:38:05.810: INFO: (12) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.600713ms)
    Aug 17 04:38:05.811: INFO: (12) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.702432ms)
    Aug 17 04:38:05.811: INFO: (12) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 13.003612ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.746775ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.014159ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.549414ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.287603ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.700877ms)
    Aug 17 04:38:05.819: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.895337ms)
    Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.857667ms)
    Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.064654ms)
    Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 9.361495ms)
    Aug 17 04:38:05.821: INFO: (13) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.299817ms)
    Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.227587ms)
    Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.30827ms)
    Aug 17 04:38:05.822: INFO: (13) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 10.136599ms)
    Aug 17 04:38:05.823: INFO: (13) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.53489ms)
    Aug 17 04:38:05.823: INFO: (13) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.227736ms)
    Aug 17 04:38:05.825: INFO: (13) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.81365ms)
    Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.389553ms)
    Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.704912ms)
    Aug 17 04:38:05.833: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 8.846672ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.595412ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.735723ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.803455ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.521188ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 9.389946ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.529674ms)
    Aug 17 04:38:05.834: INFO: (14) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 9.528285ms)
    Aug 17 04:38:05.835: INFO: (14) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.583961ms)
    Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.050259ms)
    Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.883342ms)
    Aug 17 04:38:05.837: INFO: (14) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 11.996412ms)
    Aug 17 04:38:05.838: INFO: (14) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.960692ms)
    Aug 17 04:38:05.838: INFO: (14) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.726144ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.977441ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.173475ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.912668ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.161469ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 7.920657ms)
    Aug 17 04:38:05.846: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.072095ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 10.842366ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 10.651486ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.6295ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 10.674798ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 10.75462ms)
    Aug 17 04:38:05.849: INFO: (15) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 10.568268ms)
    Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.283452ms)
    Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.988389ms)
    Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.13976ms)
    Aug 17 04:38:05.850: INFO: (15) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.206563ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 10.232466ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 10.06814ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 10.037489ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 9.919936ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.979755ms)
    Aug 17 04:38:05.861: INFO: (16) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 10.080554ms)
    Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 10.960144ms)
    Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 10.856526ms)
    Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 11.272913ms)
    Aug 17 04:38:05.862: INFO: (16) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 11.595957ms)
    Aug 17 04:38:05.863: INFO: (16) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 12.650975ms)
    Aug 17 04:38:05.863: INFO: (16) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 12.80675ms)
    Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 12.698486ms)
    Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 12.778179ms)
    Aug 17 04:38:05.864: INFO: (16) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 12.875942ms)
    Aug 17 04:38:05.865: INFO: (16) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 14.55952ms)
    Aug 17 04:38:05.871: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 5.948554ms)
    Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 6.529934ms)
    Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 6.488768ms)
    Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 6.946186ms)
    Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.200621ms)
    Aug 17 04:38:05.873: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 6.920571ms)
    Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.470688ms)
    Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.369916ms)
    Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 7.760734ms)
    Aug 17 04:38:05.874: INFO: (17) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 8.254553ms)
    Aug 17 04:38:05.876: INFO: (17) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.599696ms)
    Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.71859ms)
    Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.333536ms)
    Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 11.896626ms)
    Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 11.199961ms)
    Aug 17 04:38:05.877: INFO: (17) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.263887ms)
    Aug 17 04:38:05.885: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 7.150113ms)
    Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.185164ms)
    Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.158274ms)
    Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.687009ms)
    Aug 17 04:38:05.886: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 9.010055ms)
    Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.848459ms)
    Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.523633ms)
    Aug 17 04:38:05.887: INFO: (18) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 9.101337ms)
    Aug 17 04:38:05.888: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 9.264147ms)
    Aug 17 04:38:05.888: INFO: (18) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 9.920213ms)
    Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 11.568458ms)
    Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 11.024477ms)
    Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 11.974698ms)
    Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 12.025713ms)
    Aug 17 04:38:05.890: INFO: (18) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 12.208362ms)
    Aug 17 04:38:05.892: INFO: (18) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 13.375959ms)
    Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z/proxy/rewriteme">test</a> (200; 7.04204ms)
    Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 7.677893ms)
    Aug 17 04:38:05.899: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 7.05358ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:162/proxy/: bar (200; 8.009966ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:462/proxy/: tls qux (200; 8.330609ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/http:proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">... (200; 8.68823ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:443/proxy/tlsrewritem... (200; 8.173468ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/: <a href="/api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:1080/proxy/rewriteme">test<... (200; 8.753468ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/https:proxy-service-hn2pb-npq6z:460/proxy/: tls baz (200; 8.611843ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/pods/proxy-service-hn2pb-npq6z:160/proxy/: foo (200; 8.674606ms)
    Aug 17 04:38:05.901: INFO: (19) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname2/proxy/: tls qux (200; 8.887171ms)
    Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname2/proxy/: bar (200; 10.573416ms)
    Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname1/proxy/: foo (200; 10.887696ms)
    Aug 17 04:38:05.903: INFO: (19) /api/v1/namespaces/proxy-2634/services/https:proxy-service-hn2pb:tlsportname1/proxy/: tls baz (200; 10.4502ms)
    Aug 17 04:38:05.909: INFO: (19) /api/v1/namespaces/proxy-2634/services/proxy-service-hn2pb:portname1/proxy/: foo (200; 15.891506ms)
    Aug 17 04:38:05.909: INFO: (19) /api/v1/namespaces/proxy-2634/services/http:proxy-service-hn2pb:portname2/proxy/: bar (200; 16.990606ms)
    STEP: deleting ReplicationController proxy-service-hn2pb in namespace proxy-2634, will wait for the garbage collector to delete the pods 08/17/23 04:38:05.909
    Aug 17 04:38:05.971: INFO: Deleting ReplicationController proxy-service-hn2pb took: 7.50181ms
    Aug 17 04:38:06.072: INFO: Terminating ReplicationController proxy-service-hn2pb pods took: 100.277582ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:08.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2634" for this suite. 08/17/23 04:38:08.58
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:08.588
Aug 17 04:38:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:38:08.589
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:08.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:08.605
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 17 04:38:08.621: INFO: created pod
Aug 17 04:38:08.621: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3241" to be "Succeeded or Failed"
Aug 17 04:38:08.625: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771167ms
Aug 17 04:38:10.631: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009213483s
Aug 17 04:38:12.632: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010344247s
STEP: Saw pod success 08/17/23 04:38:12.632
Aug 17 04:38:12.632: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 17 04:38:42.633: INFO: polling logs
Aug 17 04:38:42.644: INFO: Pod logs: 
I0817 04:38:09.718142       1 log.go:198] OK: Got token
I0817 04:38:09.718220       1 log.go:198] validating with in-cluster discovery
I0817 04:38:09.718637       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0817 04:38:09.718679       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3241:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692247688, NotBefore:1692247088, IssuedAt:1692247088, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3241", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"42126088-0820-4faa-bd7a-03d26df9447f"}}}
I0817 04:38:09.735630       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0817 04:38:09.747901       1 log.go:198] OK: Validated signature on JWT
I0817 04:38:09.748148       1 log.go:198] OK: Got valid claims from token!
I0817 04:38:09.748204       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3241:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692247688, NotBefore:1692247088, IssuedAt:1692247088, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3241", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"42126088-0820-4faa-bd7a-03d26df9447f"}}}

Aug 17 04:38:42.644: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:42.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3241" for this suite. 08/17/23 04:38:42.657
------------------------------
• [SLOW TEST] [34.076 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:08.588
    Aug 17 04:38:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename svcaccounts 08/17/23 04:38:08.589
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:08.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:08.605
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 17 04:38:08.621: INFO: created pod
    Aug 17 04:38:08.621: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3241" to be "Succeeded or Failed"
    Aug 17 04:38:08.625: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771167ms
    Aug 17 04:38:10.631: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009213483s
    Aug 17 04:38:12.632: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010344247s
    STEP: Saw pod success 08/17/23 04:38:12.632
    Aug 17 04:38:12.632: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 17 04:38:42.633: INFO: polling logs
    Aug 17 04:38:42.644: INFO: Pod logs: 
    I0817 04:38:09.718142       1 log.go:198] OK: Got token
    I0817 04:38:09.718220       1 log.go:198] validating with in-cluster discovery
    I0817 04:38:09.718637       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0817 04:38:09.718679       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3241:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692247688, NotBefore:1692247088, IssuedAt:1692247088, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3241", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"42126088-0820-4faa-bd7a-03d26df9447f"}}}
    I0817 04:38:09.735630       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0817 04:38:09.747901       1 log.go:198] OK: Validated signature on JWT
    I0817 04:38:09.748148       1 log.go:198] OK: Got valid claims from token!
    I0817 04:38:09.748204       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3241:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692247688, NotBefore:1692247088, IssuedAt:1692247088, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3241", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"42126088-0820-4faa-bd7a-03d26df9447f"}}}

    Aug 17 04:38:42.644: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:42.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3241" for this suite. 08/17/23 04:38:42.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:42.665
Aug 17 04:38:42.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename daemonsets 08/17/23 04:38:42.666
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:42.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:42.681
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Aug 17 04:38:42.712: INFO: Create a RollingUpdate DaemonSet
Aug 17 04:38:42.718: INFO: Check that daemon pods launch on every node of the cluster
Aug 17 04:38:42.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:38:42.726: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
Aug 17 04:38:43.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 17 04:38:43.738: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
Aug 17 04:38:44.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 17 04:38:44.740: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Aug 17 04:38:44.740: INFO: Update the DaemonSet to trigger a rollout
Aug 17 04:38:44.750: INFO: Updating DaemonSet daemon-set
Aug 17 04:38:46.772: INFO: Roll back the DaemonSet before rollout is complete
Aug 17 04:38:46.781: INFO: Updating DaemonSet daemon-set
Aug 17 04:38:46.781: INFO: Make sure DaemonSet rollback is complete
Aug 17 04:38:46.785: INFO: Wrong image for pod: daemon-set-mb7h9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 17 04:38:46.785: INFO: Pod daemon-set-mb7h9 is not available
Aug 17 04:38:50.796: INFO: Pod daemon-set-47bmx is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/17/23 04:38:50.813
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3245, will wait for the garbage collector to delete the pods 08/17/23 04:38:50.813
Aug 17 04:38:50.875: INFO: Deleting DaemonSet.extensions daemon-set took: 7.253263ms
Aug 17 04:38:50.975: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.317876ms
Aug 17 04:38:52.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 17 04:38:52.681: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 17 04:38:52.684: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"254369"},"items":null}

Aug 17 04:38:52.688: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"254369"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:52.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3245" for this suite. 08/17/23 04:38:52.714
------------------------------
• [SLOW TEST] [10.056 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:42.665
    Aug 17 04:38:42.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename daemonsets 08/17/23 04:38:42.666
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:42.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:42.681
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Aug 17 04:38:42.712: INFO: Create a RollingUpdate DaemonSet
    Aug 17 04:38:42.718: INFO: Check that daemon pods launch on every node of the cluster
    Aug 17 04:38:42.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:38:42.726: INFO: Node ske-rhel-6c9d465fc4xbjl6l-wz7jz is running 0 daemon pod, expected 1
    Aug 17 04:38:43.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 17 04:38:43.738: INFO: Node ske-ubuntu-79fff84d86x69988-vjwlx is running 0 daemon pod, expected 1
    Aug 17 04:38:44.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 17 04:38:44.740: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Aug 17 04:38:44.740: INFO: Update the DaemonSet to trigger a rollout
    Aug 17 04:38:44.750: INFO: Updating DaemonSet daemon-set
    Aug 17 04:38:46.772: INFO: Roll back the DaemonSet before rollout is complete
    Aug 17 04:38:46.781: INFO: Updating DaemonSet daemon-set
    Aug 17 04:38:46.781: INFO: Make sure DaemonSet rollback is complete
    Aug 17 04:38:46.785: INFO: Wrong image for pod: daemon-set-mb7h9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 17 04:38:46.785: INFO: Pod daemon-set-mb7h9 is not available
    Aug 17 04:38:50.796: INFO: Pod daemon-set-47bmx is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/17/23 04:38:50.813
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3245, will wait for the garbage collector to delete the pods 08/17/23 04:38:50.813
    Aug 17 04:38:50.875: INFO: Deleting DaemonSet.extensions daemon-set took: 7.253263ms
    Aug 17 04:38:50.975: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.317876ms
    Aug 17 04:38:52.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 17 04:38:52.681: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 17 04:38:52.684: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"254369"},"items":null}

    Aug 17 04:38:52.688: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"254369"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:52.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3245" for this suite. 08/17/23 04:38:52.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:52.722
Aug 17 04:38:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename endpointslice 08/17/23 04:38:52.723
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:52.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:52.74
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/17/23 04:38:52.744
STEP: getting /apis/discovery.k8s.io 08/17/23 04:38:52.749
STEP: getting /apis/discovery.k8s.iov1 08/17/23 04:38:52.751
STEP: creating 08/17/23 04:38:52.753
STEP: getting 08/17/23 04:38:52.77
STEP: listing 08/17/23 04:38:52.774
STEP: watching 08/17/23 04:38:52.778
Aug 17 04:38:52.778: INFO: starting watch
STEP: cluster-wide listing 08/17/23 04:38:52.78
STEP: cluster-wide watching 08/17/23 04:38:52.784
Aug 17 04:38:52.784: INFO: starting watch
STEP: patching 08/17/23 04:38:52.786
STEP: updating 08/17/23 04:38:52.793
Aug 17 04:38:52.802: INFO: waiting for watch events with expected annotations
Aug 17 04:38:52.802: INFO: saw patched and updated annotations
STEP: deleting 08/17/23 04:38:52.802
STEP: deleting a collection 08/17/23 04:38:52.816
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 17 04:38:52.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9415" for this suite. 08/17/23 04:38:52.838
------------------------------
• [0.125 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:52.722
    Aug 17 04:38:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename endpointslice 08/17/23 04:38:52.723
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:52.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:52.74
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/17/23 04:38:52.744
    STEP: getting /apis/discovery.k8s.io 08/17/23 04:38:52.749
    STEP: getting /apis/discovery.k8s.iov1 08/17/23 04:38:52.751
    STEP: creating 08/17/23 04:38:52.753
    STEP: getting 08/17/23 04:38:52.77
    STEP: listing 08/17/23 04:38:52.774
    STEP: watching 08/17/23 04:38:52.778
    Aug 17 04:38:52.778: INFO: starting watch
    STEP: cluster-wide listing 08/17/23 04:38:52.78
    STEP: cluster-wide watching 08/17/23 04:38:52.784
    Aug 17 04:38:52.784: INFO: starting watch
    STEP: patching 08/17/23 04:38:52.786
    STEP: updating 08/17/23 04:38:52.793
    Aug 17 04:38:52.802: INFO: waiting for watch events with expected annotations
    Aug 17 04:38:52.802: INFO: saw patched and updated annotations
    STEP: deleting 08/17/23 04:38:52.802
    STEP: deleting a collection 08/17/23 04:38:52.816
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:38:52.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9415" for this suite. 08/17/23 04:38:52.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:38:52.85
Aug 17 04:38:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename container-probe 08/17/23 04:38:52.851
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:52.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:52.87
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 17 04:38:52.883: INFO: Waiting up to 5m0s for pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c" in namespace "container-probe-8547" to be "running and ready"
Aug 17 04:38:52.888: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945241ms
Aug 17 04:38:52.888: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Pending, waiting for it to be Running (with Ready = true)
Aug 17 04:38:54.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 2.010519115s
Aug 17 04:38:54.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:38:56.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 4.011042114s
Aug 17 04:38:56.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:38:58.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 6.010552299s
Aug 17 04:38:58.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:00.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 8.010512804s
Aug 17 04:39:00.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:02.895: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 10.011547087s
Aug 17 04:39:02.895: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:04.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 12.01111234s
Aug 17 04:39:04.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:06.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 14.011337184s
Aug 17 04:39:06.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:08.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 16.011154579s
Aug 17 04:39:08.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:10.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 18.010887759s
Aug 17 04:39:10.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:12.893: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 20.010270593s
Aug 17 04:39:12.893: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
Aug 17 04:39:14.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=true. Elapsed: 22.010873539s
Aug 17 04:39:14.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = true)
Aug 17 04:39:14.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c" satisfied condition "running and ready"
Aug 17 04:39:14.898: INFO: Container started at 2023-08-17 04:38:53 +0000 UTC, pod became ready at 2023-08-17 04:39:13 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 17 04:39:14.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8547" for this suite. 08/17/23 04:39:14.906
------------------------------
• [SLOW TEST] [22.065 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:38:52.85
    Aug 17 04:38:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename container-probe 08/17/23 04:38:52.851
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:38:52.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:38:52.87
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 17 04:38:52.883: INFO: Waiting up to 5m0s for pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c" in namespace "container-probe-8547" to be "running and ready"
    Aug 17 04:38:52.888: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945241ms
    Aug 17 04:38:52.888: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Pending, waiting for it to be Running (with Ready = true)
    Aug 17 04:38:54.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 2.010519115s
    Aug 17 04:38:54.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:38:56.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 4.011042114s
    Aug 17 04:38:56.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:38:58.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 6.010552299s
    Aug 17 04:38:58.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:00.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 8.010512804s
    Aug 17 04:39:00.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:02.895: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 10.011547087s
    Aug 17 04:39:02.895: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:04.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 12.01111234s
    Aug 17 04:39:04.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:06.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 14.011337184s
    Aug 17 04:39:06.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:08.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 16.011154579s
    Aug 17 04:39:08.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:10.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 18.010887759s
    Aug 17 04:39:10.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:12.893: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=false. Elapsed: 20.010270593s
    Aug 17 04:39:12.893: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = false)
    Aug 17 04:39:14.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c": Phase="Running", Reason="", readiness=true. Elapsed: 22.010873539s
    Aug 17 04:39:14.894: INFO: The phase of Pod test-webserver-d3556235-f219-4187-bb30-358a790c400c is Running (Ready = true)
    Aug 17 04:39:14.894: INFO: Pod "test-webserver-d3556235-f219-4187-bb30-358a790c400c" satisfied condition "running and ready"
    Aug 17 04:39:14.898: INFO: Container started at 2023-08-17 04:38:53 +0000 UTC, pod became ready at 2023-08-17 04:39:13 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:39:14.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8547" for this suite. 08/17/23 04:39:14.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:39:14.92
Aug 17 04:39:14.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename disruption 08/17/23 04:39:14.921
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:14.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:14.942
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/17/23 04:39:14.953
STEP: Updating PodDisruptionBudget status 08/17/23 04:39:16.964
STEP: Waiting for all pods to be running 08/17/23 04:39:16.972
Aug 17 04:39:16.977: INFO: running pods: 0 < 1
STEP: locating a running pod 08/17/23 04:39:18.984
STEP: Waiting for the pdb to be processed 08/17/23 04:39:18.999
STEP: Patching PodDisruptionBudget status 08/17/23 04:39:19.009
STEP: Waiting for the pdb to be processed 08/17/23 04:39:19.022
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 17 04:39:19.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1853" for this suite. 08/17/23 04:39:19.035
------------------------------
• [4.125 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:39:14.92
    Aug 17 04:39:14.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename disruption 08/17/23 04:39:14.921
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:14.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:14.942
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/17/23 04:39:14.953
    STEP: Updating PodDisruptionBudget status 08/17/23 04:39:16.964
    STEP: Waiting for all pods to be running 08/17/23 04:39:16.972
    Aug 17 04:39:16.977: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/17/23 04:39:18.984
    STEP: Waiting for the pdb to be processed 08/17/23 04:39:18.999
    STEP: Patching PodDisruptionBudget status 08/17/23 04:39:19.009
    STEP: Waiting for the pdb to be processed 08/17/23 04:39:19.022
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:39:19.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1853" for this suite. 08/17/23 04:39:19.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:39:19.048
Aug 17 04:39:19.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename statefulset 08/17/23 04:39:19.048
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:19.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:19.075
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9146 08/17/23 04:39:19.08
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 17 04:39:19.097: INFO: Found 0 stateful pods, waiting for 1
Aug 17 04:39:29.103: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/17/23 04:39:29.112
W0817 04:39:29.121544      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 17 04:39:29.131: INFO: Found 1 stateful pods, waiting for 2
Aug 17 04:39:39.138: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 17 04:39:39.138: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/17/23 04:39:39.152
STEP: Delete all of the StatefulSets 08/17/23 04:39:39.157
STEP: Verify that StatefulSets have been deleted 08/17/23 04:39:39.166
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 17 04:39:39.171: INFO: Deleting all statefulset in ns statefulset-9146
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 17 04:39:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9146" for this suite. 08/17/23 04:39:39.195
------------------------------
• [SLOW TEST] [20.157 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:39:19.048
    Aug 17 04:39:19.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename statefulset 08/17/23 04:39:19.048
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:19.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:19.075
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9146 08/17/23 04:39:19.08
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 17 04:39:19.097: INFO: Found 0 stateful pods, waiting for 1
    Aug 17 04:39:29.103: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/17/23 04:39:29.112
    W0817 04:39:29.121544      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 17 04:39:29.131: INFO: Found 1 stateful pods, waiting for 2
    Aug 17 04:39:39.138: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 17 04:39:39.138: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/17/23 04:39:39.152
    STEP: Delete all of the StatefulSets 08/17/23 04:39:39.157
    STEP: Verify that StatefulSets have been deleted 08/17/23 04:39:39.166
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 17 04:39:39.171: INFO: Deleting all statefulset in ns statefulset-9146
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:39:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9146" for this suite. 08/17/23 04:39:39.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:39:39.216
Aug 17 04:39:39.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename gc 08/17/23 04:39:39.216
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:39.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:39.237
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/17/23 04:39:39.252
STEP: delete the rc 08/17/23 04:39:44.268
STEP: wait for the rc to be deleted 08/17/23 04:39:44.277
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/17/23 04:39:49.299
STEP: Gathering metrics 08/17/23 04:40:19.318
W0817 04:40:19.332795      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 17 04:40:19.332: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 17 04:40:19.333: INFO: Deleting pod "simpletest.rc-27sdq" in namespace "gc-4014"
Aug 17 04:40:19.346: INFO: Deleting pod "simpletest.rc-29mjt" in namespace "gc-4014"
Aug 17 04:40:19.376: INFO: Deleting pod "simpletest.rc-2b648" in namespace "gc-4014"
Aug 17 04:40:19.389: INFO: Deleting pod "simpletest.rc-2rhpw" in namespace "gc-4014"
Aug 17 04:40:19.407: INFO: Deleting pod "simpletest.rc-2tpdm" in namespace "gc-4014"
Aug 17 04:40:19.441: INFO: Deleting pod "simpletest.rc-2vcpx" in namespace "gc-4014"
Aug 17 04:40:19.459: INFO: Deleting pod "simpletest.rc-4f5s6" in namespace "gc-4014"
Aug 17 04:40:19.473: INFO: Deleting pod "simpletest.rc-4ltqw" in namespace "gc-4014"
Aug 17 04:40:19.489: INFO: Deleting pod "simpletest.rc-4mrn4" in namespace "gc-4014"
Aug 17 04:40:19.573: INFO: Deleting pod "simpletest.rc-4tfnc" in namespace "gc-4014"
Aug 17 04:40:19.618: INFO: Deleting pod "simpletest.rc-52gwg" in namespace "gc-4014"
Aug 17 04:40:19.678: INFO: Deleting pod "simpletest.rc-57n8k" in namespace "gc-4014"
Aug 17 04:40:19.695: INFO: Deleting pod "simpletest.rc-5dxfl" in namespace "gc-4014"
Aug 17 04:40:19.708: INFO: Deleting pod "simpletest.rc-5x8nn" in namespace "gc-4014"
Aug 17 04:40:19.753: INFO: Deleting pod "simpletest.rc-5z9z9" in namespace "gc-4014"
Aug 17 04:40:19.775: INFO: Deleting pod "simpletest.rc-5zkcb" in namespace "gc-4014"
Aug 17 04:40:19.826: INFO: Deleting pod "simpletest.rc-6ltk4" in namespace "gc-4014"
Aug 17 04:40:19.842: INFO: Deleting pod "simpletest.rc-6mldl" in namespace "gc-4014"
Aug 17 04:40:19.853: INFO: Deleting pod "simpletest.rc-6tqqk" in namespace "gc-4014"
Aug 17 04:40:19.925: INFO: Deleting pod "simpletest.rc-6xqpl" in namespace "gc-4014"
Aug 17 04:40:19.945: INFO: Deleting pod "simpletest.rc-7925k" in namespace "gc-4014"
Aug 17 04:40:19.959: INFO: Deleting pod "simpletest.rc-7xj4x" in namespace "gc-4014"
Aug 17 04:40:19.973: INFO: Deleting pod "simpletest.rc-8nx42" in namespace "gc-4014"
Aug 17 04:40:19.987: INFO: Deleting pod "simpletest.rc-8xf2m" in namespace "gc-4014"
Aug 17 04:40:20.004: INFO: Deleting pod "simpletest.rc-92fsx" in namespace "gc-4014"
Aug 17 04:40:20.018: INFO: Deleting pod "simpletest.rc-95lf6" in namespace "gc-4014"
Aug 17 04:40:20.037: INFO: Deleting pod "simpletest.rc-9c9xf" in namespace "gc-4014"
Aug 17 04:40:20.118: INFO: Deleting pod "simpletest.rc-9qrl8" in namespace "gc-4014"
Aug 17 04:40:20.133: INFO: Deleting pod "simpletest.rc-b8kjs" in namespace "gc-4014"
Aug 17 04:40:20.177: INFO: Deleting pod "simpletest.rc-bds8p" in namespace "gc-4014"
Aug 17 04:40:20.195: INFO: Deleting pod "simpletest.rc-bnx8b" in namespace "gc-4014"
Aug 17 04:40:20.213: INFO: Deleting pod "simpletest.rc-c6mnx" in namespace "gc-4014"
Aug 17 04:40:20.236: INFO: Deleting pod "simpletest.rc-c8xrl" in namespace "gc-4014"
Aug 17 04:40:20.257: INFO: Deleting pod "simpletest.rc-cbs7m" in namespace "gc-4014"
Aug 17 04:40:20.270: INFO: Deleting pod "simpletest.rc-cgx9s" in namespace "gc-4014"
Aug 17 04:40:20.285: INFO: Deleting pod "simpletest.rc-chxrr" in namespace "gc-4014"
Aug 17 04:40:20.303: INFO: Deleting pod "simpletest.rc-cpt5z" in namespace "gc-4014"
Aug 17 04:40:20.318: INFO: Deleting pod "simpletest.rc-cpts7" in namespace "gc-4014"
Aug 17 04:40:20.345: INFO: Deleting pod "simpletest.rc-cs24k" in namespace "gc-4014"
Aug 17 04:40:20.368: INFO: Deleting pod "simpletest.rc-dbkjj" in namespace "gc-4014"
Aug 17 04:40:20.381: INFO: Deleting pod "simpletest.rc-dzr4s" in namespace "gc-4014"
Aug 17 04:40:20.396: INFO: Deleting pod "simpletest.rc-fkzfj" in namespace "gc-4014"
Aug 17 04:40:20.407: INFO: Deleting pod "simpletest.rc-ft2zj" in namespace "gc-4014"
Aug 17 04:40:20.430: INFO: Deleting pod "simpletest.rc-fz774" in namespace "gc-4014"
Aug 17 04:40:20.452: INFO: Deleting pod "simpletest.rc-g5vbs" in namespace "gc-4014"
Aug 17 04:40:20.463: INFO: Deleting pod "simpletest.rc-g7mvr" in namespace "gc-4014"
Aug 17 04:40:20.477: INFO: Deleting pod "simpletest.rc-gb7rz" in namespace "gc-4014"
Aug 17 04:40:20.493: INFO: Deleting pod "simpletest.rc-gxc55" in namespace "gc-4014"
Aug 17 04:40:20.508: INFO: Deleting pod "simpletest.rc-gzc4c" in namespace "gc-4014"
Aug 17 04:40:20.522: INFO: Deleting pod "simpletest.rc-h6vbv" in namespace "gc-4014"
Aug 17 04:40:20.545: INFO: Deleting pod "simpletest.rc-j4wwf" in namespace "gc-4014"
Aug 17 04:40:20.569: INFO: Deleting pod "simpletest.rc-j6zdx" in namespace "gc-4014"
Aug 17 04:40:20.595: INFO: Deleting pod "simpletest.rc-jbt5g" in namespace "gc-4014"
Aug 17 04:40:20.625: INFO: Deleting pod "simpletest.rc-jhhxt" in namespace "gc-4014"
Aug 17 04:40:20.648: INFO: Deleting pod "simpletest.rc-jkphz" in namespace "gc-4014"
Aug 17 04:40:20.671: INFO: Deleting pod "simpletest.rc-kjvgg" in namespace "gc-4014"
Aug 17 04:40:20.691: INFO: Deleting pod "simpletest.rc-kvl9s" in namespace "gc-4014"
Aug 17 04:40:20.707: INFO: Deleting pod "simpletest.rc-kxpwz" in namespace "gc-4014"
Aug 17 04:40:20.727: INFO: Deleting pod "simpletest.rc-l9wpj" in namespace "gc-4014"
Aug 17 04:40:20.772: INFO: Deleting pod "simpletest.rc-lblb5" in namespace "gc-4014"
Aug 17 04:40:20.788: INFO: Deleting pod "simpletest.rc-lnths" in namespace "gc-4014"
Aug 17 04:40:20.800: INFO: Deleting pod "simpletest.rc-lxzz8" in namespace "gc-4014"
Aug 17 04:40:20.811: INFO: Deleting pod "simpletest.rc-mnwdp" in namespace "gc-4014"
Aug 17 04:40:20.832: INFO: Deleting pod "simpletest.rc-mpgqm" in namespace "gc-4014"
Aug 17 04:40:20.860: INFO: Deleting pod "simpletest.rc-mzrr7" in namespace "gc-4014"
Aug 17 04:40:20.875: INFO: Deleting pod "simpletest.rc-ngc2n" in namespace "gc-4014"
Aug 17 04:40:20.887: INFO: Deleting pod "simpletest.rc-nhrlv" in namespace "gc-4014"
Aug 17 04:40:20.905: INFO: Deleting pod "simpletest.rc-ntjzz" in namespace "gc-4014"
Aug 17 04:40:20.916: INFO: Deleting pod "simpletest.rc-p2tqd" in namespace "gc-4014"
Aug 17 04:40:20.929: INFO: Deleting pod "simpletest.rc-p42p9" in namespace "gc-4014"
Aug 17 04:40:20.946: INFO: Deleting pod "simpletest.rc-p6p2l" in namespace "gc-4014"
Aug 17 04:40:20.978: INFO: Deleting pod "simpletest.rc-pkbrg" in namespace "gc-4014"
Aug 17 04:40:20.991: INFO: Deleting pod "simpletest.rc-ps7st" in namespace "gc-4014"
Aug 17 04:40:21.011: INFO: Deleting pod "simpletest.rc-pstbr" in namespace "gc-4014"
Aug 17 04:40:21.031: INFO: Deleting pod "simpletest.rc-ql2zf" in namespace "gc-4014"
Aug 17 04:40:21.072: INFO: Deleting pod "simpletest.rc-qqktf" in namespace "gc-4014"
Aug 17 04:40:21.086: INFO: Deleting pod "simpletest.rc-qv52c" in namespace "gc-4014"
Aug 17 04:40:21.095: INFO: Deleting pod "simpletest.rc-qx8jt" in namespace "gc-4014"
Aug 17 04:40:21.107: INFO: Deleting pod "simpletest.rc-rb4wt" in namespace "gc-4014"
Aug 17 04:40:21.119: INFO: Deleting pod "simpletest.rc-rhbdx" in namespace "gc-4014"
Aug 17 04:40:21.130: INFO: Deleting pod "simpletest.rc-rjkb6" in namespace "gc-4014"
Aug 17 04:40:21.147: INFO: Deleting pod "simpletest.rc-rnv8k" in namespace "gc-4014"
Aug 17 04:40:21.185: INFO: Deleting pod "simpletest.rc-s5jv9" in namespace "gc-4014"
Aug 17 04:40:21.202: INFO: Deleting pod "simpletest.rc-skm7p" in namespace "gc-4014"
Aug 17 04:40:21.226: INFO: Deleting pod "simpletest.rc-slhjm" in namespace "gc-4014"
Aug 17 04:40:21.238: INFO: Deleting pod "simpletest.rc-sqm6t" in namespace "gc-4014"
Aug 17 04:40:21.249: INFO: Deleting pod "simpletest.rc-t47mg" in namespace "gc-4014"
Aug 17 04:40:21.274: INFO: Deleting pod "simpletest.rc-t6t94" in namespace "gc-4014"
Aug 17 04:40:21.314: INFO: Deleting pod "simpletest.rc-tj4nt" in namespace "gc-4014"
Aug 17 04:40:21.361: INFO: Deleting pod "simpletest.rc-v7khs" in namespace "gc-4014"
Aug 17 04:40:21.413: INFO: Deleting pod "simpletest.rc-v8h2h" in namespace "gc-4014"
Aug 17 04:40:21.468: INFO: Deleting pod "simpletest.rc-v9qdf" in namespace "gc-4014"
Aug 17 04:40:21.515: INFO: Deleting pod "simpletest.rc-vdk2q" in namespace "gc-4014"
Aug 17 04:40:21.563: INFO: Deleting pod "simpletest.rc-vz56p" in namespace "gc-4014"
Aug 17 04:40:21.619: INFO: Deleting pod "simpletest.rc-wrfkj" in namespace "gc-4014"
Aug 17 04:40:21.673: INFO: Deleting pod "simpletest.rc-wzbnz" in namespace "gc-4014"
Aug 17 04:40:21.720: INFO: Deleting pod "simpletest.rc-xbftw" in namespace "gc-4014"
Aug 17 04:40:21.763: INFO: Deleting pod "simpletest.rc-xcbxz" in namespace "gc-4014"
Aug 17 04:40:21.816: INFO: Deleting pod "simpletest.rc-z5z9k" in namespace "gc-4014"
Aug 17 04:40:21.867: INFO: Deleting pod "simpletest.rc-zptqp" in namespace "gc-4014"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 17 04:40:21.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4014" for this suite. 08/17/23 04:40:21.982
------------------------------
• [SLOW TEST] [42.796 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:39:39.216
    Aug 17 04:39:39.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename gc 08/17/23 04:39:39.216
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:39:39.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:39:39.237
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/17/23 04:39:39.252
    STEP: delete the rc 08/17/23 04:39:44.268
    STEP: wait for the rc to be deleted 08/17/23 04:39:44.277
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/17/23 04:39:49.299
    STEP: Gathering metrics 08/17/23 04:40:19.318
    W0817 04:40:19.332795      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 17 04:40:19.332: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 17 04:40:19.333: INFO: Deleting pod "simpletest.rc-27sdq" in namespace "gc-4014"
    Aug 17 04:40:19.346: INFO: Deleting pod "simpletest.rc-29mjt" in namespace "gc-4014"
    Aug 17 04:40:19.376: INFO: Deleting pod "simpletest.rc-2b648" in namespace "gc-4014"
    Aug 17 04:40:19.389: INFO: Deleting pod "simpletest.rc-2rhpw" in namespace "gc-4014"
    Aug 17 04:40:19.407: INFO: Deleting pod "simpletest.rc-2tpdm" in namespace "gc-4014"
    Aug 17 04:40:19.441: INFO: Deleting pod "simpletest.rc-2vcpx" in namespace "gc-4014"
    Aug 17 04:40:19.459: INFO: Deleting pod "simpletest.rc-4f5s6" in namespace "gc-4014"
    Aug 17 04:40:19.473: INFO: Deleting pod "simpletest.rc-4ltqw" in namespace "gc-4014"
    Aug 17 04:40:19.489: INFO: Deleting pod "simpletest.rc-4mrn4" in namespace "gc-4014"
    Aug 17 04:40:19.573: INFO: Deleting pod "simpletest.rc-4tfnc" in namespace "gc-4014"
    Aug 17 04:40:19.618: INFO: Deleting pod "simpletest.rc-52gwg" in namespace "gc-4014"
    Aug 17 04:40:19.678: INFO: Deleting pod "simpletest.rc-57n8k" in namespace "gc-4014"
    Aug 17 04:40:19.695: INFO: Deleting pod "simpletest.rc-5dxfl" in namespace "gc-4014"
    Aug 17 04:40:19.708: INFO: Deleting pod "simpletest.rc-5x8nn" in namespace "gc-4014"
    Aug 17 04:40:19.753: INFO: Deleting pod "simpletest.rc-5z9z9" in namespace "gc-4014"
    Aug 17 04:40:19.775: INFO: Deleting pod "simpletest.rc-5zkcb" in namespace "gc-4014"
    Aug 17 04:40:19.826: INFO: Deleting pod "simpletest.rc-6ltk4" in namespace "gc-4014"
    Aug 17 04:40:19.842: INFO: Deleting pod "simpletest.rc-6mldl" in namespace "gc-4014"
    Aug 17 04:40:19.853: INFO: Deleting pod "simpletest.rc-6tqqk" in namespace "gc-4014"
    Aug 17 04:40:19.925: INFO: Deleting pod "simpletest.rc-6xqpl" in namespace "gc-4014"
    Aug 17 04:40:19.945: INFO: Deleting pod "simpletest.rc-7925k" in namespace "gc-4014"
    Aug 17 04:40:19.959: INFO: Deleting pod "simpletest.rc-7xj4x" in namespace "gc-4014"
    Aug 17 04:40:19.973: INFO: Deleting pod "simpletest.rc-8nx42" in namespace "gc-4014"
    Aug 17 04:40:19.987: INFO: Deleting pod "simpletest.rc-8xf2m" in namespace "gc-4014"
    Aug 17 04:40:20.004: INFO: Deleting pod "simpletest.rc-92fsx" in namespace "gc-4014"
    Aug 17 04:40:20.018: INFO: Deleting pod "simpletest.rc-95lf6" in namespace "gc-4014"
    Aug 17 04:40:20.037: INFO: Deleting pod "simpletest.rc-9c9xf" in namespace "gc-4014"
    Aug 17 04:40:20.118: INFO: Deleting pod "simpletest.rc-9qrl8" in namespace "gc-4014"
    Aug 17 04:40:20.133: INFO: Deleting pod "simpletest.rc-b8kjs" in namespace "gc-4014"
    Aug 17 04:40:20.177: INFO: Deleting pod "simpletest.rc-bds8p" in namespace "gc-4014"
    Aug 17 04:40:20.195: INFO: Deleting pod "simpletest.rc-bnx8b" in namespace "gc-4014"
    Aug 17 04:40:20.213: INFO: Deleting pod "simpletest.rc-c6mnx" in namespace "gc-4014"
    Aug 17 04:40:20.236: INFO: Deleting pod "simpletest.rc-c8xrl" in namespace "gc-4014"
    Aug 17 04:40:20.257: INFO: Deleting pod "simpletest.rc-cbs7m" in namespace "gc-4014"
    Aug 17 04:40:20.270: INFO: Deleting pod "simpletest.rc-cgx9s" in namespace "gc-4014"
    Aug 17 04:40:20.285: INFO: Deleting pod "simpletest.rc-chxrr" in namespace "gc-4014"
    Aug 17 04:40:20.303: INFO: Deleting pod "simpletest.rc-cpt5z" in namespace "gc-4014"
    Aug 17 04:40:20.318: INFO: Deleting pod "simpletest.rc-cpts7" in namespace "gc-4014"
    Aug 17 04:40:20.345: INFO: Deleting pod "simpletest.rc-cs24k" in namespace "gc-4014"
    Aug 17 04:40:20.368: INFO: Deleting pod "simpletest.rc-dbkjj" in namespace "gc-4014"
    Aug 17 04:40:20.381: INFO: Deleting pod "simpletest.rc-dzr4s" in namespace "gc-4014"
    Aug 17 04:40:20.396: INFO: Deleting pod "simpletest.rc-fkzfj" in namespace "gc-4014"
    Aug 17 04:40:20.407: INFO: Deleting pod "simpletest.rc-ft2zj" in namespace "gc-4014"
    Aug 17 04:40:20.430: INFO: Deleting pod "simpletest.rc-fz774" in namespace "gc-4014"
    Aug 17 04:40:20.452: INFO: Deleting pod "simpletest.rc-g5vbs" in namespace "gc-4014"
    Aug 17 04:40:20.463: INFO: Deleting pod "simpletest.rc-g7mvr" in namespace "gc-4014"
    Aug 17 04:40:20.477: INFO: Deleting pod "simpletest.rc-gb7rz" in namespace "gc-4014"
    Aug 17 04:40:20.493: INFO: Deleting pod "simpletest.rc-gxc55" in namespace "gc-4014"
    Aug 17 04:40:20.508: INFO: Deleting pod "simpletest.rc-gzc4c" in namespace "gc-4014"
    Aug 17 04:40:20.522: INFO: Deleting pod "simpletest.rc-h6vbv" in namespace "gc-4014"
    Aug 17 04:40:20.545: INFO: Deleting pod "simpletest.rc-j4wwf" in namespace "gc-4014"
    Aug 17 04:40:20.569: INFO: Deleting pod "simpletest.rc-j6zdx" in namespace "gc-4014"
    Aug 17 04:40:20.595: INFO: Deleting pod "simpletest.rc-jbt5g" in namespace "gc-4014"
    Aug 17 04:40:20.625: INFO: Deleting pod "simpletest.rc-jhhxt" in namespace "gc-4014"
    Aug 17 04:40:20.648: INFO: Deleting pod "simpletest.rc-jkphz" in namespace "gc-4014"
    Aug 17 04:40:20.671: INFO: Deleting pod "simpletest.rc-kjvgg" in namespace "gc-4014"
    Aug 17 04:40:20.691: INFO: Deleting pod "simpletest.rc-kvl9s" in namespace "gc-4014"
    Aug 17 04:40:20.707: INFO: Deleting pod "simpletest.rc-kxpwz" in namespace "gc-4014"
    Aug 17 04:40:20.727: INFO: Deleting pod "simpletest.rc-l9wpj" in namespace "gc-4014"
    Aug 17 04:40:20.772: INFO: Deleting pod "simpletest.rc-lblb5" in namespace "gc-4014"
    Aug 17 04:40:20.788: INFO: Deleting pod "simpletest.rc-lnths" in namespace "gc-4014"
    Aug 17 04:40:20.800: INFO: Deleting pod "simpletest.rc-lxzz8" in namespace "gc-4014"
    Aug 17 04:40:20.811: INFO: Deleting pod "simpletest.rc-mnwdp" in namespace "gc-4014"
    Aug 17 04:40:20.832: INFO: Deleting pod "simpletest.rc-mpgqm" in namespace "gc-4014"
    Aug 17 04:40:20.860: INFO: Deleting pod "simpletest.rc-mzrr7" in namespace "gc-4014"
    Aug 17 04:40:20.875: INFO: Deleting pod "simpletest.rc-ngc2n" in namespace "gc-4014"
    Aug 17 04:40:20.887: INFO: Deleting pod "simpletest.rc-nhrlv" in namespace "gc-4014"
    Aug 17 04:40:20.905: INFO: Deleting pod "simpletest.rc-ntjzz" in namespace "gc-4014"
    Aug 17 04:40:20.916: INFO: Deleting pod "simpletest.rc-p2tqd" in namespace "gc-4014"
    Aug 17 04:40:20.929: INFO: Deleting pod "simpletest.rc-p42p9" in namespace "gc-4014"
    Aug 17 04:40:20.946: INFO: Deleting pod "simpletest.rc-p6p2l" in namespace "gc-4014"
    Aug 17 04:40:20.978: INFO: Deleting pod "simpletest.rc-pkbrg" in namespace "gc-4014"
    Aug 17 04:40:20.991: INFO: Deleting pod "simpletest.rc-ps7st" in namespace "gc-4014"
    Aug 17 04:40:21.011: INFO: Deleting pod "simpletest.rc-pstbr" in namespace "gc-4014"
    Aug 17 04:40:21.031: INFO: Deleting pod "simpletest.rc-ql2zf" in namespace "gc-4014"
    Aug 17 04:40:21.072: INFO: Deleting pod "simpletest.rc-qqktf" in namespace "gc-4014"
    Aug 17 04:40:21.086: INFO: Deleting pod "simpletest.rc-qv52c" in namespace "gc-4014"
    Aug 17 04:40:21.095: INFO: Deleting pod "simpletest.rc-qx8jt" in namespace "gc-4014"
    Aug 17 04:40:21.107: INFO: Deleting pod "simpletest.rc-rb4wt" in namespace "gc-4014"
    Aug 17 04:40:21.119: INFO: Deleting pod "simpletest.rc-rhbdx" in namespace "gc-4014"
    Aug 17 04:40:21.130: INFO: Deleting pod "simpletest.rc-rjkb6" in namespace "gc-4014"
    Aug 17 04:40:21.147: INFO: Deleting pod "simpletest.rc-rnv8k" in namespace "gc-4014"
    Aug 17 04:40:21.185: INFO: Deleting pod "simpletest.rc-s5jv9" in namespace "gc-4014"
    Aug 17 04:40:21.202: INFO: Deleting pod "simpletest.rc-skm7p" in namespace "gc-4014"
    Aug 17 04:40:21.226: INFO: Deleting pod "simpletest.rc-slhjm" in namespace "gc-4014"
    Aug 17 04:40:21.238: INFO: Deleting pod "simpletest.rc-sqm6t" in namespace "gc-4014"
    Aug 17 04:40:21.249: INFO: Deleting pod "simpletest.rc-t47mg" in namespace "gc-4014"
    Aug 17 04:40:21.274: INFO: Deleting pod "simpletest.rc-t6t94" in namespace "gc-4014"
    Aug 17 04:40:21.314: INFO: Deleting pod "simpletest.rc-tj4nt" in namespace "gc-4014"
    Aug 17 04:40:21.361: INFO: Deleting pod "simpletest.rc-v7khs" in namespace "gc-4014"
    Aug 17 04:40:21.413: INFO: Deleting pod "simpletest.rc-v8h2h" in namespace "gc-4014"
    Aug 17 04:40:21.468: INFO: Deleting pod "simpletest.rc-v9qdf" in namespace "gc-4014"
    Aug 17 04:40:21.515: INFO: Deleting pod "simpletest.rc-vdk2q" in namespace "gc-4014"
    Aug 17 04:40:21.563: INFO: Deleting pod "simpletest.rc-vz56p" in namespace "gc-4014"
    Aug 17 04:40:21.619: INFO: Deleting pod "simpletest.rc-wrfkj" in namespace "gc-4014"
    Aug 17 04:40:21.673: INFO: Deleting pod "simpletest.rc-wzbnz" in namespace "gc-4014"
    Aug 17 04:40:21.720: INFO: Deleting pod "simpletest.rc-xbftw" in namespace "gc-4014"
    Aug 17 04:40:21.763: INFO: Deleting pod "simpletest.rc-xcbxz" in namespace "gc-4014"
    Aug 17 04:40:21.816: INFO: Deleting pod "simpletest.rc-z5z9k" in namespace "gc-4014"
    Aug 17 04:40:21.867: INFO: Deleting pod "simpletest.rc-zptqp" in namespace "gc-4014"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:40:21.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4014" for this suite. 08/17/23 04:40:21.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:40:22.014
Aug 17 04:40:22.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename resourcequota 08/17/23 04:40:22.018
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:40:22.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:40:22.045
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/17/23 04:40:39.057
STEP: Creating a ResourceQuota 08/17/23 04:40:44.066
STEP: Ensuring resource quota status is calculated 08/17/23 04:40:44.073
STEP: Creating a ConfigMap 08/17/23 04:40:46.08
STEP: Ensuring resource quota status captures configMap creation 08/17/23 04:40:46.09
STEP: Deleting a ConfigMap 08/17/23 04:40:48.096
STEP: Ensuring resource quota status released usage 08/17/23 04:40:48.104
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 17 04:40:50.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5324" for this suite. 08/17/23 04:40:50.121
------------------------------
• [SLOW TEST] [28.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:40:22.014
    Aug 17 04:40:22.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename resourcequota 08/17/23 04:40:22.018
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:40:22.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:40:22.045
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/17/23 04:40:39.057
    STEP: Creating a ResourceQuota 08/17/23 04:40:44.066
    STEP: Ensuring resource quota status is calculated 08/17/23 04:40:44.073
    STEP: Creating a ConfigMap 08/17/23 04:40:46.08
    STEP: Ensuring resource quota status captures configMap creation 08/17/23 04:40:46.09
    STEP: Deleting a ConfigMap 08/17/23 04:40:48.096
    STEP: Ensuring resource quota status released usage 08/17/23 04:40:48.104
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:40:50.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5324" for this suite. 08/17/23 04:40:50.121
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/17/23 04:40:50.13
Aug 17 04:40:50.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
STEP: Building a namespace api object, basename tables 08/17/23 04:40:50.131
STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:40:50.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:40:50.151
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 17 04:40:50.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3025" for this suite. 08/17/23 04:40:50.169
------------------------------
• [0.048 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/17/23 04:40:50.13
    Aug 17 04:40:50.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2331764595
    STEP: Building a namespace api object, basename tables 08/17/23 04:40:50.131
    STEP: Waiting for a default service account to be provisioned in namespace 08/17/23 04:40:50.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/17/23 04:40:50.151
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 17 04:40:50.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3025" for this suite. 08/17/23 04:40:50.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 17 04:40:50.181: INFO: Running AfterSuite actions on node 1
Aug 17 04:40:50.181: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 17 04:40:50.181: INFO: Running AfterSuite actions on node 1
    Aug 17 04:40:50.181: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.087 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6052.969 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h40m53.40993851s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

