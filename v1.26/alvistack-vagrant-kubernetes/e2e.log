I1214 15:36:22.018566      14 e2e.go:126] Starting e2e run "9a41bc61-18a6-47a0-82d3-e8ecc018d4c2" on Ginkgo node 1
Dec 14 15:36:22.052: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1671032181 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec 14 15:36:22.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 15:36:22.281: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 14 15:36:22.305: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 14 15:36:22.333: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 15:36:22.333: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec 14 15:36:22.334: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 15:36:22.339: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 14 15:36:22.339: INFO: e2e test version: v1.26.0
Dec 14 15:36:22.340: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec 14 15:36:22.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 15:36:22.346: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.072 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec 14 15:36:22.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 15:36:22.281: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Dec 14 15:36:22.305: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Dec 14 15:36:22.333: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec 14 15:36:22.333: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Dec 14 15:36:22.334: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec 14 15:36:22.339: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Dec 14 15:36:22.339: INFO: e2e test version: v1.26.0
    Dec 14 15:36:22.340: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec 14 15:36:22.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 15:36:22.346: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:36:22.374
Dec 14 15:36:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 15:36:22.375
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:36:22.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:36:22.419
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2296 12/14/22 15:36:22.423
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 12/14/22 15:36:22.429
Dec 14 15:36:22.444: INFO: Found 0 stateful pods, waiting for 3
Dec 14 15:36:32.454: INFO: Found 1 stateful pods, waiting for 3
Dec 14 15:36:42.453: INFO: Found 1 stateful pods, waiting for 3
Dec 14 15:36:52.457: INFO: Found 2 stateful pods, waiting for 3
Dec 14 15:37:02.451: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:02.451: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:02.451: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 14 15:37:12.456: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:12.457: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:12.457: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/14/22 15:37:12.478
Dec 14 15:37:12.510: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/14/22 15:37:12.51
STEP: Not applying an update when the partition is greater than the number of replicas 12/14/22 15:37:22.541
STEP: Performing a canary update 12/14/22 15:37:22.541
Dec 14 15:37:22.583: INFO: Updating stateful set ss2
Dec 14 15:37:22.596: INFO: Waiting for Pod statefulset-2296/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 12/14/22 15:37:32.615
Dec 14 15:37:32.677: INFO: Found 1 stateful pods, waiting for 3
Dec 14 15:37:42.690: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:42.690: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 15:37:42.690: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 12/14/22 15:37:42.71
Dec 14 15:37:42.736: INFO: Updating stateful set ss2
Dec 14 15:37:42.754: INFO: Waiting for Pod statefulset-2296/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Dec 14 15:37:52.808: INFO: Updating stateful set ss2
Dec 14 15:37:52.818: INFO: Waiting for StatefulSet statefulset-2296/ss2 to complete update
Dec 14 15:37:52.818: INFO: Waiting for Pod statefulset-2296/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 15:38:02.842: INFO: Deleting all statefulset in ns statefulset-2296
Dec 14 15:38:02.848: INFO: Scaling statefulset ss2 to 0
Dec 14 15:38:12.878: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 15:38:12.885: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 15:38:12.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2296" for this suite. 12/14/22 15:38:12.909
------------------------------
• [SLOW TEST] [110.545 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:36:22.374
    Dec 14 15:36:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 15:36:22.375
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:36:22.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:36:22.419
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2296 12/14/22 15:36:22.423
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 12/14/22 15:36:22.429
    Dec 14 15:36:22.444: INFO: Found 0 stateful pods, waiting for 3
    Dec 14 15:36:32.454: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 15:36:42.453: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 15:36:52.457: INFO: Found 2 stateful pods, waiting for 3
    Dec 14 15:37:02.451: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:02.451: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:02.451: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Dec 14 15:37:12.456: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:12.457: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:12.457: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/14/22 15:37:12.478
    Dec 14 15:37:12.510: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/14/22 15:37:12.51
    STEP: Not applying an update when the partition is greater than the number of replicas 12/14/22 15:37:22.541
    STEP: Performing a canary update 12/14/22 15:37:22.541
    Dec 14 15:37:22.583: INFO: Updating stateful set ss2
    Dec 14 15:37:22.596: INFO: Waiting for Pod statefulset-2296/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 12/14/22 15:37:32.615
    Dec 14 15:37:32.677: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 15:37:42.690: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:42.690: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 15:37:42.690: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 12/14/22 15:37:42.71
    Dec 14 15:37:42.736: INFO: Updating stateful set ss2
    Dec 14 15:37:42.754: INFO: Waiting for Pod statefulset-2296/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Dec 14 15:37:52.808: INFO: Updating stateful set ss2
    Dec 14 15:37:52.818: INFO: Waiting for StatefulSet statefulset-2296/ss2 to complete update
    Dec 14 15:37:52.818: INFO: Waiting for Pod statefulset-2296/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 15:38:02.842: INFO: Deleting all statefulset in ns statefulset-2296
    Dec 14 15:38:02.848: INFO: Scaling statefulset ss2 to 0
    Dec 14 15:38:12.878: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 15:38:12.885: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:38:12.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2296" for this suite. 12/14/22 15:38:12.909
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:38:12.923
Dec 14 15:38:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 15:38:12.928
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:38:12.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:38:12.963
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c in namespace container-probe-1303 12/14/22 15:38:12.967
Dec 14 15:38:12.982: INFO: Waiting up to 5m0s for pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c" in namespace "container-probe-1303" to be "not pending"
Dec 14 15:38:12.986: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20235ms
Dec 14 15:38:15.001: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019320728s
Dec 14 15:38:16.996: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013983337s
Dec 14 15:38:18.995: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01268359s
Dec 14 15:38:20.998: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Running", Reason="", readiness=true. Elapsed: 8.016272966s
Dec 14 15:38:20.998: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c" satisfied condition "not pending"
Dec 14 15:38:20.998: INFO: Started pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c in namespace container-probe-1303
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:38:20.999
Dec 14 15:38:21.005: INFO: Initial restart count of pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c is 0
Dec 14 15:38:45.125: INFO: Restart count of pod container-probe-1303/liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c is now 1 (24.119180064s elapsed)
STEP: deleting the pod 12/14/22 15:38:45.125
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 15:38:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1303" for this suite. 12/14/22 15:38:45.157
------------------------------
• [SLOW TEST] [32.254 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:38:12.923
    Dec 14 15:38:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 15:38:12.928
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:38:12.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:38:12.963
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c in namespace container-probe-1303 12/14/22 15:38:12.967
    Dec 14 15:38:12.982: INFO: Waiting up to 5m0s for pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c" in namespace "container-probe-1303" to be "not pending"
    Dec 14 15:38:12.986: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20235ms
    Dec 14 15:38:15.001: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019320728s
    Dec 14 15:38:16.996: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013983337s
    Dec 14 15:38:18.995: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01268359s
    Dec 14 15:38:20.998: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c": Phase="Running", Reason="", readiness=true. Elapsed: 8.016272966s
    Dec 14 15:38:20.998: INFO: Pod "liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c" satisfied condition "not pending"
    Dec 14 15:38:20.998: INFO: Started pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c in namespace container-probe-1303
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:38:20.999
    Dec 14 15:38:21.005: INFO: Initial restart count of pod liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c is 0
    Dec 14 15:38:45.125: INFO: Restart count of pod container-probe-1303/liveness-ab08dea1-f297-4149-a9af-bb0acff4bd1c is now 1 (24.119180064s elapsed)
    STEP: deleting the pod 12/14/22 15:38:45.125
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:38:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1303" for this suite. 12/14/22 15:38:45.157
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:38:45.182
Dec 14 15:38:45.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename cronjob 12/14/22 15:38:45.185
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:38:45.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:38:45.22
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 12/14/22 15:38:45.224
STEP: Ensuring a job is scheduled 12/14/22 15:38:45.236
STEP: Ensuring exactly one is scheduled 12/14/22 15:39:01.247
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 15:39:01.254
STEP: Ensuring no more jobs are scheduled 12/14/22 15:39:01.259
STEP: Removing cronjob 12/14/22 15:44:01.273
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 14 15:44:01.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7539" for this suite. 12/14/22 15:44:01.294
------------------------------
• [SLOW TEST] [316.121 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:38:45.182
    Dec 14 15:38:45.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename cronjob 12/14/22 15:38:45.185
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:38:45.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:38:45.22
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 12/14/22 15:38:45.224
    STEP: Ensuring a job is scheduled 12/14/22 15:38:45.236
    STEP: Ensuring exactly one is scheduled 12/14/22 15:39:01.247
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 15:39:01.254
    STEP: Ensuring no more jobs are scheduled 12/14/22 15:39:01.259
    STEP: Removing cronjob 12/14/22 15:44:01.273
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:44:01.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7539" for this suite. 12/14/22 15:44:01.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:44:01.306
Dec 14 15:44:01.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename containers 12/14/22 15:44:01.311
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:01.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:01.349
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 12/14/22 15:44:01.354
Dec 14 15:44:01.375: INFO: Waiting up to 5m0s for pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a" in namespace "containers-8804" to be "Succeeded or Failed"
Dec 14 15:44:01.385: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.892441ms
Dec 14 15:44:03.399: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022966097s
Dec 14 15:44:05.394: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018113557s
STEP: Saw pod success 12/14/22 15:44:05.394
Dec 14 15:44:05.395: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a" satisfied condition "Succeeded or Failed"
Dec 14 15:44:05.400: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a container agnhost-container: <nil>
STEP: delete the pod 12/14/22 15:44:05.434
Dec 14 15:44:05.456: INFO: Waiting for pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a to disappear
Dec 14 15:44:05.460: INFO: Pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 14 15:44:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8804" for this suite. 12/14/22 15:44:05.466
------------------------------
• [4.168 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:44:01.306
    Dec 14 15:44:01.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename containers 12/14/22 15:44:01.311
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:01.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:01.349
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 12/14/22 15:44:01.354
    Dec 14 15:44:01.375: INFO: Waiting up to 5m0s for pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a" in namespace "containers-8804" to be "Succeeded or Failed"
    Dec 14 15:44:01.385: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.892441ms
    Dec 14 15:44:03.399: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022966097s
    Dec 14 15:44:05.394: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018113557s
    STEP: Saw pod success 12/14/22 15:44:05.394
    Dec 14 15:44:05.395: INFO: Pod "client-containers-30f224d1-6d1b-48dc-a083-30378f52987a" satisfied condition "Succeeded or Failed"
    Dec 14 15:44:05.400: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 15:44:05.434
    Dec 14 15:44:05.456: INFO: Waiting for pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a to disappear
    Dec 14 15:44:05.460: INFO: Pod client-containers-30f224d1-6d1b-48dc-a083-30378f52987a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:44:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8804" for this suite. 12/14/22 15:44:05.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:44:05.476
Dec 14 15:44:05.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 15:44:05.478
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:05.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:05.512
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-c4a861e9-53ae-45f9-86d0-6bc3764de513 12/14/22 15:44:05.523
STEP: Creating configMap with name cm-test-opt-upd-3c968a43-b66b-4a3a-93f6-c85e80021219 12/14/22 15:44:05.533
STEP: Creating the pod 12/14/22 15:44:05.538
Dec 14 15:44:05.551: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f" in namespace "projected-4551" to be "running and ready"
Dec 14 15:44:05.557: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362304ms
Dec 14 15:44:05.557: INFO: The phase of Pod pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:44:07.564: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012378799s
Dec 14 15:44:07.564: INFO: The phase of Pod pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f is Running (Ready = true)
Dec 14 15:44:07.564: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c4a861e9-53ae-45f9-86d0-6bc3764de513 12/14/22 15:44:07.604
STEP: Updating configmap cm-test-opt-upd-3c968a43-b66b-4a3a-93f6-c85e80021219 12/14/22 15:44:07.616
STEP: Creating configMap with name cm-test-opt-create-cb61e98d-6b1f-446a-a92f-768d0763db37 12/14/22 15:44:07.625
STEP: waiting to observe update in volume 12/14/22 15:44:07.631
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 15:44:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4551" for this suite. 12/14/22 15:44:11.696
------------------------------
• [SLOW TEST] [6.229 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:44:05.476
    Dec 14 15:44:05.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 15:44:05.478
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:05.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:05.512
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-c4a861e9-53ae-45f9-86d0-6bc3764de513 12/14/22 15:44:05.523
    STEP: Creating configMap with name cm-test-opt-upd-3c968a43-b66b-4a3a-93f6-c85e80021219 12/14/22 15:44:05.533
    STEP: Creating the pod 12/14/22 15:44:05.538
    Dec 14 15:44:05.551: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f" in namespace "projected-4551" to be "running and ready"
    Dec 14 15:44:05.557: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362304ms
    Dec 14 15:44:05.557: INFO: The phase of Pod pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 15:44:07.564: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012378799s
    Dec 14 15:44:07.564: INFO: The phase of Pod pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f is Running (Ready = true)
    Dec 14 15:44:07.564: INFO: Pod "pod-projected-configmaps-441ed77c-6886-44ec-91a8-38e6e61da31f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c4a861e9-53ae-45f9-86d0-6bc3764de513 12/14/22 15:44:07.604
    STEP: Updating configmap cm-test-opt-upd-3c968a43-b66b-4a3a-93f6-c85e80021219 12/14/22 15:44:07.616
    STEP: Creating configMap with name cm-test-opt-create-cb61e98d-6b1f-446a-a92f-768d0763db37 12/14/22 15:44:07.625
    STEP: waiting to observe update in volume 12/14/22 15:44:07.631
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:44:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4551" for this suite. 12/14/22 15:44:11.696
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:44:11.709
Dec 14 15:44:11.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context-test 12/14/22 15:44:11.711
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:11.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:11.744
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Dec 14 15:44:11.759: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf" in namespace "security-context-test-2875" to be "Succeeded or Failed"
Dec 14 15:44:11.763: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088757ms
Dec 14 15:44:13.772: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01312134s
Dec 14 15:44:15.770: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010980131s
Dec 14 15:44:15.770: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 15:44:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2875" for this suite. 12/14/22 15:44:15.781
------------------------------
• [4.087 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:44:11.709
    Dec 14 15:44:11.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context-test 12/14/22 15:44:11.711
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:11.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:11.744
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Dec 14 15:44:11.759: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf" in namespace "security-context-test-2875" to be "Succeeded or Failed"
    Dec 14 15:44:11.763: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088757ms
    Dec 14 15:44:13.772: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01312134s
    Dec 14 15:44:15.770: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010980131s
    Dec 14 15:44:15.770: INFO: Pod "busybox-user-65534-dc91ba43-cef8-471d-adbd-c4c0431156cf" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:44:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2875" for this suite. 12/14/22 15:44:15.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:44:15.806
Dec 14 15:44:15.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 15:44:15.808
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:15.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:15.842
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 in namespace container-probe-2108 12/14/22 15:44:15.848
Dec 14 15:44:15.862: INFO: Waiting up to 5m0s for pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56" in namespace "container-probe-2108" to be "not pending"
Dec 14 15:44:15.867: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.580958ms
Dec 14 15:44:17.876: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56": Phase="Running", Reason="", readiness=true. Elapsed: 2.013691835s
Dec 14 15:44:17.876: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56" satisfied condition "not pending"
Dec 14 15:44:17.877: INFO: Started pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 in namespace container-probe-2108
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:44:17.877
Dec 14 15:44:17.883: INFO: Initial restart count of pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 is 0
Dec 14 15:45:08.135: INFO: Restart count of pod container-probe-2108/busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 is now 1 (50.252566457s elapsed)
STEP: deleting the pod 12/14/22 15:45:08.136
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:08.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2108" for this suite. 12/14/22 15:45:08.163
------------------------------
• [SLOW TEST] [52.369 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:44:15.806
    Dec 14 15:44:15.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 15:44:15.808
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:44:15.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:44:15.842
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 in namespace container-probe-2108 12/14/22 15:44:15.848
    Dec 14 15:44:15.862: INFO: Waiting up to 5m0s for pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56" in namespace "container-probe-2108" to be "not pending"
    Dec 14 15:44:15.867: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.580958ms
    Dec 14 15:44:17.876: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56": Phase="Running", Reason="", readiness=true. Elapsed: 2.013691835s
    Dec 14 15:44:17.876: INFO: Pod "busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56" satisfied condition "not pending"
    Dec 14 15:44:17.877: INFO: Started pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 in namespace container-probe-2108
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:44:17.877
    Dec 14 15:44:17.883: INFO: Initial restart count of pod busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 is 0
    Dec 14 15:45:08.135: INFO: Restart count of pod container-probe-2108/busybox-ea22fd95-e1d0-4337-a6bf-16d39a499b56 is now 1 (50.252566457s elapsed)
    STEP: deleting the pod 12/14/22 15:45:08.136
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:08.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2108" for this suite. 12/14/22 15:45:08.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:08.18
Dec 14 15:45:08.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename lease-test 12/14/22 15:45:08.183
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:08.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:08.216
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:08.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-8708" for this suite. 12/14/22 15:45:08.3
------------------------------
• [0.130 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:08.18
    Dec 14 15:45:08.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename lease-test 12/14/22 15:45:08.183
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:08.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:08.216
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:08.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-8708" for this suite. 12/14/22 15:45:08.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:08.311
Dec 14 15:45:08.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 15:45:08.314
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:08.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:08.343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 12/14/22 15:45:08.36
STEP: watching for Pod to be ready 12/14/22 15:45:08.374
Dec 14 15:45:08.376: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 14 15:45:08.389: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
Dec 14 15:45:08.413: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
Dec 14 15:45:09.489: INFO: Found Pod pod-test in namespace pods-4639 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 12/14/22 15:45:09.497
STEP: getting the Pod and ensuring that it's patched 12/14/22 15:45:09.517
STEP: replacing the Pod's status Ready condition to False 12/14/22 15:45:09.521
STEP: check the Pod again to ensure its Ready conditions are False 12/14/22 15:45:09.54
STEP: deleting the Pod via a Collection with a LabelSelector 12/14/22 15:45:09.541
STEP: watching for the Pod to be deleted 12/14/22 15:45:09.555
Dec 14 15:45:09.558: INFO: observed event type MODIFIED
Dec 14 15:45:11.517: INFO: observed event type MODIFIED
Dec 14 15:45:12.505: INFO: observed event type MODIFIED
Dec 14 15:45:12.521: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4639" for this suite. 12/14/22 15:45:12.541
------------------------------
• [4.245 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:08.311
    Dec 14 15:45:08.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 15:45:08.314
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:08.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:08.343
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 12/14/22 15:45:08.36
    STEP: watching for Pod to be ready 12/14/22 15:45:08.374
    Dec 14 15:45:08.376: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Dec 14 15:45:08.389: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
    Dec 14 15:45:08.413: INFO: observed Pod pod-test in namespace pods-4639 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
    Dec 14 15:45:09.489: INFO: Found Pod pod-test in namespace pods-4639 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:45:08 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 12/14/22 15:45:09.497
    STEP: getting the Pod and ensuring that it's patched 12/14/22 15:45:09.517
    STEP: replacing the Pod's status Ready condition to False 12/14/22 15:45:09.521
    STEP: check the Pod again to ensure its Ready conditions are False 12/14/22 15:45:09.54
    STEP: deleting the Pod via a Collection with a LabelSelector 12/14/22 15:45:09.541
    STEP: watching for the Pod to be deleted 12/14/22 15:45:09.555
    Dec 14 15:45:09.558: INFO: observed event type MODIFIED
    Dec 14 15:45:11.517: INFO: observed event type MODIFIED
    Dec 14 15:45:12.505: INFO: observed event type MODIFIED
    Dec 14 15:45:12.521: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4639" for this suite. 12/14/22 15:45:12.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:12.561
Dec 14 15:45:12.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 15:45:12.564
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:12.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:12.59
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 15:45:12.595
Dec 14 15:45:12.614: INFO: Waiting up to 5m0s for pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376" in namespace "emptydir-657" to be "Succeeded or Failed"
Dec 14 15:45:12.619: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881341ms
Dec 14 15:45:14.628: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01443816s
Dec 14 15:45:16.627: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013674284s
STEP: Saw pod success 12/14/22 15:45:16.628
Dec 14 15:45:16.628: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376" satisfied condition "Succeeded or Failed"
Dec 14 15:45:16.635: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 container test-container: <nil>
STEP: delete the pod 12/14/22 15:45:16.645
Dec 14 15:45:16.659: INFO: Waiting for pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 to disappear
Dec 14 15:45:16.666: INFO: Pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:16.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-657" for this suite. 12/14/22 15:45:16.671
------------------------------
• [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:12.561
    Dec 14 15:45:12.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 15:45:12.564
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:12.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:12.59
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 15:45:12.595
    Dec 14 15:45:12.614: INFO: Waiting up to 5m0s for pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376" in namespace "emptydir-657" to be "Succeeded or Failed"
    Dec 14 15:45:12.619: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881341ms
    Dec 14 15:45:14.628: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01443816s
    Dec 14 15:45:16.627: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013674284s
    STEP: Saw pod success 12/14/22 15:45:16.628
    Dec 14 15:45:16.628: INFO: Pod "pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376" satisfied condition "Succeeded or Failed"
    Dec 14 15:45:16.635: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 container test-container: <nil>
    STEP: delete the pod 12/14/22 15:45:16.645
    Dec 14 15:45:16.659: INFO: Waiting for pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 to disappear
    Dec 14 15:45:16.666: INFO: Pod pod-e029c60f-7b5d-4281-9d0c-3672b3cd4376 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:16.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-657" for this suite. 12/14/22 15:45:16.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:16.685
Dec 14 15:45:16.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 15:45:16.687
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:16.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:16.728
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1567 12/14/22 15:45:16.737
STEP: creating service affinity-nodeport in namespace services-1567 12/14/22 15:45:16.738
STEP: creating replication controller affinity-nodeport in namespace services-1567 12/14/22 15:45:16.764
I1214 15:45:16.771664      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1567, replica count: 3
I1214 15:45:19.830851      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 15:45:22.831160      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 15:45:25.831991      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:45:25.850: INFO: Creating new exec pod
Dec 14 15:45:25.866: INFO: Waiting up to 5m0s for pod "execpod-affinityc8g7x" in namespace "services-1567" to be "running"
Dec 14 15:45:25.872: INFO: Pod "execpod-affinityc8g7x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.602925ms
Dec 14 15:45:27.882: INFO: Pod "execpod-affinityc8g7x": Phase="Running", Reason="", readiness=true. Elapsed: 2.01598457s
Dec 14 15:45:27.882: INFO: Pod "execpod-affinityc8g7x" satisfied condition "running"
Dec 14 15:45:28.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Dec 14 15:45:29.316: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 14 15:45:29.316: INFO: stdout: ""
Dec 14 15:45:29.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 10.233.57.252 80'
Dec 14 15:45:29.537: INFO: stderr: "+ nc -v -z -w 2 10.233.57.252 80\nConnection to 10.233.57.252 80 port [tcp/http] succeeded!\n"
Dec 14 15:45:29.537: INFO: stdout: ""
Dec 14 15:45:29.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31246'
Dec 14 15:45:29.739: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31246\nConnection to 192.168.121.56 31246 port [tcp/*] succeeded!\n"
Dec 14 15:45:29.739: INFO: stdout: ""
Dec 14 15:45:29.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31246'
Dec 14 15:45:29.966: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31246\nConnection to 192.168.121.16 31246 port [tcp/*] succeeded!\n"
Dec 14 15:45:29.966: INFO: stdout: ""
Dec 14 15:45:29.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31246/ ; done'
Dec 14 15:45:30.340: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n"
Dec 14 15:45:30.341: INFO: stdout: "\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f"
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
Dec 14 15:45:30.341: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1567, will wait for the garbage collector to delete the pods 12/14/22 15:45:30.372
Dec 14 15:45:30.456: INFO: Deleting ReplicationController affinity-nodeport took: 11.945208ms
Dec 14 15:45:30.556: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.463768ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:32.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1567" for this suite. 12/14/22 15:45:32.798
------------------------------
• [SLOW TEST] [16.123 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:16.685
    Dec 14 15:45:16.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 15:45:16.687
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:16.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:16.728
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1567 12/14/22 15:45:16.737
    STEP: creating service affinity-nodeport in namespace services-1567 12/14/22 15:45:16.738
    STEP: creating replication controller affinity-nodeport in namespace services-1567 12/14/22 15:45:16.764
    I1214 15:45:16.771664      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1567, replica count: 3
    I1214 15:45:19.830851      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 15:45:22.831160      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 15:45:25.831991      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 15:45:25.850: INFO: Creating new exec pod
    Dec 14 15:45:25.866: INFO: Waiting up to 5m0s for pod "execpod-affinityc8g7x" in namespace "services-1567" to be "running"
    Dec 14 15:45:25.872: INFO: Pod "execpod-affinityc8g7x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.602925ms
    Dec 14 15:45:27.882: INFO: Pod "execpod-affinityc8g7x": Phase="Running", Reason="", readiness=true. Elapsed: 2.01598457s
    Dec 14 15:45:27.882: INFO: Pod "execpod-affinityc8g7x" satisfied condition "running"
    Dec 14 15:45:28.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Dec 14 15:45:29.316: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Dec 14 15:45:29.316: INFO: stdout: ""
    Dec 14 15:45:29.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 10.233.57.252 80'
    Dec 14 15:45:29.537: INFO: stderr: "+ nc -v -z -w 2 10.233.57.252 80\nConnection to 10.233.57.252 80 port [tcp/http] succeeded!\n"
    Dec 14 15:45:29.537: INFO: stdout: ""
    Dec 14 15:45:29.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31246'
    Dec 14 15:45:29.739: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31246\nConnection to 192.168.121.56 31246 port [tcp/*] succeeded!\n"
    Dec 14 15:45:29.739: INFO: stdout: ""
    Dec 14 15:45:29.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31246'
    Dec 14 15:45:29.966: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31246\nConnection to 192.168.121.16 31246 port [tcp/*] succeeded!\n"
    Dec 14 15:45:29.966: INFO: stdout: ""
    Dec 14 15:45:29.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-1567 exec execpod-affinityc8g7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31246/ ; done'
    Dec 14 15:45:30.340: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31246/\n"
    Dec 14 15:45:30.341: INFO: stdout: "\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f\naffinity-nodeport-hbz5f"
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Received response from host: affinity-nodeport-hbz5f
    Dec 14 15:45:30.341: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1567, will wait for the garbage collector to delete the pods 12/14/22 15:45:30.372
    Dec 14 15:45:30.456: INFO: Deleting ReplicationController affinity-nodeport took: 11.945208ms
    Dec 14 15:45:30.556: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.463768ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:32.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1567" for this suite. 12/14/22 15:45:32.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:32.814
Dec 14 15:45:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 15:45:32.817
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:32.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:32.851
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 12/14/22 15:45:32.856
STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 15:45:32.867
STEP: delete the deployment 12/14/22 15:45:33.387
STEP: wait for all rs to be garbage collected 12/14/22 15:45:33.398
STEP: expected 0 rs, got 1 rs 12/14/22 15:45:33.407
STEP: expected 0 pods, got 2 pods 12/14/22 15:45:33.412
STEP: Gathering metrics 12/14/22 15:45:33.925
Dec 14 15:45:33.971: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 15:45:33.976: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.311643ms
Dec 14 15:45:33.976: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 15:45:33.977: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 15:45:34.131: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 15:45:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7561" for this suite. 12/14/22 15:45:34.143
------------------------------
• [1.340 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:32.814
    Dec 14 15:45:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 15:45:32.817
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:32.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:32.851
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 12/14/22 15:45:32.856
    STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 15:45:32.867
    STEP: delete the deployment 12/14/22 15:45:33.387
    STEP: wait for all rs to be garbage collected 12/14/22 15:45:33.398
    STEP: expected 0 rs, got 1 rs 12/14/22 15:45:33.407
    STEP: expected 0 pods, got 2 pods 12/14/22 15:45:33.412
    STEP: Gathering metrics 12/14/22 15:45:33.925
    Dec 14 15:45:33.971: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 15:45:33.976: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.311643ms
    Dec 14 15:45:33.976: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 15:45:33.977: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 15:45:34.131: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:45:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7561" for this suite. 12/14/22 15:45:34.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:45:34.16
Dec 14 15:45:34.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 15:45:34.162
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:34.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:34.196
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-r4cvt" 12/14/22 15:45:34.209
Dec 14 15:45:34.223: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard cpu limit of 500m
Dec 14 15:45:34.223: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.223
STEP: Confirm /status for "e2e-rq-status-r4cvt" resourceQuota via watch 12/14/22 15:45:34.238
Dec 14 15:45:34.240: INFO: observed resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList(nil)
Dec 14 15:45:34.240: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec 14 15:45:34.240: INFO: ResourceQuota "e2e-rq-status-r4cvt" /status was updated
STEP: Patching hard spec values for cpu & memory 12/14/22 15:45:34.244
Dec 14 15:45:34.253: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard cpu limit of 1
Dec 14 15:45:34.253: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.253
STEP: Confirm /status for "e2e-rq-status-r4cvt" resourceQuota via watch 12/14/22 15:45:34.266
Dec 14 15:45:34.270: INFO: observed resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec 14 15:45:34.270: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Dec 14 15:45:34.270: INFO: ResourceQuota "e2e-rq-status-r4cvt" /status was patched
STEP: Get "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.27
Dec 14 15:45:34.276: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard cpu of 1
Dec 14 15:45:34.276: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-r4cvt" /status before checking Spec is unchanged 12/14/22 15:45:34.284
Dec 14 15:45:34.291: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard cpu of 2
Dec 14 15:45:34.292: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard memory of 2Gi
Dec 14 15:45:34.294: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Dec 14 15:49:29.309: INFO: ResourceQuota "e2e-rq-status-r4cvt" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:29.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1809" for this suite. 12/14/22 15:49:29.32
------------------------------
• [SLOW TEST] [235.173 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:45:34.16
    Dec 14 15:45:34.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 15:45:34.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:45:34.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:45:34.196
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-r4cvt" 12/14/22 15:45:34.209
    Dec 14 15:45:34.223: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard cpu limit of 500m
    Dec 14 15:45:34.223: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.223
    STEP: Confirm /status for "e2e-rq-status-r4cvt" resourceQuota via watch 12/14/22 15:45:34.238
    Dec 14 15:45:34.240: INFO: observed resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList(nil)
    Dec 14 15:45:34.240: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec 14 15:45:34.240: INFO: ResourceQuota "e2e-rq-status-r4cvt" /status was updated
    STEP: Patching hard spec values for cpu & memory 12/14/22 15:45:34.244
    Dec 14 15:45:34.253: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard cpu limit of 1
    Dec 14 15:45:34.253: INFO: Resource quota "e2e-rq-status-r4cvt" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.253
    STEP: Confirm /status for "e2e-rq-status-r4cvt" resourceQuota via watch 12/14/22 15:45:34.266
    Dec 14 15:45:34.270: INFO: observed resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec 14 15:45:34.270: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Dec 14 15:45:34.270: INFO: ResourceQuota "e2e-rq-status-r4cvt" /status was patched
    STEP: Get "e2e-rq-status-r4cvt" /status 12/14/22 15:45:34.27
    Dec 14 15:45:34.276: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard cpu of 1
    Dec 14 15:45:34.276: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-r4cvt" /status before checking Spec is unchanged 12/14/22 15:45:34.284
    Dec 14 15:45:34.291: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard cpu of 2
    Dec 14 15:45:34.292: INFO: Resourcequota "e2e-rq-status-r4cvt" reports status: hard memory of 2Gi
    Dec 14 15:45:34.294: INFO: Found resourceQuota "e2e-rq-status-r4cvt" in namespace "resourcequota-1809" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Dec 14 15:49:29.309: INFO: ResourceQuota "e2e-rq-status-r4cvt" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:29.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1809" for this suite. 12/14/22 15:49:29.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:29.35
Dec 14 15:49:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 15:49:29.353
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:29.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:29.39
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5405 12/14/22 15:49:29.396
STEP: changing the ExternalName service to type=NodePort 12/14/22 15:49:29.406
STEP: creating replication controller externalname-service in namespace services-5405 12/14/22 15:49:29.442
I1214 15:49:29.452766      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5405, replica count: 2
I1214 15:49:32.504106      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:49:32.504: INFO: Creating new exec pod
Dec 14 15:49:32.521: INFO: Waiting up to 5m0s for pod "execpodrftvm" in namespace "services-5405" to be "running"
Dec 14 15:49:32.527: INFO: Pod "execpodrftvm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.291884ms
Dec 14 15:49:34.535: INFO: Pod "execpodrftvm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01325804s
Dec 14 15:49:34.535: INFO: Pod "execpodrftvm" satisfied condition "running"
Dec 14 15:49:35.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec 14 15:49:35.854: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 15:49:35.854: INFO: stdout: ""
Dec 14 15:49:35.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 10.233.60.215 80'
Dec 14 15:49:36.088: INFO: stderr: "+ nc -v -z -w 2 10.233.60.215 80\nConnection to 10.233.60.215 80 port [tcp/http] succeeded!\n"
Dec 14 15:49:36.088: INFO: stdout: ""
Dec 14 15:49:36.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 192.168.121.21 31132'
Dec 14 15:49:36.337: INFO: stderr: "+ nc -v -z -w 2 192.168.121.21 31132\nConnection to 192.168.121.21 31132 port [tcp/*] succeeded!\n"
Dec 14 15:49:36.337: INFO: stdout: ""
Dec 14 15:49:36.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31132'
Dec 14 15:49:36.554: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31132\nConnection to 192.168.121.56 31132 port [tcp/*] succeeded!\n"
Dec 14 15:49:36.554: INFO: stdout: ""
Dec 14 15:49:36.554: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:36.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5405" for this suite. 12/14/22 15:49:36.607
------------------------------
• [SLOW TEST] [7.273 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:29.35
    Dec 14 15:49:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 15:49:29.353
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:29.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:29.39
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5405 12/14/22 15:49:29.396
    STEP: changing the ExternalName service to type=NodePort 12/14/22 15:49:29.406
    STEP: creating replication controller externalname-service in namespace services-5405 12/14/22 15:49:29.442
    I1214 15:49:29.452766      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5405, replica count: 2
    I1214 15:49:32.504106      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 15:49:32.504: INFO: Creating new exec pod
    Dec 14 15:49:32.521: INFO: Waiting up to 5m0s for pod "execpodrftvm" in namespace "services-5405" to be "running"
    Dec 14 15:49:32.527: INFO: Pod "execpodrftvm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.291884ms
    Dec 14 15:49:34.535: INFO: Pod "execpodrftvm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01325804s
    Dec 14 15:49:34.535: INFO: Pod "execpodrftvm" satisfied condition "running"
    Dec 14 15:49:35.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec 14 15:49:35.854: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 14 15:49:35.854: INFO: stdout: ""
    Dec 14 15:49:35.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 10.233.60.215 80'
    Dec 14 15:49:36.088: INFO: stderr: "+ nc -v -z -w 2 10.233.60.215 80\nConnection to 10.233.60.215 80 port [tcp/http] succeeded!\n"
    Dec 14 15:49:36.088: INFO: stdout: ""
    Dec 14 15:49:36.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 192.168.121.21 31132'
    Dec 14 15:49:36.337: INFO: stderr: "+ nc -v -z -w 2 192.168.121.21 31132\nConnection to 192.168.121.21 31132 port [tcp/*] succeeded!\n"
    Dec 14 15:49:36.337: INFO: stdout: ""
    Dec 14 15:49:36.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5405 exec execpodrftvm -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31132'
    Dec 14 15:49:36.554: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31132\nConnection to 192.168.121.56 31132 port [tcp/*] succeeded!\n"
    Dec 14 15:49:36.554: INFO: stdout: ""
    Dec 14 15:49:36.554: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:36.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5405" for this suite. 12/14/22 15:49:36.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:36.629
Dec 14 15:49:36.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 15:49:36.631
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:36.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:36.745
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 12/14/22 15:49:36.754
Dec 14 15:49:36.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: rename a version 12/14/22 15:49:41.448
STEP: check the new version name is served 12/14/22 15:49:41.477
STEP: check the old version name is removed 12/14/22 15:49:43.705
STEP: check the other version is not changed 12/14/22 15:49:44.594
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7252" for this suite. 12/14/22 15:49:48.512
------------------------------
• [SLOW TEST] [11.897 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:36.629
    Dec 14 15:49:36.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 15:49:36.631
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:36.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:36.745
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 12/14/22 15:49:36.754
    Dec 14 15:49:36.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: rename a version 12/14/22 15:49:41.448
    STEP: check the new version name is served 12/14/22 15:49:41.477
    STEP: check the old version name is removed 12/14/22 15:49:43.705
    STEP: check the other version is not changed 12/14/22 15:49:44.594
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7252" for this suite. 12/14/22 15:49:48.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:48.532
Dec 14 15:49:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 15:49:48.535
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:48.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:48.57
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 12/14/22 15:49:48.584
Dec 14 15:49:48.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c" in namespace "projected-131" to be "Succeeded or Failed"
Dec 14 15:49:48.605: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090279ms
Dec 14 15:49:50.613: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013865325s
Dec 14 15:49:52.615: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015452648s
STEP: Saw pod success 12/14/22 15:49:52.615
Dec 14 15:49:52.615: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c" satisfied condition "Succeeded or Failed"
Dec 14 15:49:52.620: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c container client-container: <nil>
STEP: delete the pod 12/14/22 15:49:52.648
Dec 14 15:49:52.663: INFO: Waiting for pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c to disappear
Dec 14 15:49:52.667: INFO: Pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:52.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-131" for this suite. 12/14/22 15:49:52.673
------------------------------
• [4.150 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:48.532
    Dec 14 15:49:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 15:49:48.535
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:48.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:48.57
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 12/14/22 15:49:48.584
    Dec 14 15:49:48.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c" in namespace "projected-131" to be "Succeeded or Failed"
    Dec 14 15:49:48.605: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090279ms
    Dec 14 15:49:50.613: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013865325s
    Dec 14 15:49:52.615: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015452648s
    STEP: Saw pod success 12/14/22 15:49:52.615
    Dec 14 15:49:52.615: INFO: Pod "downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c" satisfied condition "Succeeded or Failed"
    Dec 14 15:49:52.620: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c container client-container: <nil>
    STEP: delete the pod 12/14/22 15:49:52.648
    Dec 14 15:49:52.663: INFO: Waiting for pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c to disappear
    Dec 14 15:49:52.667: INFO: Pod downwardapi-volume-1a6d4777-0d66-4c61-93f1-884d48ee382c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:52.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-131" for this suite. 12/14/22 15:49:52.673
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:52.683
Dec 14 15:49:52.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename podtemplate 12/14/22 15:49:52.688
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.714
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 12/14/22 15:49:52.719
STEP: Replace a pod template 12/14/22 15:49:52.728
Dec 14 15:49:52.741: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:52.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9874" for this suite. 12/14/22 15:49:52.749
------------------------------
• [0.078 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:52.683
    Dec 14 15:49:52.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename podtemplate 12/14/22 15:49:52.688
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.714
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 12/14/22 15:49:52.719
    STEP: Replace a pod template 12/14/22 15:49:52.728
    Dec 14 15:49:52.741: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:52.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9874" for this suite. 12/14/22 15:49:52.749
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:52.761
Dec 14 15:49:52.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 15:49:52.764
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.798
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 12/14/22 15:49:52.803
STEP: submitting the pod to kubernetes 12/14/22 15:49:52.803
STEP: verifying QOS class is set on the pod 12/14/22 15:49:52.815
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:52.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-935" for this suite. 12/14/22 15:49:52.831
------------------------------
• [0.080 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:52.761
    Dec 14 15:49:52.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 15:49:52.764
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.798
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 12/14/22 15:49:52.803
    STEP: submitting the pod to kubernetes 12/14/22 15:49:52.803
    STEP: verifying QOS class is set on the pod 12/14/22 15:49:52.815
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:52.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-935" for this suite. 12/14/22 15:49:52.831
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:52.842
Dec 14 15:49:52.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 15:49:52.844
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.873
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Dec 14 15:49:52.893: INFO: Waiting up to 2m0s for pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" in namespace "var-expansion-669" to be "container 0 failed with reason CreateContainerConfigError"
Dec 14 15:49:52.897: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925914ms
Dec 14 15:49:54.908: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014639542s
Dec 14 15:49:54.908: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 14 15:49:54.908: INFO: Deleting pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" in namespace "var-expansion-669"
Dec 14 15:49:54.920: INFO: Wait up to 5m0s for pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:56.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-669" for this suite. 12/14/22 15:49:56.944
------------------------------
• [4.112 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:52.842
    Dec 14 15:49:52.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 15:49:52.844
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:52.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:52.873
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Dec 14 15:49:52.893: INFO: Waiting up to 2m0s for pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" in namespace "var-expansion-669" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 14 15:49:52.897: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925914ms
    Dec 14 15:49:54.908: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014639542s
    Dec 14 15:49:54.908: INFO: Pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 14 15:49:54.908: INFO: Deleting pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" in namespace "var-expansion-669"
    Dec 14 15:49:54.920: INFO: Wait up to 5m0s for pod "var-expansion-c9f403f1-3f90-4c4d-8e88-37712b4fff42" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:56.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-669" for this suite. 12/14/22 15:49:56.944
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:56.954
Dec 14 15:49:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename podtemplate 12/14/22 15:49:56.956
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:56.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:56.986
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 12/14/22 15:49:56.99
Dec 14 15:49:56.998: INFO: created test-podtemplate-1
Dec 14 15:49:57.008: INFO: created test-podtemplate-2
Dec 14 15:49:57.014: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 12/14/22 15:49:57.015
STEP: delete collection of pod templates 12/14/22 15:49:57.02
Dec 14 15:49:57.020: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 12/14/22 15:49:57.043
Dec 14 15:49:57.044: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 14 15:49:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9825" for this suite. 12/14/22 15:49:57.055
------------------------------
• [0.114 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:56.954
    Dec 14 15:49:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename podtemplate 12/14/22 15:49:56.956
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:56.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:56.986
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 12/14/22 15:49:56.99
    Dec 14 15:49:56.998: INFO: created test-podtemplate-1
    Dec 14 15:49:57.008: INFO: created test-podtemplate-2
    Dec 14 15:49:57.014: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 12/14/22 15:49:57.015
    STEP: delete collection of pod templates 12/14/22 15:49:57.02
    Dec 14 15:49:57.020: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 12/14/22 15:49:57.043
    Dec 14 15:49:57.044: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:49:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9825" for this suite. 12/14/22 15:49:57.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:49:57.076
Dec 14 15:49:57.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 15:49:57.077
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:57.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:57.108
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-b2ae9207-184a-4dfb-9751-043218dea170 12/14/22 15:49:57.112
STEP: Creating a pod to test consume secrets 12/14/22 15:49:57.118
Dec 14 15:49:57.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9" in namespace "projected-2512" to be "Succeeded or Failed"
Dec 14 15:49:57.132: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.92284ms
Dec 14 15:49:59.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Running", Reason="", readiness=false. Elapsed: 2.013486305s
Dec 14 15:50:01.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013175294s
STEP: Saw pod success 12/14/22 15:50:01.142
Dec 14 15:50:01.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9" satisfied condition "Succeeded or Failed"
Dec 14 15:50:01.149: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 15:50:01.164
Dec 14 15:50:01.189: INFO: Waiting for pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 to disappear
Dec 14 15:50:01.193: INFO: Pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:01.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2512" for this suite. 12/14/22 15:50:01.199
------------------------------
• [4.133 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:49:57.076
    Dec 14 15:49:57.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 15:49:57.077
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:49:57.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:49:57.108
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-b2ae9207-184a-4dfb-9751-043218dea170 12/14/22 15:49:57.112
    STEP: Creating a pod to test consume secrets 12/14/22 15:49:57.118
    Dec 14 15:49:57.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9" in namespace "projected-2512" to be "Succeeded or Failed"
    Dec 14 15:49:57.132: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.92284ms
    Dec 14 15:49:59.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Running", Reason="", readiness=false. Elapsed: 2.013486305s
    Dec 14 15:50:01.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013175294s
    STEP: Saw pod success 12/14/22 15:50:01.142
    Dec 14 15:50:01.142: INFO: Pod "pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9" satisfied condition "Succeeded or Failed"
    Dec 14 15:50:01.149: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 15:50:01.164
    Dec 14 15:50:01.189: INFO: Waiting for pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 to disappear
    Dec 14 15:50:01.193: INFO: Pod pod-projected-secrets-93a7a95f-664f-4f78-ae0f-3dae24532cb9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:01.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2512" for this suite. 12/14/22 15:50:01.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:01.214
Dec 14 15:50:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 15:50:01.217
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:01.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:01.254
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Dec 14 15:50:01.275: INFO: Waiting up to 2m0s for pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" in namespace "var-expansion-6893" to be "container 0 failed with reason CreateContainerConfigError"
Dec 14 15:50:01.303: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574": Phase="Pending", Reason="", readiness=false. Elapsed: 27.688625ms
Dec 14 15:50:03.314: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03858816s
Dec 14 15:50:03.314: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 14 15:50:03.314: INFO: Deleting pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" in namespace "var-expansion-6893"
Dec 14 15:50:03.328: INFO: Wait up to 5m0s for pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6893" for this suite. 12/14/22 15:50:05.359
------------------------------
• [4.154 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:01.214
    Dec 14 15:50:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 15:50:01.217
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:01.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:01.254
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Dec 14 15:50:01.275: INFO: Waiting up to 2m0s for pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" in namespace "var-expansion-6893" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 14 15:50:01.303: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574": Phase="Pending", Reason="", readiness=false. Elapsed: 27.688625ms
    Dec 14 15:50:03.314: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03858816s
    Dec 14 15:50:03.314: INFO: Pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 14 15:50:03.314: INFO: Deleting pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" in namespace "var-expansion-6893"
    Dec 14 15:50:03.328: INFO: Wait up to 5m0s for pod "var-expansion-506389bf-fd5c-463a-bb7f-6bda8f54d574" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6893" for this suite. 12/14/22 15:50:05.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:05.378
Dec 14 15:50:05.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 15:50:05.38
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:05.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:05.404
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 15:50:05.427
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:06.272
STEP: Deploying the webhook pod 12/14/22 15:50:06.284
STEP: Wait for the deployment to be ready 12/14/22 15:50:06.3
Dec 14 15:50:06.308: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 15:50:08.332
STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:08.349
Dec 14 15:50:09.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Dec 14 15:50:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/14/22 15:50:09.891
STEP: Creating a custom resource that should be denied by the webhook 12/14/22 15:50:09.921
STEP: Creating a custom resource whose deletion would be denied by the webhook 12/14/22 15:50:12.023
STEP: Updating the custom resource with disallowed data should be denied 12/14/22 15:50:12.037
STEP: Deleting the custom resource should be denied 12/14/22 15:50:12.055
STEP: Remove the offending key and value from the custom resource data 12/14/22 15:50:12.067
STEP: Deleting the updated custom resource should be successful 12/14/22 15:50:12.083
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6779" for this suite. 12/14/22 15:50:12.748
STEP: Destroying namespace "webhook-6779-markers" for this suite. 12/14/22 15:50:12.764
------------------------------
• [SLOW TEST] [7.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:05.378
    Dec 14 15:50:05.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 15:50:05.38
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:05.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:05.404
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 15:50:05.427
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:06.272
    STEP: Deploying the webhook pod 12/14/22 15:50:06.284
    STEP: Wait for the deployment to be ready 12/14/22 15:50:06.3
    Dec 14 15:50:06.308: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 15:50:08.332
    STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:08.349
    Dec 14 15:50:09.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Dec 14 15:50:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/14/22 15:50:09.891
    STEP: Creating a custom resource that should be denied by the webhook 12/14/22 15:50:09.921
    STEP: Creating a custom resource whose deletion would be denied by the webhook 12/14/22 15:50:12.023
    STEP: Updating the custom resource with disallowed data should be denied 12/14/22 15:50:12.037
    STEP: Deleting the custom resource should be denied 12/14/22 15:50:12.055
    STEP: Remove the offending key and value from the custom resource data 12/14/22 15:50:12.067
    STEP: Deleting the updated custom resource should be successful 12/14/22 15:50:12.083
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6779" for this suite. 12/14/22 15:50:12.748
    STEP: Destroying namespace "webhook-6779-markers" for this suite. 12/14/22 15:50:12.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:12.812
Dec 14 15:50:12.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 15:50:12.826
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:12.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:12.866
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 15:50:12.894
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:13.626
STEP: Deploying the webhook pod 12/14/22 15:50:13.634
STEP: Wait for the deployment to be ready 12/14/22 15:50:13.651
Dec 14 15:50:13.660: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 15:50:15.683
STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:15.702
Dec 14 15:50:16.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 12/14/22 15:50:16.713
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.739
STEP: Updating a validating webhook configuration's rules to not include the create operation 12/14/22 15:50:16.756
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.777
STEP: Patching a validating webhook configuration's rules to include the create operation 12/14/22 15:50:16.797
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.808
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:16.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9962" for this suite. 12/14/22 15:50:17.028
STEP: Destroying namespace "webhook-9962-markers" for this suite. 12/14/22 15:50:17.04
------------------------------
• [4.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:12.812
    Dec 14 15:50:12.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 15:50:12.826
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:12.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:12.866
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 15:50:12.894
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:13.626
    STEP: Deploying the webhook pod 12/14/22 15:50:13.634
    STEP: Wait for the deployment to be ready 12/14/22 15:50:13.651
    Dec 14 15:50:13.660: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 15:50:15.683
    STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:15.702
    Dec 14 15:50:16.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 12/14/22 15:50:16.713
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.739
    STEP: Updating a validating webhook configuration's rules to not include the create operation 12/14/22 15:50:16.756
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.777
    STEP: Patching a validating webhook configuration's rules to include the create operation 12/14/22 15:50:16.797
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 15:50:16.808
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:16.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9962" for this suite. 12/14/22 15:50:17.028
    STEP: Destroying namespace "webhook-9962-markers" for this suite. 12/14/22 15:50:17.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:17.078
Dec 14 15:50:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename prestop 12/14/22 15:50:17.081
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:17.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:17.119
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3091 12/14/22 15:50:17.122
STEP: Waiting for pods to come up. 12/14/22 15:50:17.136
Dec 14 15:50:17.137: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3091" to be "running"
Dec 14 15:50:17.140: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773121ms
Dec 14 15:50:19.149: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.012391448s
Dec 14 15:50:19.150: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3091 12/14/22 15:50:19.155
Dec 14 15:50:19.163: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3091" to be "running"
Dec 14 15:50:19.181: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.79812ms
Dec 14 15:50:21.189: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.025962224s
Dec 14 15:50:21.189: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 12/14/22 15:50:21.189
Dec 14 15:50:26.211: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 12/14/22 15:50:26.211
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:26.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-3091" for this suite. 12/14/22 15:50:26.255
------------------------------
• [SLOW TEST] [9.188 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:17.078
    Dec 14 15:50:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename prestop 12/14/22 15:50:17.081
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:17.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:17.119
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3091 12/14/22 15:50:17.122
    STEP: Waiting for pods to come up. 12/14/22 15:50:17.136
    Dec 14 15:50:17.137: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3091" to be "running"
    Dec 14 15:50:17.140: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773121ms
    Dec 14 15:50:19.149: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.012391448s
    Dec 14 15:50:19.150: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3091 12/14/22 15:50:19.155
    Dec 14 15:50:19.163: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3091" to be "running"
    Dec 14 15:50:19.181: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.79812ms
    Dec 14 15:50:21.189: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.025962224s
    Dec 14 15:50:21.189: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 12/14/22 15:50:21.189
    Dec 14 15:50:26.211: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 12/14/22 15:50:26.211
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:26.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-3091" for this suite. 12/14/22 15:50:26.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:26.274
Dec 14 15:50:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 15:50:26.276
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:26.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:26.305
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 15:50:26.322
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:26.939
STEP: Deploying the webhook pod 12/14/22 15:50:26.952
STEP: Wait for the deployment to be ready 12/14/22 15:50:26.97
Dec 14 15:50:26.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 15:50:29.018
STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:29.034
Dec 14 15:50:30.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 12/14/22 15:50:30.129
STEP: Creating a configMap that should be mutated 12/14/22 15:50:30.154
STEP: Deleting the collection of validation webhooks 12/14/22 15:50:30.196
STEP: Creating a configMap that should not be mutated 12/14/22 15:50:30.27
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-742" for this suite. 12/14/22 15:50:30.349
STEP: Destroying namespace "webhook-742-markers" for this suite. 12/14/22 15:50:30.359
------------------------------
• [4.100 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:26.274
    Dec 14 15:50:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 15:50:26.276
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:26.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:26.305
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 15:50:26.322
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:50:26.939
    STEP: Deploying the webhook pod 12/14/22 15:50:26.952
    STEP: Wait for the deployment to be ready 12/14/22 15:50:26.97
    Dec 14 15:50:26.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 15:50:29.018
    STEP: Verifying the service has paired with the endpoint 12/14/22 15:50:29.034
    Dec 14 15:50:30.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 12/14/22 15:50:30.129
    STEP: Creating a configMap that should be mutated 12/14/22 15:50:30.154
    STEP: Deleting the collection of validation webhooks 12/14/22 15:50:30.196
    STEP: Creating a configMap that should not be mutated 12/14/22 15:50:30.27
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-742" for this suite. 12/14/22 15:50:30.349
    STEP: Destroying namespace "webhook-742-markers" for this suite. 12/14/22 15:50:30.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:30.418
Dec 14 15:50:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 15:50:30.423
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.45
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 12/14/22 15:50:30.46
STEP: waiting for available Endpoint 12/14/22 15:50:30.467
STEP: listing all Endpoints 12/14/22 15:50:30.47
STEP: updating the Endpoint 12/14/22 15:50:30.475
STEP: fetching the Endpoint 12/14/22 15:50:30.485
STEP: patching the Endpoint 12/14/22 15:50:30.491
STEP: fetching the Endpoint 12/14/22 15:50:30.5
STEP: deleting the Endpoint by Collection 12/14/22 15:50:30.506
STEP: waiting for Endpoint deletion 12/14/22 15:50:30.518
STEP: fetching the Endpoint 12/14/22 15:50:30.522
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:30.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7265" for this suite. 12/14/22 15:50:30.539
------------------------------
• [0.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:30.418
    Dec 14 15:50:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 15:50:30.423
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.45
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 12/14/22 15:50:30.46
    STEP: waiting for available Endpoint 12/14/22 15:50:30.467
    STEP: listing all Endpoints 12/14/22 15:50:30.47
    STEP: updating the Endpoint 12/14/22 15:50:30.475
    STEP: fetching the Endpoint 12/14/22 15:50:30.485
    STEP: patching the Endpoint 12/14/22 15:50:30.491
    STEP: fetching the Endpoint 12/14/22 15:50:30.5
    STEP: deleting the Endpoint by Collection 12/14/22 15:50:30.506
    STEP: waiting for Endpoint deletion 12/14/22 15:50:30.518
    STEP: fetching the Endpoint 12/14/22 15:50:30.522
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:30.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7265" for this suite. 12/14/22 15:50:30.539
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:30.562
Dec 14 15:50:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 15:50:30.565
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.609
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 12/14/22 15:50:30.615
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:30.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8784" for this suite. 12/14/22 15:50:30.629
------------------------------
• [0.080 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:30.562
    Dec 14 15:50:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 15:50:30.565
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.609
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 12/14/22 15:50:30.615
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:30.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8784" for this suite. 12/14/22 15:50:30.629
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:30.646
Dec 14 15:50:30.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 15:50:30.648
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.682
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972 12/14/22 15:50:30.687
Dec 14 15:50:30.701: INFO: Pod name my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Found 0 pods out of 1
Dec 14 15:50:35.708: INFO: Pod name my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Found 1 pods out of 1
Dec 14 15:50:35.708: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972" are running
Dec 14 15:50:35.708: INFO: Waiting up to 5m0s for pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" in namespace "replication-controller-9648" to be "running"
Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs": Phase="Running", Reason="", readiness=true. Elapsed: 3.293956ms
Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" satisfied condition "running"
Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:30 +0000 UTC Reason: Message:}])
Dec 14 15:50:35.712: INFO: Trying to dial the pod
Dec 14 15:50:40.739: INFO: Controller my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Got expected result from replica 1 [my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs]: "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 15:50:40.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9648" for this suite. 12/14/22 15:50:40.751
------------------------------
• [SLOW TEST] [10.116 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:30.646
    Dec 14 15:50:30.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 15:50:30.648
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:30.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:30.682
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972 12/14/22 15:50:30.687
    Dec 14 15:50:30.701: INFO: Pod name my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Found 0 pods out of 1
    Dec 14 15:50:35.708: INFO: Pod name my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Found 1 pods out of 1
    Dec 14 15:50:35.708: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972" are running
    Dec 14 15:50:35.708: INFO: Waiting up to 5m0s for pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" in namespace "replication-controller-9648" to be "running"
    Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs": Phase="Running", Reason="", readiness=true. Elapsed: 3.293956ms
    Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" satisfied condition "running"
    Dec 14 15:50:35.712: INFO: Pod "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:30 +0000 UTC Reason: Message:}])
    Dec 14 15:50:35.712: INFO: Trying to dial the pod
    Dec 14 15:50:40.739: INFO: Controller my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972: Got expected result from replica 1 [my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs]: "my-hostname-basic-617176d3-c80f-41fa-bfc2-4b9adc192972-v9qvs", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:50:40.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9648" for this suite. 12/14/22 15:50:40.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:50:40.766
Dec 14 15:50:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 15:50:40.769
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:40.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:40.801
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1383 12/14/22 15:50:40.806
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-1383 12/14/22 15:50:40.822
Dec 14 15:50:40.837: INFO: Found 0 stateful pods, waiting for 1
Dec 14 15:50:50.843: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 12/14/22 15:50:50.854
STEP: Getting /status 12/14/22 15:50:50.874
Dec 14 15:50:50.883: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 12/14/22 15:50:50.883
Dec 14 15:50:50.901: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 12/14/22 15:50:50.901
Dec 14 15:50:50.915: INFO: Observed &StatefulSet event: ADDED
Dec 14 15:50:50.915: INFO: Found Statefulset ss in namespace statefulset-1383 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 15:50:50.915: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 12/14/22 15:50:50.915
Dec 14 15:50:50.915: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 15:50:50.931: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 12/14/22 15:50:50.931
Dec 14 15:50:50.935: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 15:50:50.936: INFO: Deleting all statefulset in ns statefulset-1383
Dec 14 15:50:50.942: INFO: Scaling statefulset ss to 0
Dec 14 15:51:00.981: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 15:51:00.999: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:01.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1383" for this suite. 12/14/22 15:51:01.031
------------------------------
• [SLOW TEST] [20.279 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:50:40.766
    Dec 14 15:50:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 15:50:40.769
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:50:40.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:50:40.801
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1383 12/14/22 15:50:40.806
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-1383 12/14/22 15:50:40.822
    Dec 14 15:50:40.837: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 15:50:50.843: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 12/14/22 15:50:50.854
    STEP: Getting /status 12/14/22 15:50:50.874
    Dec 14 15:50:50.883: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 12/14/22 15:50:50.883
    Dec 14 15:50:50.901: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 12/14/22 15:50:50.901
    Dec 14 15:50:50.915: INFO: Observed &StatefulSet event: ADDED
    Dec 14 15:50:50.915: INFO: Found Statefulset ss in namespace statefulset-1383 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 15:50:50.915: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 12/14/22 15:50:50.915
    Dec 14 15:50:50.915: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 15:50:50.931: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 12/14/22 15:50:50.931
    Dec 14 15:50:50.935: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 15:50:50.936: INFO: Deleting all statefulset in ns statefulset-1383
    Dec 14 15:50:50.942: INFO: Scaling statefulset ss to 0
    Dec 14 15:51:00.981: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 15:51:00.999: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:01.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1383" for this suite. 12/14/22 15:51:01.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:01.054
Dec 14 15:51:01.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename runtimeclass 12/14/22 15:51:01.057
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:01.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:01.096
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3296-delete-me 12/14/22 15:51:01.107
STEP: Waiting for the RuntimeClass to disappear 12/14/22 15:51:01.117
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:01.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3296" for this suite. 12/14/22 15:51:01.137
------------------------------
• [0.091 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:01.054
    Dec 14 15:51:01.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 15:51:01.057
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:01.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:01.096
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3296-delete-me 12/14/22 15:51:01.107
    STEP: Waiting for the RuntimeClass to disappear 12/14/22 15:51:01.117
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:01.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3296" for this suite. 12/14/22 15:51:01.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:01.15
Dec 14 15:51:01.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 15:51:01.152
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:01.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:01.191
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 15:51:01.215
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:51:01.886
STEP: Deploying the webhook pod 12/14/22 15:51:01.907
STEP: Wait for the deployment to be ready 12/14/22 15:51:01.927
Dec 14 15:51:01.942: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 15:51:03.968
STEP: Verifying the service has paired with the endpoint 12/14/22 15:51:03.99
Dec 14 15:51:04.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 12/14/22 15:51:05
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/14/22 15:51:05.003
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 15:51:05.004
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/14/22 15:51:05.004
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/14/22 15:51:05.007
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 15:51:05.008
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 15:51:05.01
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:05.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1122" for this suite. 12/14/22 15:51:05.091
STEP: Destroying namespace "webhook-1122-markers" for this suite. 12/14/22 15:51:05.1
------------------------------
• [3.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:01.15
    Dec 14 15:51:01.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 15:51:01.152
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:01.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:01.191
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 15:51:01.215
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:51:01.886
    STEP: Deploying the webhook pod 12/14/22 15:51:01.907
    STEP: Wait for the deployment to be ready 12/14/22 15:51:01.927
    Dec 14 15:51:01.942: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 15:51:03.968
    STEP: Verifying the service has paired with the endpoint 12/14/22 15:51:03.99
    Dec 14 15:51:04.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 12/14/22 15:51:05
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/14/22 15:51:05.003
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 15:51:05.004
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/14/22 15:51:05.004
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/14/22 15:51:05.007
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 15:51:05.008
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/14/22 15:51:05.01
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:05.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1122" for this suite. 12/14/22 15:51:05.091
    STEP: Destroying namespace "webhook-1122-markers" for this suite. 12/14/22 15:51:05.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:05.125
Dec 14 15:51:05.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename runtimeclass 12/14/22 15:51:05.129
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:05.157
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:05.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2408" for this suite. 12/14/22 15:51:05.19
------------------------------
• [0.074 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:05.125
    Dec 14 15:51:05.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 15:51:05.129
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:05.157
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:05.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2408" for this suite. 12/14/22 15:51:05.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:05.2
Dec 14 15:51:05.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 15:51:05.202
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:05.224
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 12/14/22 15:51:05.227
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.306
STEP: Creating a pod in the namespace 12/14/22 15:51:05.312
STEP: Waiting for the pod to have running status 12/14/22 15:51:05.322
Dec 14 15:51:05.323: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9633" to be "running"
Dec 14 15:51:05.330: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.84719ms
Dec 14 15:51:07.339: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016193167s
Dec 14 15:51:09.339: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016571011s
Dec 14 15:51:09.340: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/14/22 15:51:09.34
STEP: Waiting for the namespace to be removed. 12/14/22 15:51:09.352
STEP: Recreating the namespace 12/14/22 15:51:20.364
STEP: Verifying there are no pods in the namespace 12/14/22 15:51:20.391
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:20.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4528" for this suite. 12/14/22 15:51:20.402
STEP: Destroying namespace "nsdeletetest-9633" for this suite. 12/14/22 15:51:20.412
Dec 14 15:51:20.420: INFO: Namespace nsdeletetest-9633 was already deleted
STEP: Destroying namespace "nsdeletetest-236" for this suite. 12/14/22 15:51:20.42
------------------------------
• [SLOW TEST] [15.230 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:05.2
    Dec 14 15:51:05.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 15:51:05.202
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:05.224
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 12/14/22 15:51:05.227
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:05.306
    STEP: Creating a pod in the namespace 12/14/22 15:51:05.312
    STEP: Waiting for the pod to have running status 12/14/22 15:51:05.322
    Dec 14 15:51:05.323: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9633" to be "running"
    Dec 14 15:51:05.330: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.84719ms
    Dec 14 15:51:07.339: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016193167s
    Dec 14 15:51:09.339: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016571011s
    Dec 14 15:51:09.340: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/14/22 15:51:09.34
    STEP: Waiting for the namespace to be removed. 12/14/22 15:51:09.352
    STEP: Recreating the namespace 12/14/22 15:51:20.364
    STEP: Verifying there are no pods in the namespace 12/14/22 15:51:20.391
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:20.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4528" for this suite. 12/14/22 15:51:20.402
    STEP: Destroying namespace "nsdeletetest-9633" for this suite. 12/14/22 15:51:20.412
    Dec 14 15:51:20.420: INFO: Namespace nsdeletetest-9633 was already deleted
    STEP: Destroying namespace "nsdeletetest-236" for this suite. 12/14/22 15:51:20.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:20.436
Dec 14 15:51:20.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context 12/14/22 15:51:20.441
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:20.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:20.469
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 15:51:20.473
Dec 14 15:51:20.484: INFO: Waiting up to 5m0s for pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659" in namespace "security-context-1084" to be "Succeeded or Failed"
Dec 14 15:51:20.488: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79082ms
Dec 14 15:51:22.497: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012869023s
Dec 14 15:51:24.497: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01314609s
STEP: Saw pod success 12/14/22 15:51:24.497
Dec 14 15:51:24.498: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659" satisfied condition "Succeeded or Failed"
Dec 14 15:51:24.506: INFO: Trying to get logs from node iet9eich7uhu-3 pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 container test-container: <nil>
STEP: delete the pod 12/14/22 15:51:24.521
Dec 14 15:51:24.542: INFO: Waiting for pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 to disappear
Dec 14 15:51:24.549: INFO: Pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:24.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1084" for this suite. 12/14/22 15:51:24.555
------------------------------
• [4.128 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:20.436
    Dec 14 15:51:20.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context 12/14/22 15:51:20.441
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:20.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:20.469
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 15:51:20.473
    Dec 14 15:51:20.484: INFO: Waiting up to 5m0s for pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659" in namespace "security-context-1084" to be "Succeeded or Failed"
    Dec 14 15:51:20.488: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79082ms
    Dec 14 15:51:22.497: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012869023s
    Dec 14 15:51:24.497: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01314609s
    STEP: Saw pod success 12/14/22 15:51:24.497
    Dec 14 15:51:24.498: INFO: Pod "security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659" satisfied condition "Succeeded or Failed"
    Dec 14 15:51:24.506: INFO: Trying to get logs from node iet9eich7uhu-3 pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 container test-container: <nil>
    STEP: delete the pod 12/14/22 15:51:24.521
    Dec 14 15:51:24.542: INFO: Waiting for pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 to disappear
    Dec 14 15:51:24.549: INFO: Pod security-context-7fd2ea0e-9f45-4a3d-8f81-779412c7c659 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:24.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1084" for this suite. 12/14/22 15:51:24.555
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:24.568
Dec 14 15:51:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 15:51:24.571
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:24.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:24.606
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 12/14/22 15:51:24.615
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/14/22 15:51:24.616
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 15:51:24.617
STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/14/22 15:51:24.617
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/14/22 15:51:24.618
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 15:51:24.619
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 15:51:24.621
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:24.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2910" for this suite. 12/14/22 15:51:24.628
------------------------------
• [0.068 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:24.568
    Dec 14 15:51:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 15:51:24.571
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:24.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:24.606
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 12/14/22 15:51:24.615
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/14/22 15:51:24.616
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/14/22 15:51:24.617
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/14/22 15:51:24.617
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/14/22 15:51:24.618
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 15:51:24.619
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/14/22 15:51:24.621
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:24.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2910" for this suite. 12/14/22 15:51:24.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:24.642
Dec 14 15:51:24.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 15:51:24.644
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:24.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:24.666
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 15:51:24.671
Dec 14 15:51:24.687: INFO: Waiting up to 5m0s for pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef" in namespace "emptydir-158" to be "Succeeded or Failed"
Dec 14 15:51:24.693: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.630237ms
Dec 14 15:51:26.703: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016604582s
Dec 14 15:51:28.701: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013798358s
STEP: Saw pod success 12/14/22 15:51:28.701
Dec 14 15:51:28.702: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef" satisfied condition "Succeeded or Failed"
Dec 14 15:51:28.707: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef container test-container: <nil>
STEP: delete the pod 12/14/22 15:51:28.719
Dec 14 15:51:28.738: INFO: Waiting for pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef to disappear
Dec 14 15:51:28.743: INFO: Pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 15:51:28.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-158" for this suite. 12/14/22 15:51:28.749
------------------------------
• [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:24.642
    Dec 14 15:51:24.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 15:51:24.644
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:24.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:24.666
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/14/22 15:51:24.671
    Dec 14 15:51:24.687: INFO: Waiting up to 5m0s for pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef" in namespace "emptydir-158" to be "Succeeded or Failed"
    Dec 14 15:51:24.693: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.630237ms
    Dec 14 15:51:26.703: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016604582s
    Dec 14 15:51:28.701: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013798358s
    STEP: Saw pod success 12/14/22 15:51:28.701
    Dec 14 15:51:28.702: INFO: Pod "pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef" satisfied condition "Succeeded or Failed"
    Dec 14 15:51:28.707: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef container test-container: <nil>
    STEP: delete the pod 12/14/22 15:51:28.719
    Dec 14 15:51:28.738: INFO: Waiting for pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef to disappear
    Dec 14 15:51:28.743: INFO: Pod pod-87622096-6ac7-4b9a-ae0b-59f88fed9cef no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:51:28.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-158" for this suite. 12/14/22 15:51:28.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:51:28.762
Dec 14 15:51:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename cronjob 12/14/22 15:51:28.765
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:28.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:28.799
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 12/14/22 15:51:28.806
STEP: Ensuring no jobs are scheduled 12/14/22 15:51:28.815
STEP: Ensuring no job exists by listing jobs explicitly 12/14/22 15:56:28.828
STEP: Removing cronjob 12/14/22 15:56:28.835
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:28.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9105" for this suite. 12/14/22 15:56:28.856
------------------------------
• [SLOW TEST] [300.102 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:51:28.762
    Dec 14 15:51:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename cronjob 12/14/22 15:51:28.765
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:51:28.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:51:28.799
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 12/14/22 15:51:28.806
    STEP: Ensuring no jobs are scheduled 12/14/22 15:51:28.815
    STEP: Ensuring no job exists by listing jobs explicitly 12/14/22 15:56:28.828
    STEP: Removing cronjob 12/14/22 15:56:28.835
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:28.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9105" for this suite. 12/14/22 15:56:28.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:28.87
Dec 14 15:56:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 15:56:28.873
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:28.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:28.907
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 15:56:28.912
Dec 14 15:56:28.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-4752 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Dec 14 15:56:29.058: INFO: stderr: ""
Dec 14 15:56:29.058: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 15:56:29.058
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Dec 14 15:56:29.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-4752 delete pods e2e-test-httpd-pod'
Dec 14 15:56:31.590: INFO: stderr: ""
Dec 14 15:56:31.591: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4752" for this suite. 12/14/22 15:56:31.599
------------------------------
• [2.740 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:28.87
    Dec 14 15:56:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 15:56:28.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:28.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:28.907
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 15:56:28.912
    Dec 14 15:56:28.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-4752 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Dec 14 15:56:29.058: INFO: stderr: ""
    Dec 14 15:56:29.058: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 15:56:29.058
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Dec 14 15:56:29.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-4752 delete pods e2e-test-httpd-pod'
    Dec 14 15:56:31.590: INFO: stderr: ""
    Dec 14 15:56:31.591: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4752" for this suite. 12/14/22 15:56:31.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:31.618
Dec 14 15:56:31.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 15:56:31.62
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.66
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 12/14/22 15:56:31.664
STEP: Getting a ResourceQuota 12/14/22 15:56:31.674
STEP: Listing all ResourceQuotas with LabelSelector 12/14/22 15:56:31.678
STEP: Patching the ResourceQuota 12/14/22 15:56:31.684
STEP: Deleting a Collection of ResourceQuotas 12/14/22 15:56:31.693
STEP: Verifying the deleted ResourceQuota 12/14/22 15:56:31.703
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:31.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5683" for this suite. 12/14/22 15:56:31.714
------------------------------
• [0.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:31.618
    Dec 14 15:56:31.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 15:56:31.62
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.66
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 12/14/22 15:56:31.664
    STEP: Getting a ResourceQuota 12/14/22 15:56:31.674
    STEP: Listing all ResourceQuotas with LabelSelector 12/14/22 15:56:31.678
    STEP: Patching the ResourceQuota 12/14/22 15:56:31.684
    STEP: Deleting a Collection of ResourceQuotas 12/14/22 15:56:31.693
    STEP: Verifying the deleted ResourceQuota 12/14/22 15:56:31.703
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:31.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5683" for this suite. 12/14/22 15:56:31.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:31.728
Dec 14 15:56:31.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename limitrange 12/14/22 15:56:31.73
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.761
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-vkq8p" in namespace "limitrange-8242" 12/14/22 15:56:31.766
STEP: Creating another limitRange in another namespace 12/14/22 15:56:31.787
Dec 14 15:56:31.815: INFO: Namespace "e2e-limitrange-vkq8p-2626" created
Dec 14 15:56:31.815: INFO: Creating LimitRange "e2e-limitrange-vkq8p" in namespace "e2e-limitrange-vkq8p-2626"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vkq8p" 12/14/22 15:56:31.82
Dec 14 15:56:31.825: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-vkq8p" in "limitrange-8242" namespace 12/14/22 15:56:31.825
Dec 14 15:56:31.834: INFO: LimitRange "e2e-limitrange-vkq8p" has been patched
STEP: Delete LimitRange "e2e-limitrange-vkq8p" by Collection with labelSelector: "e2e-limitrange-vkq8p=patched" 12/14/22 15:56:31.834
STEP: Confirm that the limitRange "e2e-limitrange-vkq8p" has been deleted 12/14/22 15:56:31.845
Dec 14 15:56:31.846: INFO: Requesting list of LimitRange to confirm quantity
Dec 14 15:56:31.849: INFO: Found 0 LimitRange with label "e2e-limitrange-vkq8p=patched"
Dec 14 15:56:31.849: INFO: LimitRange "e2e-limitrange-vkq8p" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vkq8p" 12/14/22 15:56:31.849
Dec 14 15:56:31.853: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:31.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8242" for this suite. 12/14/22 15:56:31.858
STEP: Destroying namespace "e2e-limitrange-vkq8p-2626" for this suite. 12/14/22 15:56:31.875
------------------------------
• [0.159 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:31.728
    Dec 14 15:56:31.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename limitrange 12/14/22 15:56:31.73
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.761
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-vkq8p" in namespace "limitrange-8242" 12/14/22 15:56:31.766
    STEP: Creating another limitRange in another namespace 12/14/22 15:56:31.787
    Dec 14 15:56:31.815: INFO: Namespace "e2e-limitrange-vkq8p-2626" created
    Dec 14 15:56:31.815: INFO: Creating LimitRange "e2e-limitrange-vkq8p" in namespace "e2e-limitrange-vkq8p-2626"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vkq8p" 12/14/22 15:56:31.82
    Dec 14 15:56:31.825: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-vkq8p" in "limitrange-8242" namespace 12/14/22 15:56:31.825
    Dec 14 15:56:31.834: INFO: LimitRange "e2e-limitrange-vkq8p" has been patched
    STEP: Delete LimitRange "e2e-limitrange-vkq8p" by Collection with labelSelector: "e2e-limitrange-vkq8p=patched" 12/14/22 15:56:31.834
    STEP: Confirm that the limitRange "e2e-limitrange-vkq8p" has been deleted 12/14/22 15:56:31.845
    Dec 14 15:56:31.846: INFO: Requesting list of LimitRange to confirm quantity
    Dec 14 15:56:31.849: INFO: Found 0 LimitRange with label "e2e-limitrange-vkq8p=patched"
    Dec 14 15:56:31.849: INFO: LimitRange "e2e-limitrange-vkq8p" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vkq8p" 12/14/22 15:56:31.849
    Dec 14 15:56:31.853: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:31.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8242" for this suite. 12/14/22 15:56:31.858
    STEP: Destroying namespace "e2e-limitrange-vkq8p-2626" for this suite. 12/14/22 15:56:31.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:31.887
Dec 14 15:56:31.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 15:56:31.891
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.924
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 12/14/22 15:56:31.928
Dec 14 15:56:31.945: INFO: Waiting up to 5m0s for pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2" in namespace "emptydir-7314" to be "Succeeded or Failed"
Dec 14 15:56:31.953: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.880835ms
Dec 14 15:56:33.960: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Running", Reason="", readiness=false. Elapsed: 2.015574598s
Dec 14 15:56:35.962: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016800766s
STEP: Saw pod success 12/14/22 15:56:35.962
Dec 14 15:56:35.963: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2" satisfied condition "Succeeded or Failed"
Dec 14 15:56:35.968: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 container test-container: <nil>
STEP: delete the pod 12/14/22 15:56:35.999
Dec 14 15:56:36.019: INFO: Waiting for pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 to disappear
Dec 14 15:56:36.026: INFO: Pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:36.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7314" for this suite. 12/14/22 15:56:36.034
------------------------------
• [4.159 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:31.887
    Dec 14 15:56:31.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 15:56:31.891
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:31.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:31.924
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 12/14/22 15:56:31.928
    Dec 14 15:56:31.945: INFO: Waiting up to 5m0s for pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2" in namespace "emptydir-7314" to be "Succeeded or Failed"
    Dec 14 15:56:31.953: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.880835ms
    Dec 14 15:56:33.960: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Running", Reason="", readiness=false. Elapsed: 2.015574598s
    Dec 14 15:56:35.962: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016800766s
    STEP: Saw pod success 12/14/22 15:56:35.962
    Dec 14 15:56:35.963: INFO: Pod "pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2" satisfied condition "Succeeded or Failed"
    Dec 14 15:56:35.968: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 container test-container: <nil>
    STEP: delete the pod 12/14/22 15:56:35.999
    Dec 14 15:56:36.019: INFO: Waiting for pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 to disappear
    Dec 14 15:56:36.026: INFO: Pod pod-41ca8f4c-cdfb-4ae4-9cbc-f76f9a47eab2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:36.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7314" for this suite. 12/14/22 15:56:36.034
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:36.047
Dec 14 15:56:36.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 15:56:36.051
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:36.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:36.08
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 12/14/22 15:56:36.084
Dec 14 15:56:36.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5495 api-versions'
Dec 14 15:56:36.250: INFO: stderr: ""
Dec 14 15:56:36.250: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:36.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5495" for this suite. 12/14/22 15:56:36.258
------------------------------
• [0.220 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:36.047
    Dec 14 15:56:36.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 15:56:36.051
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:36.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:36.08
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 12/14/22 15:56:36.084
    Dec 14 15:56:36.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5495 api-versions'
    Dec 14 15:56:36.250: INFO: stderr: ""
    Dec 14 15:56:36.250: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:36.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5495" for this suite. 12/14/22 15:56:36.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:36.271
Dec 14 15:56:36.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 15:56:36.275
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:36.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:36.309
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-eec8b173-5c6e-4437-a704-a27d9f504cda 12/14/22 15:56:36.313
STEP: Creating a pod to test consume secrets 12/14/22 15:56:36.32
Dec 14 15:56:36.330: INFO: Waiting up to 5m0s for pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274" in namespace "secrets-4987" to be "Succeeded or Failed"
Dec 14 15:56:36.334: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Pending", Reason="", readiness=false. Elapsed: 3.804866ms
Dec 14 15:56:38.342: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011752643s
Dec 14 15:56:40.347: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016775612s
STEP: Saw pod success 12/14/22 15:56:40.347
Dec 14 15:56:40.347: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274" satisfied condition "Succeeded or Failed"
Dec 14 15:56:40.354: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 container secret-env-test: <nil>
STEP: delete the pod 12/14/22 15:56:40.365
Dec 14 15:56:40.380: INFO: Waiting for pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 to disappear
Dec 14 15:56:40.385: INFO: Pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:40.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4987" for this suite. 12/14/22 15:56:40.392
------------------------------
• [4.133 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:36.271
    Dec 14 15:56:36.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 15:56:36.275
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:36.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:36.309
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-eec8b173-5c6e-4437-a704-a27d9f504cda 12/14/22 15:56:36.313
    STEP: Creating a pod to test consume secrets 12/14/22 15:56:36.32
    Dec 14 15:56:36.330: INFO: Waiting up to 5m0s for pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274" in namespace "secrets-4987" to be "Succeeded or Failed"
    Dec 14 15:56:36.334: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Pending", Reason="", readiness=false. Elapsed: 3.804866ms
    Dec 14 15:56:38.342: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011752643s
    Dec 14 15:56:40.347: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016775612s
    STEP: Saw pod success 12/14/22 15:56:40.347
    Dec 14 15:56:40.347: INFO: Pod "pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274" satisfied condition "Succeeded or Failed"
    Dec 14 15:56:40.354: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 container secret-env-test: <nil>
    STEP: delete the pod 12/14/22 15:56:40.365
    Dec 14 15:56:40.380: INFO: Waiting for pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 to disappear
    Dec 14 15:56:40.385: INFO: Pod pod-secrets-aaa4c58e-56fc-4198-87cd-a8d630e92274 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:40.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4987" for this suite. 12/14/22 15:56:40.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:40.405
Dec 14 15:56:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 15:56:40.409
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:40.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:40.435
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 12/14/22 15:56:40.439
Dec 14 15:56:40.451: INFO: Waiting up to 5m0s for pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218" in namespace "var-expansion-2728" to be "Succeeded or Failed"
Dec 14 15:56:40.456: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Pending", Reason="", readiness=false. Elapsed: 4.676196ms
Dec 14 15:56:42.467: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015415594s
Dec 14 15:56:44.465: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013294933s
STEP: Saw pod success 12/14/22 15:56:44.465
Dec 14 15:56:44.465: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218" satisfied condition "Succeeded or Failed"
Dec 14 15:56:44.472: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 container dapi-container: <nil>
STEP: delete the pod 12/14/22 15:56:44.483
Dec 14 15:56:44.504: INFO: Waiting for pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 to disappear
Dec 14 15:56:44.509: INFO: Pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:44.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2728" for this suite. 12/14/22 15:56:44.519
------------------------------
• [4.122 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:40.405
    Dec 14 15:56:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 15:56:40.409
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:40.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:40.435
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 12/14/22 15:56:40.439
    Dec 14 15:56:40.451: INFO: Waiting up to 5m0s for pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218" in namespace "var-expansion-2728" to be "Succeeded or Failed"
    Dec 14 15:56:40.456: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Pending", Reason="", readiness=false. Elapsed: 4.676196ms
    Dec 14 15:56:42.467: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015415594s
    Dec 14 15:56:44.465: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013294933s
    STEP: Saw pod success 12/14/22 15:56:44.465
    Dec 14 15:56:44.465: INFO: Pod "var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218" satisfied condition "Succeeded or Failed"
    Dec 14 15:56:44.472: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 15:56:44.483
    Dec 14 15:56:44.504: INFO: Waiting for pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 to disappear
    Dec 14 15:56:44.509: INFO: Pod var-expansion-99e8eff8-48d6-40e8-a8e3-396274128218 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:44.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2728" for this suite. 12/14/22 15:56:44.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:44.531
Dec 14 15:56:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 15:56:44.533
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:44.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:44.565
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 15:56:44.588
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:56:44.965
STEP: Deploying the webhook pod 12/14/22 15:56:44.98
STEP: Wait for the deployment to be ready 12/14/22 15:56:44.997
Dec 14 15:56:45.006: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 15:56:47.024
STEP: Verifying the service has paired with the endpoint 12/14/22 15:56:47.039
Dec 14 15:56:48.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/14/22 15:56:48.049
STEP: create a namespace for the webhook 12/14/22 15:56:48.079
STEP: create a configmap should be unconditionally rejected by the webhook 12/14/22 15:56:48.092
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 15:56:48.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-423" for this suite. 12/14/22 15:56:48.2
STEP: Destroying namespace "webhook-423-markers" for this suite. 12/14/22 15:56:48.213
------------------------------
• [3.692 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:44.531
    Dec 14 15:56:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 15:56:44.533
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:44.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:44.565
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 15:56:44.588
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 15:56:44.965
    STEP: Deploying the webhook pod 12/14/22 15:56:44.98
    STEP: Wait for the deployment to be ready 12/14/22 15:56:44.997
    Dec 14 15:56:45.006: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 15:56:47.024
    STEP: Verifying the service has paired with the endpoint 12/14/22 15:56:47.039
    Dec 14 15:56:48.040: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/14/22 15:56:48.049
    STEP: create a namespace for the webhook 12/14/22 15:56:48.079
    STEP: create a configmap should be unconditionally rejected by the webhook 12/14/22 15:56:48.092
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:56:48.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-423" for this suite. 12/14/22 15:56:48.2
    STEP: Destroying namespace "webhook-423-markers" for this suite. 12/14/22 15:56:48.213
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:56:48.224
Dec 14 15:56:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename subpath 12/14/22 15:56:48.228
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:48.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:48.254
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 15:56:48.258
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-7z7k 12/14/22 15:56:48.273
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 15:56:48.273
Dec 14 15:56:48.286: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7z7k" in namespace "subpath-8876" to be "Succeeded or Failed"
Dec 14 15:56:48.290: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929469ms
Dec 14 15:56:50.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014200425s
Dec 14 15:56:52.298: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 4.012808113s
Dec 14 15:56:54.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 6.015946247s
Dec 14 15:56:56.299: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 8.012942682s
Dec 14 15:56:58.299: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 10.01308978s
Dec 14 15:57:00.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 12.014608895s
Dec 14 15:57:02.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 14.016621267s
Dec 14 15:57:04.303: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 16.017003376s
Dec 14 15:57:06.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 18.016091683s
Dec 14 15:57:08.301: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 20.01494376s
Dec 14 15:57:10.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=false. Elapsed: 22.016102818s
Dec 14 15:57:12.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014679322s
STEP: Saw pod success 12/14/22 15:57:12.301
Dec 14 15:57:12.302: INFO: Pod "pod-subpath-test-configmap-7z7k" satisfied condition "Succeeded or Failed"
Dec 14 15:57:12.308: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-configmap-7z7k container test-container-subpath-configmap-7z7k: <nil>
STEP: delete the pod 12/14/22 15:57:12.324
Dec 14 15:57:12.344: INFO: Waiting for pod pod-subpath-test-configmap-7z7k to disappear
Dec 14 15:57:12.349: INFO: Pod pod-subpath-test-configmap-7z7k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7z7k 12/14/22 15:57:12.349
Dec 14 15:57:12.349: INFO: Deleting pod "pod-subpath-test-configmap-7z7k" in namespace "subpath-8876"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 14 15:57:12.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8876" for this suite. 12/14/22 15:57:12.362
------------------------------
• [SLOW TEST] [24.149 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:56:48.224
    Dec 14 15:56:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename subpath 12/14/22 15:56:48.228
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:56:48.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:56:48.254
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 15:56:48.258
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-7z7k 12/14/22 15:56:48.273
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 15:56:48.273
    Dec 14 15:56:48.286: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7z7k" in namespace "subpath-8876" to be "Succeeded or Failed"
    Dec 14 15:56:48.290: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929469ms
    Dec 14 15:56:50.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014200425s
    Dec 14 15:56:52.298: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 4.012808113s
    Dec 14 15:56:54.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 6.015946247s
    Dec 14 15:56:56.299: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 8.012942682s
    Dec 14 15:56:58.299: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 10.01308978s
    Dec 14 15:57:00.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 12.014608895s
    Dec 14 15:57:02.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 14.016621267s
    Dec 14 15:57:04.303: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 16.017003376s
    Dec 14 15:57:06.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 18.016091683s
    Dec 14 15:57:08.301: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=true. Elapsed: 20.01494376s
    Dec 14 15:57:10.302: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Running", Reason="", readiness=false. Elapsed: 22.016102818s
    Dec 14 15:57:12.300: INFO: Pod "pod-subpath-test-configmap-7z7k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014679322s
    STEP: Saw pod success 12/14/22 15:57:12.301
    Dec 14 15:57:12.302: INFO: Pod "pod-subpath-test-configmap-7z7k" satisfied condition "Succeeded or Failed"
    Dec 14 15:57:12.308: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-configmap-7z7k container test-container-subpath-configmap-7z7k: <nil>
    STEP: delete the pod 12/14/22 15:57:12.324
    Dec 14 15:57:12.344: INFO: Waiting for pod pod-subpath-test-configmap-7z7k to disappear
    Dec 14 15:57:12.349: INFO: Pod pod-subpath-test-configmap-7z7k no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-7z7k 12/14/22 15:57:12.349
    Dec 14 15:57:12.349: INFO: Deleting pod "pod-subpath-test-configmap-7z7k" in namespace "subpath-8876"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:57:12.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8876" for this suite. 12/14/22 15:57:12.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:57:12.381
Dec 14 15:57:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubelet-test 12/14/22 15:57:12.384
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:12.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:12.423
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 12/14/22 15:57:12.438
Dec 14 15:57:12.439: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9" in namespace "kubelet-test-5706" to be "completed"
Dec 14 15:57:12.443: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097725ms
Dec 14 15:57:14.455: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015971893s
Dec 14 15:57:16.452: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013132151s
Dec 14 15:57:16.452: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 14 15:57:16.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5706" for this suite. 12/14/22 15:57:16.477
------------------------------
• [4.107 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:57:12.381
    Dec 14 15:57:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 15:57:12.384
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:12.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:12.423
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 12/14/22 15:57:12.438
    Dec 14 15:57:12.439: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9" in namespace "kubelet-test-5706" to be "completed"
    Dec 14 15:57:12.443: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097725ms
    Dec 14 15:57:14.455: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015971893s
    Dec 14 15:57:16.452: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013132151s
    Dec 14 15:57:16.452: INFO: Pod "agnhost-host-aliasesb5a03abd-88cb-4bc8-bc73-ad3a9c4a1eb9" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:57:16.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5706" for this suite. 12/14/22 15:57:16.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:57:16.499
Dec 14 15:57:16.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 15:57:16.501
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:16.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:16.535
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-c4b37761-e164-43e5-a5f7-c1e706ae3438 12/14/22 15:57:16.54
STEP: Creating a pod to test consume configMaps 12/14/22 15:57:16.549
Dec 14 15:57:16.567: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2" in namespace "configmap-7227" to be "Succeeded or Failed"
Dec 14 15:57:16.581: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.65913ms
Dec 14 15:57:18.587: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019746887s
Dec 14 15:57:20.590: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022721908s
STEP: Saw pod success 12/14/22 15:57:20.59
Dec 14 15:57:20.591: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2" satisfied condition "Succeeded or Failed"
Dec 14 15:57:20.597: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 15:57:20.61
Dec 14 15:57:20.627: INFO: Waiting for pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 to disappear
Dec 14 15:57:20.631: INFO: Pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 15:57:20.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7227" for this suite. 12/14/22 15:57:20.637
------------------------------
• [4.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:57:16.499
    Dec 14 15:57:16.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 15:57:16.501
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:16.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:16.535
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-c4b37761-e164-43e5-a5f7-c1e706ae3438 12/14/22 15:57:16.54
    STEP: Creating a pod to test consume configMaps 12/14/22 15:57:16.549
    Dec 14 15:57:16.567: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2" in namespace "configmap-7227" to be "Succeeded or Failed"
    Dec 14 15:57:16.581: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.65913ms
    Dec 14 15:57:18.587: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019746887s
    Dec 14 15:57:20.590: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022721908s
    STEP: Saw pod success 12/14/22 15:57:20.59
    Dec 14 15:57:20.591: INFO: Pod "pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2" satisfied condition "Succeeded or Failed"
    Dec 14 15:57:20.597: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 15:57:20.61
    Dec 14 15:57:20.627: INFO: Waiting for pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 to disappear
    Dec 14 15:57:20.631: INFO: Pod pod-configmaps-e6f043b7-72d8-4731-8226-52dc39c8bea2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 15:57:20.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7227" for this suite. 12/14/22 15:57:20.637
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 15:57:20.651
Dec 14 15:57:20.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 15:57:20.654
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:20.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:20.677
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea in namespace container-probe-9106 12/14/22 15:57:20.681
Dec 14 15:57:20.693: INFO: Waiting up to 5m0s for pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea" in namespace "container-probe-9106" to be "not pending"
Dec 14 15:57:20.697: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552134ms
Dec 14 15:57:22.705: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.01229305s
Dec 14 15:57:22.705: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea" satisfied condition "not pending"
Dec 14 15:57:22.706: INFO: Started pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea in namespace container-probe-9106
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:57:22.706
Dec 14 15:57:22.713: INFO: Initial restart count of pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea is 0
STEP: deleting the pod 12/14/22 16:01:24.068
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 16:01:24.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9106" for this suite. 12/14/22 16:01:24.124
------------------------------
• [SLOW TEST] [243.485 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 15:57:20.651
    Dec 14 15:57:20.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 15:57:20.654
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 15:57:20.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 15:57:20.677
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea in namespace container-probe-9106 12/14/22 15:57:20.681
    Dec 14 15:57:20.693: INFO: Waiting up to 5m0s for pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea" in namespace "container-probe-9106" to be "not pending"
    Dec 14 15:57:20.697: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552134ms
    Dec 14 15:57:22.705: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.01229305s
    Dec 14 15:57:22.705: INFO: Pod "busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea" satisfied condition "not pending"
    Dec 14 15:57:22.706: INFO: Started pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea in namespace container-probe-9106
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 15:57:22.706
    Dec 14 15:57:22.713: INFO: Initial restart count of pod busybox-fde71755-ee81-4c04-ac6a-3df0bcc179ea is 0
    STEP: deleting the pod 12/14/22 16:01:24.068
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:01:24.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9106" for this suite. 12/14/22 16:01:24.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:01:24.137
Dec 14 16:01:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:01:24.145
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:24.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:24.176
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 12/14/22 16:01:24.181
Dec 14 16:01:24.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 create -f -'
Dec 14 16:01:25.468: INFO: stderr: ""
Dec 14 16:01:25.468: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:01:25.468
Dec 14 16:01:25.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:01:25.655: INFO: stderr: ""
Dec 14 16:01:25.655: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
Dec 14 16:01:25.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:01:25.849: INFO: stderr: ""
Dec 14 16:01:25.849: INFO: stdout: ""
Dec 14 16:01:25.849: INFO: update-demo-nautilus-bttt8 is created but not running
Dec 14 16:01:30.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:01:31.063: INFO: stderr: ""
Dec 14 16:01:31.063: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
Dec 14 16:01:31.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:01:31.206: INFO: stderr: ""
Dec 14 16:01:31.206: INFO: stdout: ""
Dec 14 16:01:31.206: INFO: update-demo-nautilus-bttt8 is created but not running
Dec 14 16:01:36.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:01:36.361: INFO: stderr: ""
Dec 14 16:01:36.367: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
Dec 14 16:01:36.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:01:36.506: INFO: stderr: ""
Dec 14 16:01:36.506: INFO: stdout: "true"
Dec 14 16:01:36.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:01:36.648: INFO: stderr: ""
Dec 14 16:01:36.648: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:01:36.648: INFO: validating pod update-demo-nautilus-bttt8
Dec 14 16:01:36.666: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:01:36.666: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:01:36.666: INFO: update-demo-nautilus-bttt8 is verified up and running
Dec 14 16:01:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-m78bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:01:36.784: INFO: stderr: ""
Dec 14 16:01:36.784: INFO: stdout: "true"
Dec 14 16:01:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-m78bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:01:36.918: INFO: stderr: ""
Dec 14 16:01:36.918: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:01:36.918: INFO: validating pod update-demo-nautilus-m78bb
Dec 14 16:01:36.935: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:01:36.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:01:36.935: INFO: update-demo-nautilus-m78bb is verified up and running
STEP: using delete to clean up resources 12/14/22 16:01:36.935
Dec 14 16:01:36.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 delete --grace-period=0 --force -f -'
Dec 14 16:01:37.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 16:01:37.070: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 16:01:37.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get rc,svc -l name=update-demo --no-headers'
Dec 14 16:01:37.252: INFO: stderr: "No resources found in kubectl-2700 namespace.\n"
Dec 14 16:01:37.252: INFO: stdout: ""
Dec 14 16:01:37.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 16:01:37.410: INFO: stderr: ""
Dec 14 16:01:37.410: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:01:37.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2700" for this suite. 12/14/22 16:01:37.422
------------------------------
• [SLOW TEST] [13.297 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:01:24.137
    Dec 14 16:01:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:01:24.145
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:24.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:24.176
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 12/14/22 16:01:24.181
    Dec 14 16:01:24.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 create -f -'
    Dec 14 16:01:25.468: INFO: stderr: ""
    Dec 14 16:01:25.468: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:01:25.468
    Dec 14 16:01:25.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:01:25.655: INFO: stderr: ""
    Dec 14 16:01:25.655: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
    Dec 14 16:01:25.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:01:25.849: INFO: stderr: ""
    Dec 14 16:01:25.849: INFO: stdout: ""
    Dec 14 16:01:25.849: INFO: update-demo-nautilus-bttt8 is created but not running
    Dec 14 16:01:30.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:01:31.063: INFO: stderr: ""
    Dec 14 16:01:31.063: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
    Dec 14 16:01:31.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:01:31.206: INFO: stderr: ""
    Dec 14 16:01:31.206: INFO: stdout: ""
    Dec 14 16:01:31.206: INFO: update-demo-nautilus-bttt8 is created but not running
    Dec 14 16:01:36.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:01:36.361: INFO: stderr: ""
    Dec 14 16:01:36.367: INFO: stdout: "update-demo-nautilus-bttt8 update-demo-nautilus-m78bb "
    Dec 14 16:01:36.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:01:36.506: INFO: stderr: ""
    Dec 14 16:01:36.506: INFO: stdout: "true"
    Dec 14 16:01:36.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-bttt8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:01:36.648: INFO: stderr: ""
    Dec 14 16:01:36.648: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:01:36.648: INFO: validating pod update-demo-nautilus-bttt8
    Dec 14 16:01:36.666: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:01:36.666: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:01:36.666: INFO: update-demo-nautilus-bttt8 is verified up and running
    Dec 14 16:01:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-m78bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:01:36.784: INFO: stderr: ""
    Dec 14 16:01:36.784: INFO: stdout: "true"
    Dec 14 16:01:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods update-demo-nautilus-m78bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:01:36.918: INFO: stderr: ""
    Dec 14 16:01:36.918: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:01:36.918: INFO: validating pod update-demo-nautilus-m78bb
    Dec 14 16:01:36.935: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:01:36.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:01:36.935: INFO: update-demo-nautilus-m78bb is verified up and running
    STEP: using delete to clean up resources 12/14/22 16:01:36.935
    Dec 14 16:01:36.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 delete --grace-period=0 --force -f -'
    Dec 14 16:01:37.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 16:01:37.070: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 14 16:01:37.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get rc,svc -l name=update-demo --no-headers'
    Dec 14 16:01:37.252: INFO: stderr: "No resources found in kubectl-2700 namespace.\n"
    Dec 14 16:01:37.252: INFO: stdout: ""
    Dec 14 16:01:37.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-2700 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 16:01:37.410: INFO: stderr: ""
    Dec 14 16:01:37.410: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:01:37.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2700" for this suite. 12/14/22 16:01:37.422
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:01:37.443
Dec 14 16:01:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:01:37.451
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:37.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:37.489
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Dec 14 16:01:37.529: INFO: created pod pod-service-account-defaultsa
Dec 14 16:01:37.529: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 16:01:37.555: INFO: created pod pod-service-account-mountsa
Dec 14 16:01:37.555: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 16:01:37.565: INFO: created pod pod-service-account-nomountsa
Dec 14 16:01:37.565: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 16:01:37.577: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 16:01:37.577: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 16:01:37.586: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 16:01:37.587: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 16:01:37.601: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 16:01:37.601: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 16:01:37.617: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 16:01:37.618: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 16:01:37.628: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 16:01:37.628: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 16:01:37.648: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 16:01:37.648: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:01:37.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7729" for this suite. 12/14/22 16:01:37.657
------------------------------
• [0.246 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:01:37.443
    Dec 14 16:01:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:01:37.451
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:37.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:37.489
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Dec 14 16:01:37.529: INFO: created pod pod-service-account-defaultsa
    Dec 14 16:01:37.529: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Dec 14 16:01:37.555: INFO: created pod pod-service-account-mountsa
    Dec 14 16:01:37.555: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Dec 14 16:01:37.565: INFO: created pod pod-service-account-nomountsa
    Dec 14 16:01:37.565: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Dec 14 16:01:37.577: INFO: created pod pod-service-account-defaultsa-mountspec
    Dec 14 16:01:37.577: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Dec 14 16:01:37.586: INFO: created pod pod-service-account-mountsa-mountspec
    Dec 14 16:01:37.587: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Dec 14 16:01:37.601: INFO: created pod pod-service-account-nomountsa-mountspec
    Dec 14 16:01:37.601: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Dec 14 16:01:37.617: INFO: created pod pod-service-account-defaultsa-nomountspec
    Dec 14 16:01:37.618: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Dec 14 16:01:37.628: INFO: created pod pod-service-account-mountsa-nomountspec
    Dec 14 16:01:37.628: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Dec 14 16:01:37.648: INFO: created pod pod-service-account-nomountsa-nomountspec
    Dec 14 16:01:37.648: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:01:37.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7729" for this suite. 12/14/22 16:01:37.657
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:01:37.69
Dec 14 16:01:37.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:01:37.695
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:37.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:37.801
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 12/14/22 16:01:37.804
Dec 14 16:01:37.816: INFO: Waiting up to 5m0s for pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14" in namespace "downward-api-8824" to be "running and ready"
Dec 14 16:01:37.837: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 20.383996ms
Dec 14 16:01:37.837: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:01:39.842: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026163559s
Dec 14 16:01:39.843: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:01:41.845: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028439332s
Dec 14 16:01:41.845: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:01:43.844: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Running", Reason="", readiness=true. Elapsed: 6.027923223s
Dec 14 16:01:43.844: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Running (Ready = true)
Dec 14 16:01:43.844: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14" satisfied condition "running and ready"
Dec 14 16:01:44.404: INFO: Successfully updated pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:01:46.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8824" for this suite. 12/14/22 16:01:46.442
------------------------------
• [SLOW TEST] [8.762 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:01:37.69
    Dec 14 16:01:37.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:01:37.695
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:37.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:37.801
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 12/14/22 16:01:37.804
    Dec 14 16:01:37.816: INFO: Waiting up to 5m0s for pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14" in namespace "downward-api-8824" to be "running and ready"
    Dec 14 16:01:37.837: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 20.383996ms
    Dec 14 16:01:37.837: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:01:39.842: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026163559s
    Dec 14 16:01:39.843: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:01:41.845: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028439332s
    Dec 14 16:01:41.845: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:01:43.844: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14": Phase="Running", Reason="", readiness=true. Elapsed: 6.027923223s
    Dec 14 16:01:43.844: INFO: The phase of Pod labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14 is Running (Ready = true)
    Dec 14 16:01:43.844: INFO: Pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14" satisfied condition "running and ready"
    Dec 14 16:01:44.404: INFO: Successfully updated pod "labelsupdate7360a230-3c1e-4caa-bc58-dcb935102c14"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:01:46.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8824" for this suite. 12/14/22 16:01:46.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:01:46.459
Dec 14 16:01:46.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 16:01:46.46
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:46.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:46.502
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 12/14/22 16:01:46.507
STEP: waiting for pod running 12/14/22 16:01:46.528
Dec 14 16:01:46.529: INFO: Waiting up to 2m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815" to be "running"
Dec 14 16:01:46.533: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286586ms
Dec 14 16:01:48.540: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Running", Reason="", readiness=true. Elapsed: 2.010855001s
Dec 14 16:01:48.540: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" satisfied condition "running"
STEP: creating a file in subpath 12/14/22 16:01:48.54
Dec 14 16:01:48.547: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-815 PodName:var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:01:48.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:01:48.548: INFO: ExecWithOptions: Clientset creation
Dec 14 16:01:48.549: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-815/pods/var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 12/14/22 16:01:48.656
Dec 14 16:01:48.665: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-815 PodName:var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:01:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:01:48.666: INFO: ExecWithOptions: Clientset creation
Dec 14 16:01:48.666: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-815/pods/var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 12/14/22 16:01:48.766
Dec 14 16:01:49.290: INFO: Successfully updated pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848"
STEP: waiting for annotated pod running 12/14/22 16:01:49.29
Dec 14 16:01:49.291: INFO: Waiting up to 2m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815" to be "running"
Dec 14 16:01:49.299: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Running", Reason="", readiness=true. Elapsed: 8.069726ms
Dec 14 16:01:49.299: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 16:01:49.299
Dec 14 16:01:49.299: INFO: Deleting pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815"
Dec 14 16:01:49.312: INFO: Wait up to 5m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:23.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-815" for this suite. 12/14/22 16:02:23.339
------------------------------
• [SLOW TEST] [36.895 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:01:46.459
    Dec 14 16:01:46.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 16:01:46.46
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:01:46.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:01:46.502
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 12/14/22 16:01:46.507
    STEP: waiting for pod running 12/14/22 16:01:46.528
    Dec 14 16:01:46.529: INFO: Waiting up to 2m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815" to be "running"
    Dec 14 16:01:46.533: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286586ms
    Dec 14 16:01:48.540: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Running", Reason="", readiness=true. Elapsed: 2.010855001s
    Dec 14 16:01:48.540: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" satisfied condition "running"
    STEP: creating a file in subpath 12/14/22 16:01:48.54
    Dec 14 16:01:48.547: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-815 PodName:var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:01:48.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:01:48.548: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:01:48.549: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-815/pods/var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 12/14/22 16:01:48.656
    Dec 14 16:01:48.665: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-815 PodName:var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:01:48.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:01:48.666: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:01:48.666: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-815/pods/var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 12/14/22 16:01:48.766
    Dec 14 16:01:49.290: INFO: Successfully updated pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848"
    STEP: waiting for annotated pod running 12/14/22 16:01:49.29
    Dec 14 16:01:49.291: INFO: Waiting up to 2m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815" to be "running"
    Dec 14 16:01:49.299: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848": Phase="Running", Reason="", readiness=true. Elapsed: 8.069726ms
    Dec 14 16:01:49.299: INFO: Pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 16:01:49.299
    Dec 14 16:01:49.299: INFO: Deleting pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" in namespace "var-expansion-815"
    Dec 14 16:01:49.312: INFO: Wait up to 5m0s for pod "var-expansion-bf30dbb3-aeb7-46ce-8bd9-8ebabe15b848" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:23.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-815" for this suite. 12/14/22 16:02:23.339
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:23.363
Dec 14 16:02:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename watch 12/14/22 16:02:23.368
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.405
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 12/14/22 16:02:23.45
STEP: creating a new configmap 12/14/22 16:02:23.455
STEP: modifying the configmap once 12/14/22 16:02:23.468
STEP: closing the watch once it receives two notifications 12/14/22 16:02:23.483
Dec 14 16:02:23.484: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7020 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:02:23.484: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7021 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 12/14/22 16:02:23.485
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/14/22 16:02:23.499
STEP: deleting the configmap 12/14/22 16:02:23.501
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/14/22 16:02:23.511
Dec 14 16:02:23.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7022 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:02:23.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7023 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:23.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5567" for this suite. 12/14/22 16:02:23.522
------------------------------
• [0.170 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:23.363
    Dec 14 16:02:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename watch 12/14/22 16:02:23.368
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.405
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 12/14/22 16:02:23.45
    STEP: creating a new configmap 12/14/22 16:02:23.455
    STEP: modifying the configmap once 12/14/22 16:02:23.468
    STEP: closing the watch once it receives two notifications 12/14/22 16:02:23.483
    Dec 14 16:02:23.484: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7020 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:02:23.484: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7021 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 12/14/22 16:02:23.485
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/14/22 16:02:23.499
    STEP: deleting the configmap 12/14/22 16:02:23.501
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/14/22 16:02:23.511
    Dec 14 16:02:23.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7022 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:02:23.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5567  70fb857b-b694-474b-a293-52820d4b62c4 7023 0 2022-12-14 16:02:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-14 16:02:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:23.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5567" for this suite. 12/14/22 16:02:23.522
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:23.535
Dec 14 16:02:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename csiinlinevolumes 12/14/22 16:02:23.538
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.571
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 12/14/22 16:02:23.576
STEP: getting 12/14/22 16:02:23.604
STEP: listing in namespace 12/14/22 16:02:23.62
STEP: patching 12/14/22 16:02:23.629
STEP: deleting 12/14/22 16:02:23.647
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:23.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3588" for this suite. 12/14/22 16:02:23.688
------------------------------
• [0.162 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:23.535
    Dec 14 16:02:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename csiinlinevolumes 12/14/22 16:02:23.538
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.571
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 12/14/22 16:02:23.576
    STEP: getting 12/14/22 16:02:23.604
    STEP: listing in namespace 12/14/22 16:02:23.62
    STEP: patching 12/14/22 16:02:23.629
    STEP: deleting 12/14/22 16:02:23.647
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:23.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3588" for this suite. 12/14/22 16:02:23.688
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:23.7
Dec 14 16:02:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename cronjob 12/14/22 16:02:23.702
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.733
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 12/14/22 16:02:23.736
STEP: creating 12/14/22 16:02:23.737
STEP: getting 12/14/22 16:02:23.744
STEP: listing 12/14/22 16:02:23.748
STEP: watching 12/14/22 16:02:23.754
Dec 14 16:02:23.754: INFO: starting watch
STEP: cluster-wide listing 12/14/22 16:02:23.755
STEP: cluster-wide watching 12/14/22 16:02:23.762
Dec 14 16:02:23.763: INFO: starting watch
STEP: patching 12/14/22 16:02:23.765
STEP: updating 12/14/22 16:02:23.799
Dec 14 16:02:23.814: INFO: waiting for watch events with expected annotations
Dec 14 16:02:23.814: INFO: saw patched and updated annotations
STEP: patching /status 12/14/22 16:02:23.815
STEP: updating /status 12/14/22 16:02:23.827
STEP: get /status 12/14/22 16:02:23.849
STEP: deleting 12/14/22 16:02:23.861
STEP: deleting a collection 12/14/22 16:02:23.905
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3351" for this suite. 12/14/22 16:02:23.952
------------------------------
• [0.272 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:23.7
    Dec 14 16:02:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename cronjob 12/14/22 16:02:23.702
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:23.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:23.733
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 12/14/22 16:02:23.736
    STEP: creating 12/14/22 16:02:23.737
    STEP: getting 12/14/22 16:02:23.744
    STEP: listing 12/14/22 16:02:23.748
    STEP: watching 12/14/22 16:02:23.754
    Dec 14 16:02:23.754: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 16:02:23.755
    STEP: cluster-wide watching 12/14/22 16:02:23.762
    Dec 14 16:02:23.763: INFO: starting watch
    STEP: patching 12/14/22 16:02:23.765
    STEP: updating 12/14/22 16:02:23.799
    Dec 14 16:02:23.814: INFO: waiting for watch events with expected annotations
    Dec 14 16:02:23.814: INFO: saw patched and updated annotations
    STEP: patching /status 12/14/22 16:02:23.815
    STEP: updating /status 12/14/22 16:02:23.827
    STEP: get /status 12/14/22 16:02:23.849
    STEP: deleting 12/14/22 16:02:23.861
    STEP: deleting a collection 12/14/22 16:02:23.905
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3351" for this suite. 12/14/22 16:02:23.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:23.972
Dec 14 16:02:23.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:02:23.99
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:24.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:24.03
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/14/22 16:02:24.033
Dec 14 16:02:24.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:02:26.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:34.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6945" for this suite. 12/14/22 16:02:34.059
------------------------------
• [SLOW TEST] [10.098 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:23.972
    Dec 14 16:02:23.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:02:23.99
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:24.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:24.03
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/14/22 16:02:24.033
    Dec 14 16:02:24.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:02:26.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:34.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6945" for this suite. 12/14/22 16:02:34.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:34.076
Dec 14 16:02:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:02:34.08
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:34.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:34.112
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 12/14/22 16:02:34.117
Dec 14 16:02:34.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7179 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 12/14/22 16:02:34.223
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7179" for this suite. 12/14/22 16:02:34.255
------------------------------
• [0.187 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:34.076
    Dec 14 16:02:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:02:34.08
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:34.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:34.112
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 12/14/22 16:02:34.117
    Dec 14 16:02:34.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7179 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 12/14/22 16:02:34.223
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:34.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7179" for this suite. 12/14/22 16:02:34.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:34.265
Dec 14 16:02:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:02:34.267
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:34.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:34.298
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9388 12/14/22 16:02:34.303
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[] 12/14/22 16:02:34.318
Dec 14 16:02:34.340: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9388 12/14/22 16:02:34.34
Dec 14 16:02:34.353: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9388" to be "running and ready"
Dec 14 16:02:34.357: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573664ms
Dec 14 16:02:34.357: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:02:36.368: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014271026s
Dec 14 16:02:36.368: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 16:02:36.368: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod1:[100]] 12/14/22 16:02:36.375
Dec 14 16:02:36.396: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9388 12/14/22 16:02:36.396
Dec 14 16:02:36.407: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9388" to be "running and ready"
Dec 14 16:02:36.411: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330066ms
Dec 14 16:02:36.411: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:02:38.426: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01908778s
Dec 14 16:02:38.426: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 16:02:38.426: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod1:[100] pod2:[101]] 12/14/22 16:02:38.432
Dec 14 16:02:38.453: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 12/14/22 16:02:38.453
Dec 14 16:02:38.454: INFO: Creating new exec pod
Dec 14 16:02:38.460: INFO: Waiting up to 5m0s for pod "execpodzt9qx" in namespace "services-9388" to be "running"
Dec 14 16:02:38.465: INFO: Pod "execpodzt9qx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504887ms
Dec 14 16:02:40.474: INFO: Pod "execpodzt9qx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013709868s
Dec 14 16:02:40.474: INFO: Pod "execpodzt9qx" satisfied condition "running"
Dec 14 16:02:41.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Dec 14 16:02:41.762: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec 14 16:02:41.762: INFO: stdout: ""
Dec 14 16:02:41.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 10.233.4.31 80'
Dec 14 16:02:41.995: INFO: stderr: "+ nc -v -z -w 2 10.233.4.31 80\nConnection to 10.233.4.31 80 port [tcp/http] succeeded!\n"
Dec 14 16:02:41.995: INFO: stdout: ""
Dec 14 16:02:41.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Dec 14 16:02:42.225: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec 14 16:02:42.225: INFO: stdout: ""
Dec 14 16:02:42.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 10.233.4.31 81'
Dec 14 16:02:42.433: INFO: stderr: "+ nc -v -z -w 2 10.233.4.31 81\nConnection to 10.233.4.31 81 port [tcp/*] succeeded!\n"
Dec 14 16:02:42.433: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9388 12/14/22 16:02:42.433
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod2:[101]] 12/14/22 16:02:42.453
Dec 14 16:02:42.494: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9388 12/14/22 16:02:42.494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[] 12/14/22 16:02:42.538
Dec 14 16:02:43.583: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9388" for this suite. 12/14/22 16:02:43.625
------------------------------
• [SLOW TEST] [9.382 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:34.265
    Dec 14 16:02:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:02:34.267
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:34.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:34.298
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9388 12/14/22 16:02:34.303
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[] 12/14/22 16:02:34.318
    Dec 14 16:02:34.340: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9388 12/14/22 16:02:34.34
    Dec 14 16:02:34.353: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9388" to be "running and ready"
    Dec 14 16:02:34.357: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573664ms
    Dec 14 16:02:34.357: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:02:36.368: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014271026s
    Dec 14 16:02:36.368: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 16:02:36.368: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod1:[100]] 12/14/22 16:02:36.375
    Dec 14 16:02:36.396: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9388 12/14/22 16:02:36.396
    Dec 14 16:02:36.407: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9388" to be "running and ready"
    Dec 14 16:02:36.411: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330066ms
    Dec 14 16:02:36.411: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:02:38.426: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01908778s
    Dec 14 16:02:38.426: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 16:02:38.426: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod1:[100] pod2:[101]] 12/14/22 16:02:38.432
    Dec 14 16:02:38.453: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 12/14/22 16:02:38.453
    Dec 14 16:02:38.454: INFO: Creating new exec pod
    Dec 14 16:02:38.460: INFO: Waiting up to 5m0s for pod "execpodzt9qx" in namespace "services-9388" to be "running"
    Dec 14 16:02:38.465: INFO: Pod "execpodzt9qx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504887ms
    Dec 14 16:02:40.474: INFO: Pod "execpodzt9qx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013709868s
    Dec 14 16:02:40.474: INFO: Pod "execpodzt9qx" satisfied condition "running"
    Dec 14 16:02:41.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Dec 14 16:02:41.762: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Dec 14 16:02:41.762: INFO: stdout: ""
    Dec 14 16:02:41.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 10.233.4.31 80'
    Dec 14 16:02:41.995: INFO: stderr: "+ nc -v -z -w 2 10.233.4.31 80\nConnection to 10.233.4.31 80 port [tcp/http] succeeded!\n"
    Dec 14 16:02:41.995: INFO: stdout: ""
    Dec 14 16:02:41.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Dec 14 16:02:42.225: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Dec 14 16:02:42.225: INFO: stdout: ""
    Dec 14 16:02:42.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9388 exec execpodzt9qx -- /bin/sh -x -c nc -v -z -w 2 10.233.4.31 81'
    Dec 14 16:02:42.433: INFO: stderr: "+ nc -v -z -w 2 10.233.4.31 81\nConnection to 10.233.4.31 81 port [tcp/*] succeeded!\n"
    Dec 14 16:02:42.433: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9388 12/14/22 16:02:42.433
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[pod2:[101]] 12/14/22 16:02:42.453
    Dec 14 16:02:42.494: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9388 12/14/22 16:02:42.494
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9388 to expose endpoints map[] 12/14/22 16:02:42.538
    Dec 14 16:02:43.583: INFO: successfully validated that service multi-endpoint-test in namespace services-9388 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9388" for this suite. 12/14/22 16:02:43.625
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:43.649
Dec 14 16:02:43.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 16:02:43.651
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:43.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:43.695
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 12/14/22 16:02:43.7
Dec 14 16:02:43.714: INFO: Waiting up to 5m0s for pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669" in namespace "var-expansion-6480" to be "Succeeded or Failed"
Dec 14 16:02:43.726: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 12.166619ms
Dec 14 16:02:45.738: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024578591s
Dec 14 16:02:47.734: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020011428s
Dec 14 16:02:49.735: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021367141s
Dec 14 16:02:51.736: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022029781s
STEP: Saw pod success 12/14/22 16:02:51.736
Dec 14 16:02:51.737: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669" satisfied condition "Succeeded or Failed"
Dec 14 16:02:51.745: INFO: Trying to get logs from node iet9eich7uhu-1 pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:02:51.771
Dec 14 16:02:51.807: INFO: Waiting for pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 to disappear
Dec 14 16:02:51.814: INFO: Pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:51.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6480" for this suite. 12/14/22 16:02:51.822
------------------------------
• [SLOW TEST] [8.207 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:43.649
    Dec 14 16:02:43.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 16:02:43.651
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:43.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:43.695
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 12/14/22 16:02:43.7
    Dec 14 16:02:43.714: INFO: Waiting up to 5m0s for pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669" in namespace "var-expansion-6480" to be "Succeeded or Failed"
    Dec 14 16:02:43.726: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 12.166619ms
    Dec 14 16:02:45.738: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024578591s
    Dec 14 16:02:47.734: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020011428s
    Dec 14 16:02:49.735: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021367141s
    Dec 14 16:02:51.736: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022029781s
    STEP: Saw pod success 12/14/22 16:02:51.736
    Dec 14 16:02:51.737: INFO: Pod "var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669" satisfied condition "Succeeded or Failed"
    Dec 14 16:02:51.745: INFO: Trying to get logs from node iet9eich7uhu-1 pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:02:51.771
    Dec 14 16:02:51.807: INFO: Waiting for pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 to disappear
    Dec 14 16:02:51.814: INFO: Pod var-expansion-d5a401a7-c931-4d37-ba87-4bb872935669 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:51.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6480" for this suite. 12/14/22 16:02:51.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:51.858
Dec 14 16:02:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 16:02:51.867
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:51.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:51.901
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 12/14/22 16:02:51.911
STEP: waiting for RC to be added 12/14/22 16:02:51.921
STEP: waiting for available Replicas 12/14/22 16:02:51.922
STEP: patching ReplicationController 12/14/22 16:02:53.358
STEP: waiting for RC to be modified 12/14/22 16:02:53.375
STEP: patching ReplicationController status 12/14/22 16:02:53.375
STEP: waiting for RC to be modified 12/14/22 16:02:53.384
STEP: waiting for available Replicas 12/14/22 16:02:53.385
STEP: fetching ReplicationController status 12/14/22 16:02:53.393
STEP: patching ReplicationController scale 12/14/22 16:02:53.397
STEP: waiting for RC to be modified 12/14/22 16:02:53.406
STEP: waiting for ReplicationController's scale to be the max amount 12/14/22 16:02:53.407
STEP: fetching ReplicationController; ensuring that it's patched 12/14/22 16:02:54.554
STEP: updating ReplicationController status 12/14/22 16:02:54.559
STEP: waiting for RC to be modified 12/14/22 16:02:54.576
STEP: listing all ReplicationControllers 12/14/22 16:02:54.578
STEP: checking that ReplicationController has expected values 12/14/22 16:02:54.611
STEP: deleting ReplicationControllers by collection 12/14/22 16:02:54.612
STEP: waiting for ReplicationController to have a DELETED watchEvent 12/14/22 16:02:54.629
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:54.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9477" for this suite. 12/14/22 16:02:54.725
------------------------------
• [2.877 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:51.858
    Dec 14 16:02:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 16:02:51.867
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:51.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:51.901
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 12/14/22 16:02:51.911
    STEP: waiting for RC to be added 12/14/22 16:02:51.921
    STEP: waiting for available Replicas 12/14/22 16:02:51.922
    STEP: patching ReplicationController 12/14/22 16:02:53.358
    STEP: waiting for RC to be modified 12/14/22 16:02:53.375
    STEP: patching ReplicationController status 12/14/22 16:02:53.375
    STEP: waiting for RC to be modified 12/14/22 16:02:53.384
    STEP: waiting for available Replicas 12/14/22 16:02:53.385
    STEP: fetching ReplicationController status 12/14/22 16:02:53.393
    STEP: patching ReplicationController scale 12/14/22 16:02:53.397
    STEP: waiting for RC to be modified 12/14/22 16:02:53.406
    STEP: waiting for ReplicationController's scale to be the max amount 12/14/22 16:02:53.407
    STEP: fetching ReplicationController; ensuring that it's patched 12/14/22 16:02:54.554
    STEP: updating ReplicationController status 12/14/22 16:02:54.559
    STEP: waiting for RC to be modified 12/14/22 16:02:54.576
    STEP: listing all ReplicationControllers 12/14/22 16:02:54.578
    STEP: checking that ReplicationController has expected values 12/14/22 16:02:54.611
    STEP: deleting ReplicationControllers by collection 12/14/22 16:02:54.612
    STEP: waiting for ReplicationController to have a DELETED watchEvent 12/14/22 16:02:54.629
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:54.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9477" for this suite. 12/14/22 16:02:54.725
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:54.736
Dec 14 16:02:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 16:02:54.738
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:54.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:54.764
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 12/14/22 16:02:54.768
Dec 14 16:02:54.781: INFO: Waiting up to 5m0s for pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e" in namespace "var-expansion-6594" to be "Succeeded or Failed"
Dec 14 16:02:54.785: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.655894ms
Dec 14 16:02:56.794: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01319614s
Dec 14 16:02:58.793: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012163133s
STEP: Saw pod success 12/14/22 16:02:58.793
Dec 14 16:02:58.794: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e" satisfied condition "Succeeded or Failed"
Dec 14 16:02:58.800: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:02:58.811
Dec 14 16:02:58.826: INFO: Waiting for pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e to disappear
Dec 14 16:02:58.829: INFO: Pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 16:02:58.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6594" for this suite. 12/14/22 16:02:58.835
------------------------------
• [4.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:54.736
    Dec 14 16:02:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 16:02:54.738
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:54.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:54.764
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 12/14/22 16:02:54.768
    Dec 14 16:02:54.781: INFO: Waiting up to 5m0s for pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e" in namespace "var-expansion-6594" to be "Succeeded or Failed"
    Dec 14 16:02:54.785: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.655894ms
    Dec 14 16:02:56.794: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01319614s
    Dec 14 16:02:58.793: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012163133s
    STEP: Saw pod success 12/14/22 16:02:58.793
    Dec 14 16:02:58.794: INFO: Pod "var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e" satisfied condition "Succeeded or Failed"
    Dec 14 16:02:58.800: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:02:58.811
    Dec 14 16:02:58.826: INFO: Waiting for pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e to disappear
    Dec 14 16:02:58.829: INFO: Pod var-expansion-d32524fd-2259-4840-985c-98a6fea2d24e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:02:58.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6594" for this suite. 12/14/22 16:02:58.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:02:58.85
Dec 14 16:02:58.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:02:58.851
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:58.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:58.88
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Dec 14 16:02:58.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: creating the pod 12/14/22 16:02:58.886
STEP: submitting the pod to kubernetes 12/14/22 16:02:58.887
Dec 14 16:02:58.900: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e" in namespace "pods-710" to be "running and ready"
Dec 14 16:02:58.910: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.525543ms
Dec 14 16:02:58.910: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:03:00.918: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01762772s
Dec 14 16:03:00.918: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:03:02.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018501142s
Dec 14 16:03:02.919: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:03:04.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Running", Reason="", readiness=true. Elapsed: 6.01880059s
Dec 14 16:03:04.919: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Running (Ready = true)
Dec 14 16:03:04.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:04.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-710" for this suite. 12/14/22 16:03:04.981
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:02:58.85
    Dec 14 16:02:58.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:02:58.851
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:02:58.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:02:58.88
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Dec 14 16:02:58.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: creating the pod 12/14/22 16:02:58.886
    STEP: submitting the pod to kubernetes 12/14/22 16:02:58.887
    Dec 14 16:02:58.900: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e" in namespace "pods-710" to be "running and ready"
    Dec 14 16:02:58.910: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.525543ms
    Dec 14 16:02:58.910: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:03:00.918: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01762772s
    Dec 14 16:03:00.918: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:03:02.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018501142s
    Dec 14 16:03:02.919: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:03:04.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e": Phase="Running", Reason="", readiness=true. Elapsed: 6.01880059s
    Dec 14 16:03:04.919: INFO: The phase of Pod pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e is Running (Ready = true)
    Dec 14 16:03:04.919: INFO: Pod "pod-logs-websocket-ff3c6583-0d1d-4405-9c55-3b21ae4e878e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:04.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-710" for this suite. 12/14/22 16:03:04.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:04.994
Dec 14 16:03:04.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename init-container 12/14/22 16:03:04.998
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:05.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:05.037
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 12/14/22 16:03:05.042
Dec 14 16:03:05.042: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5405" for this suite. 12/14/22 16:03:10.488
------------------------------
• [SLOW TEST] [5.505 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:04.994
    Dec 14 16:03:04.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename init-container 12/14/22 16:03:04.998
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:05.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:05.037
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 12/14/22 16:03:05.042
    Dec 14 16:03:05.042: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5405" for this suite. 12/14/22 16:03:10.488
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:10.501
Dec 14 16:03:10.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename proxy 12/14/22 16:03:10.504
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:10.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:10.54
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Dec 14 16:03:10.551: INFO: Creating pod...
Dec 14 16:03:10.566: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5021" to be "running"
Dec 14 16:03:10.577: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.220421ms
Dec 14 16:03:12.589: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.023682083s
Dec 14 16:03:12.590: INFO: Pod "agnhost" satisfied condition "running"
Dec 14 16:03:12.590: INFO: Creating service...
Dec 14 16:03:12.614: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/DELETE
Dec 14 16:03:12.629: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:03:12.629: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/GET
Dec 14 16:03:12.634: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 16:03:12.635: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/HEAD
Dec 14 16:03:12.639: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 16:03:12.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 14 16:03:12.644: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:03:12.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/PATCH
Dec 14 16:03:12.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:03:12.654: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/POST
Dec 14 16:03:12.664: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:03:12.664: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/PUT
Dec 14 16:03:12.682: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 16:03:12.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/DELETE
Dec 14 16:03:12.691: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:03:12.691: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/GET
Dec 14 16:03:12.701: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 16:03:12.701: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/HEAD
Dec 14 16:03:12.708: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 16:03:12.708: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/OPTIONS
Dec 14 16:03:12.716: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:03:12.717: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/PATCH
Dec 14 16:03:12.726: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:03:12.726: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/POST
Dec 14 16:03:12.734: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:03:12.735: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/PUT
Dec 14 16:03:12.751: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:12.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5021" for this suite. 12/14/22 16:03:12.761
------------------------------
• [2.272 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:10.501
    Dec 14 16:03:10.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename proxy 12/14/22 16:03:10.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:10.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:10.54
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Dec 14 16:03:10.551: INFO: Creating pod...
    Dec 14 16:03:10.566: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5021" to be "running"
    Dec 14 16:03:10.577: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.220421ms
    Dec 14 16:03:12.589: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.023682083s
    Dec 14 16:03:12.590: INFO: Pod "agnhost" satisfied condition "running"
    Dec 14 16:03:12.590: INFO: Creating service...
    Dec 14 16:03:12.614: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/DELETE
    Dec 14 16:03:12.629: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 16:03:12.629: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/GET
    Dec 14 16:03:12.634: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 14 16:03:12.635: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/HEAD
    Dec 14 16:03:12.639: INFO: http.Client request:HEAD | StatusCode:200
    Dec 14 16:03:12.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/OPTIONS
    Dec 14 16:03:12.644: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 16:03:12.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/PATCH
    Dec 14 16:03:12.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 16:03:12.654: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/POST
    Dec 14 16:03:12.664: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 16:03:12.664: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/pods/agnhost/proxy/some/path/with/PUT
    Dec 14 16:03:12.682: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 16:03:12.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/DELETE
    Dec 14 16:03:12.691: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 16:03:12.691: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/GET
    Dec 14 16:03:12.701: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 14 16:03:12.701: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/HEAD
    Dec 14 16:03:12.708: INFO: http.Client request:HEAD | StatusCode:200
    Dec 14 16:03:12.708: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/OPTIONS
    Dec 14 16:03:12.716: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 16:03:12.717: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/PATCH
    Dec 14 16:03:12.726: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 16:03:12.726: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/POST
    Dec 14 16:03:12.734: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 16:03:12.735: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5021/services/test-service/proxy/some/path/with/PUT
    Dec 14 16:03:12.751: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:12.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5021" for this suite. 12/14/22 16:03:12.761
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:12.778
Dec 14 16:03:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 16:03:12.785
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:12.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:12.815
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 12/14/22 16:03:12.82
STEP: Ensuring active pods == parallelism 12/14/22 16:03:12.829
STEP: delete a job 12/14/22 16:03:14.838
STEP: deleting Job.batch foo in namespace job-5800, will wait for the garbage collector to delete the pods 12/14/22 16:03:14.839
Dec 14 16:03:14.909: INFO: Deleting Job.batch foo took: 12.265928ms
Dec 14 16:03:15.010: INFO: Terminating Job.batch foo pods took: 100.902874ms
STEP: Ensuring job was deleted 12/14/22 16:03:47.711
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5800" for this suite. 12/14/22 16:03:47.73
------------------------------
• [SLOW TEST] [34.963 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:12.778
    Dec 14 16:03:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 16:03:12.785
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:12.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:12.815
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 12/14/22 16:03:12.82
    STEP: Ensuring active pods == parallelism 12/14/22 16:03:12.829
    STEP: delete a job 12/14/22 16:03:14.838
    STEP: deleting Job.batch foo in namespace job-5800, will wait for the garbage collector to delete the pods 12/14/22 16:03:14.839
    Dec 14 16:03:14.909: INFO: Deleting Job.batch foo took: 12.265928ms
    Dec 14 16:03:15.010: INFO: Terminating Job.batch foo pods took: 100.902874ms
    STEP: Ensuring job was deleted 12/14/22 16:03:47.711
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5800" for this suite. 12/14/22 16:03:47.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:47.742
Dec 14 16:03:47.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:03:47.746
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:47.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:47.779
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-dsmnn"  12/14/22 16:03:47.783
Dec 14 16:03:47.792: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-dsmnn"  12/14/22 16:03:47.792
Dec 14 16:03:47.814: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:47.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4355" for this suite. 12/14/22 16:03:47.819
------------------------------
• [0.086 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:47.742
    Dec 14 16:03:47.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:03:47.746
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:47.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:47.779
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-dsmnn"  12/14/22 16:03:47.783
    Dec 14 16:03:47.792: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-dsmnn"  12/14/22 16:03:47.792
    Dec 14 16:03:47.814: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:47.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4355" for this suite. 12/14/22 16:03:47.819
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:47.829
Dec 14 16:03:47.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:03:47.831
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:47.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:47.864
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 12/14/22 16:03:47.867
Dec 14 16:03:47.880: INFO: Waiting up to 5m0s for pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b" in namespace "downward-api-1116" to be "Succeeded or Failed"
Dec 14 16:03:47.885: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.806558ms
Dec 14 16:03:49.893: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012399573s
Dec 14 16:03:51.891: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010246515s
STEP: Saw pod success 12/14/22 16:03:51.891
Dec 14 16:03:51.891: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b" satisfied condition "Succeeded or Failed"
Dec 14 16:03:51.896: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:03:51.909
Dec 14 16:03:51.948: INFO: Waiting for pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b to disappear
Dec 14 16:03:51.953: INFO: Pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:03:51.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1116" for this suite. 12/14/22 16:03:51.961
------------------------------
• [4.145 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:47.829
    Dec 14 16:03:47.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:03:47.831
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:47.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:47.864
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 12/14/22 16:03:47.867
    Dec 14 16:03:47.880: INFO: Waiting up to 5m0s for pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b" in namespace "downward-api-1116" to be "Succeeded or Failed"
    Dec 14 16:03:47.885: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.806558ms
    Dec 14 16:03:49.893: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012399573s
    Dec 14 16:03:51.891: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010246515s
    STEP: Saw pod success 12/14/22 16:03:51.891
    Dec 14 16:03:51.891: INFO: Pod "downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b" satisfied condition "Succeeded or Failed"
    Dec 14 16:03:51.896: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:03:51.909
    Dec 14 16:03:51.948: INFO: Waiting for pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b to disappear
    Dec 14 16:03:51.953: INFO: Pod downward-api-5b02cf6f-fa6a-498c-91c1-48eb32d0dd1b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:03:51.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1116" for this suite. 12/14/22 16:03:51.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:03:51.976
Dec 14 16:03:51.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-webhook 12/14/22 16:03:51.982
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:52.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:52.03
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/14/22 16:03:52.034
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 16:03:53.587
STEP: Deploying the custom resource conversion webhook pod 12/14/22 16:03:53.602
STEP: Wait for the deployment to be ready 12/14/22 16:03:53.619
Dec 14 16:03:53.631: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 16:03:55.664
STEP: Verifying the service has paired with the endpoint 12/14/22 16:03:55.693
Dec 14 16:03:56.694: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Dec 14 16:03:56.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Creating a v1 custom resource 12/14/22 16:03:59.522
STEP: v2 custom resource should be converted 12/14/22 16:03:59.53
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:04:00.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6936" for this suite. 12/14/22 16:04:00.152
------------------------------
• [SLOW TEST] [8.232 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:03:51.976
    Dec 14 16:03:51.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-webhook 12/14/22 16:03:51.982
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:03:52.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:03:52.03
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/14/22 16:03:52.034
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 16:03:53.587
    STEP: Deploying the custom resource conversion webhook pod 12/14/22 16:03:53.602
    STEP: Wait for the deployment to be ready 12/14/22 16:03:53.619
    Dec 14 16:03:53.631: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 16:03:55.664
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:03:55.693
    Dec 14 16:03:56.694: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Dec 14 16:03:56.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Creating a v1 custom resource 12/14/22 16:03:59.522
    STEP: v2 custom resource should be converted 12/14/22 16:03:59.53
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:04:00.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6936" for this suite. 12/14/22 16:04:00.152
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:04:00.209
Dec 14 16:04:00.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename ingress 12/14/22 16:04:00.219
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:00.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:00.255
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 12/14/22 16:04:00.263
STEP: getting /apis/networking.k8s.io 12/14/22 16:04:00.267
STEP: getting /apis/networking.k8s.iov1 12/14/22 16:04:00.27
STEP: creating 12/14/22 16:04:00.271
STEP: getting 12/14/22 16:04:00.305
STEP: listing 12/14/22 16:04:00.314
STEP: watching 12/14/22 16:04:00.323
Dec 14 16:04:00.324: INFO: starting watch
STEP: cluster-wide listing 12/14/22 16:04:00.327
STEP: cluster-wide watching 12/14/22 16:04:00.34
Dec 14 16:04:00.340: INFO: starting watch
STEP: patching 12/14/22 16:04:00.343
STEP: updating 12/14/22 16:04:00.355
Dec 14 16:04:00.370: INFO: waiting for watch events with expected annotations
Dec 14 16:04:00.370: INFO: saw patched and updated annotations
STEP: patching /status 12/14/22 16:04:00.371
STEP: updating /status 12/14/22 16:04:00.38
STEP: get /status 12/14/22 16:04:00.395
STEP: deleting 12/14/22 16:04:00.4
STEP: deleting a collection 12/14/22 16:04:00.422
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:04:00.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-10" for this suite. 12/14/22 16:04:00.455
------------------------------
• [0.261 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:04:00.209
    Dec 14 16:04:00.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename ingress 12/14/22 16:04:00.219
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:00.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:00.255
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 12/14/22 16:04:00.263
    STEP: getting /apis/networking.k8s.io 12/14/22 16:04:00.267
    STEP: getting /apis/networking.k8s.iov1 12/14/22 16:04:00.27
    STEP: creating 12/14/22 16:04:00.271
    STEP: getting 12/14/22 16:04:00.305
    STEP: listing 12/14/22 16:04:00.314
    STEP: watching 12/14/22 16:04:00.323
    Dec 14 16:04:00.324: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 16:04:00.327
    STEP: cluster-wide watching 12/14/22 16:04:00.34
    Dec 14 16:04:00.340: INFO: starting watch
    STEP: patching 12/14/22 16:04:00.343
    STEP: updating 12/14/22 16:04:00.355
    Dec 14 16:04:00.370: INFO: waiting for watch events with expected annotations
    Dec 14 16:04:00.370: INFO: saw patched and updated annotations
    STEP: patching /status 12/14/22 16:04:00.371
    STEP: updating /status 12/14/22 16:04:00.38
    STEP: get /status 12/14/22 16:04:00.395
    STEP: deleting 12/14/22 16:04:00.4
    STEP: deleting a collection 12/14/22 16:04:00.422
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:04:00.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-10" for this suite. 12/14/22 16:04:00.455
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:04:00.472
Dec 14 16:04:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:04:00.475
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:00.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:00.505
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:04:00.509
Dec 14 16:04:00.523: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa" in namespace "downward-api-3651" to be "Succeeded or Failed"
Dec 14 16:04:00.528: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382618ms
Dec 14 16:04:02.550: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027245428s
Dec 14 16:04:04.537: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544624s
STEP: Saw pod success 12/14/22 16:04:04.537
Dec 14 16:04:04.537: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa" satisfied condition "Succeeded or Failed"
Dec 14 16:04:04.543: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa container client-container: <nil>
STEP: delete the pod 12/14/22 16:04:04.559
Dec 14 16:04:04.580: INFO: Waiting for pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa to disappear
Dec 14 16:04:04.584: INFO: Pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:04:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3651" for this suite. 12/14/22 16:04:04.591
------------------------------
• [4.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:04:00.472
    Dec 14 16:04:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:04:00.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:00.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:00.505
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:04:00.509
    Dec 14 16:04:00.523: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa" in namespace "downward-api-3651" to be "Succeeded or Failed"
    Dec 14 16:04:00.528: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382618ms
    Dec 14 16:04:02.550: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027245428s
    Dec 14 16:04:04.537: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013544624s
    STEP: Saw pod success 12/14/22 16:04:04.537
    Dec 14 16:04:04.537: INFO: Pod "downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa" satisfied condition "Succeeded or Failed"
    Dec 14 16:04:04.543: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa container client-container: <nil>
    STEP: delete the pod 12/14/22 16:04:04.559
    Dec 14 16:04:04.580: INFO: Waiting for pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa to disappear
    Dec 14 16:04:04.584: INFO: Pod downwardapi-volume-c78791a6-0777-475f-ab89-bd3014155eaa no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:04:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3651" for this suite. 12/14/22 16:04:04.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:04:04.614
Dec 14 16:04:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:04:04.616
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:04.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:04.653
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 12/14/22 16:04:04.667
STEP: delete the rc 12/14/22 16:04:09.709
STEP: wait for the rc to be deleted 12/14/22 16:04:09.737
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/14/22 16:04:14.754
STEP: Gathering metrics 12/14/22 16:04:44.783
Dec 14 16:04:44.828: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 16:04:44.833: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.686489ms
Dec 14 16:04:44.833: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 16:04:44.833: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 16:04:44.998: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 16:04:44.998: INFO: Deleting pod "simpletest.rc-2mwp9" in namespace "gc-8491"
Dec 14 16:04:45.022: INFO: Deleting pod "simpletest.rc-2vnvx" in namespace "gc-8491"
Dec 14 16:04:45.071: INFO: Deleting pod "simpletest.rc-4dtvs" in namespace "gc-8491"
Dec 14 16:04:45.102: INFO: Deleting pod "simpletest.rc-4l8kw" in namespace "gc-8491"
Dec 14 16:04:45.145: INFO: Deleting pod "simpletest.rc-4lkrn" in namespace "gc-8491"
Dec 14 16:04:45.205: INFO: Deleting pod "simpletest.rc-4pllt" in namespace "gc-8491"
Dec 14 16:04:45.259: INFO: Deleting pod "simpletest.rc-4s6xt" in namespace "gc-8491"
Dec 14 16:04:45.352: INFO: Deleting pod "simpletest.rc-58xqg" in namespace "gc-8491"
Dec 14 16:04:45.432: INFO: Deleting pod "simpletest.rc-5p6g5" in namespace "gc-8491"
Dec 14 16:04:45.472: INFO: Deleting pod "simpletest.rc-5qlfl" in namespace "gc-8491"
Dec 14 16:04:45.497: INFO: Deleting pod "simpletest.rc-65grt" in namespace "gc-8491"
Dec 14 16:04:45.525: INFO: Deleting pod "simpletest.rc-665gc" in namespace "gc-8491"
Dec 14 16:04:45.553: INFO: Deleting pod "simpletest.rc-69h2c" in namespace "gc-8491"
Dec 14 16:04:45.597: INFO: Deleting pod "simpletest.rc-6f24z" in namespace "gc-8491"
Dec 14 16:04:45.628: INFO: Deleting pod "simpletest.rc-6jsns" in namespace "gc-8491"
Dec 14 16:04:45.650: INFO: Deleting pod "simpletest.rc-6sf2x" in namespace "gc-8491"
Dec 14 16:04:45.683: INFO: Deleting pod "simpletest.rc-74xw9" in namespace "gc-8491"
Dec 14 16:04:45.713: INFO: Deleting pod "simpletest.rc-77cn8" in namespace "gc-8491"
Dec 14 16:04:45.734: INFO: Deleting pod "simpletest.rc-7nwk5" in namespace "gc-8491"
Dec 14 16:04:45.769: INFO: Deleting pod "simpletest.rc-7qgvl" in namespace "gc-8491"
Dec 14 16:04:45.803: INFO: Deleting pod "simpletest.rc-7thsp" in namespace "gc-8491"
Dec 14 16:04:45.845: INFO: Deleting pod "simpletest.rc-84lv5" in namespace "gc-8491"
Dec 14 16:04:45.879: INFO: Deleting pod "simpletest.rc-85kq9" in namespace "gc-8491"
Dec 14 16:04:45.908: INFO: Deleting pod "simpletest.rc-8x9zd" in namespace "gc-8491"
Dec 14 16:04:45.941: INFO: Deleting pod "simpletest.rc-9gwkx" in namespace "gc-8491"
Dec 14 16:04:45.971: INFO: Deleting pod "simpletest.rc-9kkkt" in namespace "gc-8491"
Dec 14 16:04:46.098: INFO: Deleting pod "simpletest.rc-9njht" in namespace "gc-8491"
Dec 14 16:04:46.128: INFO: Deleting pod "simpletest.rc-9t9lv" in namespace "gc-8491"
Dec 14 16:04:46.183: INFO: Deleting pod "simpletest.rc-bqxhm" in namespace "gc-8491"
Dec 14 16:04:46.243: INFO: Deleting pod "simpletest.rc-btcgs" in namespace "gc-8491"
Dec 14 16:04:46.306: INFO: Deleting pod "simpletest.rc-c87t7" in namespace "gc-8491"
Dec 14 16:04:46.334: INFO: Deleting pod "simpletest.rc-cdxdr" in namespace "gc-8491"
Dec 14 16:04:46.398: INFO: Deleting pod "simpletest.rc-cpt9k" in namespace "gc-8491"
Dec 14 16:04:46.430: INFO: Deleting pod "simpletest.rc-cw58z" in namespace "gc-8491"
Dec 14 16:04:46.463: INFO: Deleting pod "simpletest.rc-czzff" in namespace "gc-8491"
Dec 14 16:04:46.522: INFO: Deleting pod "simpletest.rc-d2wnj" in namespace "gc-8491"
Dec 14 16:04:46.574: INFO: Deleting pod "simpletest.rc-d6lrh" in namespace "gc-8491"
Dec 14 16:04:46.626: INFO: Deleting pod "simpletest.rc-d9mz2" in namespace "gc-8491"
Dec 14 16:04:46.662: INFO: Deleting pod "simpletest.rc-dc4jj" in namespace "gc-8491"
Dec 14 16:04:46.704: INFO: Deleting pod "simpletest.rc-dfr2f" in namespace "gc-8491"
Dec 14 16:04:46.763: INFO: Deleting pod "simpletest.rc-fdtkw" in namespace "gc-8491"
Dec 14 16:04:46.797: INFO: Deleting pod "simpletest.rc-fj2zl" in namespace "gc-8491"
Dec 14 16:04:46.833: INFO: Deleting pod "simpletest.rc-flh5g" in namespace "gc-8491"
Dec 14 16:04:46.880: INFO: Deleting pod "simpletest.rc-g4t5z" in namespace "gc-8491"
Dec 14 16:04:46.930: INFO: Deleting pod "simpletest.rc-ghzcv" in namespace "gc-8491"
Dec 14 16:04:46.967: INFO: Deleting pod "simpletest.rc-gjphx" in namespace "gc-8491"
Dec 14 16:04:47.077: INFO: Deleting pod "simpletest.rc-gql2z" in namespace "gc-8491"
Dec 14 16:04:47.158: INFO: Deleting pod "simpletest.rc-gwp49" in namespace "gc-8491"
Dec 14 16:04:47.247: INFO: Deleting pod "simpletest.rc-hj69c" in namespace "gc-8491"
Dec 14 16:04:47.286: INFO: Deleting pod "simpletest.rc-hkvxk" in namespace "gc-8491"
Dec 14 16:04:47.308: INFO: Deleting pod "simpletest.rc-hpdmz" in namespace "gc-8491"
Dec 14 16:04:47.361: INFO: Deleting pod "simpletest.rc-hzd7w" in namespace "gc-8491"
Dec 14 16:04:47.389: INFO: Deleting pod "simpletest.rc-jkpz2" in namespace "gc-8491"
Dec 14 16:04:47.428: INFO: Deleting pod "simpletest.rc-jm6k2" in namespace "gc-8491"
Dec 14 16:04:47.514: INFO: Deleting pod "simpletest.rc-k7vgs" in namespace "gc-8491"
Dec 14 16:04:47.550: INFO: Deleting pod "simpletest.rc-klkh6" in namespace "gc-8491"
Dec 14 16:04:47.592: INFO: Deleting pod "simpletest.rc-kvx48" in namespace "gc-8491"
Dec 14 16:04:47.643: INFO: Deleting pod "simpletest.rc-kzl9h" in namespace "gc-8491"
Dec 14 16:04:47.684: INFO: Deleting pod "simpletest.rc-l7qlg" in namespace "gc-8491"
Dec 14 16:04:47.725: INFO: Deleting pod "simpletest.rc-l9q7f" in namespace "gc-8491"
Dec 14 16:04:47.794: INFO: Deleting pod "simpletest.rc-lnmmt" in namespace "gc-8491"
Dec 14 16:04:47.986: INFO: Deleting pod "simpletest.rc-mk6hj" in namespace "gc-8491"
Dec 14 16:04:48.065: INFO: Deleting pod "simpletest.rc-mr89l" in namespace "gc-8491"
Dec 14 16:04:48.155: INFO: Deleting pod "simpletest.rc-mzxmt" in namespace "gc-8491"
Dec 14 16:04:48.204: INFO: Deleting pod "simpletest.rc-n5hhc" in namespace "gc-8491"
Dec 14 16:04:48.268: INFO: Deleting pod "simpletest.rc-nh6wb" in namespace "gc-8491"
Dec 14 16:04:48.306: INFO: Deleting pod "simpletest.rc-p85w9" in namespace "gc-8491"
Dec 14 16:04:48.336: INFO: Deleting pod "simpletest.rc-pfsp8" in namespace "gc-8491"
Dec 14 16:04:48.365: INFO: Deleting pod "simpletest.rc-pxf5c" in namespace "gc-8491"
Dec 14 16:04:48.402: INFO: Deleting pod "simpletest.rc-pzd4g" in namespace "gc-8491"
Dec 14 16:04:48.425: INFO: Deleting pod "simpletest.rc-q9v44" in namespace "gc-8491"
Dec 14 16:04:48.456: INFO: Deleting pod "simpletest.rc-qmdhr" in namespace "gc-8491"
Dec 14 16:04:48.481: INFO: Deleting pod "simpletest.rc-qmrn2" in namespace "gc-8491"
Dec 14 16:04:48.511: INFO: Deleting pod "simpletest.rc-qn4p2" in namespace "gc-8491"
Dec 14 16:04:48.541: INFO: Deleting pod "simpletest.rc-r7r2k" in namespace "gc-8491"
Dec 14 16:04:48.587: INFO: Deleting pod "simpletest.rc-rjskg" in namespace "gc-8491"
Dec 14 16:04:48.640: INFO: Deleting pod "simpletest.rc-rkw4p" in namespace "gc-8491"
Dec 14 16:04:48.698: INFO: Deleting pod "simpletest.rc-sfbjn" in namespace "gc-8491"
Dec 14 16:04:48.761: INFO: Deleting pod "simpletest.rc-sjmkg" in namespace "gc-8491"
Dec 14 16:04:48.799: INFO: Deleting pod "simpletest.rc-tdf68" in namespace "gc-8491"
Dec 14 16:04:48.844: INFO: Deleting pod "simpletest.rc-tmcc9" in namespace "gc-8491"
Dec 14 16:04:48.871: INFO: Deleting pod "simpletest.rc-v9lfn" in namespace "gc-8491"
Dec 14 16:04:48.928: INFO: Deleting pod "simpletest.rc-vcfhd" in namespace "gc-8491"
Dec 14 16:04:48.993: INFO: Deleting pod "simpletest.rc-vj9x7" in namespace "gc-8491"
Dec 14 16:04:49.056: INFO: Deleting pod "simpletest.rc-vm6hl" in namespace "gc-8491"
Dec 14 16:04:49.078: INFO: Deleting pod "simpletest.rc-vxbqc" in namespace "gc-8491"
Dec 14 16:04:49.110: INFO: Deleting pod "simpletest.rc-w6vc2" in namespace "gc-8491"
Dec 14 16:04:49.182: INFO: Deleting pod "simpletest.rc-wbs5h" in namespace "gc-8491"
Dec 14 16:04:49.253: INFO: Deleting pod "simpletest.rc-wlmcj" in namespace "gc-8491"
Dec 14 16:04:49.351: INFO: Deleting pod "simpletest.rc-wn4jv" in namespace "gc-8491"
Dec 14 16:04:49.463: INFO: Deleting pod "simpletest.rc-wpb7k" in namespace "gc-8491"
Dec 14 16:04:49.526: INFO: Deleting pod "simpletest.rc-wvdsv" in namespace "gc-8491"
Dec 14 16:04:49.565: INFO: Deleting pod "simpletest.rc-wzxmq" in namespace "gc-8491"
Dec 14 16:04:49.606: INFO: Deleting pod "simpletest.rc-x8rb2" in namespace "gc-8491"
Dec 14 16:04:49.655: INFO: Deleting pod "simpletest.rc-xhc24" in namespace "gc-8491"
Dec 14 16:04:49.710: INFO: Deleting pod "simpletest.rc-xkwfh" in namespace "gc-8491"
Dec 14 16:04:49.762: INFO: Deleting pod "simpletest.rc-xn7kk" in namespace "gc-8491"
Dec 14 16:04:49.794: INFO: Deleting pod "simpletest.rc-zhl6q" in namespace "gc-8491"
Dec 14 16:04:49.881: INFO: Deleting pod "simpletest.rc-zqpck" in namespace "gc-8491"
Dec 14 16:04:49.925: INFO: Deleting pod "simpletest.rc-zxzw7" in namespace "gc-8491"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:04:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8491" for this suite. 12/14/22 16:04:49.984
------------------------------
• [SLOW TEST] [45.394 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:04:04.614
    Dec 14 16:04:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:04:04.616
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:04.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:04.653
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 12/14/22 16:04:04.667
    STEP: delete the rc 12/14/22 16:04:09.709
    STEP: wait for the rc to be deleted 12/14/22 16:04:09.737
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/14/22 16:04:14.754
    STEP: Gathering metrics 12/14/22 16:04:44.783
    Dec 14 16:04:44.828: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 16:04:44.833: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.686489ms
    Dec 14 16:04:44.833: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 16:04:44.833: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 16:04:44.998: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 14 16:04:44.998: INFO: Deleting pod "simpletest.rc-2mwp9" in namespace "gc-8491"
    Dec 14 16:04:45.022: INFO: Deleting pod "simpletest.rc-2vnvx" in namespace "gc-8491"
    Dec 14 16:04:45.071: INFO: Deleting pod "simpletest.rc-4dtvs" in namespace "gc-8491"
    Dec 14 16:04:45.102: INFO: Deleting pod "simpletest.rc-4l8kw" in namespace "gc-8491"
    Dec 14 16:04:45.145: INFO: Deleting pod "simpletest.rc-4lkrn" in namespace "gc-8491"
    Dec 14 16:04:45.205: INFO: Deleting pod "simpletest.rc-4pllt" in namespace "gc-8491"
    Dec 14 16:04:45.259: INFO: Deleting pod "simpletest.rc-4s6xt" in namespace "gc-8491"
    Dec 14 16:04:45.352: INFO: Deleting pod "simpletest.rc-58xqg" in namespace "gc-8491"
    Dec 14 16:04:45.432: INFO: Deleting pod "simpletest.rc-5p6g5" in namespace "gc-8491"
    Dec 14 16:04:45.472: INFO: Deleting pod "simpletest.rc-5qlfl" in namespace "gc-8491"
    Dec 14 16:04:45.497: INFO: Deleting pod "simpletest.rc-65grt" in namespace "gc-8491"
    Dec 14 16:04:45.525: INFO: Deleting pod "simpletest.rc-665gc" in namespace "gc-8491"
    Dec 14 16:04:45.553: INFO: Deleting pod "simpletest.rc-69h2c" in namespace "gc-8491"
    Dec 14 16:04:45.597: INFO: Deleting pod "simpletest.rc-6f24z" in namespace "gc-8491"
    Dec 14 16:04:45.628: INFO: Deleting pod "simpletest.rc-6jsns" in namespace "gc-8491"
    Dec 14 16:04:45.650: INFO: Deleting pod "simpletest.rc-6sf2x" in namespace "gc-8491"
    Dec 14 16:04:45.683: INFO: Deleting pod "simpletest.rc-74xw9" in namespace "gc-8491"
    Dec 14 16:04:45.713: INFO: Deleting pod "simpletest.rc-77cn8" in namespace "gc-8491"
    Dec 14 16:04:45.734: INFO: Deleting pod "simpletest.rc-7nwk5" in namespace "gc-8491"
    Dec 14 16:04:45.769: INFO: Deleting pod "simpletest.rc-7qgvl" in namespace "gc-8491"
    Dec 14 16:04:45.803: INFO: Deleting pod "simpletest.rc-7thsp" in namespace "gc-8491"
    Dec 14 16:04:45.845: INFO: Deleting pod "simpletest.rc-84lv5" in namespace "gc-8491"
    Dec 14 16:04:45.879: INFO: Deleting pod "simpletest.rc-85kq9" in namespace "gc-8491"
    Dec 14 16:04:45.908: INFO: Deleting pod "simpletest.rc-8x9zd" in namespace "gc-8491"
    Dec 14 16:04:45.941: INFO: Deleting pod "simpletest.rc-9gwkx" in namespace "gc-8491"
    Dec 14 16:04:45.971: INFO: Deleting pod "simpletest.rc-9kkkt" in namespace "gc-8491"
    Dec 14 16:04:46.098: INFO: Deleting pod "simpletest.rc-9njht" in namespace "gc-8491"
    Dec 14 16:04:46.128: INFO: Deleting pod "simpletest.rc-9t9lv" in namespace "gc-8491"
    Dec 14 16:04:46.183: INFO: Deleting pod "simpletest.rc-bqxhm" in namespace "gc-8491"
    Dec 14 16:04:46.243: INFO: Deleting pod "simpletest.rc-btcgs" in namespace "gc-8491"
    Dec 14 16:04:46.306: INFO: Deleting pod "simpletest.rc-c87t7" in namespace "gc-8491"
    Dec 14 16:04:46.334: INFO: Deleting pod "simpletest.rc-cdxdr" in namespace "gc-8491"
    Dec 14 16:04:46.398: INFO: Deleting pod "simpletest.rc-cpt9k" in namespace "gc-8491"
    Dec 14 16:04:46.430: INFO: Deleting pod "simpletest.rc-cw58z" in namespace "gc-8491"
    Dec 14 16:04:46.463: INFO: Deleting pod "simpletest.rc-czzff" in namespace "gc-8491"
    Dec 14 16:04:46.522: INFO: Deleting pod "simpletest.rc-d2wnj" in namespace "gc-8491"
    Dec 14 16:04:46.574: INFO: Deleting pod "simpletest.rc-d6lrh" in namespace "gc-8491"
    Dec 14 16:04:46.626: INFO: Deleting pod "simpletest.rc-d9mz2" in namespace "gc-8491"
    Dec 14 16:04:46.662: INFO: Deleting pod "simpletest.rc-dc4jj" in namespace "gc-8491"
    Dec 14 16:04:46.704: INFO: Deleting pod "simpletest.rc-dfr2f" in namespace "gc-8491"
    Dec 14 16:04:46.763: INFO: Deleting pod "simpletest.rc-fdtkw" in namespace "gc-8491"
    Dec 14 16:04:46.797: INFO: Deleting pod "simpletest.rc-fj2zl" in namespace "gc-8491"
    Dec 14 16:04:46.833: INFO: Deleting pod "simpletest.rc-flh5g" in namespace "gc-8491"
    Dec 14 16:04:46.880: INFO: Deleting pod "simpletest.rc-g4t5z" in namespace "gc-8491"
    Dec 14 16:04:46.930: INFO: Deleting pod "simpletest.rc-ghzcv" in namespace "gc-8491"
    Dec 14 16:04:46.967: INFO: Deleting pod "simpletest.rc-gjphx" in namespace "gc-8491"
    Dec 14 16:04:47.077: INFO: Deleting pod "simpletest.rc-gql2z" in namespace "gc-8491"
    Dec 14 16:04:47.158: INFO: Deleting pod "simpletest.rc-gwp49" in namespace "gc-8491"
    Dec 14 16:04:47.247: INFO: Deleting pod "simpletest.rc-hj69c" in namespace "gc-8491"
    Dec 14 16:04:47.286: INFO: Deleting pod "simpletest.rc-hkvxk" in namespace "gc-8491"
    Dec 14 16:04:47.308: INFO: Deleting pod "simpletest.rc-hpdmz" in namespace "gc-8491"
    Dec 14 16:04:47.361: INFO: Deleting pod "simpletest.rc-hzd7w" in namespace "gc-8491"
    Dec 14 16:04:47.389: INFO: Deleting pod "simpletest.rc-jkpz2" in namespace "gc-8491"
    Dec 14 16:04:47.428: INFO: Deleting pod "simpletest.rc-jm6k2" in namespace "gc-8491"
    Dec 14 16:04:47.514: INFO: Deleting pod "simpletest.rc-k7vgs" in namespace "gc-8491"
    Dec 14 16:04:47.550: INFO: Deleting pod "simpletest.rc-klkh6" in namespace "gc-8491"
    Dec 14 16:04:47.592: INFO: Deleting pod "simpletest.rc-kvx48" in namespace "gc-8491"
    Dec 14 16:04:47.643: INFO: Deleting pod "simpletest.rc-kzl9h" in namespace "gc-8491"
    Dec 14 16:04:47.684: INFO: Deleting pod "simpletest.rc-l7qlg" in namespace "gc-8491"
    Dec 14 16:04:47.725: INFO: Deleting pod "simpletest.rc-l9q7f" in namespace "gc-8491"
    Dec 14 16:04:47.794: INFO: Deleting pod "simpletest.rc-lnmmt" in namespace "gc-8491"
    Dec 14 16:04:47.986: INFO: Deleting pod "simpletest.rc-mk6hj" in namespace "gc-8491"
    Dec 14 16:04:48.065: INFO: Deleting pod "simpletest.rc-mr89l" in namespace "gc-8491"
    Dec 14 16:04:48.155: INFO: Deleting pod "simpletest.rc-mzxmt" in namespace "gc-8491"
    Dec 14 16:04:48.204: INFO: Deleting pod "simpletest.rc-n5hhc" in namespace "gc-8491"
    Dec 14 16:04:48.268: INFO: Deleting pod "simpletest.rc-nh6wb" in namespace "gc-8491"
    Dec 14 16:04:48.306: INFO: Deleting pod "simpletest.rc-p85w9" in namespace "gc-8491"
    Dec 14 16:04:48.336: INFO: Deleting pod "simpletest.rc-pfsp8" in namespace "gc-8491"
    Dec 14 16:04:48.365: INFO: Deleting pod "simpletest.rc-pxf5c" in namespace "gc-8491"
    Dec 14 16:04:48.402: INFO: Deleting pod "simpletest.rc-pzd4g" in namespace "gc-8491"
    Dec 14 16:04:48.425: INFO: Deleting pod "simpletest.rc-q9v44" in namespace "gc-8491"
    Dec 14 16:04:48.456: INFO: Deleting pod "simpletest.rc-qmdhr" in namespace "gc-8491"
    Dec 14 16:04:48.481: INFO: Deleting pod "simpletest.rc-qmrn2" in namespace "gc-8491"
    Dec 14 16:04:48.511: INFO: Deleting pod "simpletest.rc-qn4p2" in namespace "gc-8491"
    Dec 14 16:04:48.541: INFO: Deleting pod "simpletest.rc-r7r2k" in namespace "gc-8491"
    Dec 14 16:04:48.587: INFO: Deleting pod "simpletest.rc-rjskg" in namespace "gc-8491"
    Dec 14 16:04:48.640: INFO: Deleting pod "simpletest.rc-rkw4p" in namespace "gc-8491"
    Dec 14 16:04:48.698: INFO: Deleting pod "simpletest.rc-sfbjn" in namespace "gc-8491"
    Dec 14 16:04:48.761: INFO: Deleting pod "simpletest.rc-sjmkg" in namespace "gc-8491"
    Dec 14 16:04:48.799: INFO: Deleting pod "simpletest.rc-tdf68" in namespace "gc-8491"
    Dec 14 16:04:48.844: INFO: Deleting pod "simpletest.rc-tmcc9" in namespace "gc-8491"
    Dec 14 16:04:48.871: INFO: Deleting pod "simpletest.rc-v9lfn" in namespace "gc-8491"
    Dec 14 16:04:48.928: INFO: Deleting pod "simpletest.rc-vcfhd" in namespace "gc-8491"
    Dec 14 16:04:48.993: INFO: Deleting pod "simpletest.rc-vj9x7" in namespace "gc-8491"
    Dec 14 16:04:49.056: INFO: Deleting pod "simpletest.rc-vm6hl" in namespace "gc-8491"
    Dec 14 16:04:49.078: INFO: Deleting pod "simpletest.rc-vxbqc" in namespace "gc-8491"
    Dec 14 16:04:49.110: INFO: Deleting pod "simpletest.rc-w6vc2" in namespace "gc-8491"
    Dec 14 16:04:49.182: INFO: Deleting pod "simpletest.rc-wbs5h" in namespace "gc-8491"
    Dec 14 16:04:49.253: INFO: Deleting pod "simpletest.rc-wlmcj" in namespace "gc-8491"
    Dec 14 16:04:49.351: INFO: Deleting pod "simpletest.rc-wn4jv" in namespace "gc-8491"
    Dec 14 16:04:49.463: INFO: Deleting pod "simpletest.rc-wpb7k" in namespace "gc-8491"
    Dec 14 16:04:49.526: INFO: Deleting pod "simpletest.rc-wvdsv" in namespace "gc-8491"
    Dec 14 16:04:49.565: INFO: Deleting pod "simpletest.rc-wzxmq" in namespace "gc-8491"
    Dec 14 16:04:49.606: INFO: Deleting pod "simpletest.rc-x8rb2" in namespace "gc-8491"
    Dec 14 16:04:49.655: INFO: Deleting pod "simpletest.rc-xhc24" in namespace "gc-8491"
    Dec 14 16:04:49.710: INFO: Deleting pod "simpletest.rc-xkwfh" in namespace "gc-8491"
    Dec 14 16:04:49.762: INFO: Deleting pod "simpletest.rc-xn7kk" in namespace "gc-8491"
    Dec 14 16:04:49.794: INFO: Deleting pod "simpletest.rc-zhl6q" in namespace "gc-8491"
    Dec 14 16:04:49.881: INFO: Deleting pod "simpletest.rc-zqpck" in namespace "gc-8491"
    Dec 14 16:04:49.925: INFO: Deleting pod "simpletest.rc-zxzw7" in namespace "gc-8491"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:04:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8491" for this suite. 12/14/22 16:04:49.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:04:50.015
Dec 14 16:04:50.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:04:50.082
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:50.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:50.142
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:04:50.147
Dec 14 16:04:50.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984" in namespace "downward-api-9981" to be "Succeeded or Failed"
Dec 14 16:04:50.196: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 15.773715ms
Dec 14 16:04:52.202: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022225184s
Dec 14 16:04:54.205: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024929641s
Dec 14 16:04:56.202: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021953595s
Dec 14 16:04:58.204: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Running", Reason="", readiness=true. Elapsed: 8.023805676s
Dec 14 16:05:00.203: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023255203s
STEP: Saw pod success 12/14/22 16:05:00.203
Dec 14 16:05:00.204: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984" satisfied condition "Succeeded or Failed"
Dec 14 16:05:00.210: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 container client-container: <nil>
STEP: delete the pod 12/14/22 16:05:00.226
Dec 14 16:05:00.241: INFO: Waiting for pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 to disappear
Dec 14 16:05:00.247: INFO: Pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:00.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9981" for this suite. 12/14/22 16:05:00.255
------------------------------
• [SLOW TEST] [10.248 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:04:50.015
    Dec 14 16:04:50.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:04:50.082
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:04:50.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:04:50.142
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:04:50.147
    Dec 14 16:04:50.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984" in namespace "downward-api-9981" to be "Succeeded or Failed"
    Dec 14 16:04:50.196: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 15.773715ms
    Dec 14 16:04:52.202: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022225184s
    Dec 14 16:04:54.205: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024929641s
    Dec 14 16:04:56.202: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021953595s
    Dec 14 16:04:58.204: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Running", Reason="", readiness=true. Elapsed: 8.023805676s
    Dec 14 16:05:00.203: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023255203s
    STEP: Saw pod success 12/14/22 16:05:00.203
    Dec 14 16:05:00.204: INFO: Pod "downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984" satisfied condition "Succeeded or Failed"
    Dec 14 16:05:00.210: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:05:00.226
    Dec 14 16:05:00.241: INFO: Waiting for pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 to disappear
    Dec 14 16:05:00.247: INFO: Pod downwardapi-volume-dea00dbd-2201-4841-9f13-28fe4b2f2984 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:00.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9981" for this suite. 12/14/22 16:05:00.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:00.268
Dec 14 16:05:00.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:05:00.271
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:00.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:00.302
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 12/14/22 16:05:00.307
STEP: setting up watch 12/14/22 16:05:00.308
STEP: submitting the pod to kubernetes 12/14/22 16:05:00.414
STEP: verifying the pod is in kubernetes 12/14/22 16:05:00.429
STEP: verifying pod creation was observed 12/14/22 16:05:00.433
Dec 14 16:05:00.433: INFO: Waiting up to 5m0s for pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5" in namespace "pods-6693" to be "running"
Dec 14 16:05:00.450: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.099885ms
Dec 14 16:05:02.465: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5": Phase="Running", Reason="", readiness=true. Elapsed: 2.031705204s
Dec 14 16:05:02.465: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 16:05:02.473
STEP: verifying pod deletion was observed 12/14/22 16:05:02.488
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:04.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6693" for this suite. 12/14/22 16:05:04.724
------------------------------
• [4.467 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:00.268
    Dec 14 16:05:00.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:05:00.271
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:00.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:00.302
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 12/14/22 16:05:00.307
    STEP: setting up watch 12/14/22 16:05:00.308
    STEP: submitting the pod to kubernetes 12/14/22 16:05:00.414
    STEP: verifying the pod is in kubernetes 12/14/22 16:05:00.429
    STEP: verifying pod creation was observed 12/14/22 16:05:00.433
    Dec 14 16:05:00.433: INFO: Waiting up to 5m0s for pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5" in namespace "pods-6693" to be "running"
    Dec 14 16:05:00.450: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.099885ms
    Dec 14 16:05:02.465: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5": Phase="Running", Reason="", readiness=true. Elapsed: 2.031705204s
    Dec 14 16:05:02.465: INFO: Pod "pod-submit-remove-22d83622-8519-4e21-ab35-3298a566cad5" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 16:05:02.473
    STEP: verifying pod deletion was observed 12/14/22 16:05:02.488
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:04.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6693" for this suite. 12/14/22 16:05:04.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:04.753
Dec 14 16:05:04.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:05:04.757
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:04.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:04.791
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-906a5d05-c514-4bf2-a89a-77b66a01c5ab 12/14/22 16:05:04.796
STEP: Creating a pod to test consume configMaps 12/14/22 16:05:04.803
Dec 14 16:05:04.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd" in namespace "configmap-8665" to be "Succeeded or Failed"
Dec 14 16:05:04.819: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097256ms
Dec 14 16:05:06.832: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017573473s
Dec 14 16:05:08.829: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014671527s
STEP: Saw pod success 12/14/22 16:05:08.83
Dec 14 16:05:08.830: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd" satisfied condition "Succeeded or Failed"
Dec 14 16:05:08.835: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:05:08.845
Dec 14 16:05:08.866: INFO: Waiting for pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd to disappear
Dec 14 16:05:08.870: INFO: Pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8665" for this suite. 12/14/22 16:05:08.884
------------------------------
• [4.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:04.753
    Dec 14 16:05:04.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:05:04.757
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:04.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:04.791
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-906a5d05-c514-4bf2-a89a-77b66a01c5ab 12/14/22 16:05:04.796
    STEP: Creating a pod to test consume configMaps 12/14/22 16:05:04.803
    Dec 14 16:05:04.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd" in namespace "configmap-8665" to be "Succeeded or Failed"
    Dec 14 16:05:04.819: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097256ms
    Dec 14 16:05:06.832: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017573473s
    Dec 14 16:05:08.829: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014671527s
    STEP: Saw pod success 12/14/22 16:05:08.83
    Dec 14 16:05:08.830: INFO: Pod "pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd" satisfied condition "Succeeded or Failed"
    Dec 14 16:05:08.835: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:05:08.845
    Dec 14 16:05:08.866: INFO: Waiting for pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd to disappear
    Dec 14 16:05:08.870: INFO: Pod pod-configmaps-65b0d758-2811-42e8-9a96-0b801ff0badd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8665" for this suite. 12/14/22 16:05:08.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:08.899
Dec 14 16:05:08.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 16:05:08.901
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:08.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:08.935
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6616.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 12/14/22 16:05:08.941
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6616.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 12/14/22 16:05:08.941
STEP: creating a pod to probe /etc/hosts 12/14/22 16:05:08.941
STEP: submitting the pod to kubernetes 12/14/22 16:05:08.942
Dec 14 16:05:08.958: INFO: Waiting up to 15m0s for pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b" in namespace "dns-6616" to be "running"
Dec 14 16:05:08.963: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.685339ms
Dec 14 16:05:10.970: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012321388s
Dec 14 16:05:12.973: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014637635s
Dec 14 16:05:14.968: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009948396s
Dec 14 16:05:16.970: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012280342s
Dec 14 16:05:18.989: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030894005s
Dec 14 16:05:20.974: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015988535s
Dec 14 16:05:22.979: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Running", Reason="", readiness=true. Elapsed: 14.021314974s
Dec 14 16:05:22.980: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b" satisfied condition "running"
STEP: retrieving the pod 12/14/22 16:05:22.98
STEP: looking for the results for each expected name from probers 12/14/22 16:05:22.988
Dec 14 16:05:23.012: INFO: DNS probes using dns-6616/dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b succeeded

STEP: deleting the pod 12/14/22 16:05:23.012
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:23.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6616" for this suite. 12/14/22 16:05:23.057
------------------------------
• [SLOW TEST] [14.170 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:08.899
    Dec 14 16:05:08.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 16:05:08.901
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:08.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:08.935
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6616.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     12/14/22 16:05:08.941
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6616.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     12/14/22 16:05:08.941
    STEP: creating a pod to probe /etc/hosts 12/14/22 16:05:08.941
    STEP: submitting the pod to kubernetes 12/14/22 16:05:08.942
    Dec 14 16:05:08.958: INFO: Waiting up to 15m0s for pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b" in namespace "dns-6616" to be "running"
    Dec 14 16:05:08.963: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.685339ms
    Dec 14 16:05:10.970: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012321388s
    Dec 14 16:05:12.973: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014637635s
    Dec 14 16:05:14.968: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009948396s
    Dec 14 16:05:16.970: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012280342s
    Dec 14 16:05:18.989: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030894005s
    Dec 14 16:05:20.974: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015988535s
    Dec 14 16:05:22.979: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b": Phase="Running", Reason="", readiness=true. Elapsed: 14.021314974s
    Dec 14 16:05:22.980: INFO: Pod "dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 16:05:22.98
    STEP: looking for the results for each expected name from probers 12/14/22 16:05:22.988
    Dec 14 16:05:23.012: INFO: DNS probes using dns-6616/dns-test-d1c0c6fe-3f8a-4cde-874e-951f243efb6b succeeded

    STEP: deleting the pod 12/14/22 16:05:23.012
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:23.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6616" for this suite. 12/14/22 16:05:23.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:23.074
Dec 14 16:05:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:05:23.081
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:23.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:23.113
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 12/14/22 16:05:23.118
STEP: Creating a ResourceQuota 12/14/22 16:05:28.126
STEP: Ensuring resource quota status is calculated 12/14/22 16:05:28.135
STEP: Creating a ReplicationController 12/14/22 16:05:30.146
STEP: Ensuring resource quota status captures replication controller creation 12/14/22 16:05:30.164
STEP: Deleting a ReplicationController 12/14/22 16:05:32.173
STEP: Ensuring resource quota status released usage 12/14/22 16:05:32.185
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:34.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5797" for this suite. 12/14/22 16:05:34.2
------------------------------
• [SLOW TEST] [11.136 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:23.074
    Dec 14 16:05:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:05:23.081
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:23.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:23.113
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 12/14/22 16:05:23.118
    STEP: Creating a ResourceQuota 12/14/22 16:05:28.126
    STEP: Ensuring resource quota status is calculated 12/14/22 16:05:28.135
    STEP: Creating a ReplicationController 12/14/22 16:05:30.146
    STEP: Ensuring resource quota status captures replication controller creation 12/14/22 16:05:30.164
    STEP: Deleting a ReplicationController 12/14/22 16:05:32.173
    STEP: Ensuring resource quota status released usage 12/14/22 16:05:32.185
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:34.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5797" for this suite. 12/14/22 16:05:34.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:34.212
Dec 14 16:05:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 16:05:34.214
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:34.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:34.242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-phb69" 12/14/22 16:05:34.246
Dec 14 16:05:34.273: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-phb69-6549" 12/14/22 16:05:34.273
Dec 14 16:05:34.286: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-phb69-6549" 12/14/22 16:05:34.286
Dec 14 16:05:34.296: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:34.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-218" for this suite. 12/14/22 16:05:34.304
STEP: Destroying namespace "e2e-ns-phb69-6549" for this suite. 12/14/22 16:05:34.313
------------------------------
• [0.110 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:34.212
    Dec 14 16:05:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 16:05:34.214
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:34.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:34.242
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-phb69" 12/14/22 16:05:34.246
    Dec 14 16:05:34.273: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-phb69-6549" 12/14/22 16:05:34.273
    Dec 14 16:05:34.286: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-phb69-6549" 12/14/22 16:05:34.286
    Dec 14 16:05:34.296: INFO: Namespace "e2e-ns-phb69-6549" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:34.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-218" for this suite. 12/14/22 16:05:34.304
    STEP: Destroying namespace "e2e-ns-phb69-6549" for this suite. 12/14/22 16:05:34.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:34.33
Dec 14 16:05:34.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:05:34.331
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:34.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:34.364
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-c2c0b708-e58c-493d-b1c8-8c40107ede75 12/14/22 16:05:34.37
STEP: Creating a pod to test consume configMaps 12/14/22 16:05:34.377
Dec 14 16:05:34.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7" in namespace "projected-7400" to be "Succeeded or Failed"
Dec 14 16:05:34.424: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.075581ms
Dec 14 16:05:36.431: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Running", Reason="", readiness=false. Elapsed: 2.03215338s
Dec 14 16:05:38.436: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036859503s
STEP: Saw pod success 12/14/22 16:05:38.436
Dec 14 16:05:38.437: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7" satisfied condition "Succeeded or Failed"
Dec 14 16:05:38.454: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:05:38.474
Dec 14 16:05:38.497: INFO: Waiting for pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 to disappear
Dec 14 16:05:38.501: INFO: Pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:05:38.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7400" for this suite. 12/14/22 16:05:38.509
------------------------------
• [4.194 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:34.33
    Dec 14 16:05:34.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:05:34.331
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:34.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:34.364
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-c2c0b708-e58c-493d-b1c8-8c40107ede75 12/14/22 16:05:34.37
    STEP: Creating a pod to test consume configMaps 12/14/22 16:05:34.377
    Dec 14 16:05:34.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7" in namespace "projected-7400" to be "Succeeded or Failed"
    Dec 14 16:05:34.424: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.075581ms
    Dec 14 16:05:36.431: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Running", Reason="", readiness=false. Elapsed: 2.03215338s
    Dec 14 16:05:38.436: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036859503s
    STEP: Saw pod success 12/14/22 16:05:38.436
    Dec 14 16:05:38.437: INFO: Pod "pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7" satisfied condition "Succeeded or Failed"
    Dec 14 16:05:38.454: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:05:38.474
    Dec 14 16:05:38.497: INFO: Waiting for pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 to disappear
    Dec 14 16:05:38.501: INFO: Pod pod-projected-configmaps-348b9527-8c04-4d13-8b77-d9859b8ba7f7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:05:38.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7400" for this suite. 12/14/22 16:05:38.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:05:38.53
Dec 14 16:05:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename taint-single-pod 12/14/22 16:05:38.532
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:38.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:38.558
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Dec 14 16:05:38.566: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:06:38.599: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Dec 14 16:06:38.606: INFO: Starting informer...
STEP: Starting pod... 12/14/22 16:06:38.606
Dec 14 16:06:38.827: INFO: Pod is running on iet9eich7uhu-3. Tainting Node
STEP: Trying to apply a taint on the Node 12/14/22 16:06:38.827
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 16:06:38.846
STEP: Waiting short time to make sure Pod is queued for deletion 12/14/22 16:06:38.852
Dec 14 16:06:38.852: INFO: Pod wasn't evicted. Proceeding
Dec 14 16:06:38.852: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 16:06:38.902
STEP: Waiting some time to make sure that toleration time passed. 12/14/22 16:06:38.935
Dec 14 16:07:53.936: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:07:53.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3049" for this suite. 12/14/22 16:07:53.95
------------------------------
• [SLOW TEST] [135.430 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:05:38.53
    Dec 14 16:05:38.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename taint-single-pod 12/14/22 16:05:38.532
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:05:38.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:05:38.558
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Dec 14 16:05:38.566: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 16:06:38.599: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Dec 14 16:06:38.606: INFO: Starting informer...
    STEP: Starting pod... 12/14/22 16:06:38.606
    Dec 14 16:06:38.827: INFO: Pod is running on iet9eich7uhu-3. Tainting Node
    STEP: Trying to apply a taint on the Node 12/14/22 16:06:38.827
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 16:06:38.846
    STEP: Waiting short time to make sure Pod is queued for deletion 12/14/22 16:06:38.852
    Dec 14 16:06:38.852: INFO: Pod wasn't evicted. Proceeding
    Dec 14 16:06:38.852: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 16:06:38.902
    STEP: Waiting some time to make sure that toleration time passed. 12/14/22 16:06:38.935
    Dec 14 16:07:53.936: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:07:53.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3049" for this suite. 12/14/22 16:07:53.95
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:07:53.962
Dec 14 16:07:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename containers 12/14/22 16:07:53.969
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:07:54.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:07:54.006
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 12/14/22 16:07:54.009
Dec 14 16:07:54.019: INFO: Waiting up to 5m0s for pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53" in namespace "containers-8183" to be "Succeeded or Failed"
Dec 14 16:07:54.024: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785642ms
Dec 14 16:07:56.032: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Running", Reason="", readiness=false. Elapsed: 2.012822442s
Dec 14 16:07:58.056: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037287527s
STEP: Saw pod success 12/14/22 16:07:58.056
Dec 14 16:07:58.056: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53" satisfied condition "Succeeded or Failed"
Dec 14 16:07:58.068: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:07:58.098
Dec 14 16:07:58.127: INFO: Waiting for pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 to disappear
Dec 14 16:07:58.136: INFO: Pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:07:58.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8183" for this suite. 12/14/22 16:07:58.143
------------------------------
• [4.190 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:07:53.962
    Dec 14 16:07:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename containers 12/14/22 16:07:53.969
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:07:54.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:07:54.006
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 12/14/22 16:07:54.009
    Dec 14 16:07:54.019: INFO: Waiting up to 5m0s for pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53" in namespace "containers-8183" to be "Succeeded or Failed"
    Dec 14 16:07:54.024: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785642ms
    Dec 14 16:07:56.032: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Running", Reason="", readiness=false. Elapsed: 2.012822442s
    Dec 14 16:07:58.056: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037287527s
    STEP: Saw pod success 12/14/22 16:07:58.056
    Dec 14 16:07:58.056: INFO: Pod "client-containers-daa34c41-816d-433e-a2a7-2539804d3d53" satisfied condition "Succeeded or Failed"
    Dec 14 16:07:58.068: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:07:58.098
    Dec 14 16:07:58.127: INFO: Waiting for pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 to disappear
    Dec 14 16:07:58.136: INFO: Pod client-containers-daa34c41-816d-433e-a2a7-2539804d3d53 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:07:58.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8183" for this suite. 12/14/22 16:07:58.143
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:07:58.153
Dec 14 16:07:58.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-runtime 12/14/22 16:07:58.156
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:07:58.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:07:58.19
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 12/14/22 16:07:58.194
STEP: wait for the container to reach Failed 12/14/22 16:07:58.206
STEP: get the container status 12/14/22 16:08:02.261
STEP: the container should be terminated 12/14/22 16:08:02.27
STEP: the termination message should be set 12/14/22 16:08:02.271
Dec 14 16:08:02.271: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/14/22 16:08:02.271
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 14 16:08:02.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3378" for this suite. 12/14/22 16:08:02.306
------------------------------
• [4.164 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:07:58.153
    Dec 14 16:07:58.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-runtime 12/14/22 16:07:58.156
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:07:58.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:07:58.19
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 12/14/22 16:07:58.194
    STEP: wait for the container to reach Failed 12/14/22 16:07:58.206
    STEP: get the container status 12/14/22 16:08:02.261
    STEP: the container should be terminated 12/14/22 16:08:02.27
    STEP: the termination message should be set 12/14/22 16:08:02.271
    Dec 14 16:08:02.271: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/14/22 16:08:02.271
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:08:02.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3378" for this suite. 12/14/22 16:08:02.306
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:08:02.318
Dec 14 16:08:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:08:02.322
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:08:02.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:08:02.363
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 14 16:08:02.386: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:09:02.432: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 12/14/22 16:09:02.44
Dec 14 16:09:02.477: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 16:09:02.484: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 16:09:02.518: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 16:09:02.543: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 14 16:09:02.594: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 14 16:09:02.604: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/14/22 16:09:02.604
Dec 14 16:09:02.604: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:02.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936535ms
Dec 14 16:09:04.619: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015000808s
Dec 14 16:09:06.621: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016663639s
Dec 14 16:09:08.618: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013964734s
Dec 14 16:09:10.620: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.015830015s
Dec 14 16:09:10.620: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 14 16:09:10.620: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:10.625: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.761969ms
Dec 14 16:09:10.625: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 16:09:10.625: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:10.631: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.638252ms
Dec 14 16:09:12.639: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309466s
Dec 14 16:09:14.643: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017223785s
Dec 14 16:09:14.643: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 16:09:14.643: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:14.659: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.397705ms
Dec 14 16:09:14.659: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 16:09:14.659: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:14.666: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.30725ms
Dec 14 16:09:14.666: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 16:09:14.666: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:14.671: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.017044ms
Dec 14 16:09:14.671: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/14/22 16:09:14.671
Dec 14 16:09:14.685: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8401" to be "running"
Dec 14 16:09:14.705: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.197708ms
Dec 14 16:09:16.713: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028700882s
Dec 14 16:09:18.713: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028508582s
Dec 14 16:09:18.714: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:09:18.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8401" for this suite. 12/14/22 16:09:18.827
------------------------------
• [SLOW TEST] [76.521 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:08:02.318
    Dec 14 16:08:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:08:02.322
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:08:02.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:08:02.363
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 14 16:08:02.386: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 16:09:02.432: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 12/14/22 16:09:02.44
    Dec 14 16:09:02.477: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 14 16:09:02.484: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 14 16:09:02.518: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 14 16:09:02.543: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 14 16:09:02.594: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 14 16:09:02.604: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/14/22 16:09:02.604
    Dec 14 16:09:02.604: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:02.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936535ms
    Dec 14 16:09:04.619: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015000808s
    Dec 14 16:09:06.621: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016663639s
    Dec 14 16:09:08.618: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013964734s
    Dec 14 16:09:10.620: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.015830015s
    Dec 14 16:09:10.620: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 14 16:09:10.620: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:10.625: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.761969ms
    Dec 14 16:09:10.625: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 16:09:10.625: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:10.631: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.638252ms
    Dec 14 16:09:12.639: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309466s
    Dec 14 16:09:14.643: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017223785s
    Dec 14 16:09:14.643: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 16:09:14.643: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:14.659: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.397705ms
    Dec 14 16:09:14.659: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 16:09:14.659: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:14.666: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.30725ms
    Dec 14 16:09:14.666: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 16:09:14.666: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:14.671: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.017044ms
    Dec 14 16:09:14.671: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/14/22 16:09:14.671
    Dec 14 16:09:14.685: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8401" to be "running"
    Dec 14 16:09:14.705: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.197708ms
    Dec 14 16:09:16.713: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028700882s
    Dec 14 16:09:18.713: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028508582s
    Dec 14 16:09:18.714: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:09:18.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8401" for this suite. 12/14/22 16:09:18.827
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:09:18.839
Dec 14 16:09:18.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename endpointslice 12/14/22 16:09:18.842
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:18.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:18.87
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 12/14/22 16:09:24.104
STEP: referencing matching pods with named port 12/14/22 16:09:29.131
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/14/22 16:09:34.146
STEP: recreating EndpointSlices after they've been deleted 12/14/22 16:09:39.162
Dec 14 16:09:39.205: INFO: EndpointSlice for Service endpointslice-4761/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 14 16:09:49.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4761" for this suite. 12/14/22 16:09:49.245
------------------------------
• [SLOW TEST] [30.416 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:09:18.839
    Dec 14 16:09:18.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename endpointslice 12/14/22 16:09:18.842
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:18.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:18.87
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 12/14/22 16:09:24.104
    STEP: referencing matching pods with named port 12/14/22 16:09:29.131
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/14/22 16:09:34.146
    STEP: recreating EndpointSlices after they've been deleted 12/14/22 16:09:39.162
    Dec 14 16:09:39.205: INFO: EndpointSlice for Service endpointslice-4761/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:09:49.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4761" for this suite. 12/14/22 16:09:49.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:09:49.262
Dec 14 16:09:49.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-webhook 12/14/22 16:09:49.265
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:49.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:49.304
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/14/22 16:09:49.307
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 16:09:49.86
STEP: Deploying the custom resource conversion webhook pod 12/14/22 16:09:49.87
STEP: Wait for the deployment to be ready 12/14/22 16:09:49.883
Dec 14 16:09:49.908: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 16:09:51.926
STEP: Verifying the service has paired with the endpoint 12/14/22 16:09:51.944
Dec 14 16:09:52.945: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Dec 14 16:09:52.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Creating a v1 custom resource 12/14/22 16:09:55.719
STEP: Create a v2 custom resource 12/14/22 16:09:55.745
STEP: List CRs in v1 12/14/22 16:09:55.942
STEP: List CRs in v2 12/14/22 16:09:55.958
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:09:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5480" for this suite. 12/14/22 16:09:56.575
------------------------------
• [SLOW TEST] [7.324 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:09:49.262
    Dec 14 16:09:49.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-webhook 12/14/22 16:09:49.265
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:49.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:49.304
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/14/22 16:09:49.307
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/14/22 16:09:49.86
    STEP: Deploying the custom resource conversion webhook pod 12/14/22 16:09:49.87
    STEP: Wait for the deployment to be ready 12/14/22 16:09:49.883
    Dec 14 16:09:49.908: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 16:09:51.926
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:09:51.944
    Dec 14 16:09:52.945: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Dec 14 16:09:52.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Creating a v1 custom resource 12/14/22 16:09:55.719
    STEP: Create a v2 custom resource 12/14/22 16:09:55.745
    STEP: List CRs in v1 12/14/22 16:09:55.942
    STEP: List CRs in v2 12/14/22 16:09:55.958
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:09:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5480" for this suite. 12/14/22 16:09:56.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:09:56.588
Dec 14 16:09:56.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:09:56.591
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:56.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:56.625
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1628/secret-test-837b27cb-1392-4508-8512-15a8cb781798 12/14/22 16:09:56.634
STEP: Creating a pod to test consume secrets 12/14/22 16:09:56.652
Dec 14 16:09:56.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a" in namespace "secrets-1628" to be "Succeeded or Failed"
Dec 14 16:09:56.676: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76065ms
Dec 14 16:09:58.684: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548287s
Dec 14 16:10:00.684: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013294126s
STEP: Saw pod success 12/14/22 16:10:00.685
Dec 14 16:10:00.685: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a" satisfied condition "Succeeded or Failed"
Dec 14 16:10:00.691: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a container env-test: <nil>
STEP: delete the pod 12/14/22 16:10:00.72
Dec 14 16:10:00.737: INFO: Waiting for pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a to disappear
Dec 14 16:10:00.741: INFO: Pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:00.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1628" for this suite. 12/14/22 16:10:00.748
------------------------------
• [4.169 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:09:56.588
    Dec 14 16:09:56.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:09:56.591
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:09:56.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:09:56.625
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1628/secret-test-837b27cb-1392-4508-8512-15a8cb781798 12/14/22 16:09:56.634
    STEP: Creating a pod to test consume secrets 12/14/22 16:09:56.652
    Dec 14 16:09:56.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a" in namespace "secrets-1628" to be "Succeeded or Failed"
    Dec 14 16:09:56.676: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76065ms
    Dec 14 16:09:58.684: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548287s
    Dec 14 16:10:00.684: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013294126s
    STEP: Saw pod success 12/14/22 16:10:00.685
    Dec 14 16:10:00.685: INFO: Pod "pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a" satisfied condition "Succeeded or Failed"
    Dec 14 16:10:00.691: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a container env-test: <nil>
    STEP: delete the pod 12/14/22 16:10:00.72
    Dec 14 16:10:00.737: INFO: Waiting for pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a to disappear
    Dec 14 16:10:00.741: INFO: Pod pod-configmaps-6704df4b-cd6b-4a9a-8192-fa78855e654a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:00.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1628" for this suite. 12/14/22 16:10:00.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:00.764
Dec 14 16:10:00.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context 12/14/22 16:10:00.766
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:00.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:00.794
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 16:10:00.799
Dec 14 16:10:00.820: INFO: Waiting up to 5m0s for pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b" in namespace "security-context-7962" to be "Succeeded or Failed"
Dec 14 16:10:00.829: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.226673ms
Dec 14 16:10:02.839: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018639296s
Dec 14 16:10:04.837: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017063612s
STEP: Saw pod success 12/14/22 16:10:04.837
Dec 14 16:10:04.838: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b" satisfied condition "Succeeded or Failed"
Dec 14 16:10:04.842: INFO: Trying to get logs from node iet9eich7uhu-3 pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b container test-container: <nil>
STEP: delete the pod 12/14/22 16:10:04.853
Dec 14 16:10:04.872: INFO: Waiting for pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b to disappear
Dec 14 16:10:04.876: INFO: Pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:04.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7962" for this suite. 12/14/22 16:10:04.883
------------------------------
• [4.127 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:00.764
    Dec 14 16:10:00.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context 12/14/22 16:10:00.766
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:00.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:00.794
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/14/22 16:10:00.799
    Dec 14 16:10:00.820: INFO: Waiting up to 5m0s for pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b" in namespace "security-context-7962" to be "Succeeded or Failed"
    Dec 14 16:10:00.829: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.226673ms
    Dec 14 16:10:02.839: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018639296s
    Dec 14 16:10:04.837: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017063612s
    STEP: Saw pod success 12/14/22 16:10:04.837
    Dec 14 16:10:04.838: INFO: Pod "security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b" satisfied condition "Succeeded or Failed"
    Dec 14 16:10:04.842: INFO: Trying to get logs from node iet9eich7uhu-3 pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b container test-container: <nil>
    STEP: delete the pod 12/14/22 16:10:04.853
    Dec 14 16:10:04.872: INFO: Waiting for pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b to disappear
    Dec 14 16:10:04.876: INFO: Pod security-context-1d4da0d9-b13f-44f4-b85c-92f892d3fe8b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:04.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7962" for this suite. 12/14/22 16:10:04.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:04.901
Dec 14 16:10:04.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:10:04.902
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:04.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:04.944
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Dec 14 16:10:04.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: creating the pod 12/14/22 16:10:04.95
STEP: submitting the pod to kubernetes 12/14/22 16:10:04.95
Dec 14 16:10:04.967: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce" in namespace "pods-9463" to be "running and ready"
Dec 14 16:10:04.971: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643259ms
Dec 14 16:10:04.971: INFO: The phase of Pod pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:10:06.977: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce": Phase="Running", Reason="", readiness=true. Elapsed: 2.009605639s
Dec 14 16:10:06.977: INFO: The phase of Pod pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce is Running (Ready = true)
Dec 14 16:10:06.977: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:07.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9463" for this suite. 12/14/22 16:10:07.085
------------------------------
• [2.194 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:04.901
    Dec 14 16:10:04.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:10:04.902
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:04.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:04.944
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Dec 14 16:10:04.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: creating the pod 12/14/22 16:10:04.95
    STEP: submitting the pod to kubernetes 12/14/22 16:10:04.95
    Dec 14 16:10:04.967: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce" in namespace "pods-9463" to be "running and ready"
    Dec 14 16:10:04.971: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643259ms
    Dec 14 16:10:04.971: INFO: The phase of Pod pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:10:06.977: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce": Phase="Running", Reason="", readiness=true. Elapsed: 2.009605639s
    Dec 14 16:10:06.977: INFO: The phase of Pod pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce is Running (Ready = true)
    Dec 14 16:10:06.977: INFO: Pod "pod-exec-websocket-c1a73bd5-fdff-41ac-9e0b-7489b35f2cce" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:07.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9463" for this suite. 12/14/22 16:10:07.085
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:07.096
Dec 14 16:10:07.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:10:07.099
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:07.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:07.132
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:10:07.141
Dec 14 16:10:07.153: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4925" to be "running and ready"
Dec 14 16:10:07.159: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.931562ms
Dec 14 16:10:07.159: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:10:09.172: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018673966s
Dec 14 16:10:09.172: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 16:10:09.172: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 12/14/22 16:10:09.18
Dec 14 16:10:09.189: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4925" to be "running and ready"
Dec 14 16:10:09.192: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.251757ms
Dec 14 16:10:09.192: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:10:11.203: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01417669s
Dec 14 16:10:11.204: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Dec 14 16:10:11.204: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/14/22 16:10:11.209
STEP: delete the pod with lifecycle hook 12/14/22 16:10:11.235
Dec 14 16:10:11.248: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 16:10:11.254: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 16:10:13.256: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 16:10:13.279: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4925" for this suite. 12/14/22 16:10:13.307
------------------------------
• [SLOW TEST] [6.227 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:07.096
    Dec 14 16:10:07.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:10:07.099
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:07.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:07.132
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:10:07.141
    Dec 14 16:10:07.153: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4925" to be "running and ready"
    Dec 14 16:10:07.159: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.931562ms
    Dec 14 16:10:07.159: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:10:09.172: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018673966s
    Dec 14 16:10:09.172: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 16:10:09.172: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 12/14/22 16:10:09.18
    Dec 14 16:10:09.189: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4925" to be "running and ready"
    Dec 14 16:10:09.192: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.251757ms
    Dec 14 16:10:09.192: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:10:11.203: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01417669s
    Dec 14 16:10:11.204: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Dec 14 16:10:11.204: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/14/22 16:10:11.209
    STEP: delete the pod with lifecycle hook 12/14/22 16:10:11.235
    Dec 14 16:10:11.248: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 14 16:10:11.254: INFO: Pod pod-with-poststart-http-hook still exists
    Dec 14 16:10:13.256: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 14 16:10:13.279: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4925" for this suite. 12/14/22 16:10:13.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:13.326
Dec 14 16:10:13.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:10:13.329
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:13.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:13.373
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:10:13.379
Dec 14 16:10:13.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef" in namespace "projected-9888" to be "Succeeded or Failed"
Dec 14 16:10:13.420: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Pending", Reason="", readiness=false. Elapsed: 20.344728ms
Dec 14 16:10:15.430: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030175368s
Dec 14 16:10:17.431: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030747192s
STEP: Saw pod success 12/14/22 16:10:17.431
Dec 14 16:10:17.431: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef" satisfied condition "Succeeded or Failed"
Dec 14 16:10:17.437: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef container client-container: <nil>
STEP: delete the pod 12/14/22 16:10:17.452
Dec 14 16:10:17.481: INFO: Waiting for pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef to disappear
Dec 14 16:10:17.484: INFO: Pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:17.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9888" for this suite. 12/14/22 16:10:17.491
------------------------------
• [4.174 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:13.326
    Dec 14 16:10:13.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:10:13.329
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:13.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:13.373
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:10:13.379
    Dec 14 16:10:13.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef" in namespace "projected-9888" to be "Succeeded or Failed"
    Dec 14 16:10:13.420: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Pending", Reason="", readiness=false. Elapsed: 20.344728ms
    Dec 14 16:10:15.430: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030175368s
    Dec 14 16:10:17.431: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030747192s
    STEP: Saw pod success 12/14/22 16:10:17.431
    Dec 14 16:10:17.431: INFO: Pod "downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef" satisfied condition "Succeeded or Failed"
    Dec 14 16:10:17.437: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef container client-container: <nil>
    STEP: delete the pod 12/14/22 16:10:17.452
    Dec 14 16:10:17.481: INFO: Waiting for pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef to disappear
    Dec 14 16:10:17.484: INFO: Pod downwardapi-volume-f43e2c83-ad61-4009-85ae-06a95f97ecef no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:17.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9888" for this suite. 12/14/22 16:10:17.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:17.506
Dec 14 16:10:17.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename endpointslice 12/14/22 16:10:17.509
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:17.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:17.54
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 12/14/22 16:10:17.548
STEP: getting /apis/discovery.k8s.io 12/14/22 16:10:17.551
STEP: getting /apis/discovery.k8s.iov1 12/14/22 16:10:17.554
STEP: creating 12/14/22 16:10:17.556
STEP: getting 12/14/22 16:10:17.574
STEP: listing 12/14/22 16:10:17.577
STEP: watching 12/14/22 16:10:17.581
Dec 14 16:10:17.581: INFO: starting watch
STEP: cluster-wide listing 12/14/22 16:10:17.583
STEP: cluster-wide watching 12/14/22 16:10:17.587
Dec 14 16:10:17.587: INFO: starting watch
STEP: patching 12/14/22 16:10:17.589
STEP: updating 12/14/22 16:10:17.596
Dec 14 16:10:17.606: INFO: waiting for watch events with expected annotations
Dec 14 16:10:17.606: INFO: saw patched and updated annotations
STEP: deleting 12/14/22 16:10:17.606
STEP: deleting a collection 12/14/22 16:10:17.622
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 14 16:10:17.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5637" for this suite. 12/14/22 16:10:17.652
------------------------------
• [0.157 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:17.506
    Dec 14 16:10:17.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename endpointslice 12/14/22 16:10:17.509
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:17.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:17.54
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 12/14/22 16:10:17.548
    STEP: getting /apis/discovery.k8s.io 12/14/22 16:10:17.551
    STEP: getting /apis/discovery.k8s.iov1 12/14/22 16:10:17.554
    STEP: creating 12/14/22 16:10:17.556
    STEP: getting 12/14/22 16:10:17.574
    STEP: listing 12/14/22 16:10:17.577
    STEP: watching 12/14/22 16:10:17.581
    Dec 14 16:10:17.581: INFO: starting watch
    STEP: cluster-wide listing 12/14/22 16:10:17.583
    STEP: cluster-wide watching 12/14/22 16:10:17.587
    Dec 14 16:10:17.587: INFO: starting watch
    STEP: patching 12/14/22 16:10:17.589
    STEP: updating 12/14/22 16:10:17.596
    Dec 14 16:10:17.606: INFO: waiting for watch events with expected annotations
    Dec 14 16:10:17.606: INFO: saw patched and updated annotations
    STEP: deleting 12/14/22 16:10:17.606
    STEP: deleting a collection 12/14/22 16:10:17.622
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:10:17.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5637" for this suite. 12/14/22 16:10:17.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:10:17.67
Dec 14 16:10:17.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-watch 12/14/22 16:10:17.671
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:17.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:17.71
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Dec 14 16:10:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Creating first CR  12/14/22 16:10:20.35
Dec 14 16:10:20.357: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:20Z]] name:name1 resourceVersion:10999 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 12/14/22 16:10:30.361
Dec 14 16:10:30.376: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:30Z]] name:name2 resourceVersion:11030 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 12/14/22 16:10:40.377
Dec 14 16:10:40.392: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:40Z]] name:name1 resourceVersion:11048 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 12/14/22 16:10:50.394
Dec 14 16:10:50.409: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:50Z]] name:name2 resourceVersion:11069 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 12/14/22 16:11:00.41
Dec 14 16:11:00.428: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:40Z]] name:name1 resourceVersion:11089 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 12/14/22 16:11:10.429
Dec 14 16:11:10.448: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:50Z]] name:name2 resourceVersion:11106 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:20.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2577" for this suite. 12/14/22 16:11:20.989
------------------------------
• [SLOW TEST] [63.330 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:10:17.67
    Dec 14 16:10:17.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-watch 12/14/22 16:10:17.671
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:10:17.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:10:17.71
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Dec 14 16:10:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Creating first CR  12/14/22 16:10:20.35
    Dec 14 16:10:20.357: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:20Z]] name:name1 resourceVersion:10999 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 12/14/22 16:10:30.361
    Dec 14 16:10:30.376: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:30Z]] name:name2 resourceVersion:11030 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 12/14/22 16:10:40.377
    Dec 14 16:10:40.392: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:40Z]] name:name1 resourceVersion:11048 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 12/14/22 16:10:50.394
    Dec 14 16:10:50.409: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:50Z]] name:name2 resourceVersion:11069 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 12/14/22 16:11:00.41
    Dec 14 16:11:00.428: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:40Z]] name:name1 resourceVersion:11089 uid:866e1a2c-1a05-4304-875f-40770add4c1d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 12/14/22 16:11:10.429
    Dec 14 16:11:10.448: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T16:10:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T16:10:50Z]] name:name2 resourceVersion:11106 uid:14db48fa-314a-4c2b-9a7b-f44adcf527a2] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:20.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2577" for this suite. 12/14/22 16:11:20.989
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:21.001
Dec 14 16:11:21.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:21.004
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:21.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:21.073
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:11:21.078
Dec 14 16:11:21.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6" in namespace "downward-api-913" to be "Succeeded or Failed"
Dec 14 16:11:21.098: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.341845ms
Dec 14 16:11:23.110: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016236556s
Dec 14 16:11:25.109: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015013783s
STEP: Saw pod success 12/14/22 16:11:25.109
Dec 14 16:11:25.110: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6" satisfied condition "Succeeded or Failed"
Dec 14 16:11:25.115: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 container client-container: <nil>
STEP: delete the pod 12/14/22 16:11:25.127
Dec 14 16:11:25.147: INFO: Waiting for pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 to disappear
Dec 14 16:11:25.152: INFO: Pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:25.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-913" for this suite. 12/14/22 16:11:25.159
------------------------------
• [4.172 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:21.001
    Dec 14 16:11:21.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:21.004
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:21.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:21.073
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:11:21.078
    Dec 14 16:11:21.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6" in namespace "downward-api-913" to be "Succeeded or Failed"
    Dec 14 16:11:21.098: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.341845ms
    Dec 14 16:11:23.110: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016236556s
    Dec 14 16:11:25.109: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015013783s
    STEP: Saw pod success 12/14/22 16:11:25.109
    Dec 14 16:11:25.110: INFO: Pod "downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:25.115: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:11:25.127
    Dec 14 16:11:25.147: INFO: Waiting for pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 to disappear
    Dec 14 16:11:25.152: INFO: Pod downwardapi-volume-7548c99b-7c98-47fe-bb92-a1e8f5c298f6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:25.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-913" for this suite. 12/14/22 16:11:25.159
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:25.174
Dec 14 16:11:25.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename csistoragecapacity 12/14/22 16:11:25.177
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:25.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:25.219
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 12/14/22 16:11:25.223
STEP: getting /apis/storage.k8s.io 12/14/22 16:11:25.227
STEP: getting /apis/storage.k8s.io/v1 12/14/22 16:11:25.229
STEP: creating 12/14/22 16:11:25.231
STEP: watching 12/14/22 16:11:25.263
Dec 14 16:11:25.263: INFO: starting watch
STEP: getting 12/14/22 16:11:25.276
STEP: listing in namespace 12/14/22 16:11:25.282
STEP: listing across namespaces 12/14/22 16:11:25.287
STEP: patching 12/14/22 16:11:25.291
STEP: updating 12/14/22 16:11:25.301
Dec 14 16:11:25.310: INFO: waiting for watch events with expected annotations in namespace
Dec 14 16:11:25.310: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 12/14/22 16:11:25.311
STEP: deleting a collection 12/14/22 16:11:25.325
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:25.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8755" for this suite. 12/14/22 16:11:25.362
------------------------------
• [0.206 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:25.174
    Dec 14 16:11:25.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename csistoragecapacity 12/14/22 16:11:25.177
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:25.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:25.219
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 12/14/22 16:11:25.223
    STEP: getting /apis/storage.k8s.io 12/14/22 16:11:25.227
    STEP: getting /apis/storage.k8s.io/v1 12/14/22 16:11:25.229
    STEP: creating 12/14/22 16:11:25.231
    STEP: watching 12/14/22 16:11:25.263
    Dec 14 16:11:25.263: INFO: starting watch
    STEP: getting 12/14/22 16:11:25.276
    STEP: listing in namespace 12/14/22 16:11:25.282
    STEP: listing across namespaces 12/14/22 16:11:25.287
    STEP: patching 12/14/22 16:11:25.291
    STEP: updating 12/14/22 16:11:25.301
    Dec 14 16:11:25.310: INFO: waiting for watch events with expected annotations in namespace
    Dec 14 16:11:25.310: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 12/14/22 16:11:25.311
    STEP: deleting a collection 12/14/22 16:11:25.325
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:25.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8755" for this suite. 12/14/22 16:11:25.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:25.383
Dec 14 16:11:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:11:25.385
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:25.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:25.413
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c9cd6fd1-5e7a-4d1e-bb84-2902c113c529 12/14/22 16:11:25.415
STEP: Creating a pod to test consume configMaps 12/14/22 16:11:25.423
Dec 14 16:11:25.437: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d" in namespace "projected-1403" to be "Succeeded or Failed"
Dec 14 16:11:25.447: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187437ms
Dec 14 16:11:27.459: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02204011s
Dec 14 16:11:29.455: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018290336s
STEP: Saw pod success 12/14/22 16:11:29.456
Dec 14 16:11:29.456: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d" satisfied condition "Succeeded or Failed"
Dec 14 16:11:29.462: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:11:29.473
Dec 14 16:11:29.497: INFO: Waiting for pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d to disappear
Dec 14 16:11:29.501: INFO: Pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1403" for this suite. 12/14/22 16:11:29.508
------------------------------
• [4.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:25.383
    Dec 14 16:11:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:11:25.385
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:25.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:25.413
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c9cd6fd1-5e7a-4d1e-bb84-2902c113c529 12/14/22 16:11:25.415
    STEP: Creating a pod to test consume configMaps 12/14/22 16:11:25.423
    Dec 14 16:11:25.437: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d" in namespace "projected-1403" to be "Succeeded or Failed"
    Dec 14 16:11:25.447: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187437ms
    Dec 14 16:11:27.459: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02204011s
    Dec 14 16:11:29.455: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018290336s
    STEP: Saw pod success 12/14/22 16:11:29.456
    Dec 14 16:11:29.456: INFO: Pod "pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:29.462: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:11:29.473
    Dec 14 16:11:29.497: INFO: Waiting for pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d to disappear
    Dec 14 16:11:29.501: INFO: Pod pod-projected-configmaps-9002c8db-d60c-4122-8d4c-a1a6e53e7c4d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1403" for this suite. 12/14/22 16:11:29.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:29.52
Dec 14 16:11:29.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:11:29.523
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:29.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:29.553
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-f1a06eaa-c42c-4071-b8f5-6236412bc19b 12/14/22 16:11:29.555
STEP: Creating a pod to test consume configMaps 12/14/22 16:11:29.565
Dec 14 16:11:29.577: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6" in namespace "configmap-3087" to be "Succeeded or Failed"
Dec 14 16:11:29.583: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234138ms
Dec 14 16:11:31.598: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021023729s
Dec 14 16:11:33.591: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01348529s
STEP: Saw pod success 12/14/22 16:11:33.591
Dec 14 16:11:33.593: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6" satisfied condition "Succeeded or Failed"
Dec 14 16:11:33.598: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:11:33.611
Dec 14 16:11:33.635: INFO: Waiting for pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 to disappear
Dec 14 16:11:33.640: INFO: Pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:33.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3087" for this suite. 12/14/22 16:11:33.648
------------------------------
• [4.139 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:29.52
    Dec 14 16:11:29.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:11:29.523
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:29.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:29.553
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-f1a06eaa-c42c-4071-b8f5-6236412bc19b 12/14/22 16:11:29.555
    STEP: Creating a pod to test consume configMaps 12/14/22 16:11:29.565
    Dec 14 16:11:29.577: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6" in namespace "configmap-3087" to be "Succeeded or Failed"
    Dec 14 16:11:29.583: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234138ms
    Dec 14 16:11:31.598: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021023729s
    Dec 14 16:11:33.591: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01348529s
    STEP: Saw pod success 12/14/22 16:11:33.591
    Dec 14 16:11:33.593: INFO: Pod "pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:33.598: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:11:33.611
    Dec 14 16:11:33.635: INFO: Waiting for pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 to disappear
    Dec 14 16:11:33.640: INFO: Pod pod-configmaps-9e562fe7-1845-40ca-b064-e764379756a6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:33.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3087" for this suite. 12/14/22 16:11:33.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:33.665
Dec 14 16:11:33.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:33.668
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:33.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:33.713
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 12/14/22 16:11:33.717
Dec 14 16:11:33.727: INFO: Waiting up to 5m0s for pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0" in namespace "downward-api-1277" to be "Succeeded or Failed"
Dec 14 16:11:33.732: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.249015ms
Dec 14 16:11:35.740: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012814836s
Dec 14 16:11:37.739: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011910378s
STEP: Saw pod success 12/14/22 16:11:37.74
Dec 14 16:11:37.740: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0" satisfied condition "Succeeded or Failed"
Dec 14 16:11:37.746: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:11:37.76
Dec 14 16:11:37.797: INFO: Waiting for pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 to disappear
Dec 14 16:11:37.806: INFO: Pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:37.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1277" for this suite. 12/14/22 16:11:37.813
------------------------------
• [4.163 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:33.665
    Dec 14 16:11:33.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:33.668
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:33.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:33.713
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 12/14/22 16:11:33.717
    Dec 14 16:11:33.727: INFO: Waiting up to 5m0s for pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0" in namespace "downward-api-1277" to be "Succeeded or Failed"
    Dec 14 16:11:33.732: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.249015ms
    Dec 14 16:11:35.740: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012814836s
    Dec 14 16:11:37.739: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011910378s
    STEP: Saw pod success 12/14/22 16:11:37.74
    Dec 14 16:11:37.740: INFO: Pod "downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:37.746: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:11:37.76
    Dec 14 16:11:37.797: INFO: Waiting for pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 to disappear
    Dec 14 16:11:37.806: INFO: Pod downward-api-252a8933-92af-4dd2-ae97-7a1a508868d0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:37.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1277" for this suite. 12/14/22 16:11:37.813
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:37.83
Dec 14 16:11:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:11:37.832
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:37.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:37.861
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 12/14/22 16:11:37.865
STEP: submitting the pod to kubernetes 12/14/22 16:11:37.866
Dec 14 16:11:37.877: INFO: Waiting up to 5m0s for pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" in namespace "pods-4614" to be "running and ready"
Dec 14 16:11:37.881: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78205ms
Dec 14 16:11:37.881: INFO: The phase of Pod pod-update-82890cf3-a99e-4110-a678-0b6fd38377db is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:11:39.895: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Running", Reason="", readiness=true. Elapsed: 2.017834004s
Dec 14 16:11:39.895: INFO: The phase of Pod pod-update-82890cf3-a99e-4110-a678-0b6fd38377db is Running (Ready = true)
Dec 14 16:11:39.895: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/14/22 16:11:39.9
STEP: updating the pod 12/14/22 16:11:39.904
Dec 14 16:11:40.432: INFO: Successfully updated pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db"
Dec 14 16:11:40.432: INFO: Waiting up to 5m0s for pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" in namespace "pods-4614" to be "running"
Dec 14 16:11:40.436: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Running", Reason="", readiness=true. Elapsed: 4.116351ms
Dec 14 16:11:40.436: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 12/14/22 16:11:40.436
Dec 14 16:11:40.441: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:40.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4614" for this suite. 12/14/22 16:11:40.448
------------------------------
• [2.627 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:37.83
    Dec 14 16:11:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:11:37.832
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:37.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:37.861
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 12/14/22 16:11:37.865
    STEP: submitting the pod to kubernetes 12/14/22 16:11:37.866
    Dec 14 16:11:37.877: INFO: Waiting up to 5m0s for pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" in namespace "pods-4614" to be "running and ready"
    Dec 14 16:11:37.881: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78205ms
    Dec 14 16:11:37.881: INFO: The phase of Pod pod-update-82890cf3-a99e-4110-a678-0b6fd38377db is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:11:39.895: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Running", Reason="", readiness=true. Elapsed: 2.017834004s
    Dec 14 16:11:39.895: INFO: The phase of Pod pod-update-82890cf3-a99e-4110-a678-0b6fd38377db is Running (Ready = true)
    Dec 14 16:11:39.895: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/14/22 16:11:39.9
    STEP: updating the pod 12/14/22 16:11:39.904
    Dec 14 16:11:40.432: INFO: Successfully updated pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db"
    Dec 14 16:11:40.432: INFO: Waiting up to 5m0s for pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" in namespace "pods-4614" to be "running"
    Dec 14 16:11:40.436: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db": Phase="Running", Reason="", readiness=true. Elapsed: 4.116351ms
    Dec 14 16:11:40.436: INFO: Pod "pod-update-82890cf3-a99e-4110-a678-0b6fd38377db" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 12/14/22 16:11:40.436
    Dec 14 16:11:40.441: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:40.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4614" for this suite. 12/14/22 16:11:40.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:40.468
Dec 14 16:11:40.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sysctl 12/14/22 16:11:40.47
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:40.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:40.502
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 12/14/22 16:11:40.542
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7885" for this suite. 12/14/22 16:11:40.563
------------------------------
• [0.108 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:40.468
    Dec 14 16:11:40.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sysctl 12/14/22 16:11:40.47
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:40.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:40.502
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 12/14/22 16:11:40.542
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7885" for this suite. 12/14/22 16:11:40.563
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:40.582
Dec 14 16:11:40.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:11:40.585
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:40.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:40.623
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Dec 14 16:11:40.647: INFO: Waiting up to 5m0s for pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e" in namespace "svcaccounts-5262" to be "running"
Dec 14 16:11:40.652: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802447ms
Dec 14 16:11:42.658: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011324424s
Dec 14 16:11:42.659: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e" satisfied condition "running"
STEP: reading a file in the container 12/14/22 16:11:42.659
Dec 14 16:11:42.660: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 12/14/22 16:11:42.911
Dec 14 16:11:42.912: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 12/14/22 16:11:43.166
Dec 14 16:11:43.167: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec 14 16:11:43.389: INFO: Got root ca configmap in namespace "svcaccounts-5262"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:43.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5262" for this suite. 12/14/22 16:11:43.405
------------------------------
• [2.839 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:40.582
    Dec 14 16:11:40.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:11:40.585
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:40.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:40.623
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Dec 14 16:11:40.647: INFO: Waiting up to 5m0s for pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e" in namespace "svcaccounts-5262" to be "running"
    Dec 14 16:11:40.652: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802447ms
    Dec 14 16:11:42.658: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011324424s
    Dec 14 16:11:42.659: INFO: Pod "pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e" satisfied condition "running"
    STEP: reading a file in the container 12/14/22 16:11:42.659
    Dec 14 16:11:42.660: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 12/14/22 16:11:42.911
    Dec 14 16:11:42.912: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 12/14/22 16:11:43.166
    Dec 14 16:11:43.167: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5262 pod-service-account-aa28130d-2dd6-4e60-92c4-e813ee3a9a2e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Dec 14 16:11:43.389: INFO: Got root ca configmap in namespace "svcaccounts-5262"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:43.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5262" for this suite. 12/14/22 16:11:43.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:43.422
Dec 14 16:11:43.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:11:43.427
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:43.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:43.465
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Dec 14 16:11:43.483: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e" in namespace "kubelet-test-1150" to be "running and ready"
Dec 14 16:11:43.488: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.486363ms
Dec 14 16:11:43.488: INFO: The phase of Pod busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:11:45.495: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012717226s
Dec 14 16:11:45.495: INFO: The phase of Pod busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e is Running (Ready = true)
Dec 14 16:11:45.495: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1150" for this suite. 12/14/22 16:11:45.518
------------------------------
• [2.139 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:43.422
    Dec 14 16:11:43.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:11:43.427
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:43.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:43.465
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Dec 14 16:11:43.483: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e" in namespace "kubelet-test-1150" to be "running and ready"
    Dec 14 16:11:43.488: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.486363ms
    Dec 14 16:11:43.488: INFO: The phase of Pod busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:11:45.495: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012717226s
    Dec 14 16:11:45.495: INFO: The phase of Pod busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e is Running (Ready = true)
    Dec 14 16:11:45.495: INFO: Pod "busybox-readonly-fs8e394ae6-4e91-4fef-bf8c-a4c12d6d765e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:45.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1150" for this suite. 12/14/22 16:11:45.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:45.566
Dec 14 16:11:45.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:45.57
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:45.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:45.601
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:11:45.604
Dec 14 16:11:45.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616" in namespace "downward-api-6512" to be "Succeeded or Failed"
Dec 14 16:11:45.622: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Pending", Reason="", readiness=false. Elapsed: 3.858644ms
Dec 14 16:11:47.633: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014446739s
Dec 14 16:11:49.635: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016621164s
STEP: Saw pod success 12/14/22 16:11:49.635
Dec 14 16:11:49.636: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616" satisfied condition "Succeeded or Failed"
Dec 14 16:11:49.640: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 container client-container: <nil>
STEP: delete the pod 12/14/22 16:11:49.651
Dec 14 16:11:49.667: INFO: Waiting for pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 to disappear
Dec 14 16:11:49.676: INFO: Pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:49.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6512" for this suite. 12/14/22 16:11:49.683
------------------------------
• [4.129 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:45.566
    Dec 14 16:11:45.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:11:45.57
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:45.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:45.601
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:11:45.604
    Dec 14 16:11:45.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616" in namespace "downward-api-6512" to be "Succeeded or Failed"
    Dec 14 16:11:45.622: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Pending", Reason="", readiness=false. Elapsed: 3.858644ms
    Dec 14 16:11:47.633: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014446739s
    Dec 14 16:11:49.635: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016621164s
    STEP: Saw pod success 12/14/22 16:11:49.635
    Dec 14 16:11:49.636: INFO: Pod "downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:49.640: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:11:49.651
    Dec 14 16:11:49.667: INFO: Waiting for pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 to disappear
    Dec 14 16:11:49.676: INFO: Pod downwardapi-volume-e80a4bfb-1569-42b8-816c-793743c37616 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:49.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6512" for this suite. 12/14/22 16:11:49.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:49.697
Dec 14 16:11:49.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption 12/14/22 16:11:49.7
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:49.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:49.733
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 12/14/22 16:11:49.745
STEP: Updating PodDisruptionBudget status 12/14/22 16:11:51.758
STEP: Waiting for all pods to be running 12/14/22 16:11:51.774
Dec 14 16:11:51.781: INFO: running pods: 0 < 1
STEP: locating a running pod 12/14/22 16:11:53.79
STEP: Waiting for the pdb to be processed 12/14/22 16:11:53.806
STEP: Patching PodDisruptionBudget status 12/14/22 16:11:53.822
STEP: Waiting for the pdb to be processed 12/14/22 16:11:53.857
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7299" for this suite. 12/14/22 16:11:53.873
------------------------------
• [4.186 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:49.697
    Dec 14 16:11:49.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption 12/14/22 16:11:49.7
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:49.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:49.733
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 12/14/22 16:11:49.745
    STEP: Updating PodDisruptionBudget status 12/14/22 16:11:51.758
    STEP: Waiting for all pods to be running 12/14/22 16:11:51.774
    Dec 14 16:11:51.781: INFO: running pods: 0 < 1
    STEP: locating a running pod 12/14/22 16:11:53.79
    STEP: Waiting for the pdb to be processed 12/14/22 16:11:53.806
    STEP: Patching PodDisruptionBudget status 12/14/22 16:11:53.822
    STEP: Waiting for the pdb to be processed 12/14/22 16:11:53.857
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7299" for this suite. 12/14/22 16:11:53.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:53.885
Dec 14 16:11:53.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:11:53.888
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:53.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:53.915
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 12/14/22 16:11:53.919
Dec 14 16:11:53.919: INFO: Creating e2e-svc-a-97qcp
Dec 14 16:11:53.939: INFO: Creating e2e-svc-b-kwkhr
Dec 14 16:11:53.975: INFO: Creating e2e-svc-c-gpv6v
STEP: deleting service collection 12/14/22 16:11:54.003
Dec 14 16:11:54.079: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9997" for this suite. 12/14/22 16:11:54.086
------------------------------
• [0.222 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:53.885
    Dec 14 16:11:53.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:11:53.888
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:53.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:53.915
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 12/14/22 16:11:53.919
    Dec 14 16:11:53.919: INFO: Creating e2e-svc-a-97qcp
    Dec 14 16:11:53.939: INFO: Creating e2e-svc-b-kwkhr
    Dec 14 16:11:53.975: INFO: Creating e2e-svc-c-gpv6v
    STEP: deleting service collection 12/14/22 16:11:54.003
    Dec 14 16:11:54.079: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9997" for this suite. 12/14/22 16:11:54.086
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:54.108
Dec 14 16:11:54.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:11:54.111
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:54.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:54.147
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 12/14/22 16:11:54.151
STEP: Getting a ResourceQuota 12/14/22 16:11:54.159
STEP: Updating a ResourceQuota 12/14/22 16:11:54.164
STEP: Verifying a ResourceQuota was modified 12/14/22 16:11:54.188
STEP: Deleting a ResourceQuota 12/14/22 16:11:54.196
STEP: Verifying the deleted ResourceQuota 12/14/22 16:11:54.212
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:54.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5151" for this suite. 12/14/22 16:11:54.22
------------------------------
• [0.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:54.108
    Dec 14 16:11:54.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:11:54.111
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:54.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:54.147
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 12/14/22 16:11:54.151
    STEP: Getting a ResourceQuota 12/14/22 16:11:54.159
    STEP: Updating a ResourceQuota 12/14/22 16:11:54.164
    STEP: Verifying a ResourceQuota was modified 12/14/22 16:11:54.188
    STEP: Deleting a ResourceQuota 12/14/22 16:11:54.196
    STEP: Verifying the deleted ResourceQuota 12/14/22 16:11:54.212
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:54.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5151" for this suite. 12/14/22 16:11:54.22
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:54.234
Dec 14 16:11:54.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename containers 12/14/22 16:11:54.243
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:54.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:54.279
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 12/14/22 16:11:54.284
Dec 14 16:11:54.303: INFO: Waiting up to 5m0s for pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c" in namespace "containers-6889" to be "Succeeded or Failed"
Dec 14 16:11:54.308: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525058ms
Dec 14 16:11:56.316: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013307431s
Dec 14 16:11:58.316: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013104823s
STEP: Saw pod success 12/14/22 16:11:58.316
Dec 14 16:11:58.317: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c" satisfied condition "Succeeded or Failed"
Dec 14 16:11:58.323: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-13851c06-e669-4260-8a1d-073651d3075c container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:11:58.333
Dec 14 16:11:58.354: INFO: Waiting for pod client-containers-13851c06-e669-4260-8a1d-073651d3075c to disappear
Dec 14 16:11:58.358: INFO: Pod client-containers-13851c06-e669-4260-8a1d-073651d3075c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:11:58.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6889" for this suite. 12/14/22 16:11:58.364
------------------------------
• [4.138 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:54.234
    Dec 14 16:11:54.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename containers 12/14/22 16:11:54.243
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:54.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:54.279
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 12/14/22 16:11:54.284
    Dec 14 16:11:54.303: INFO: Waiting up to 5m0s for pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c" in namespace "containers-6889" to be "Succeeded or Failed"
    Dec 14 16:11:54.308: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525058ms
    Dec 14 16:11:56.316: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013307431s
    Dec 14 16:11:58.316: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013104823s
    STEP: Saw pod success 12/14/22 16:11:58.316
    Dec 14 16:11:58.317: INFO: Pod "client-containers-13851c06-e669-4260-8a1d-073651d3075c" satisfied condition "Succeeded or Failed"
    Dec 14 16:11:58.323: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-containers-13851c06-e669-4260-8a1d-073651d3075c container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:11:58.333
    Dec 14 16:11:58.354: INFO: Waiting for pod client-containers-13851c06-e669-4260-8a1d-073651d3075c to disappear
    Dec 14 16:11:58.358: INFO: Pod client-containers-13851c06-e669-4260-8a1d-073651d3075c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:11:58.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6889" for this suite. 12/14/22 16:11:58.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:11:58.375
Dec 14 16:11:58.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:11:58.377
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:58.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:58.41
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Dec 14 16:11:58.415: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 16:11:58.432: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:12:03.440: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:12:03.441
Dec 14 16:12:03.441: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 16:12:03.455: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 16:12:03.468: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 16:12:05.489: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 16:12:05.494: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:12:05.509: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1240  e6598a47-33c4-474f-9813-6e82f205a24f 11564 1 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d77218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:12:03 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-14 16:12:05 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:12:05.515: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1240  8fc90bbd-8ebb-4266-aa99-977b549e1086 11554 1 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e6598a47-33c4-474f-9813-6e82f205a24f 0xc003d77717 0xc003d77718}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6598a47-33c4-474f-9813-6e82f205a24f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d777c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:12:05.515: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 16:12:05.515: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1240  f85b5101-2117-4bd8-a51b-4d691d49685c 11563 2 2022-12-14 16:11:58 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e6598a47-33c4-474f-9813-6e82f205a24f 0xc003d775e7 0xc003d775e8}] [] [{e2e.test Update apps/v1 2022-12-14 16:11:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6598a47-33c4-474f-9813-6e82f205a24f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d776a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:12:05.525: INFO: Pod "test-rolling-update-deployment-7549d9f46d-f2bqf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-f2bqf test-rolling-update-deployment-7549d9f46d- deployment-1240  14b5c02e-ab41-4467-b513-5c7cb9fdefbe 11553 0 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8fc90bbd-8ebb-4266-aa99-977b549e1086 0xc003928ef7 0xc003928ef8}] [] [{kube-controller-manager Update v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fc90bbd-8ebb-4266-aa99-977b549e1086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dmqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dmqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.120,StartTime:2022-12-14 16:12:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:12:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://e08646d1ae65122c8a6b87aafd9d72ade74fe33d7e34e42bd95dc1619661d473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1240" for this suite. 12/14/22 16:12:05.535
------------------------------
• [SLOW TEST] [7.174 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:11:58.375
    Dec 14 16:11:58.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:11:58.377
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:11:58.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:11:58.41
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Dec 14 16:11:58.415: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Dec 14 16:11:58.432: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 16:12:03.440: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:12:03.441
    Dec 14 16:12:03.441: INFO: Creating deployment "test-rolling-update-deployment"
    Dec 14 16:12:03.455: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Dec 14 16:12:03.468: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Dec 14 16:12:05.489: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Dec 14 16:12:05.494: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:12:05.509: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1240  e6598a47-33c4-474f-9813-6e82f205a24f 11564 1 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d77218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:12:03 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-14 16:12:05 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 16:12:05.515: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1240  8fc90bbd-8ebb-4266-aa99-977b549e1086 11554 1 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e6598a47-33c4-474f-9813-6e82f205a24f 0xc003d77717 0xc003d77718}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6598a47-33c4-474f-9813-6e82f205a24f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d777c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:12:05.515: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Dec 14 16:12:05.515: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1240  f85b5101-2117-4bd8-a51b-4d691d49685c 11563 2 2022-12-14 16:11:58 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e6598a47-33c4-474f-9813-6e82f205a24f 0xc003d775e7 0xc003d775e8}] [] [{e2e.test Update apps/v1 2022-12-14 16:11:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6598a47-33c4-474f-9813-6e82f205a24f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d776a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:12:05.525: INFO: Pod "test-rolling-update-deployment-7549d9f46d-f2bqf" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-f2bqf test-rolling-update-deployment-7549d9f46d- deployment-1240  14b5c02e-ab41-4467-b513-5c7cb9fdefbe 11553 0 2022-12-14 16:12:03 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8fc90bbd-8ebb-4266-aa99-977b549e1086 0xc003928ef7 0xc003928ef8}] [] [{kube-controller-manager Update v1 2022-12-14 16:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fc90bbd-8ebb-4266-aa99-977b549e1086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:12:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dmqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dmqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:12:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.120,StartTime:2022-12-14 16:12:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:12:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://e08646d1ae65122c8a6b87aafd9d72ade74fe33d7e34e42bd95dc1619661d473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1240" for this suite. 12/14/22 16:12:05.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:05.554
Dec 14 16:12:05.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:12:05.556
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:05.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:05.59
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 16:12:05.634
Dec 14 16:12:05.646: INFO: Waiting up to 5m0s for pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a" in namespace "emptydir-8074" to be "Succeeded or Failed"
Dec 14 16:12:05.650: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829731ms
Dec 14 16:12:07.654: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008489081s
Dec 14 16:12:09.656: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009791513s
STEP: Saw pod success 12/14/22 16:12:09.656
Dec 14 16:12:09.656: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a" satisfied condition "Succeeded or Failed"
Dec 14 16:12:09.661: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a container test-container: <nil>
STEP: delete the pod 12/14/22 16:12:09.67
Dec 14 16:12:09.689: INFO: Waiting for pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a to disappear
Dec 14 16:12:09.693: INFO: Pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:09.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8074" for this suite. 12/14/22 16:12:09.699
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:05.554
    Dec 14 16:12:05.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:12:05.556
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:05.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:05.59
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 16:12:05.634
    Dec 14 16:12:05.646: INFO: Waiting up to 5m0s for pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a" in namespace "emptydir-8074" to be "Succeeded or Failed"
    Dec 14 16:12:05.650: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829731ms
    Dec 14 16:12:07.654: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008489081s
    Dec 14 16:12:09.656: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009791513s
    STEP: Saw pod success 12/14/22 16:12:09.656
    Dec 14 16:12:09.656: INFO: Pod "pod-2681ed20-46a5-4fdc-b279-1a29f985a27a" satisfied condition "Succeeded or Failed"
    Dec 14 16:12:09.661: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a container test-container: <nil>
    STEP: delete the pod 12/14/22 16:12:09.67
    Dec 14 16:12:09.689: INFO: Waiting for pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a to disappear
    Dec 14 16:12:09.693: INFO: Pod pod-2681ed20-46a5-4fdc-b279-1a29f985a27a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:09.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8074" for this suite. 12/14/22 16:12:09.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:09.712
Dec 14 16:12:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:12:09.714
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:09.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:09.744
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 12/14/22 16:12:09.747
STEP: watching for the ServiceAccount to be added 12/14/22 16:12:09.766
STEP: patching the ServiceAccount 12/14/22 16:12:09.769
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/14/22 16:12:09.795
STEP: deleting the ServiceAccount 12/14/22 16:12:09.808
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5198" for this suite. 12/14/22 16:12:09.84
------------------------------
• [0.141 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:09.712
    Dec 14 16:12:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:12:09.714
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:09.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:09.744
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 12/14/22 16:12:09.747
    STEP: watching for the ServiceAccount to be added 12/14/22 16:12:09.766
    STEP: patching the ServiceAccount 12/14/22 16:12:09.769
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/14/22 16:12:09.795
    STEP: deleting the ServiceAccount 12/14/22 16:12:09.808
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5198" for this suite. 12/14/22 16:12:09.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:09.855
Dec 14 16:12:09.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 16:12:09.858
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:09.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:09.884
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Dec 14 16:12:09.911: INFO: Waiting up to 5m0s for pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a" in namespace "emptydir-wrapper-4290" to be "running and ready"
Dec 14 16:12:09.922: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274061ms
Dec 14 16:12:09.922: INFO: The phase of Pod pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:12:11.933: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a": Phase="Running", Reason="", readiness=true. Elapsed: 2.021671088s
Dec 14 16:12:11.933: INFO: The phase of Pod pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a is Running (Ready = true)
Dec 14 16:12:11.933: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a" satisfied condition "running and ready"
STEP: Cleaning up the secret 12/14/22 16:12:11.938
STEP: Cleaning up the configmap 12/14/22 16:12:11.948
STEP: Cleaning up the pod 12/14/22 16:12:11.956
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:11.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4290" for this suite. 12/14/22 16:12:11.984
------------------------------
• [2.138 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:09.855
    Dec 14 16:12:09.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 16:12:09.858
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:09.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:09.884
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Dec 14 16:12:09.911: INFO: Waiting up to 5m0s for pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a" in namespace "emptydir-wrapper-4290" to be "running and ready"
    Dec 14 16:12:09.922: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274061ms
    Dec 14 16:12:09.922: INFO: The phase of Pod pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:12:11.933: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a": Phase="Running", Reason="", readiness=true. Elapsed: 2.021671088s
    Dec 14 16:12:11.933: INFO: The phase of Pod pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a is Running (Ready = true)
    Dec 14 16:12:11.933: INFO: Pod "pod-secrets-3e83ecf6-379c-4b4d-9563-297af9f9767a" satisfied condition "running and ready"
    STEP: Cleaning up the secret 12/14/22 16:12:11.938
    STEP: Cleaning up the configmap 12/14/22 16:12:11.948
    STEP: Cleaning up the pod 12/14/22 16:12:11.956
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:11.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4290" for this suite. 12/14/22 16:12:11.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:11.997
Dec 14 16:12:11.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:12:12.004
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:12.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:12.037
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3547 12/14/22 16:12:12.042
STEP: creating a selector 12/14/22 16:12:12.042
STEP: Creating the service pods in kubernetes 12/14/22 16:12:12.042
Dec 14 16:12:12.042: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 16:12:12.102: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3547" to be "running and ready"
Dec 14 16:12:12.130: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.89489ms
Dec 14 16:12:12.130: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:12:14.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.034812576s
Dec 14 16:12:14.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:12:16.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037463499s
Dec 14 16:12:16.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:12:18.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.034348488s
Dec 14 16:12:18.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:12:20.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042170076s
Dec 14 16:12:20.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:12:22.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.035540201s
Dec 14 16:12:22.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:12:24.138: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.036030596s
Dec 14 16:12:24.138: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 16:12:24.138: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 16:12:24.143: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3547" to be "running and ready"
Dec 14 16:12:24.147: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.736464ms
Dec 14 16:12:24.147: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 16:12:24.147: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 14 16:12:24.158: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3547" to be "running and ready"
Dec 14 16:12:24.166: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.073188ms
Dec 14 16:12:24.166: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 14 16:12:24.166: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 16:12:24.172
Dec 14 16:12:24.188: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3547" to be "running"
Dec 14 16:12:24.194: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270299ms
Dec 14 16:12:26.205: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016519437s
Dec 14 16:12:26.205: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 16:12:26.212: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3547" to be "running"
Dec 14 16:12:26.219: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.66467ms
Dec 14 16:12:26.219: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 14 16:12:26.224: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 16:12:26.224: INFO: Going to poll 10.233.64.52 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:12:26.230: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:12:26.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:12:26.231: INFO: ExecWithOptions: Clientset creation
Dec 14 16:12:26.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:12:26.382: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 16:12:26.382: INFO: Going to poll 10.233.66.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:12:26.388: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:12:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:12:26.390: INFO: ExecWithOptions: Clientset creation
Dec 14 16:12:26.390: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:12:26.502: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 16:12:26.502: INFO: Going to poll 10.233.67.123 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:12:26.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.67.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:12:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:12:26.511: INFO: ExecWithOptions: Clientset creation
Dec 14 16:12:26.511: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.67.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:12:26.606: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:26.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3547" for this suite. 12/14/22 16:12:26.616
------------------------------
• [SLOW TEST] [14.635 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:11.997
    Dec 14 16:12:11.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:12:12.004
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:12.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:12.037
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3547 12/14/22 16:12:12.042
    STEP: creating a selector 12/14/22 16:12:12.042
    STEP: Creating the service pods in kubernetes 12/14/22 16:12:12.042
    Dec 14 16:12:12.042: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 16:12:12.102: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3547" to be "running and ready"
    Dec 14 16:12:12.130: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.89489ms
    Dec 14 16:12:12.130: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:12:14.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.034812576s
    Dec 14 16:12:14.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:12:16.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037463499s
    Dec 14 16:12:16.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:12:18.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.034348488s
    Dec 14 16:12:18.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:12:20.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042170076s
    Dec 14 16:12:20.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:12:22.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.035540201s
    Dec 14 16:12:22.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:12:24.138: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.036030596s
    Dec 14 16:12:24.138: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 16:12:24.138: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 16:12:24.143: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3547" to be "running and ready"
    Dec 14 16:12:24.147: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.736464ms
    Dec 14 16:12:24.147: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 16:12:24.147: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 14 16:12:24.158: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3547" to be "running and ready"
    Dec 14 16:12:24.166: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.073188ms
    Dec 14 16:12:24.166: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 14 16:12:24.166: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 16:12:24.172
    Dec 14 16:12:24.188: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3547" to be "running"
    Dec 14 16:12:24.194: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270299ms
    Dec 14 16:12:26.205: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016519437s
    Dec 14 16:12:26.205: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 16:12:26.212: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3547" to be "running"
    Dec 14 16:12:26.219: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.66467ms
    Dec 14 16:12:26.219: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 14 16:12:26.224: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 14 16:12:26.224: INFO: Going to poll 10.233.64.52 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:12:26.230: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:12:26.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:12:26.231: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:12:26.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:12:26.382: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 14 16:12:26.382: INFO: Going to poll 10.233.66.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:12:26.388: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:12:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:12:26.390: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:12:26.390: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:12:26.502: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 14 16:12:26.502: INFO: Going to poll 10.233.67.123 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:12:26.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.67.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3547 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:12:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:12:26.511: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:12:26.511: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3547/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.67.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:12:26.606: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:26.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3547" for this suite. 12/14/22 16:12:26.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:26.64
Dec 14 16:12:26.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:12:26.642
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:26.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:26.68
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6460 12/14/22 16:12:26.683
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 16:12:26.702
STEP: creating service externalsvc in namespace services-6460 12/14/22 16:12:26.702
STEP: creating replication controller externalsvc in namespace services-6460 12/14/22 16:12:26.718
I1214 16:12:26.733086      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6460, replica count: 2
I1214 16:12:29.786303      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 12/14/22 16:12:29.791
Dec 14 16:12:29.830: INFO: Creating new exec pod
Dec 14 16:12:29.845: INFO: Waiting up to 5m0s for pod "execpodc5rgr" in namespace "services-6460" to be "running"
Dec 14 16:12:29.849: INFO: Pod "execpodc5rgr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521052ms
Dec 14 16:12:31.854: INFO: Pod "execpodc5rgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009711468s
Dec 14 16:12:31.854: INFO: Pod "execpodc5rgr" satisfied condition "running"
Dec 14 16:12:31.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-6460 exec execpodc5rgr -- /bin/sh -x -c nslookup nodeport-service.services-6460.svc.cluster.local'
Dec 14 16:12:32.320: INFO: stderr: "+ nslookup nodeport-service.services-6460.svc.cluster.local\n"
Dec 14 16:12:32.320: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-6460.svc.cluster.local\tcanonical name = externalsvc.services-6460.svc.cluster.local.\nName:\texternalsvc.services-6460.svc.cluster.local\nAddress: 10.233.46.178\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6460, will wait for the garbage collector to delete the pods 12/14/22 16:12:32.32
Dec 14 16:12:32.392: INFO: Deleting ReplicationController externalsvc took: 17.467956ms
Dec 14 16:12:32.492: INFO: Terminating ReplicationController externalsvc pods took: 100.405127ms
Dec 14 16:12:34.717: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:12:34.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6460" for this suite. 12/14/22 16:12:34.744
------------------------------
• [SLOW TEST] [8.120 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:26.64
    Dec 14 16:12:26.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:12:26.642
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:26.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:26.68
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-6460 12/14/22 16:12:26.683
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 16:12:26.702
    STEP: creating service externalsvc in namespace services-6460 12/14/22 16:12:26.702
    STEP: creating replication controller externalsvc in namespace services-6460 12/14/22 16:12:26.718
    I1214 16:12:26.733086      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6460, replica count: 2
    I1214 16:12:29.786303      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 12/14/22 16:12:29.791
    Dec 14 16:12:29.830: INFO: Creating new exec pod
    Dec 14 16:12:29.845: INFO: Waiting up to 5m0s for pod "execpodc5rgr" in namespace "services-6460" to be "running"
    Dec 14 16:12:29.849: INFO: Pod "execpodc5rgr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521052ms
    Dec 14 16:12:31.854: INFO: Pod "execpodc5rgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009711468s
    Dec 14 16:12:31.854: INFO: Pod "execpodc5rgr" satisfied condition "running"
    Dec 14 16:12:31.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-6460 exec execpodc5rgr -- /bin/sh -x -c nslookup nodeport-service.services-6460.svc.cluster.local'
    Dec 14 16:12:32.320: INFO: stderr: "+ nslookup nodeport-service.services-6460.svc.cluster.local\n"
    Dec 14 16:12:32.320: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-6460.svc.cluster.local\tcanonical name = externalsvc.services-6460.svc.cluster.local.\nName:\texternalsvc.services-6460.svc.cluster.local\nAddress: 10.233.46.178\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6460, will wait for the garbage collector to delete the pods 12/14/22 16:12:32.32
    Dec 14 16:12:32.392: INFO: Deleting ReplicationController externalsvc took: 17.467956ms
    Dec 14 16:12:32.492: INFO: Terminating ReplicationController externalsvc pods took: 100.405127ms
    Dec 14 16:12:34.717: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:12:34.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6460" for this suite. 12/14/22 16:12:34.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:12:34.775
Dec 14 16:12:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 16:12:34.781
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:34.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:34.812
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/14/22 16:12:34.817
STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:12:35.19
Dec 14 16:12:35.208: INFO: Pod name wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65: Found 0 pods out of 5
Dec 14 16:12:40.231: INFO: Pod name wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 16:12:40.233
Dec 14 16:12:40.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:40.255: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 21.432437ms
Dec 14 16:12:42.267: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032882967s
Dec 14 16:12:44.267: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03251112s
Dec 14 16:12:46.272: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038154062s
Dec 14 16:12:48.263: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028797779s
Dec 14 16:12:50.268: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Running", Reason="", readiness=true. Elapsed: 10.033901976s
Dec 14 16:12:50.268: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp" satisfied condition "running"
Dec 14 16:12:50.268: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:50.275: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd": Phase="Running", Reason="", readiness=true. Elapsed: 6.751934ms
Dec 14 16:12:50.275: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd" satisfied condition "running"
Dec 14 16:12:50.275: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:50.283: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9": Phase="Running", Reason="", readiness=true. Elapsed: 7.201198ms
Dec 14 16:12:50.283: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9" satisfied condition "running"
Dec 14 16:12:50.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:50.289: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88": Phase="Running", Reason="", readiness=true. Elapsed: 6.439569ms
Dec 14 16:12:50.289: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88" satisfied condition "running"
Dec 14 16:12:50.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:50.295: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.627883ms
Dec 14 16:12:52.307: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.017670299s
Dec 14 16:12:52.307: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:12:52.307
Dec 14 16:12:52.386: INFO: Deleting ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 took: 17.549922ms
Dec 14 16:12:52.487: INFO: Terminating ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 pods took: 100.844174ms
STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:12:54.9
Dec 14 16:12:54.928: INFO: Pod name wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f: Found 0 pods out of 5
Dec 14 16:12:59.937: INFO: Pod name wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 16:12:59.938
Dec 14 16:12:59.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:12:59.944: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.293997ms
Dec 14 16:13:01.952: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014319995s
Dec 14 16:13:03.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013535871s
Dec 14 16:13:05.952: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013718127s
Dec 14 16:13:07.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013279248s
Dec 14 16:13:07.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp" satisfied condition "running"
Dec 14 16:13:07.951: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:07.955: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03406ms
Dec 14 16:13:09.966: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td": Phase="Running", Reason="", readiness=true. Elapsed: 2.014410271s
Dec 14 16:13:09.966: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td" satisfied condition "running"
Dec 14 16:13:09.966: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:09.973: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd": Phase="Running", Reason="", readiness=true. Elapsed: 6.922831ms
Dec 14 16:13:09.973: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd" satisfied condition "running"
Dec 14 16:13:09.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:09.978: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs": Phase="Running", Reason="", readiness=true. Elapsed: 4.865243ms
Dec 14 16:13:09.978: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs" satisfied condition "running"
Dec 14 16:13:09.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:09.982: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.535637ms
Dec 14 16:13:09.982: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:13:09.982
Dec 14 16:13:10.049: INFO: Deleting ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f took: 10.046197ms
Dec 14 16:13:10.150: INFO: Terminating ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f pods took: 100.938795ms
STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:13:13.062
Dec 14 16:13:13.089: INFO: Pod name wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f: Found 0 pods out of 5
Dec 14 16:13:18.102: INFO: Pod name wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/14/22 16:13:18.102
Dec 14 16:13:18.102: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:18.107: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062408ms
Dec 14 16:13:20.116: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013841796s
Dec 14 16:13:22.117: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014642128s
Dec 14 16:13:24.118: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015302243s
Dec 14 16:13:26.117: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014685885s
Dec 14 16:13:28.115: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Running", Reason="", readiness=true. Elapsed: 10.0127618s
Dec 14 16:13:28.115: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m" satisfied condition "running"
Dec 14 16:13:28.115: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:28.122: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg": Phase="Running", Reason="", readiness=true. Elapsed: 6.289932ms
Dec 14 16:13:28.122: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg" satisfied condition "running"
Dec 14 16:13:28.122: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:28.127: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c": Phase="Running", Reason="", readiness=true. Elapsed: 5.258072ms
Dec 14 16:13:28.127: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c" satisfied condition "running"
Dec 14 16:13:28.127: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:28.138: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4": Phase="Running", Reason="", readiness=true. Elapsed: 10.189505ms
Dec 14 16:13:28.138: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4" satisfied condition "running"
Dec 14 16:13:28.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq" in namespace "emptydir-wrapper-8297" to be "running"
Dec 14 16:13:28.143: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq": Phase="Running", Reason="", readiness=true. Elapsed: 5.55274ms
Dec 14 16:13:28.143: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:13:28.143
Dec 14 16:13:28.215: INFO: Deleting ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f took: 13.384351ms
Dec 14 16:13:28.316: INFO: Terminating ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f pods took: 100.803504ms
STEP: Cleaning up the configMaps 12/14/22 16:13:31.317
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:13:31.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8297" for this suite. 12/14/22 16:13:31.819
------------------------------
• [SLOW TEST] [57.052 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:12:34.775
    Dec 14 16:12:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir-wrapper 12/14/22 16:12:34.781
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:12:34.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:12:34.812
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/14/22 16:12:34.817
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:12:35.19
    Dec 14 16:12:35.208: INFO: Pod name wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65: Found 0 pods out of 5
    Dec 14 16:12:40.231: INFO: Pod name wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 16:12:40.233
    Dec 14 16:12:40.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:40.255: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 21.432437ms
    Dec 14 16:12:42.267: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032882967s
    Dec 14 16:12:44.267: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03251112s
    Dec 14 16:12:46.272: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038154062s
    Dec 14 16:12:48.263: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028797779s
    Dec 14 16:12:50.268: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp": Phase="Running", Reason="", readiness=true. Elapsed: 10.033901976s
    Dec 14 16:12:50.268: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-282dp" satisfied condition "running"
    Dec 14 16:12:50.268: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:50.275: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd": Phase="Running", Reason="", readiness=true. Elapsed: 6.751934ms
    Dec 14 16:12:50.275: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-82zwd" satisfied condition "running"
    Dec 14 16:12:50.275: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:50.283: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9": Phase="Running", Reason="", readiness=true. Elapsed: 7.201198ms
    Dec 14 16:12:50.283: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-9brp9" satisfied condition "running"
    Dec 14 16:12:50.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:50.289: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88": Phase="Running", Reason="", readiness=true. Elapsed: 6.439569ms
    Dec 14 16:12:50.289: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-p8f88" satisfied condition "running"
    Dec 14 16:12:50.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:50.295: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.627883ms
    Dec 14 16:12:52.307: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.017670299s
    Dec 14 16:12:52.307: INFO: Pod "wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65-ztzs7" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:12:52.307
    Dec 14 16:12:52.386: INFO: Deleting ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 took: 17.549922ms
    Dec 14 16:12:52.487: INFO: Terminating ReplicationController wrapped-volume-race-245b96c8-c29c-449c-b316-fdd41cb40a65 pods took: 100.844174ms
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:12:54.9
    Dec 14 16:12:54.928: INFO: Pod name wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f: Found 0 pods out of 5
    Dec 14 16:12:59.937: INFO: Pod name wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 16:12:59.938
    Dec 14 16:12:59.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:12:59.944: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.293997ms
    Dec 14 16:13:01.952: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014319995s
    Dec 14 16:13:03.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013535871s
    Dec 14 16:13:05.952: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013718127s
    Dec 14 16:13:07.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013279248s
    Dec 14 16:13:07.951: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-75hmp" satisfied condition "running"
    Dec 14 16:13:07.951: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:07.955: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03406ms
    Dec 14 16:13:09.966: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td": Phase="Running", Reason="", readiness=true. Elapsed: 2.014410271s
    Dec 14 16:13:09.966: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-bg8td" satisfied condition "running"
    Dec 14 16:13:09.966: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:09.973: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd": Phase="Running", Reason="", readiness=true. Elapsed: 6.922831ms
    Dec 14 16:13:09.973: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-lccfd" satisfied condition "running"
    Dec 14 16:13:09.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:09.978: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs": Phase="Running", Reason="", readiness=true. Elapsed: 4.865243ms
    Dec 14 16:13:09.978: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-mqxrs" satisfied condition "running"
    Dec 14 16:13:09.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:09.982: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.535637ms
    Dec 14 16:13:09.982: INFO: Pod "wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f-s8sb4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:13:09.982
    Dec 14 16:13:10.049: INFO: Deleting ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f took: 10.046197ms
    Dec 14 16:13:10.150: INFO: Terminating ReplicationController wrapped-volume-race-282d592c-098e-443f-b351-760643a9e87f pods took: 100.938795ms
    STEP: Creating RC which spawns configmap-volume pods 12/14/22 16:13:13.062
    Dec 14 16:13:13.089: INFO: Pod name wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f: Found 0 pods out of 5
    Dec 14 16:13:18.102: INFO: Pod name wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/14/22 16:13:18.102
    Dec 14 16:13:18.102: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:18.107: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062408ms
    Dec 14 16:13:20.116: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013841796s
    Dec 14 16:13:22.117: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014642128s
    Dec 14 16:13:24.118: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015302243s
    Dec 14 16:13:26.117: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014685885s
    Dec 14 16:13:28.115: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m": Phase="Running", Reason="", readiness=true. Elapsed: 10.0127618s
    Dec 14 16:13:28.115: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-72n6m" satisfied condition "running"
    Dec 14 16:13:28.115: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:28.122: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg": Phase="Running", Reason="", readiness=true. Elapsed: 6.289932ms
    Dec 14 16:13:28.122: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-dngxg" satisfied condition "running"
    Dec 14 16:13:28.122: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:28.127: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c": Phase="Running", Reason="", readiness=true. Elapsed: 5.258072ms
    Dec 14 16:13:28.127: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-fdc9c" satisfied condition "running"
    Dec 14 16:13:28.127: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:28.138: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4": Phase="Running", Reason="", readiness=true. Elapsed: 10.189505ms
    Dec 14 16:13:28.138: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-lkrw4" satisfied condition "running"
    Dec 14 16:13:28.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq" in namespace "emptydir-wrapper-8297" to be "running"
    Dec 14 16:13:28.143: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq": Phase="Running", Reason="", readiness=true. Elapsed: 5.55274ms
    Dec 14 16:13:28.143: INFO: Pod "wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f-npppq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f in namespace emptydir-wrapper-8297, will wait for the garbage collector to delete the pods 12/14/22 16:13:28.143
    Dec 14 16:13:28.215: INFO: Deleting ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f took: 13.384351ms
    Dec 14 16:13:28.316: INFO: Terminating ReplicationController wrapped-volume-race-a133e061-2cde-4732-a9ee-3f3d74fbe03f pods took: 100.803504ms
    STEP: Cleaning up the configMaps 12/14/22 16:13:31.317
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:13:31.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8297" for this suite. 12/14/22 16:13:31.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:13:31.846
Dec 14 16:13:31.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:13:31.849
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:13:31.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:13:31.88
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Dec 14 16:13:31.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:13:34.403
Dec 14 16:13:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 create -f -'
Dec 14 16:13:35.659: INFO: stderr: ""
Dec 14 16:13:35.659: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 16:13:35.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 delete e2e-test-crd-publish-openapi-4373-crds test-cr'
Dec 14 16:13:35.878: INFO: stderr: ""
Dec 14 16:13:35.878: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 16:13:35.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 apply -f -'
Dec 14 16:13:36.742: INFO: stderr: ""
Dec 14 16:13:36.742: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 16:13:36.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 delete e2e-test-crd-publish-openapi-4373-crds test-cr'
Dec 14 16:13:36.875: INFO: stderr: ""
Dec 14 16:13:36.875: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 12/14/22 16:13:36.875
Dec 14 16:13:36.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 explain e2e-test-crd-publish-openapi-4373-crds'
Dec 14 16:13:37.321: INFO: stderr: ""
Dec 14 16:13:37.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4373-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:13:40.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7209" for this suite. 12/14/22 16:13:40.083
------------------------------
• [SLOW TEST] [8.249 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:13:31.846
    Dec 14 16:13:31.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:13:31.849
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:13:31.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:13:31.88
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Dec 14 16:13:31.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:13:34.403
    Dec 14 16:13:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 create -f -'
    Dec 14 16:13:35.659: INFO: stderr: ""
    Dec 14 16:13:35.659: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 14 16:13:35.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 delete e2e-test-crd-publish-openapi-4373-crds test-cr'
    Dec 14 16:13:35.878: INFO: stderr: ""
    Dec 14 16:13:35.878: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Dec 14 16:13:35.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 apply -f -'
    Dec 14 16:13:36.742: INFO: stderr: ""
    Dec 14 16:13:36.742: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 14 16:13:36.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 --namespace=crd-publish-openapi-7209 delete e2e-test-crd-publish-openapi-4373-crds test-cr'
    Dec 14 16:13:36.875: INFO: stderr: ""
    Dec 14 16:13:36.875: INFO: stdout: "e2e-test-crd-publish-openapi-4373-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 12/14/22 16:13:36.875
    Dec 14 16:13:36.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-7209 explain e2e-test-crd-publish-openapi-4373-crds'
    Dec 14 16:13:37.321: INFO: stderr: ""
    Dec 14 16:13:37.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4373-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:13:40.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7209" for this suite. 12/14/22 16:13:40.083
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:13:40.095
Dec 14 16:13:40.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 16:13:40.097
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:13:40.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:13:40.124
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8423 12/14/22 16:13:40.127
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 12/14/22 16:13:40.137
STEP: Creating pod with conflicting port in namespace statefulset-8423 12/14/22 16:13:40.152
STEP: Waiting until pod test-pod will start running in namespace statefulset-8423 12/14/22 16:13:40.166
Dec 14 16:13:40.166: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8423" to be "running"
Dec 14 16:13:40.177: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.15658ms
Dec 14 16:13:42.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019713208s
Dec 14 16:13:42.186: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8423 12/14/22 16:13:42.186
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8423 12/14/22 16:13:42.197
Dec 14 16:13:42.215: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Pending. Waiting for statefulset controller to delete.
Dec 14 16:13:42.237: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 16:13:42.248: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 16:13:42.254: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8423
STEP: Removing pod with conflicting port in namespace statefulset-8423 12/14/22 16:13:42.254
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8423 and will be in running state 12/14/22 16:13:42.273
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 16:14:00.394: INFO: Deleting all statefulset in ns statefulset-8423
Dec 14 16:14:00.401: INFO: Scaling statefulset ss to 0
Dec 14 16:14:10.436: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:14:10.443: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:14:10.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8423" for this suite. 12/14/22 16:14:10.483
------------------------------
• [SLOW TEST] [30.400 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:13:40.095
    Dec 14 16:13:40.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 16:13:40.097
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:13:40.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:13:40.124
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8423 12/14/22 16:13:40.127
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 12/14/22 16:13:40.137
    STEP: Creating pod with conflicting port in namespace statefulset-8423 12/14/22 16:13:40.152
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8423 12/14/22 16:13:40.166
    Dec 14 16:13:40.166: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8423" to be "running"
    Dec 14 16:13:40.177: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.15658ms
    Dec 14 16:13:42.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019713208s
    Dec 14 16:13:42.186: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8423 12/14/22 16:13:42.186
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8423 12/14/22 16:13:42.197
    Dec 14 16:13:42.215: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Pending. Waiting for statefulset controller to delete.
    Dec 14 16:13:42.237: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 14 16:13:42.248: INFO: Observed stateful pod in namespace: statefulset-8423, name: ss-0, uid: 65c15c7c-86d2-422a-b826-3a8e87aaa774, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 14 16:13:42.254: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8423
    STEP: Removing pod with conflicting port in namespace statefulset-8423 12/14/22 16:13:42.254
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8423 and will be in running state 12/14/22 16:13:42.273
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 16:14:00.394: INFO: Deleting all statefulset in ns statefulset-8423
    Dec 14 16:14:00.401: INFO: Scaling statefulset ss to 0
    Dec 14 16:14:10.436: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:14:10.443: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:14:10.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8423" for this suite. 12/14/22 16:14:10.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:14:10.5
Dec 14 16:14:10.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename endpointslice 12/14/22 16:14:10.504
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:10.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:10.534
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 14 16:14:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2621" for this suite. 12/14/22 16:14:12.661
------------------------------
• [2.171 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:14:10.5
    Dec 14 16:14:10.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename endpointslice 12/14/22 16:14:10.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:10.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:10.534
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:14:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2621" for this suite. 12/14/22 16:14:12.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:14:12.675
Dec 14 16:14:12.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:14:12.679
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:12.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:12.706
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:14:12.711
Dec 14 16:14:12.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af" in namespace "projected-512" to be "Succeeded or Failed"
Dec 14 16:14:12.731: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.371766ms
Dec 14 16:14:14.740: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015029114s
Dec 14 16:14:16.739: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014052055s
STEP: Saw pod success 12/14/22 16:14:16.74
Dec 14 16:14:16.740: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af" satisfied condition "Succeeded or Failed"
Dec 14 16:14:16.746: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af container client-container: <nil>
STEP: delete the pod 12/14/22 16:14:16.776
Dec 14 16:14:16.792: INFO: Waiting for pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af to disappear
Dec 14 16:14:16.797: INFO: Pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:14:16.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-512" for this suite. 12/14/22 16:14:16.804
------------------------------
• [4.138 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:14:12.675
    Dec 14 16:14:12.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:14:12.679
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:12.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:12.706
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:14:12.711
    Dec 14 16:14:12.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af" in namespace "projected-512" to be "Succeeded or Failed"
    Dec 14 16:14:12.731: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.371766ms
    Dec 14 16:14:14.740: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015029114s
    Dec 14 16:14:16.739: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014052055s
    STEP: Saw pod success 12/14/22 16:14:16.74
    Dec 14 16:14:16.740: INFO: Pod "downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af" satisfied condition "Succeeded or Failed"
    Dec 14 16:14:16.746: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af container client-container: <nil>
    STEP: delete the pod 12/14/22 16:14:16.776
    Dec 14 16:14:16.792: INFO: Waiting for pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af to disappear
    Dec 14 16:14:16.797: INFO: Pod downwardapi-volume-e9c9efb6-b095-4d91-b5a5-e33082fc85af no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:14:16.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-512" for this suite. 12/14/22 16:14:16.804
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:14:16.814
Dec 14 16:14:16.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:14:16.816
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:16.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:16.84
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-a4f7cc82-901f-41fd-bf65-47781b17e0bd 12/14/22 16:14:16.85
STEP: Creating configMap with name cm-test-opt-upd-a0e54dcb-d210-4d98-a5bf-d26fe9b409d9 12/14/22 16:14:16.856
STEP: Creating the pod 12/14/22 16:14:16.863
Dec 14 16:14:16.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39" in namespace "configmap-1988" to be "running and ready"
Dec 14 16:14:16.883: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452578ms
Dec 14 16:14:16.883: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:14:18.894: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507247s
Dec 14 16:14:18.894: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:14:20.892: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Running", Reason="", readiness=true. Elapsed: 4.013539545s
Dec 14 16:14:20.892: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Running (Ready = true)
Dec 14 16:14:20.892: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a4f7cc82-901f-41fd-bf65-47781b17e0bd 12/14/22 16:14:20.928
STEP: Updating configmap cm-test-opt-upd-a0e54dcb-d210-4d98-a5bf-d26fe9b409d9 12/14/22 16:14:20.938
STEP: Creating configMap with name cm-test-opt-create-a14f598b-1e48-4e03-adc2-ba0915a532c5 12/14/22 16:14:20.946
STEP: waiting to observe update in volume 12/14/22 16:14:20.952
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:15:41.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1988" for this suite. 12/14/22 16:15:41.707
------------------------------
• [SLOW TEST] [84.903 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:14:16.814
    Dec 14 16:14:16.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:14:16.816
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:14:16.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:14:16.84
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-a4f7cc82-901f-41fd-bf65-47781b17e0bd 12/14/22 16:14:16.85
    STEP: Creating configMap with name cm-test-opt-upd-a0e54dcb-d210-4d98-a5bf-d26fe9b409d9 12/14/22 16:14:16.856
    STEP: Creating the pod 12/14/22 16:14:16.863
    Dec 14 16:14:16.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39" in namespace "configmap-1988" to be "running and ready"
    Dec 14 16:14:16.883: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452578ms
    Dec 14 16:14:16.883: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:14:18.894: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507247s
    Dec 14 16:14:18.894: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:14:20.892: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39": Phase="Running", Reason="", readiness=true. Elapsed: 4.013539545s
    Dec 14 16:14:20.892: INFO: The phase of Pod pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39 is Running (Ready = true)
    Dec 14 16:14:20.892: INFO: Pod "pod-configmaps-e0049b1d-95aa-47d5-8d6a-e018867cad39" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a4f7cc82-901f-41fd-bf65-47781b17e0bd 12/14/22 16:14:20.928
    STEP: Updating configmap cm-test-opt-upd-a0e54dcb-d210-4d98-a5bf-d26fe9b409d9 12/14/22 16:14:20.938
    STEP: Creating configMap with name cm-test-opt-create-a14f598b-1e48-4e03-adc2-ba0915a532c5 12/14/22 16:14:20.946
    STEP: waiting to observe update in volume 12/14/22 16:14:20.952
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:15:41.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1988" for this suite. 12/14/22 16:15:41.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:15:41.72
Dec 14 16:15:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:15:41.724
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:41.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:41.753
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 16:15:41.758
Dec 14 16:15:41.779: INFO: Waiting up to 5m0s for pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439" in namespace "emptydir-4944" to be "Succeeded or Failed"
Dec 14 16:15:41.783: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312274ms
Dec 14 16:15:43.790: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010876928s
Dec 14 16:15:45.791: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011512195s
STEP: Saw pod success 12/14/22 16:15:45.791
Dec 14 16:15:45.791: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439" satisfied condition "Succeeded or Failed"
Dec 14 16:15:45.795: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 container test-container: <nil>
STEP: delete the pod 12/14/22 16:15:45.814
Dec 14 16:15:45.827: INFO: Waiting for pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 to disappear
Dec 14 16:15:45.831: INFO: Pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:15:45.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4944" for this suite. 12/14/22 16:15:45.837
------------------------------
• [4.136 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:15:41.72
    Dec 14 16:15:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:15:41.724
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:41.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:41.753
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 16:15:41.758
    Dec 14 16:15:41.779: INFO: Waiting up to 5m0s for pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439" in namespace "emptydir-4944" to be "Succeeded or Failed"
    Dec 14 16:15:41.783: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Pending", Reason="", readiness=false. Elapsed: 4.312274ms
    Dec 14 16:15:43.790: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010876928s
    Dec 14 16:15:45.791: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011512195s
    STEP: Saw pod success 12/14/22 16:15:45.791
    Dec 14 16:15:45.791: INFO: Pod "pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439" satisfied condition "Succeeded or Failed"
    Dec 14 16:15:45.795: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:15:45.814
    Dec 14 16:15:45.827: INFO: Waiting for pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 to disappear
    Dec 14 16:15:45.831: INFO: Pod pod-d4ef4727-fe48-47e4-b52d-fb35db8e5439 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:15:45.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4944" for this suite. 12/14/22 16:15:45.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:15:45.858
Dec 14 16:15:45.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:15:45.862
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:45.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:45.885
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 12/14/22 16:15:45.894
Dec 14 16:15:45.894: INFO: Creating simple deployment test-deployment-54789
Dec 14 16:15:45.916: INFO: deployment "test-deployment-54789" doesn't have the required revision set
STEP: Getting /status 12/14/22 16:15:47.937
Dec 14 16:15:47.943: INFO: Deployment test-deployment-54789 has Conditions: [{Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 12/14/22 16:15:47.943
Dec 14 16:15:47.958: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 15, 45, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-54789-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 12/14/22 16:15:47.958
Dec 14 16:15:47.960: INFO: Observed &Deployment event: ADDED
Dec 14 16:15:47.960: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
Dec 14 16:15:47.960: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:46 +0000 UTC 2022-12-14 16:15:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54789-54bc444df" is progressing.}
Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.962: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 16:15:47.962: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
Dec 14 16:15:47.962: INFO: Found Deployment test-deployment-54789 in namespace deployment-947 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 16:15:47.962: INFO: Deployment test-deployment-54789 has an updated status
STEP: patching the Statefulset Status 12/14/22 16:15:47.962
Dec 14 16:15:47.962: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 16:15:47.973: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 12/14/22 16:15:47.973
Dec 14 16:15:47.978: INFO: Observed &Deployment event: ADDED
Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
Dec 14 16:15:47.978: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:46 +0000 UTC 2022-12-14 16:15:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54789-54bc444df" is progressing.}
Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
Dec 14 16:15:47.979: INFO: Found deployment test-deployment-54789 in namespace deployment-947 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 16:15:47.979: INFO: Deployment test-deployment-54789 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:15:47.987: INFO: Deployment "test-deployment-54789":
&Deployment{ObjectMeta:{test-deployment-54789  deployment-947  e2f751e4-ee8a-48b6-afaf-242f2bd73797 13295 1 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036083b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:15:47.994: INFO: New ReplicaSet "test-deployment-54789-54bc444df" of Deployment "test-deployment-54789":
&ReplicaSet{ObjectMeta:{test-deployment-54789-54bc444df  deployment-947  f37c9b6d-3d59-4a00-9bca-51bf2546ee2b 13292 1 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-54789 e2f751e4-ee8a-48b6-afaf-242f2bd73797 0xc0002f8190 0xc0002f8191}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2f751e4-ee8a-48b6-afaf-242f2bd73797\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002f90d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:15:47.998: INFO: Pod "test-deployment-54789-54bc444df-slq28" is available:
&Pod{ObjectMeta:{test-deployment-54789-54bc444df-slq28 test-deployment-54789-54bc444df- deployment-947  4d70ad04-7829-48c9-a243-fe2284d6437d 13291 0 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-54789-54bc444df f37c9b6d-3d59-4a00-9bca-51bf2546ee2b 0xc003608790 0xc003608791}] [] [{kube-controller-manager Update v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f37c9b6d-3d59-4a00-9bca-51bf2546ee2b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw576,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw576,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.133,StartTime:2022-12-14 16:15:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:15:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d504de1a84ae4222959f9011c7dec429d9a215fbb31bbb970fcd1b67c0cae86a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:15:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-947" for this suite. 12/14/22 16:15:48.008
------------------------------
• [2.163 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:15:45.858
    Dec 14 16:15:45.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:15:45.862
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:45.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:45.885
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 12/14/22 16:15:45.894
    Dec 14 16:15:45.894: INFO: Creating simple deployment test-deployment-54789
    Dec 14 16:15:45.916: INFO: deployment "test-deployment-54789" doesn't have the required revision set
    STEP: Getting /status 12/14/22 16:15:47.937
    Dec 14 16:15:47.943: INFO: Deployment test-deployment-54789 has Conditions: [{Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 12/14/22 16:15:47.943
    Dec 14 16:15:47.958: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 15, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 15, 45, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-54789-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 12/14/22 16:15:47.958
    Dec 14 16:15:47.960: INFO: Observed &Deployment event: ADDED
    Dec 14 16:15:47.960: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
    Dec 14 16:15:47.960: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:46 +0000 UTC 2022-12-14 16:15:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54789-54bc444df" is progressing.}
    Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 16:15:47.961: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
    Dec 14 16:15:47.961: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.962: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 16:15:47.962: INFO: Observed Deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
    Dec 14 16:15:47.962: INFO: Found Deployment test-deployment-54789 in namespace deployment-947 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 16:15:47.962: INFO: Deployment test-deployment-54789 has an updated status
    STEP: patching the Statefulset Status 12/14/22 16:15:47.962
    Dec 14 16:15:47.962: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 16:15:47.973: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 12/14/22 16:15:47.973
    Dec 14 16:15:47.978: INFO: Observed &Deployment event: ADDED
    Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
    Dec 14 16:15:47.978: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54789-54bc444df"}
    Dec 14 16:15:47.978: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 16:15:45 +0000 UTC 2022-12-14 16:15:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:46 +0000 UTC 2022-12-14 16:15:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54789-54bc444df" is progressing.}
    Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
    Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 16:15:47 +0000 UTC 2022-12-14 16:15:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54789-54bc444df" has successfully progressed.}
    Dec 14 16:15:47.979: INFO: Observed deployment test-deployment-54789 in namespace deployment-947 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 16:15:47.979: INFO: Observed &Deployment event: MODIFIED
    Dec 14 16:15:47.979: INFO: Found deployment test-deployment-54789 in namespace deployment-947 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Dec 14 16:15:47.979: INFO: Deployment test-deployment-54789 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:15:47.987: INFO: Deployment "test-deployment-54789":
    &Deployment{ObjectMeta:{test-deployment-54789  deployment-947  e2f751e4-ee8a-48b6-afaf-242f2bd73797 13295 1 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036083b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 16:15:47.994: INFO: New ReplicaSet "test-deployment-54789-54bc444df" of Deployment "test-deployment-54789":
    &ReplicaSet{ObjectMeta:{test-deployment-54789-54bc444df  deployment-947  f37c9b6d-3d59-4a00-9bca-51bf2546ee2b 13292 1 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-54789 e2f751e4-ee8a-48b6-afaf-242f2bd73797 0xc0002f8190 0xc0002f8191}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2f751e4-ee8a-48b6-afaf-242f2bd73797\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002f90d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:15:47.998: INFO: Pod "test-deployment-54789-54bc444df-slq28" is available:
    &Pod{ObjectMeta:{test-deployment-54789-54bc444df-slq28 test-deployment-54789-54bc444df- deployment-947  4d70ad04-7829-48c9-a243-fe2284d6437d 13291 0 2022-12-14 16:15:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-54789-54bc444df f37c9b6d-3d59-4a00-9bca-51bf2546ee2b 0xc003608790 0xc003608791}] [] [{kube-controller-manager Update v1 2022-12-14 16:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f37c9b6d-3d59-4a00-9bca-51bf2546ee2b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:15:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw576,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw576,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:15:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.133,StartTime:2022-12-14 16:15:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:15:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d504de1a84ae4222959f9011c7dec429d9a215fbb31bbb970fcd1b67c0cae86a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:15:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-947" for this suite. 12/14/22 16:15:48.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:15:48.022
Dec 14 16:15:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:15:48.025
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:48.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:48.061
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 12/14/22 16:15:48.075
STEP: watching for the Service to be added 12/14/22 16:15:48.1
Dec 14 16:15:48.103: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 14 16:15:48.103: INFO: Service test-service-dwbkn created
STEP: Getting /status 12/14/22 16:15:48.103
Dec 14 16:15:48.117: INFO: Service test-service-dwbkn has LoadBalancer: {[]}
STEP: patching the ServiceStatus 12/14/22 16:15:48.117
STEP: watching for the Service to be patched 12/14/22 16:15:48.14
Dec 14 16:15:48.145: INFO: observed Service test-service-dwbkn in namespace services-7982 with annotations: map[] & LoadBalancer: {[]}
Dec 14 16:15:48.146: INFO: Found Service test-service-dwbkn in namespace services-7982 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 14 16:15:48.146: INFO: Service test-service-dwbkn has service status patched
STEP: updating the ServiceStatus 12/14/22 16:15:48.146
Dec 14 16:15:48.162: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 12/14/22 16:15:48.162
Dec 14 16:15:48.167: INFO: Observed Service test-service-dwbkn in namespace services-7982 with annotations: map[] & Conditions: {[]}
Dec 14 16:15:48.167: INFO: Observed event: &Service{ObjectMeta:{test-service-dwbkn  services-7982  80706008-eb69-4ecc-93ef-ac5e8533f75a 13305 0 2022-12-14 16:15:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.63.17,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.63.17],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 14 16:15:48.168: INFO: Found Service test-service-dwbkn in namespace services-7982 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 16:15:48.168: INFO: Service test-service-dwbkn has service status updated
STEP: patching the service 12/14/22 16:15:48.168
STEP: watching for the Service to be patched 12/14/22 16:15:48.197
Dec 14 16:15:48.200: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
Dec 14 16:15:48.200: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
Dec 14 16:15:48.201: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
Dec 14 16:15:48.201: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service:patched test-service-static:true]
Dec 14 16:15:48.201: INFO: Service test-service-dwbkn patched
STEP: deleting the service 12/14/22 16:15:48.201
STEP: watching for the Service to be deleted 12/14/22 16:15:48.23
Dec 14 16:15:48.233: INFO: Observed event: ADDED
Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
Dec 14 16:15:48.233: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 14 16:15:48.233: INFO: Service test-service-dwbkn deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:15:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7982" for this suite. 12/14/22 16:15:48.24
------------------------------
• [0.228 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:15:48.022
    Dec 14 16:15:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:15:48.025
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:48.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:48.061
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 12/14/22 16:15:48.075
    STEP: watching for the Service to be added 12/14/22 16:15:48.1
    Dec 14 16:15:48.103: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Dec 14 16:15:48.103: INFO: Service test-service-dwbkn created
    STEP: Getting /status 12/14/22 16:15:48.103
    Dec 14 16:15:48.117: INFO: Service test-service-dwbkn has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 12/14/22 16:15:48.117
    STEP: watching for the Service to be patched 12/14/22 16:15:48.14
    Dec 14 16:15:48.145: INFO: observed Service test-service-dwbkn in namespace services-7982 with annotations: map[] & LoadBalancer: {[]}
    Dec 14 16:15:48.146: INFO: Found Service test-service-dwbkn in namespace services-7982 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Dec 14 16:15:48.146: INFO: Service test-service-dwbkn has service status patched
    STEP: updating the ServiceStatus 12/14/22 16:15:48.146
    Dec 14 16:15:48.162: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 12/14/22 16:15:48.162
    Dec 14 16:15:48.167: INFO: Observed Service test-service-dwbkn in namespace services-7982 with annotations: map[] & Conditions: {[]}
    Dec 14 16:15:48.167: INFO: Observed event: &Service{ObjectMeta:{test-service-dwbkn  services-7982  80706008-eb69-4ecc-93ef-ac5e8533f75a 13305 0 2022-12-14 16:15:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-14 16:15:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.63.17,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.63.17],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Dec 14 16:15:48.168: INFO: Found Service test-service-dwbkn in namespace services-7982 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 16:15:48.168: INFO: Service test-service-dwbkn has service status updated
    STEP: patching the service 12/14/22 16:15:48.168
    STEP: watching for the Service to be patched 12/14/22 16:15:48.197
    Dec 14 16:15:48.200: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
    Dec 14 16:15:48.200: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
    Dec 14 16:15:48.201: INFO: observed Service test-service-dwbkn in namespace services-7982 with labels: map[test-service-static:true]
    Dec 14 16:15:48.201: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service:patched test-service-static:true]
    Dec 14 16:15:48.201: INFO: Service test-service-dwbkn patched
    STEP: deleting the service 12/14/22 16:15:48.201
    STEP: watching for the Service to be deleted 12/14/22 16:15:48.23
    Dec 14 16:15:48.233: INFO: Observed event: ADDED
    Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
    Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
    Dec 14 16:15:48.233: INFO: Observed event: MODIFIED
    Dec 14 16:15:48.233: INFO: Found Service test-service-dwbkn in namespace services-7982 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Dec 14 16:15:48.233: INFO: Service test-service-dwbkn deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:15:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7982" for this suite. 12/14/22 16:15:48.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:15:48.258
Dec 14 16:15:48.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:15:48.261
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:48.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:48.336
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5664 12/14/22 16:15:48.342
STEP: creating a selector 12/14/22 16:15:48.342
STEP: Creating the service pods in kubernetes 12/14/22 16:15:48.342
Dec 14 16:15:48.342: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 16:15:48.398: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5664" to be "running and ready"
Dec 14 16:15:48.403: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189031ms
Dec 14 16:15:48.404: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:15:50.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017834916s
Dec 14 16:15:50.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:15:52.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027749422s
Dec 14 16:15:52.427: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:15:54.417: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019165361s
Dec 14 16:15:54.418: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:15:56.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017944743s
Dec 14 16:15:56.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:15:58.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013800628s
Dec 14 16:15:58.412: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:00.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014533967s
Dec 14 16:16:00.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:02.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014036475s
Dec 14 16:16:02.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:04.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016913408s
Dec 14 16:16:04.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:06.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014347495s
Dec 14 16:16:06.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:08.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014354917s
Dec 14 16:16:08.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:16:10.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016115367s
Dec 14 16:16:10.415: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 16:16:10.415: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 16:16:10.420: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5664" to be "running and ready"
Dec 14 16:16:10.425: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.664176ms
Dec 14 16:16:10.425: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 16:16:10.425: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 14 16:16:10.430: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5664" to be "running and ready"
Dec 14 16:16:10.435: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.544192ms
Dec 14 16:16:10.435: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 14 16:16:10.435: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 16:16:10.439
Dec 14 16:16:10.458: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5664" to be "running"
Dec 14 16:16:10.467: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879193ms
Dec 14 16:16:12.476: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018293569s
Dec 14 16:16:12.476: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 16:16:12.485: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5664" to be "running"
Dec 14 16:16:12.490: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.811905ms
Dec 14 16:16:12.490: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 14 16:16:12.495: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 16:16:12.495: INFO: Going to poll 10.233.64.58 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:16:12.501: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:16:12.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:16:12.505: INFO: ExecWithOptions: Clientset creation
Dec 14 16:16:12.505: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.58+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:16:13.630: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 16:16:13.631: INFO: Going to poll 10.233.66.60 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:16:13.637: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.60 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:16:13.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:16:13.639: INFO: ExecWithOptions: Clientset creation
Dec 14 16:16:13.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.60+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:16:14.726: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 16:16:14.726: INFO: Going to poll 10.233.67.134 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:16:14.733: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.67.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:16:14.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:16:14.734: INFO: ExecWithOptions: Clientset creation
Dec 14 16:16:14.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.67.134+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:16:15.877: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:15.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5664" for this suite. 12/14/22 16:16:15.889
------------------------------
• [SLOW TEST] [27.642 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:15:48.258
    Dec 14 16:15:48.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:15:48.261
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:15:48.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:15:48.336
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5664 12/14/22 16:15:48.342
    STEP: creating a selector 12/14/22 16:15:48.342
    STEP: Creating the service pods in kubernetes 12/14/22 16:15:48.342
    Dec 14 16:15:48.342: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 16:15:48.398: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5664" to be "running and ready"
    Dec 14 16:15:48.403: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189031ms
    Dec 14 16:15:48.404: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:15:50.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017834916s
    Dec 14 16:15:50.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:15:52.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027749422s
    Dec 14 16:15:52.427: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:15:54.417: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019165361s
    Dec 14 16:15:54.418: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:15:56.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017944743s
    Dec 14 16:15:56.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:15:58.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013800628s
    Dec 14 16:15:58.412: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:00.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014533967s
    Dec 14 16:16:00.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:02.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014036475s
    Dec 14 16:16:02.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:04.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016913408s
    Dec 14 16:16:04.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:06.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014347495s
    Dec 14 16:16:06.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:08.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014354917s
    Dec 14 16:16:08.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:16:10.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016115367s
    Dec 14 16:16:10.415: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 16:16:10.415: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 16:16:10.420: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5664" to be "running and ready"
    Dec 14 16:16:10.425: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.664176ms
    Dec 14 16:16:10.425: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 16:16:10.425: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 14 16:16:10.430: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5664" to be "running and ready"
    Dec 14 16:16:10.435: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.544192ms
    Dec 14 16:16:10.435: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 14 16:16:10.435: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 16:16:10.439
    Dec 14 16:16:10.458: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5664" to be "running"
    Dec 14 16:16:10.467: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879193ms
    Dec 14 16:16:12.476: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018293569s
    Dec 14 16:16:12.476: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 16:16:12.485: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5664" to be "running"
    Dec 14 16:16:12.490: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.811905ms
    Dec 14 16:16:12.490: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 14 16:16:12.495: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 14 16:16:12.495: INFO: Going to poll 10.233.64.58 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:16:12.501: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:16:12.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:16:12.505: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:16:12.505: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.58+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:16:13.630: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 14 16:16:13.631: INFO: Going to poll 10.233.66.60 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:16:13.637: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.60 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:16:13.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:16:13.639: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:16:13.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.60+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:16:14.726: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 14 16:16:14.726: INFO: Going to poll 10.233.67.134 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Dec 14 16:16:14.733: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.67.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5664 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:16:14.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:16:14.734: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:16:14.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5664/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.67.134+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:16:15.877: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:15.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5664" for this suite. 12/14/22 16:16:15.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:15.935
Dec 14 16:16:15.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:16:15.942
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:15.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:15.978
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 12/14/22 16:16:15.982
STEP: Counting existing ResourceQuota 12/14/22 16:16:20.989
STEP: Creating a ResourceQuota 12/14/22 16:16:25.995
STEP: Ensuring resource quota status is calculated 12/14/22 16:16:26.006
STEP: Creating a Secret 12/14/22 16:16:28.013
STEP: Ensuring resource quota status captures secret creation 12/14/22 16:16:28.035
STEP: Deleting a secret 12/14/22 16:16:30.046
STEP: Ensuring resource quota status released usage 12/14/22 16:16:30.057
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:32.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9084" for this suite. 12/14/22 16:16:32.071
------------------------------
• [SLOW TEST] [16.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:15.935
    Dec 14 16:16:15.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:16:15.942
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:15.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:15.978
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 12/14/22 16:16:15.982
    STEP: Counting existing ResourceQuota 12/14/22 16:16:20.989
    STEP: Creating a ResourceQuota 12/14/22 16:16:25.995
    STEP: Ensuring resource quota status is calculated 12/14/22 16:16:26.006
    STEP: Creating a Secret 12/14/22 16:16:28.013
    STEP: Ensuring resource quota status captures secret creation 12/14/22 16:16:28.035
    STEP: Deleting a secret 12/14/22 16:16:30.046
    STEP: Ensuring resource quota status released usage 12/14/22 16:16:30.057
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:32.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9084" for this suite. 12/14/22 16:16:32.071
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:32.089
Dec 14 16:16:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:16:32.092
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:32.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:32.119
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 12/14/22 16:16:32.123
STEP: delete the rc 12/14/22 16:16:37.142
STEP: wait for all pods to be garbage collected 12/14/22 16:16:37.151
STEP: Gathering metrics 12/14/22 16:16:42.161
Dec 14 16:16:42.194: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 16:16:42.199: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.835571ms
Dec 14 16:16:42.199: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 16:16:42.199: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 16:16:42.312: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8380" for this suite. 12/14/22 16:16:42.321
------------------------------
• [SLOW TEST] [10.243 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:32.089
    Dec 14 16:16:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:16:32.092
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:32.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:32.119
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 12/14/22 16:16:32.123
    STEP: delete the rc 12/14/22 16:16:37.142
    STEP: wait for all pods to be garbage collected 12/14/22 16:16:37.151
    STEP: Gathering metrics 12/14/22 16:16:42.161
    Dec 14 16:16:42.194: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 16:16:42.199: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.835571ms
    Dec 14 16:16:42.199: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 16:16:42.199: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 16:16:42.312: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8380" for this suite. 12/14/22 16:16:42.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:42.339
Dec 14 16:16:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 16:16:42.341
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:42.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:42.369
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Dec 14 16:16:42.393: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:16:47.402: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:16:47.402
STEP: Scaling up "test-rs" replicaset  12/14/22 16:16:47.402
Dec 14 16:16:47.418: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 12/14/22 16:16:47.418
W1214 16:16:47.427349      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 16:16:47.434: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:16:47.477: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:16:47.502: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:16:47.533: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:16:48.617: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 2, AvailableReplicas 2
Dec 14 16:16:48.979: INFO: observed Replicaset test-rs in namespace replicaset-3136 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:48.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3136" for this suite. 12/14/22 16:16:48.988
------------------------------
• [SLOW TEST] [6.660 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:42.339
    Dec 14 16:16:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 16:16:42.341
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:42.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:42.369
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Dec 14 16:16:42.393: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 16:16:47.402: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:16:47.402
    STEP: Scaling up "test-rs" replicaset  12/14/22 16:16:47.402
    Dec 14 16:16:47.418: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 12/14/22 16:16:47.418
    W1214 16:16:47.427349      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 16:16:47.434: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 16:16:47.477: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 16:16:47.502: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 16:16:47.533: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 1, AvailableReplicas 1
    Dec 14 16:16:48.617: INFO: observed ReplicaSet test-rs in namespace replicaset-3136 with ReadyReplicas 2, AvailableReplicas 2
    Dec 14 16:16:48.979: INFO: observed Replicaset test-rs in namespace replicaset-3136 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:48.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3136" for this suite. 12/14/22 16:16:48.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:49.003
Dec 14 16:16:49.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename events 12/14/22 16:16:49.005
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:49.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:49.026
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 12/14/22 16:16:49.031
STEP: get a list of Events with a label in the current namespace 12/14/22 16:16:49.057
STEP: delete a list of events 12/14/22 16:16:49.062
Dec 14 16:16:49.062: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/14/22 16:16:49.094
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:49.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1509" for this suite. 12/14/22 16:16:49.104
------------------------------
• [0.111 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:49.003
    Dec 14 16:16:49.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename events 12/14/22 16:16:49.005
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:49.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:49.026
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 12/14/22 16:16:49.031
    STEP: get a list of Events with a label in the current namespace 12/14/22 16:16:49.057
    STEP: delete a list of events 12/14/22 16:16:49.062
    Dec 14 16:16:49.062: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/14/22 16:16:49.094
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:49.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1509" for this suite. 12/14/22 16:16:49.104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:49.114
Dec 14 16:16:49.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption 12/14/22 16:16:49.118
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:49.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:49.141
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 12/14/22 16:16:49.156
STEP: Waiting for all pods to be running 12/14/22 16:16:49.213
Dec 14 16:16:49.228: INFO: running pods: 0 < 3
Dec 14 16:16:51.238: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:53.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8979" for this suite. 12/14/22 16:16:53.25
------------------------------
• [4.149 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:49.114
    Dec 14 16:16:49.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption 12/14/22 16:16:49.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:49.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:49.141
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 12/14/22 16:16:49.156
    STEP: Waiting for all pods to be running 12/14/22 16:16:49.213
    Dec 14 16:16:49.228: INFO: running pods: 0 < 3
    Dec 14 16:16:51.238: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:53.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8979" for this suite. 12/14/22 16:16:53.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:53.268
Dec 14 16:16:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:16:53.271
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:53.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:53.308
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 16:16:53.316
Dec 14 16:16:53.329: INFO: Waiting up to 5m0s for pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387" in namespace "emptydir-7389" to be "Succeeded or Failed"
Dec 14 16:16:53.334: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632537ms
Dec 14 16:16:55.352: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023597458s
Dec 14 16:16:57.344: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015451576s
STEP: Saw pod success 12/14/22 16:16:57.345
Dec 14 16:16:57.346: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387" satisfied condition "Succeeded or Failed"
Dec 14 16:16:57.352: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 container test-container: <nil>
STEP: delete the pod 12/14/22 16:16:57.372
Dec 14 16:16:57.394: INFO: Waiting for pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 to disappear
Dec 14 16:16:57.403: INFO: Pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:16:57.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7389" for this suite. 12/14/22 16:16:57.414
------------------------------
• [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:53.268
    Dec 14 16:16:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:16:53.271
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:53.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:53.308
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 16:16:53.316
    Dec 14 16:16:53.329: INFO: Waiting up to 5m0s for pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387" in namespace "emptydir-7389" to be "Succeeded or Failed"
    Dec 14 16:16:53.334: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632537ms
    Dec 14 16:16:55.352: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023597458s
    Dec 14 16:16:57.344: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015451576s
    STEP: Saw pod success 12/14/22 16:16:57.345
    Dec 14 16:16:57.346: INFO: Pod "pod-9d07c6f7-061b-468d-b76c-68de9dbf6387" satisfied condition "Succeeded or Failed"
    Dec 14 16:16:57.352: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:16:57.372
    Dec 14 16:16:57.394: INFO: Waiting for pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 to disappear
    Dec 14 16:16:57.403: INFO: Pod pod-9d07c6f7-061b-468d-b76c-68de9dbf6387 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:16:57.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7389" for this suite. 12/14/22 16:16:57.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:16:57.428
Dec 14 16:16:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 16:16:57.433
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:57.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:57.46
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 12/14/22 16:16:57.467
STEP: Verify that the required pods have come up 12/14/22 16:16:57.477
Dec 14 16:16:57.484: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec 14 16:17:02.493: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 12/14/22 16:17:02.494
Dec 14 16:17:02.503: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 12/14/22 16:17:02.503
STEP: DeleteCollection of the ReplicaSets 12/14/22 16:17:02.511
STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/14/22 16:17:02.539
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:17:02.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4570" for this suite. 12/14/22 16:17:02.561
------------------------------
• [SLOW TEST] [5.159 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:16:57.428
    Dec 14 16:16:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 16:16:57.433
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:16:57.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:16:57.46
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 12/14/22 16:16:57.467
    STEP: Verify that the required pods have come up 12/14/22 16:16:57.477
    Dec 14 16:16:57.484: INFO: Pod name sample-pod: Found 0 pods out of 3
    Dec 14 16:17:02.493: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 12/14/22 16:17:02.494
    Dec 14 16:17:02.503: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 12/14/22 16:17:02.503
    STEP: DeleteCollection of the ReplicaSets 12/14/22 16:17:02.511
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/14/22 16:17:02.539
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:17:02.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4570" for this suite. 12/14/22 16:17:02.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:17:02.591
Dec 14 16:17:02.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:17:02.593
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:17:02.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:17:02.665
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:17:06.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3507" for this suite. 12/14/22 16:17:06.717
------------------------------
• [4.136 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:17:02.591
    Dec 14 16:17:02.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:17:02.593
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:17:02.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:17:02.665
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:17:06.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3507" for this suite. 12/14/22 16:17:06.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:17:06.728
Dec 14 16:17:06.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 16:17:06.73
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:17:06.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:17:06.768
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4244 12/14/22 16:17:06.772
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 12/14/22 16:17:06.782
STEP: Creating stateful set ss in namespace statefulset-4244 12/14/22 16:17:06.788
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4244 12/14/22 16:17:06.8
Dec 14 16:17:06.813: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:17:16.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/14/22 16:17:16.822
Dec 14 16:17:16.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:17:17.065: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:17:17.065: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:17:17.065: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:17:17.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 16:17:27.081: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:17:27.081: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:17:27.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999697s
Dec 14 16:17:28.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994270451s
Dec 14 16:17:29.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988082059s
Dec 14 16:17:30.131: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980625791s
Dec 14 16:17:31.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.970776613s
Dec 14 16:17:32.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961467221s
Dec 14 16:17:33.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954792487s
Dec 14 16:17:34.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.944352235s
Dec 14 16:17:35.175: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936027021s
Dec 14 16:17:36.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 927.480444ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4244 12/14/22 16:17:37.185
Dec 14 16:17:37.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:17:37.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:17:37.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:17:37.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:17:37.433: INFO: Found 1 stateful pods, waiting for 3
Dec 14 16:17:47.447: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:17:47.447: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:17:47.448: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 12/14/22 16:17:47.448
STEP: Scale down will halt with unhealthy stateful pod 12/14/22 16:17:47.448
Dec 14 16:17:47.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:17:47.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:17:47.700: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:17:47.700: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:17:47.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:17:47.912: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:17:47.912: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:17:47.912: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:17:47.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:17:48.175: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:17:48.175: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:17:48.175: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:17:48.175: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:17:48.180: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 14 16:17:58.196: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:17:58.196: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:17:58.197: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:17:58.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999758s
Dec 14 16:17:59.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9825422s
Dec 14 16:18:00.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972388011s
Dec 14 16:18:01.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959619269s
Dec 14 16:18:02.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931021073s
Dec 14 16:18:03.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.915972404s
Dec 14 16:18:04.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.905334504s
Dec 14 16:18:05.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.890782787s
Dec 14 16:18:06.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.881253824s
Dec 14 16:18:07.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 870.706212ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4244 12/14/22 16:18:08.355
Dec 14 16:18:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:18:08.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:18:08.623: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:18:08.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:18:08.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:18:08.832: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:18:08.832: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:18:08.832: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:18:08.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:18:09.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:18:09.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:18:09.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:18:09.055: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 12/14/22 16:18:19.088
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 16:18:19.088: INFO: Deleting all statefulset in ns statefulset-4244
Dec 14 16:18:19.096: INFO: Scaling statefulset ss to 0
Dec 14 16:18:19.120: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:18:19.125: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:18:19.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4244" for this suite. 12/14/22 16:18:19.159
------------------------------
• [SLOW TEST] [72.443 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:17:06.728
    Dec 14 16:17:06.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 16:17:06.73
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:17:06.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:17:06.768
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4244 12/14/22 16:17:06.772
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 12/14/22 16:17:06.782
    STEP: Creating stateful set ss in namespace statefulset-4244 12/14/22 16:17:06.788
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4244 12/14/22 16:17:06.8
    Dec 14 16:17:06.813: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 16:17:16.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/14/22 16:17:16.822
    Dec 14 16:17:16.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:17:17.065: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:17:17.065: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:17:17.065: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:17:17.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 14 16:17:27.081: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:17:27.081: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:17:27.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999697s
    Dec 14 16:17:28.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994270451s
    Dec 14 16:17:29.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988082059s
    Dec 14 16:17:30.131: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980625791s
    Dec 14 16:17:31.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.970776613s
    Dec 14 16:17:32.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961467221s
    Dec 14 16:17:33.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954792487s
    Dec 14 16:17:34.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.944352235s
    Dec 14 16:17:35.175: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936027021s
    Dec 14 16:17:36.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 927.480444ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4244 12/14/22 16:17:37.185
    Dec 14 16:17:37.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:17:37.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 16:17:37.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:17:37.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:17:37.433: INFO: Found 1 stateful pods, waiting for 3
    Dec 14 16:17:47.447: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 16:17:47.447: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 16:17:47.448: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 12/14/22 16:17:47.448
    STEP: Scale down will halt with unhealthy stateful pod 12/14/22 16:17:47.448
    Dec 14 16:17:47.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:17:47.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:17:47.700: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:17:47.700: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:17:47.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:17:47.912: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:17:47.912: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:17:47.912: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:17:47.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:17:48.175: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:17:48.175: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:17:48.175: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:17:48.175: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:17:48.180: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Dec 14 16:17:58.196: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:17:58.196: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:17:58.197: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:17:58.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999758s
    Dec 14 16:17:59.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9825422s
    Dec 14 16:18:00.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972388011s
    Dec 14 16:18:01.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959619269s
    Dec 14 16:18:02.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931021073s
    Dec 14 16:18:03.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.915972404s
    Dec 14 16:18:04.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.905334504s
    Dec 14 16:18:05.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.890782787s
    Dec 14 16:18:06.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.881253824s
    Dec 14 16:18:07.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 870.706212ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4244 12/14/22 16:18:08.355
    Dec 14 16:18:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:18:08.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 16:18:08.623: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:18:08.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:18:08.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:18:08.832: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 16:18:08.832: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:18:08.832: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:18:08.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-4244 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:18:09.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 16:18:09.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:18:09.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:18:09.055: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 12/14/22 16:18:19.088
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 16:18:19.088: INFO: Deleting all statefulset in ns statefulset-4244
    Dec 14 16:18:19.096: INFO: Scaling statefulset ss to 0
    Dec 14 16:18:19.120: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:18:19.125: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:18:19.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4244" for this suite. 12/14/22 16:18:19.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:18:19.173
Dec 14 16:18:19.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:18:19.178
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:19.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:19.205
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 16:18:19.208
Dec 14 16:18:19.218: INFO: Waiting up to 5m0s for pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6" in namespace "emptydir-2599" to be "Succeeded or Failed"
Dec 14 16:18:19.229: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101366ms
Dec 14 16:18:21.242: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023523146s
Dec 14 16:18:23.238: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019723391s
STEP: Saw pod success 12/14/22 16:18:23.238
Dec 14 16:18:23.239: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6" satisfied condition "Succeeded or Failed"
Dec 14 16:18:23.243: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 container test-container: <nil>
STEP: delete the pod 12/14/22 16:18:23.256
Dec 14 16:18:23.278: INFO: Waiting for pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 to disappear
Dec 14 16:18:23.283: INFO: Pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:18:23.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2599" for this suite. 12/14/22 16:18:23.289
------------------------------
• [4.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:18:19.173
    Dec 14 16:18:19.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:18:19.178
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:19.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:19.205
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 16:18:19.208
    Dec 14 16:18:19.218: INFO: Waiting up to 5m0s for pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6" in namespace "emptydir-2599" to be "Succeeded or Failed"
    Dec 14 16:18:19.229: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101366ms
    Dec 14 16:18:21.242: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023523146s
    Dec 14 16:18:23.238: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019723391s
    STEP: Saw pod success 12/14/22 16:18:23.238
    Dec 14 16:18:23.239: INFO: Pod "pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6" satisfied condition "Succeeded or Failed"
    Dec 14 16:18:23.243: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:18:23.256
    Dec 14 16:18:23.278: INFO: Waiting for pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 to disappear
    Dec 14 16:18:23.283: INFO: Pod pod-dfef23a4-56c2-48e3-9582-1bc85d57f3c6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:18:23.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2599" for this suite. 12/14/22 16:18:23.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:18:23.299
Dec 14 16:18:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:18:23.301
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:23.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:23.328
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 12/14/22 16:18:23.331
STEP: fetching the ConfigMap 12/14/22 16:18:23.345
STEP: patching the ConfigMap 12/14/22 16:18:23.35
STEP: listing all ConfigMaps in all namespaces with a label selector 12/14/22 16:18:23.358
STEP: deleting the ConfigMap by collection with a label selector 12/14/22 16:18:23.365
STEP: listing all ConfigMaps in test namespace 12/14/22 16:18:23.379
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:18:23.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7051" for this suite. 12/14/22 16:18:23.391
------------------------------
• [0.101 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:18:23.299
    Dec 14 16:18:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:18:23.301
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:23.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:23.328
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 12/14/22 16:18:23.331
    STEP: fetching the ConfigMap 12/14/22 16:18:23.345
    STEP: patching the ConfigMap 12/14/22 16:18:23.35
    STEP: listing all ConfigMaps in all namespaces with a label selector 12/14/22 16:18:23.358
    STEP: deleting the ConfigMap by collection with a label selector 12/14/22 16:18:23.365
    STEP: listing all ConfigMaps in test namespace 12/14/22 16:18:23.379
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:18:23.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7051" for this suite. 12/14/22 16:18:23.391
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:18:23.401
Dec 14 16:18:23.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename watch 12/14/22 16:18:23.403
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:23.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:23.443
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 12/14/22 16:18:23.447
STEP: creating a new configmap 12/14/22 16:18:23.449
STEP: modifying the configmap once 12/14/22 16:18:23.454
STEP: changing the label value of the configmap 12/14/22 16:18:23.466
STEP: Expecting to observe a delete notification for the watched object 12/14/22 16:18:23.483
Dec 14 16:18:23.483: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14255 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:18:23.484: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14256 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:18:23.484: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14257 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 12/14/22 16:18:23.484
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/14/22 16:18:23.502
STEP: changing the label value of the configmap back 12/14/22 16:18:33.503
STEP: modifying the configmap a third time 12/14/22 16:18:33.52
STEP: deleting the configmap 12/14/22 16:18:33.535
STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/14/22 16:18:33.545
Dec 14 16:18:33.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14316 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:18:33.546: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14317 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:18:33.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14318 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:18:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-599" for this suite. 12/14/22 16:18:33.556
------------------------------
• [SLOW TEST] [10.163 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:18:23.401
    Dec 14 16:18:23.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename watch 12/14/22 16:18:23.403
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:23.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:23.443
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 12/14/22 16:18:23.447
    STEP: creating a new configmap 12/14/22 16:18:23.449
    STEP: modifying the configmap once 12/14/22 16:18:23.454
    STEP: changing the label value of the configmap 12/14/22 16:18:23.466
    STEP: Expecting to observe a delete notification for the watched object 12/14/22 16:18:23.483
    Dec 14 16:18:23.483: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14255 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:18:23.484: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14256 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:18:23.484: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14257 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 12/14/22 16:18:23.484
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/14/22 16:18:23.502
    STEP: changing the label value of the configmap back 12/14/22 16:18:33.503
    STEP: modifying the configmap a third time 12/14/22 16:18:33.52
    STEP: deleting the configmap 12/14/22 16:18:33.535
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/14/22 16:18:33.545
    Dec 14 16:18:33.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14316 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:18:33.546: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14317 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:18:33.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  ff033a67-c2ef-4586-a5c1-1ee2c561d462 14318 0 2022-12-14 16:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-14 16:18:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:18:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-599" for this suite. 12/14/22 16:18:33.556
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:18:33.567
Dec 14 16:18:33.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 16:18:33.571
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:33.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:33.6
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Dec 14 16:18:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:18:37.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8223" for this suite. 12/14/22 16:18:37.092
------------------------------
• [3.537 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:18:33.567
    Dec 14 16:18:33.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 16:18:33.571
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:33.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:33.6
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Dec 14 16:18:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:18:37.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8223" for this suite. 12/14/22 16:18:37.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:18:37.108
Dec 14 16:18:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-pred 12/14/22 16:18:37.113
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:37.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:37.14
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 14 16:18:37.144: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 16:18:37.158: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 16:18:37.165: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
Dec 14 16:18:37.188: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.189: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:18:37.189: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.189: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:18:37.189: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.189: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:18:37.189: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.189: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:18:37.189: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.189: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 16:18:37.189: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.190: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:18:37.190: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.190: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 16:18:37.190: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:18:37.190: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:18:37.190: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:18:37.190: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
Dec 14 16:18:37.205: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.205: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:18:37.205: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.205: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:18:37.205: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.205: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:18:37.205: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.205: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:18:37.205: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.206: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 16:18:37.206: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.206: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:18:37.206: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.206: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 16:18:37.206: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:18:37.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:18:37.206: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:18:37.206: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
Dec 14 16:18:37.220: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.220: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:18:37.220: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.220: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:18:37.220: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
Dec 14 16:18:37.220: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 16:18:37.220: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:18:37.220: INFO: 	Container e2e ready: true, restart count 0
Dec 14 16:18:37.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:18:37.220: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:18:37.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:18:37.220: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 16:18:37.221
Dec 14 16:18:37.251: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-248" to be "running"
Dec 14 16:18:37.255: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774487ms
Dec 14 16:18:39.264: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012022681s
Dec 14 16:18:39.264: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 16:18:39.27
STEP: Trying to apply a random label on the found node. 12/14/22 16:18:39.293
STEP: verifying the node has the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 95 12/14/22 16:18:39.317
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/14/22 16:18:39.327
Dec 14 16:18:39.342: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-248" to be "not pending"
Dec 14 16:18:39.361: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.475845ms
Dec 14 16:18:41.370: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.027249158s
Dec 14 16:18:41.370: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.56 on the node which pod4 resides and expect not scheduled 12/14/22 16:18:41.37
Dec 14 16:18:41.384: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-248" to be "not pending"
Dec 14 16:18:41.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.557484ms
Dec 14 16:18:43.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019384983s
Dec 14 16:18:45.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017755367s
Dec 14 16:18:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018588196s
Dec 14 16:18:49.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020397206s
Dec 14 16:18:51.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020869234s
Dec 14 16:18:53.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021364515s
Dec 14 16:18:55.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01786185s
Dec 14 16:18:57.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021630591s
Dec 14 16:18:59.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018238005s
Dec 14 16:19:01.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020125185s
Dec 14 16:19:03.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019268396s
Dec 14 16:19:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019530906s
Dec 14 16:19:07.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024766371s
Dec 14 16:19:09.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020326686s
Dec 14 16:19:11.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019606633s
Dec 14 16:19:13.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019580848s
Dec 14 16:19:15.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018524833s
Dec 14 16:19:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01949138s
Dec 14 16:19:19.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019159198s
Dec 14 16:19:21.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021149317s
Dec 14 16:19:23.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.018452642s
Dec 14 16:19:25.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019452355s
Dec 14 16:19:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01789628s
Dec 14 16:19:29.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019261417s
Dec 14 16:19:31.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.020153092s
Dec 14 16:19:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020069824s
Dec 14 16:19:35.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020139128s
Dec 14 16:19:37.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018859292s
Dec 14 16:19:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.019838354s
Dec 14 16:19:41.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01845329s
Dec 14 16:19:43.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019468947s
Dec 14 16:19:45.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019255588s
Dec 14 16:19:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018866734s
Dec 14 16:19:49.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017487617s
Dec 14 16:19:51.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019436367s
Dec 14 16:19:53.400: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016128587s
Dec 14 16:19:55.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019008628s
Dec 14 16:19:57.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018521689s
Dec 14 16:19:59.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020074566s
Dec 14 16:20:01.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017636025s
Dec 14 16:20:03.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022923541s
Dec 14 16:20:05.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018810361s
Dec 14 16:20:07.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019845946s
Dec 14 16:20:09.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.020638134s
Dec 14 16:20:11.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021702863s
Dec 14 16:20:13.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020724891s
Dec 14 16:20:15.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019943776s
Dec 14 16:20:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019857198s
Dec 14 16:20:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020108798s
Dec 14 16:20:21.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019737965s
Dec 14 16:20:23.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022860215s
Dec 14 16:20:25.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01796355s
Dec 14 16:20:27.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019096084s
Dec 14 16:20:29.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019361913s
Dec 14 16:20:31.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022989859s
Dec 14 16:20:33.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.021674968s
Dec 14 16:20:35.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021346986s
Dec 14 16:20:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020052534s
Dec 14 16:20:39.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018414206s
Dec 14 16:20:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019672887s
Dec 14 16:20:43.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020594758s
Dec 14 16:20:45.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.019576145s
Dec 14 16:20:47.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.020107054s
Dec 14 16:20:49.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017993778s
Dec 14 16:20:51.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.021067608s
Dec 14 16:20:53.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.01817252s
Dec 14 16:20:55.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.019867011s
Dec 14 16:20:57.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017913611s
Dec 14 16:20:59.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018831851s
Dec 14 16:21:01.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.019307891s
Dec 14 16:21:03.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019437232s
Dec 14 16:21:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.019960127s
Dec 14 16:21:07.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.021170717s
Dec 14 16:21:09.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.025109623s
Dec 14 16:21:11.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.018062219s
Dec 14 16:21:13.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.019807773s
Dec 14 16:21:15.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.018907899s
Dec 14 16:21:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.020330879s
Dec 14 16:21:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019940831s
Dec 14 16:21:21.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022279185s
Dec 14 16:21:23.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.017455924s
Dec 14 16:21:25.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.019180322s
Dec 14 16:21:27.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.022099935s
Dec 14 16:21:29.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017538094s
Dec 14 16:21:31.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020080699s
Dec 14 16:21:33.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.016811607s
Dec 14 16:21:35.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022218481s
Dec 14 16:21:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019798874s
Dec 14 16:21:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01951276s
Dec 14 16:21:41.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.021485869s
Dec 14 16:21:43.410: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026202289s
Dec 14 16:21:45.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.019290761s
Dec 14 16:21:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.018777206s
Dec 14 16:21:49.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019457241s
Dec 14 16:21:51.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.019304798s
Dec 14 16:21:53.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.019292235s
Dec 14 16:21:55.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.019681347s
Dec 14 16:21:57.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.022371772s
Dec 14 16:21:59.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.019432925s
Dec 14 16:22:01.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018401555s
Dec 14 16:22:03.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023970407s
Dec 14 16:22:05.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.022121579s
Dec 14 16:22:07.413: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028655485s
Dec 14 16:22:09.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.021798295s
Dec 14 16:22:11.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022301106s
Dec 14 16:22:13.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018174362s
Dec 14 16:22:15.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024442036s
Dec 14 16:22:17.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.022336589s
Dec 14 16:22:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.019713407s
Dec 14 16:22:21.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.024035249s
Dec 14 16:22:23.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.019793217s
Dec 14 16:22:25.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019206565s
Dec 14 16:22:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018115233s
Dec 14 16:22:29.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019453112s
Dec 14 16:22:31.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018146583s
Dec 14 16:22:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020143972s
Dec 14 16:22:35.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01908546s
Dec 14 16:22:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019439921s
Dec 14 16:22:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.019617505s
Dec 14 16:22:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.019713346s
Dec 14 16:22:43.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01995542s
Dec 14 16:22:45.411: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.026540435s
Dec 14 16:22:47.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023531854s
Dec 14 16:22:49.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.020883447s
Dec 14 16:22:51.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.02032577s
Dec 14 16:22:53.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.02127879s
Dec 14 16:22:55.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018189178s
Dec 14 16:22:57.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016560196s
Dec 14 16:22:59.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.022181207s
Dec 14 16:23:01.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.021923789s
Dec 14 16:23:03.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019148346s
Dec 14 16:23:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.020107674s
Dec 14 16:23:07.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.020261736s
Dec 14 16:23:09.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.02018061s
Dec 14 16:23:11.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019550318s
Dec 14 16:23:13.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.01718533s
Dec 14 16:23:15.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.020821189s
Dec 14 16:23:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.019822387s
Dec 14 16:23:19.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022060151s
Dec 14 16:23:21.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.018521967s
Dec 14 16:23:23.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017602297s
Dec 14 16:23:25.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.020224705s
Dec 14 16:23:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.018043169s
Dec 14 16:23:29.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.021782389s
Dec 14 16:23:31.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023508808s
Dec 14 16:23:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019796144s
Dec 14 16:23:35.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.020464055s
Dec 14 16:23:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.019826557s
Dec 14 16:23:39.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017891217s
Dec 14 16:23:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020036565s
Dec 14 16:23:41.411: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027267901s
STEP: removing the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 off the node iet9eich7uhu-3 12/14/22 16:23:41.412
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 12/14/22 16:23:41.434
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:23:41.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-248" for this suite. 12/14/22 16:23:41.456
------------------------------
• [SLOW TEST] [304.360 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:18:37.108
    Dec 14 16:18:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-pred 12/14/22 16:18:37.113
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:18:37.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:18:37.14
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 14 16:18:37.144: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 16:18:37.158: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 16:18:37.165: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
    Dec 14 16:18:37.188: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.189: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:18:37.189: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.189: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 16:18:37.189: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.189: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 16:18:37.189: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.189: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 16:18:37.189: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.189: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Dec 14 16:18:37.189: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.190: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:18:37.190: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.190: INFO: 	Container kube-scheduler ready: true, restart count 2
    Dec 14 16:18:37.190: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:18:37.190: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:18:37.190: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 16:18:37.190: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
    Dec 14 16:18:37.205: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.205: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:18:37.205: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.205: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 16:18:37.205: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.205: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 16:18:37.205: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.205: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 16:18:37.205: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.206: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 14 16:18:37.206: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.206: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:18:37.206: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.206: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 14 16:18:37.206: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:18:37.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:18:37.206: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 16:18:37.206: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
    Dec 14 16:18:37.220: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.220: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.220: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
    Dec 14 16:18:37.220: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:18:37.220: INFO: 	Container e2e ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:18:37.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:18:37.220: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 16:18:37.221
    Dec 14 16:18:37.251: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-248" to be "running"
    Dec 14 16:18:37.255: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774487ms
    Dec 14 16:18:39.264: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012022681s
    Dec 14 16:18:39.264: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 16:18:39.27
    STEP: Trying to apply a random label on the found node. 12/14/22 16:18:39.293
    STEP: verifying the node has the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 95 12/14/22 16:18:39.317
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/14/22 16:18:39.327
    Dec 14 16:18:39.342: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-248" to be "not pending"
    Dec 14 16:18:39.361: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.475845ms
    Dec 14 16:18:41.370: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.027249158s
    Dec 14 16:18:41.370: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.56 on the node which pod4 resides and expect not scheduled 12/14/22 16:18:41.37
    Dec 14 16:18:41.384: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-248" to be "not pending"
    Dec 14 16:18:41.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.557484ms
    Dec 14 16:18:43.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019384983s
    Dec 14 16:18:45.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017755367s
    Dec 14 16:18:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018588196s
    Dec 14 16:18:49.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020397206s
    Dec 14 16:18:51.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020869234s
    Dec 14 16:18:53.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021364515s
    Dec 14 16:18:55.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01786185s
    Dec 14 16:18:57.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021630591s
    Dec 14 16:18:59.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018238005s
    Dec 14 16:19:01.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020125185s
    Dec 14 16:19:03.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019268396s
    Dec 14 16:19:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019530906s
    Dec 14 16:19:07.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024766371s
    Dec 14 16:19:09.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020326686s
    Dec 14 16:19:11.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019606633s
    Dec 14 16:19:13.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019580848s
    Dec 14 16:19:15.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018524833s
    Dec 14 16:19:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01949138s
    Dec 14 16:19:19.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019159198s
    Dec 14 16:19:21.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021149317s
    Dec 14 16:19:23.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.018452642s
    Dec 14 16:19:25.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019452355s
    Dec 14 16:19:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01789628s
    Dec 14 16:19:29.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019261417s
    Dec 14 16:19:31.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.020153092s
    Dec 14 16:19:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020069824s
    Dec 14 16:19:35.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020139128s
    Dec 14 16:19:37.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018859292s
    Dec 14 16:19:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.019838354s
    Dec 14 16:19:41.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01845329s
    Dec 14 16:19:43.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019468947s
    Dec 14 16:19:45.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019255588s
    Dec 14 16:19:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018866734s
    Dec 14 16:19:49.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017487617s
    Dec 14 16:19:51.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019436367s
    Dec 14 16:19:53.400: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016128587s
    Dec 14 16:19:55.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019008628s
    Dec 14 16:19:57.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018521689s
    Dec 14 16:19:59.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020074566s
    Dec 14 16:20:01.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017636025s
    Dec 14 16:20:03.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022923541s
    Dec 14 16:20:05.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018810361s
    Dec 14 16:20:07.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019845946s
    Dec 14 16:20:09.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.020638134s
    Dec 14 16:20:11.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021702863s
    Dec 14 16:20:13.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020724891s
    Dec 14 16:20:15.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019943776s
    Dec 14 16:20:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019857198s
    Dec 14 16:20:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020108798s
    Dec 14 16:20:21.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019737965s
    Dec 14 16:20:23.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022860215s
    Dec 14 16:20:25.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01796355s
    Dec 14 16:20:27.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019096084s
    Dec 14 16:20:29.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019361913s
    Dec 14 16:20:31.407: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022989859s
    Dec 14 16:20:33.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.021674968s
    Dec 14 16:20:35.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021346986s
    Dec 14 16:20:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020052534s
    Dec 14 16:20:39.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018414206s
    Dec 14 16:20:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019672887s
    Dec 14 16:20:43.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.020594758s
    Dec 14 16:20:45.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.019576145s
    Dec 14 16:20:47.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.020107054s
    Dec 14 16:20:49.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017993778s
    Dec 14 16:20:51.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.021067608s
    Dec 14 16:20:53.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.01817252s
    Dec 14 16:20:55.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.019867011s
    Dec 14 16:20:57.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017913611s
    Dec 14 16:20:59.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018831851s
    Dec 14 16:21:01.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.019307891s
    Dec 14 16:21:03.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019437232s
    Dec 14 16:21:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.019960127s
    Dec 14 16:21:07.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.021170717s
    Dec 14 16:21:09.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.025109623s
    Dec 14 16:21:11.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.018062219s
    Dec 14 16:21:13.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.019807773s
    Dec 14 16:21:15.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.018907899s
    Dec 14 16:21:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.020330879s
    Dec 14 16:21:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019940831s
    Dec 14 16:21:21.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022279185s
    Dec 14 16:21:23.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.017455924s
    Dec 14 16:21:25.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.019180322s
    Dec 14 16:21:27.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.022099935s
    Dec 14 16:21:29.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017538094s
    Dec 14 16:21:31.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020080699s
    Dec 14 16:21:33.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.016811607s
    Dec 14 16:21:35.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.022218481s
    Dec 14 16:21:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019798874s
    Dec 14 16:21:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01951276s
    Dec 14 16:21:41.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.021485869s
    Dec 14 16:21:43.410: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026202289s
    Dec 14 16:21:45.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.019290761s
    Dec 14 16:21:47.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.018777206s
    Dec 14 16:21:49.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019457241s
    Dec 14 16:21:51.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.019304798s
    Dec 14 16:21:53.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.019292235s
    Dec 14 16:21:55.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.019681347s
    Dec 14 16:21:57.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.022371772s
    Dec 14 16:21:59.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.019432925s
    Dec 14 16:22:01.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018401555s
    Dec 14 16:22:03.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023970407s
    Dec 14 16:22:05.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.022121579s
    Dec 14 16:22:07.413: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028655485s
    Dec 14 16:22:09.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.021798295s
    Dec 14 16:22:11.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022301106s
    Dec 14 16:22:13.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018174362s
    Dec 14 16:22:15.409: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024442036s
    Dec 14 16:22:17.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.022336589s
    Dec 14 16:22:19.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.019713407s
    Dec 14 16:22:21.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.024035249s
    Dec 14 16:22:23.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.019793217s
    Dec 14 16:22:25.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019206565s
    Dec 14 16:22:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018115233s
    Dec 14 16:22:29.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019453112s
    Dec 14 16:22:31.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018146583s
    Dec 14 16:22:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020143972s
    Dec 14 16:22:35.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01908546s
    Dec 14 16:22:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019439921s
    Dec 14 16:22:39.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.019617505s
    Dec 14 16:22:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.019713346s
    Dec 14 16:22:43.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01995542s
    Dec 14 16:22:45.411: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.026540435s
    Dec 14 16:22:47.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023531854s
    Dec 14 16:22:49.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.020883447s
    Dec 14 16:22:51.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.02032577s
    Dec 14 16:22:53.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.02127879s
    Dec 14 16:22:55.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018189178s
    Dec 14 16:22:57.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016560196s
    Dec 14 16:22:59.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.022181207s
    Dec 14 16:23:01.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.021923789s
    Dec 14 16:23:03.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019148346s
    Dec 14 16:23:05.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.020107674s
    Dec 14 16:23:07.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.020261736s
    Dec 14 16:23:09.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.02018061s
    Dec 14 16:23:11.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019550318s
    Dec 14 16:23:13.401: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.01718533s
    Dec 14 16:23:15.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.020821189s
    Dec 14 16:23:17.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.019822387s
    Dec 14 16:23:19.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022060151s
    Dec 14 16:23:21.403: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.018521967s
    Dec 14 16:23:23.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017602297s
    Dec 14 16:23:25.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.020224705s
    Dec 14 16:23:27.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.018043169s
    Dec 14 16:23:29.406: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.021782389s
    Dec 14 16:23:31.408: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.023508808s
    Dec 14 16:23:33.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019796144s
    Dec 14 16:23:35.405: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.020464055s
    Dec 14 16:23:37.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.019826557s
    Dec 14 16:23:39.402: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017891217s
    Dec 14 16:23:41.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020036565s
    Dec 14 16:23:41.411: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027267901s
    STEP: removing the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 off the node iet9eich7uhu-3 12/14/22 16:23:41.412
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-cf874c31-b0cb-4724-a427-e61c7008ea16 12/14/22 16:23:41.434
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:23:41.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-248" for this suite. 12/14/22 16:23:41.456
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:23:41.473
Dec 14 16:23:41.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename containers 12/14/22 16:23:41.477
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:41.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:41.509
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Dec 14 16:23:41.530: INFO: Waiting up to 5m0s for pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d" in namespace "containers-3679" to be "running"
Dec 14 16:23:41.543: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.704086ms
Dec 14 16:23:43.562: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d": Phase="Running", Reason="", readiness=true. Elapsed: 2.031616935s
Dec 14 16:23:43.562: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:23:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3679" for this suite. 12/14/22 16:23:43.599
------------------------------
• [2.141 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:23:41.473
    Dec 14 16:23:41.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename containers 12/14/22 16:23:41.477
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:41.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:41.509
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Dec 14 16:23:41.530: INFO: Waiting up to 5m0s for pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d" in namespace "containers-3679" to be "running"
    Dec 14 16:23:41.543: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.704086ms
    Dec 14 16:23:43.562: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d": Phase="Running", Reason="", readiness=true. Elapsed: 2.031616935s
    Dec 14 16:23:43.562: INFO: Pod "client-containers-cbadfa00-1cd7-4d11-9d16-08c6c752050d" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:23:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3679" for this suite. 12/14/22 16:23:43.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:23:43.617
Dec 14 16:23:43.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:23:43.621
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:43.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:43.66
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-7b4e400c-45bc-4714-a508-4fac7480bb2f 12/14/22 16:23:43.665
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:23:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6680" for this suite. 12/14/22 16:23:43.679
------------------------------
• [0.074 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:23:43.617
    Dec 14 16:23:43.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:23:43.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:43.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:43.66
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-7b4e400c-45bc-4714-a508-4fac7480bb2f 12/14/22 16:23:43.665
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:23:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6680" for this suite. 12/14/22 16:23:43.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:23:43.7
Dec 14 16:23:43.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:23:43.703
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:43.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:43.725
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:23:43.75
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:23:44.878
STEP: Deploying the webhook pod 12/14/22 16:23:44.895
STEP: Wait for the deployment to be ready 12/14/22 16:23:44.914
Dec 14 16:23:44.925: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 16:23:46.941
STEP: Verifying the service has paired with the endpoint 12/14/22 16:23:46.959
Dec 14 16:23:47.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/14/22 16:23:47.965
STEP: create a pod that should be updated by the webhook 12/14/22 16:23:47.995
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:23:48.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4786" for this suite. 12/14/22 16:23:48.137
STEP: Destroying namespace "webhook-4786-markers" for this suite. 12/14/22 16:23:48.167
------------------------------
• [4.481 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:23:43.7
    Dec 14 16:23:43.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:23:43.703
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:43.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:43.725
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:23:43.75
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:23:44.878
    STEP: Deploying the webhook pod 12/14/22 16:23:44.895
    STEP: Wait for the deployment to be ready 12/14/22 16:23:44.914
    Dec 14 16:23:44.925: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 16:23:46.941
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:23:46.959
    Dec 14 16:23:47.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/14/22 16:23:47.965
    STEP: create a pod that should be updated by the webhook 12/14/22 16:23:47.995
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:23:48.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4786" for this suite. 12/14/22 16:23:48.137
    STEP: Destroying namespace "webhook-4786-markers" for this suite. 12/14/22 16:23:48.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:23:48.184
Dec 14 16:23:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:23:48.186
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:48.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:48.216
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Dec 14 16:23:48.220: INFO: Creating deployment "webserver-deployment"
Dec 14 16:23:48.229: INFO: Waiting for observed generation 1
Dec 14 16:23:50.239: INFO: Waiting for all required pods to come up
Dec 14 16:23:50.246: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 12/14/22 16:23:50.246
Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lxn59" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2dlck" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5msbd" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-86lk9" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dq652" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nrdjm" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nm7dg" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.248: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-r5hc6" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.248: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rngvh" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xbjwb" in namespace "deployment-5324" to be "running"
Dec 14 16:23:50.257: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59": Phase="Pending", Reason="", readiness=false. Elapsed: 11.193093ms
Dec 14 16:23:50.258: INFO: Pod "webserver-deployment-7f5969cbc7-dq652": Phase="Pending", Reason="", readiness=false. Elapsed: 11.250231ms
Dec 14 16:23:50.260: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Pending", Reason="", readiness=false. Elapsed: 13.765015ms
Dec 14 16:23:50.261: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 13.734293ms
Dec 14 16:23:50.269: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm": Phase="Pending", Reason="", readiness=false. Elapsed: 21.779544ms
Dec 14 16:23:50.271: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.189822ms
Dec 14 16:23:50.272: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.477723ms
Dec 14 16:23:50.278: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh": Phase="Pending", Reason="", readiness=false. Elapsed: 30.28508ms
Dec 14 16:23:50.292: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd": Phase="Pending", Reason="", readiness=false. Elapsed: 45.256524ms
Dec 14 16:23:50.292: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6": Phase="Pending", Reason="", readiness=false. Elapsed: 44.362789ms
Dec 14 16:23:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59": Phase="Running", Reason="", readiness=true. Elapsed: 2.016325435s
Dec 14 16:23:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59" satisfied condition "running"
Dec 14 16:23:52.269: INFO: Pod "webserver-deployment-7f5969cbc7-dq652": Phase="Running", Reason="", readiness=true. Elapsed: 2.021866658s
Dec 14 16:23:52.269: INFO: Pod "webserver-deployment-7f5969cbc7-dq652" satisfied condition "running"
Dec 14 16:23:52.270: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023856884s
Dec 14 16:23:52.275: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027546712s
Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029024577s
Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.029205751s
Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm" satisfied condition "running"
Dec 14 16:23:52.277: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb": Phase="Running", Reason="", readiness=true. Elapsed: 2.028699687s
Dec 14 16:23:52.277: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb" satisfied condition "running"
Dec 14 16:23:52.285: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh": Phase="Running", Reason="", readiness=true. Elapsed: 2.036874831s
Dec 14 16:23:52.285: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh" satisfied condition "running"
Dec 14 16:23:52.297: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04947856s
Dec 14 16:23:52.297: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6" satisfied condition "running"
Dec 14 16:23:52.298: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.051681699s
Dec 14 16:23:52.298: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd" satisfied condition "running"
Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Running", Reason="", readiness=true. Elapsed: 4.022242962s
Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck" satisfied condition "running"
Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Running", Reason="", readiness=true. Elapsed: 4.0217248s
Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg" satisfied condition "running"
Dec 14 16:23:54.278: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Running", Reason="", readiness=true. Elapsed: 4.030758138s
Dec 14 16:23:54.278: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9" satisfied condition "running"
Dec 14 16:23:54.278: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 16:23:54.289: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 16:23:54.312: INFO: Updating deployment webserver-deployment
Dec 14 16:23:54.312: INFO: Waiting for observed generation 2
Dec 14 16:23:56.328: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 16:23:56.336: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 16:23:56.342: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 16:23:56.360: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 16:23:56.360: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 16:23:56.365: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 16:23:56.375: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 16:23:56.375: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 16:23:56.394: INFO: Updating deployment webserver-deployment
Dec 14 16:23:56.395: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 16:23:56.408: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 16:23:56.415: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:23:56.429: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5324  1a18f4e0-697f-4e25-91f0-531981f3daed 15227 3 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eeee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:23:52 +0000 UTC,LastTransitionTime:2022-12-14 16:23:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-14 16:23:54 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 16:23:56.457: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5324  351e1b0b-0ef2-4280-a7b7-d32140deb6d7 15230 3 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1a18f4e0-697f-4e25-91f0-531981f3daed 0xc003db66a7 0xc003db66a8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a18f4e0-697f-4e25-91f0-531981f3daed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db6748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:23:56.458: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 16:23:56.458: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5324  5e8596ae-ff69-4057-9547-62ad74527df0 15228 3 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1a18f4e0-697f-4e25-91f0-531981f3daed 0xc003db65b7 0xc003db65b8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a18f4e0-697f-4e25-91f0-531981f3daed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db6648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:23:56.495: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5msbd webserver-deployment-7f5969cbc7- deployment-5324  f068517f-a821-4262-b587-ceffba32f09b 15117 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef347 0xc0035ef348}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jwqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jwqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.64,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ff766b917dfca579bf4d1ea19881c9fc3a70dfb4c9047596d229609fb25793fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.496: INFO: Pod "webserver-deployment-7f5969cbc7-dq652" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dq652 webserver-deployment-7f5969cbc7- deployment-5324  6a6a320c-c8d9-4886-9c4b-a6378a059d4e 15128 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef547 0xc0035ef548}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlr4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlr4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.150,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fc3c22bed7c6044e9c89408b1df1595d092b1ca1945475c9707574c671447bc2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.497: INFO: Pod "webserver-deployment-7f5969cbc7-hqq4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqq4s webserver-deployment-7f5969cbc7- deployment-5324  67fb0f23-4fe5-4236-9ebe-a37541679765 15242 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef737 0xc0035ef738}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kxc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kxc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-jps9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jps9q webserver-deployment-7f5969cbc7- deployment-5324  ad899ccb-6e0e-4093-8c0b-98c8062cda08 15243 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef877 0xc0035ef878}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkzgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkzgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-jxmdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxmdg webserver-deployment-7f5969cbc7- deployment-5324  071e8147-4b7d-4edd-8c9e-a52cfecb622f 15237 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef9b7 0xc0035ef9b8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xm9gg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xm9gg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-l4hvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l4hvf webserver-deployment-7f5969cbc7- deployment-5324  62782c65-777d-49e6-8f10-45b8d53aca9e 15241 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efaf7 0xc0035efaf8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fqcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fqcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.500: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxn59 webserver-deployment-7f5969cbc7- deployment-5324  e362ce01-4679-4f22-be87-984f1170a065 15107 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efc60 0xc0035efc61}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hbjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hbjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.64,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3dcc4bfe92c05a837674848e72d4f2701106d3c16277630faa84525bc8516c51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.500: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nm7dg webserver-deployment-7f5969cbc7- deployment-5324  a0c343fb-c2be-43fb-a2c7-9838e41dc60e 15140 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efe47 0xc0035efe48}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llcpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llcpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.151,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1d3d8109867594caede408d59ebbfb4d4df321e451466690e4eec7b64ba6ddf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.501: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nrdjm webserver-deployment-7f5969cbc7- deployment-5324  98b5fee7-c86a-4109-a202-2b781e6e1058 15134 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4037 0xc0041d4038}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jhh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jhh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.65,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e25162f8cb333774baec25ffbd8c9e9d46b235f6d33d01e493f47e027ed4085a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.501: INFO: Pod "webserver-deployment-7f5969cbc7-nwpvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nwpvj webserver-deployment-7f5969cbc7- deployment-5324  d7387556-ba7f-4b33-8d72-7b18f0d23f07 15240 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4227 0xc0041d4228}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7ks7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7ks7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.502: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r5hc6 webserver-deployment-7f5969cbc7- deployment-5324  d7e5a8b6-8ac1-4867-a609-0997f1b4d328 15099 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4367 0xc0041d4368}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqhhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqhhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.63,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9c1fbe345ec4b6425831fd44d313403d4bc493a0b2f3577448923310cc8e6355,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.502: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rngvh webserver-deployment-7f5969cbc7- deployment-5324  ddce0598-3e5f-4d08-85f9-1cb61f8e7d42 15102 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4557 0xc0041d4558}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4mrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4mrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.65,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://39b096ad667c31b0e03c61917e18f51bd579a791f71135917923479976048b33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.503: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xbjwb webserver-deployment-7f5969cbc7- deployment-5324  7a439338-f33a-4087-8465-94a3c4b12e33 15132 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4747 0xc0041d4748}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptbgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptbgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.66,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8c1f7c52105373f98fe4b0a0297be432e9365bca6dda7041bb783df825a8526c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.503: INFO: Pod "webserver-deployment-7f5969cbc7-xpswc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xpswc webserver-deployment-7f5969cbc7- deployment-5324  f2e109d2-cb72-433e-b1be-73f3a076f5bc 15235 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4937 0xc0041d4938}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxlgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxlgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.504: INFO: Pod "webserver-deployment-7f5969cbc7-xsr6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xsr6l webserver-deployment-7f5969cbc7- deployment-5324  fcdf7136-098a-4e63-90e8-324fa07cf043 15239 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4ac0 0xc0041d4ac1}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lktnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lktnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.504: INFO: Pod "webserver-deployment-d9f79cb5-5r6p8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5r6p8 webserver-deployment-d9f79cb5- deployment-5324  539a725c-a9a1-4924-9587-ae5e2069837b 15221 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d4c0f 0xc0041d4c20}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jz4d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jz4d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.505: INFO: Pod "webserver-deployment-d9f79cb5-s8zlr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s8zlr webserver-deployment-d9f79cb5- deployment-5324  04f90dd6-15ec-43ff-ac5a-8231fe983cae 15192 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d4e17 0xc0041d4e18}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b58nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b58nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.505: INFO: Pod "webserver-deployment-d9f79cb5-t6896" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6896 webserver-deployment-d9f79cb5- deployment-5324  d4c716b2-165b-431f-8397-52c041752607 15216 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5007 0xc0041d5008}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5dxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5dxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.507: INFO: Pod "webserver-deployment-d9f79cb5-wbgzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wbgzn webserver-deployment-d9f79cb5- deployment-5324  af69f31e-2ce1-4f5d-8758-6b82fe2ba615 15236 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d51f7 0xc0041d51f8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8tg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8tg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.507: INFO: Pod "webserver-deployment-d9f79cb5-wrns2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wrns2 webserver-deployment-d9f79cb5- deployment-5324  395f0fd3-493b-4c6d-898e-48a1da86a8b5 15196 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5347 0xc0041d5348}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84588,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84588,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:23:56.508: INFO: Pod "webserver-deployment-d9f79cb5-xr8hf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xr8hf webserver-deployment-d9f79cb5- deployment-5324  174aae45-524e-4c68-9234-58e5cb1839c0 15199 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5537 0xc0041d5538}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9zx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9zx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:23:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5324" for this suite. 12/14/22 16:23:56.596
------------------------------
• [SLOW TEST] [8.475 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:23:48.184
    Dec 14 16:23:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:23:48.186
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:48.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:48.216
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Dec 14 16:23:48.220: INFO: Creating deployment "webserver-deployment"
    Dec 14 16:23:48.229: INFO: Waiting for observed generation 1
    Dec 14 16:23:50.239: INFO: Waiting for all required pods to come up
    Dec 14 16:23:50.246: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 12/14/22 16:23:50.246
    Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lxn59" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2dlck" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5msbd" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-86lk9" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dq652" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nrdjm" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.247: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nm7dg" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.248: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-r5hc6" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.248: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rngvh" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.246: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xbjwb" in namespace "deployment-5324" to be "running"
    Dec 14 16:23:50.257: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59": Phase="Pending", Reason="", readiness=false. Elapsed: 11.193093ms
    Dec 14 16:23:50.258: INFO: Pod "webserver-deployment-7f5969cbc7-dq652": Phase="Pending", Reason="", readiness=false. Elapsed: 11.250231ms
    Dec 14 16:23:50.260: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Pending", Reason="", readiness=false. Elapsed: 13.765015ms
    Dec 14 16:23:50.261: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 13.734293ms
    Dec 14 16:23:50.269: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm": Phase="Pending", Reason="", readiness=false. Elapsed: 21.779544ms
    Dec 14 16:23:50.271: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.189822ms
    Dec 14 16:23:50.272: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.477723ms
    Dec 14 16:23:50.278: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh": Phase="Pending", Reason="", readiness=false. Elapsed: 30.28508ms
    Dec 14 16:23:50.292: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd": Phase="Pending", Reason="", readiness=false. Elapsed: 45.256524ms
    Dec 14 16:23:50.292: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6": Phase="Pending", Reason="", readiness=false. Elapsed: 44.362789ms
    Dec 14 16:23:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59": Phase="Running", Reason="", readiness=true. Elapsed: 2.016325435s
    Dec 14 16:23:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59" satisfied condition "running"
    Dec 14 16:23:52.269: INFO: Pod "webserver-deployment-7f5969cbc7-dq652": Phase="Running", Reason="", readiness=true. Elapsed: 2.021866658s
    Dec 14 16:23:52.269: INFO: Pod "webserver-deployment-7f5969cbc7-dq652" satisfied condition "running"
    Dec 14 16:23:52.270: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023856884s
    Dec 14 16:23:52.275: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027546712s
    Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029024577s
    Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.029205751s
    Dec 14 16:23:52.276: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm" satisfied condition "running"
    Dec 14 16:23:52.277: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb": Phase="Running", Reason="", readiness=true. Elapsed: 2.028699687s
    Dec 14 16:23:52.277: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb" satisfied condition "running"
    Dec 14 16:23:52.285: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh": Phase="Running", Reason="", readiness=true. Elapsed: 2.036874831s
    Dec 14 16:23:52.285: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh" satisfied condition "running"
    Dec 14 16:23:52.297: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04947856s
    Dec 14 16:23:52.297: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6" satisfied condition "running"
    Dec 14 16:23:52.298: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.051681699s
    Dec 14 16:23:52.298: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd" satisfied condition "running"
    Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck": Phase="Running", Reason="", readiness=true. Elapsed: 4.022242962s
    Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-2dlck" satisfied condition "running"
    Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg": Phase="Running", Reason="", readiness=true. Elapsed: 4.0217248s
    Dec 14 16:23:54.269: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg" satisfied condition "running"
    Dec 14 16:23:54.278: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9": Phase="Running", Reason="", readiness=true. Elapsed: 4.030758138s
    Dec 14 16:23:54.278: INFO: Pod "webserver-deployment-7f5969cbc7-86lk9" satisfied condition "running"
    Dec 14 16:23:54.278: INFO: Waiting for deployment "webserver-deployment" to complete
    Dec 14 16:23:54.289: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Dec 14 16:23:54.312: INFO: Updating deployment webserver-deployment
    Dec 14 16:23:54.312: INFO: Waiting for observed generation 2
    Dec 14 16:23:56.328: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Dec 14 16:23:56.336: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Dec 14 16:23:56.342: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 16:23:56.360: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Dec 14 16:23:56.360: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Dec 14 16:23:56.365: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 16:23:56.375: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Dec 14 16:23:56.375: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Dec 14 16:23:56.394: INFO: Updating deployment webserver-deployment
    Dec 14 16:23:56.395: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Dec 14 16:23:56.408: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Dec 14 16:23:56.415: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:23:56.429: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5324  1a18f4e0-697f-4e25-91f0-531981f3daed 15227 3 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eeee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:23:52 +0000 UTC,LastTransitionTime:2022-12-14 16:23:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-14 16:23:54 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Dec 14 16:23:56.457: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5324  351e1b0b-0ef2-4280-a7b7-d32140deb6d7 15230 3 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1a18f4e0-697f-4e25-91f0-531981f3daed 0xc003db66a7 0xc003db66a8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a18f4e0-697f-4e25-91f0-531981f3daed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db6748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:23:56.458: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Dec 14 16:23:56.458: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5324  5e8596ae-ff69-4057-9547-62ad74527df0 15228 3 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1a18f4e0-697f-4e25-91f0-531981f3daed 0xc003db65b7 0xc003db65b8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a18f4e0-697f-4e25-91f0-531981f3daed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db6648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:23:56.495: INFO: Pod "webserver-deployment-7f5969cbc7-5msbd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5msbd webserver-deployment-7f5969cbc7- deployment-5324  f068517f-a821-4262-b587-ceffba32f09b 15117 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef347 0xc0035ef348}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jwqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jwqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.64,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ff766b917dfca579bf4d1ea19881c9fc3a70dfb4c9047596d229609fb25793fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.496: INFO: Pod "webserver-deployment-7f5969cbc7-dq652" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dq652 webserver-deployment-7f5969cbc7- deployment-5324  6a6a320c-c8d9-4886-9c4b-a6378a059d4e 15128 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef547 0xc0035ef548}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlr4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlr4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.150,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fc3c22bed7c6044e9c89408b1df1595d092b1ca1945475c9707574c671447bc2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.497: INFO: Pod "webserver-deployment-7f5969cbc7-hqq4s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqq4s webserver-deployment-7f5969cbc7- deployment-5324  67fb0f23-4fe5-4236-9ebe-a37541679765 15242 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef737 0xc0035ef738}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kxc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kxc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-jps9q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jps9q webserver-deployment-7f5969cbc7- deployment-5324  ad899ccb-6e0e-4093-8c0b-98c8062cda08 15243 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef877 0xc0035ef878}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkzgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkzgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-jxmdg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxmdg webserver-deployment-7f5969cbc7- deployment-5324  071e8147-4b7d-4edd-8c9e-a52cfecb622f 15237 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035ef9b7 0xc0035ef9b8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xm9gg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xm9gg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.499: INFO: Pod "webserver-deployment-7f5969cbc7-l4hvf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l4hvf webserver-deployment-7f5969cbc7- deployment-5324  62782c65-777d-49e6-8f10-45b8d53aca9e 15241 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efaf7 0xc0035efaf8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fqcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fqcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.500: INFO: Pod "webserver-deployment-7f5969cbc7-lxn59" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxn59 webserver-deployment-7f5969cbc7- deployment-5324  e362ce01-4679-4f22-be87-984f1170a065 15107 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efc60 0xc0035efc61}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hbjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hbjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.64,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3dcc4bfe92c05a837674848e72d4f2701106d3c16277630faa84525bc8516c51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.500: INFO: Pod "webserver-deployment-7f5969cbc7-nm7dg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nm7dg webserver-deployment-7f5969cbc7- deployment-5324  a0c343fb-c2be-43fb-a2c7-9838e41dc60e 15140 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0035efe47 0xc0035efe48}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llcpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llcpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.151,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1d3d8109867594caede408d59ebbfb4d4df321e451466690e4eec7b64ba6ddf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.501: INFO: Pod "webserver-deployment-7f5969cbc7-nrdjm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nrdjm webserver-deployment-7f5969cbc7- deployment-5324  98b5fee7-c86a-4109-a202-2b781e6e1058 15134 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4037 0xc0041d4038}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jhh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jhh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.65,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e25162f8cb333774baec25ffbd8c9e9d46b235f6d33d01e493f47e027ed4085a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.501: INFO: Pod "webserver-deployment-7f5969cbc7-nwpvj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nwpvj webserver-deployment-7f5969cbc7- deployment-5324  d7387556-ba7f-4b33-8d72-7b18f0d23f07 15240 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4227 0xc0041d4228}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7ks7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7ks7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.502: INFO: Pod "webserver-deployment-7f5969cbc7-r5hc6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r5hc6 webserver-deployment-7f5969cbc7- deployment-5324  d7e5a8b6-8ac1-4867-a609-0997f1b4d328 15099 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4367 0xc0041d4368}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqhhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqhhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.63,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9c1fbe345ec4b6425831fd44d313403d4bc493a0b2f3577448923310cc8e6355,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.502: INFO: Pod "webserver-deployment-7f5969cbc7-rngvh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rngvh webserver-deployment-7f5969cbc7- deployment-5324  ddce0598-3e5f-4d08-85f9-1cb61f8e7d42 15102 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4557 0xc0041d4558}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4mrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4mrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.65,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://39b096ad667c31b0e03c61917e18f51bd579a791f71135917923479976048b33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.503: INFO: Pod "webserver-deployment-7f5969cbc7-xbjwb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xbjwb webserver-deployment-7f5969cbc7- deployment-5324  7a439338-f33a-4087-8465-94a3c4b12e33 15132 0 2022-12-14 16:23:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4747 0xc0041d4748}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptbgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptbgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.66,StartTime:2022-12-14 16:23:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:23:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8c1f7c52105373f98fe4b0a0297be432e9365bca6dda7041bb783df825a8526c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.503: INFO: Pod "webserver-deployment-7f5969cbc7-xpswc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xpswc webserver-deployment-7f5969cbc7- deployment-5324  f2e109d2-cb72-433e-b1be-73f3a076f5bc 15235 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4937 0xc0041d4938}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxlgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxlgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.504: INFO: Pod "webserver-deployment-7f5969cbc7-xsr6l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xsr6l webserver-deployment-7f5969cbc7- deployment-5324  fcdf7136-098a-4e63-90e8-324fa07cf043 15239 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5e8596ae-ff69-4057-9547-62ad74527df0 0xc0041d4ac0 0xc0041d4ac1}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e8596ae-ff69-4057-9547-62ad74527df0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lktnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lktnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.504: INFO: Pod "webserver-deployment-d9f79cb5-5r6p8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5r6p8 webserver-deployment-d9f79cb5- deployment-5324  539a725c-a9a1-4924-9587-ae5e2069837b 15221 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d4c0f 0xc0041d4c20}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jz4d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jz4d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.505: INFO: Pod "webserver-deployment-d9f79cb5-s8zlr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s8zlr webserver-deployment-d9f79cb5- deployment-5324  04f90dd6-15ec-43ff-ac5a-8231fe983cae 15192 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d4e17 0xc0041d4e18}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b58nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b58nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.505: INFO: Pod "webserver-deployment-d9f79cb5-t6896" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6896 webserver-deployment-d9f79cb5- deployment-5324  d4c716b2-165b-431f-8397-52c041752607 15216 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5007 0xc0041d5008}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5dxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5dxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.507: INFO: Pod "webserver-deployment-d9f79cb5-wbgzn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wbgzn webserver-deployment-d9f79cb5- deployment-5324  af69f31e-2ce1-4f5d-8758-6b82fe2ba615 15236 0 2022-12-14 16:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d51f7 0xc0041d51f8}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8tg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8tg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.507: INFO: Pod "webserver-deployment-d9f79cb5-wrns2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wrns2 webserver-deployment-d9f79cb5- deployment-5324  395f0fd3-493b-4c6d-898e-48a1da86a8b5 15196 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5347 0xc0041d5348}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84588,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84588,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:23:56.508: INFO: Pod "webserver-deployment-d9f79cb5-xr8hf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xr8hf webserver-deployment-d9f79cb5- deployment-5324  174aae45-524e-4c68-9234-58e5cb1839c0 15199 0 2022-12-14 16:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 351e1b0b-0ef2-4280-a7b7-d32140deb6d7 0xc0041d5537 0xc0041d5538}] [] [{kube-controller-manager Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"351e1b0b-0ef2-4280-a7b7-d32140deb6d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:23:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9zx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9zx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:,StartTime:2022-12-14 16:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:23:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5324" for this suite. 12/14/22 16:23:56.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:23:56.666
Dec 14 16:23:56.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 16:23:56.668
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:56.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:56.801
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 16:24:56.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9356" for this suite. 12/14/22 16:24:56.884
------------------------------
• [SLOW TEST] [60.231 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:23:56.666
    Dec 14 16:23:56.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 16:23:56.668
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:23:56.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:23:56.801
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:24:56.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9356" for this suite. 12/14/22 16:24:56.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:24:56.9
Dec 14 16:24:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:24:56.906
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:24:56.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:24:56.949
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7b6787e9-4869-4ee2-8b21-427fb7000152 12/14/22 16:24:56.959
STEP: Creating the pod 12/14/22 16:24:56.967
Dec 14 16:24:56.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f" in namespace "projected-1037" to be "running and ready"
Dec 14 16:24:56.989: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838451ms
Dec 14 16:24:56.989: INFO: The phase of Pod pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:24:58.999: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017046159s
Dec 14 16:24:58.999: INFO: The phase of Pod pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f is Running (Ready = true)
Dec 14 16:24:58.999: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-7b6787e9-4869-4ee2-8b21-427fb7000152 12/14/22 16:24:59.016
STEP: waiting to observe update in volume 12/14/22 16:24:59.025
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:01.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1037" for this suite. 12/14/22 16:25:01.068
------------------------------
• [4.178 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:24:56.9
    Dec 14 16:24:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:24:56.906
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:24:56.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:24:56.949
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-7b6787e9-4869-4ee2-8b21-427fb7000152 12/14/22 16:24:56.959
    STEP: Creating the pod 12/14/22 16:24:56.967
    Dec 14 16:24:56.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f" in namespace "projected-1037" to be "running and ready"
    Dec 14 16:24:56.989: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.838451ms
    Dec 14 16:24:56.989: INFO: The phase of Pod pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:24:58.999: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017046159s
    Dec 14 16:24:58.999: INFO: The phase of Pod pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f is Running (Ready = true)
    Dec 14 16:24:58.999: INFO: Pod "pod-projected-configmaps-d4a21883-6183-4717-b64f-73b43e9dd97f" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-7b6787e9-4869-4ee2-8b21-427fb7000152 12/14/22 16:24:59.016
    STEP: waiting to observe update in volume 12/14/22 16:24:59.025
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:01.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1037" for this suite. 12/14/22 16:25:01.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:01.082
Dec 14 16:25:01.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:25:01.084
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:01.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:01.107
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 12/14/22 16:25:01.111
Dec 14 16:25:01.125: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee" in namespace "emptydir-8965" to be "running"
Dec 14 16:25:01.129: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701997ms
Dec 14 16:25:03.138: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.013380236s
Dec 14 16:25:03.139: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee" satisfied condition "running"
STEP: Reading file content from the nginx-container 12/14/22 16:25:03.139
Dec 14 16:25:03.139: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8965 PodName:pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:25:03.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:25:03.141: INFO: ExecWithOptions: Clientset creation
Dec 14 16:25:03.141: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-8965/pods/pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec 14 16:25:03.267: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:03.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8965" for this suite. 12/14/22 16:25:03.274
------------------------------
• [2.201 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:01.082
    Dec 14 16:25:01.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:25:01.084
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:01.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:01.107
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 12/14/22 16:25:01.111
    Dec 14 16:25:01.125: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee" in namespace "emptydir-8965" to be "running"
    Dec 14 16:25:01.129: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701997ms
    Dec 14 16:25:03.138: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.013380236s
    Dec 14 16:25:03.139: INFO: Pod "pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee" satisfied condition "running"
    STEP: Reading file content from the nginx-container 12/14/22 16:25:03.139
    Dec 14 16:25:03.139: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8965 PodName:pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:25:03.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:25:03.141: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:25:03.141: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-8965/pods/pod-sharedvolume-c988e5db-fed1-4245-a552-4728550dd0ee/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Dec 14 16:25:03.267: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:03.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8965" for this suite. 12/14/22 16:25:03.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:03.284
Dec 14 16:25:03.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 16:25:03.288
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:03.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:03.316
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1388 12/14/22 16:25:03.321
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1388 12/14/22 16:25:03.329
Dec 14 16:25:03.350: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:25:13.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 12/14/22 16:25:13.374
STEP: updating a scale subresource 12/14/22 16:25:13.381
STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 16:25:13.395
STEP: Patch a scale subresource 12/14/22 16:25:13.401
STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 16:25:13.41
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 16:25:13.435: INFO: Deleting all statefulset in ns statefulset-1388
Dec 14 16:25:13.443: INFO: Scaling statefulset ss to 0
Dec 14 16:25:23.500: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:25:23.505: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:23.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1388" for this suite. 12/14/22 16:25:23.547
------------------------------
• [SLOW TEST] [20.278 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:03.284
    Dec 14 16:25:03.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 16:25:03.288
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:03.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:03.316
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1388 12/14/22 16:25:03.321
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1388 12/14/22 16:25:03.329
    Dec 14 16:25:03.350: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 16:25:13.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 12/14/22 16:25:13.374
    STEP: updating a scale subresource 12/14/22 16:25:13.381
    STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 16:25:13.395
    STEP: Patch a scale subresource 12/14/22 16:25:13.401
    STEP: verifying the statefulset Spec.Replicas was modified 12/14/22 16:25:13.41
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 16:25:13.435: INFO: Deleting all statefulset in ns statefulset-1388
    Dec 14 16:25:13.443: INFO: Scaling statefulset ss to 0
    Dec 14 16:25:23.500: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:25:23.505: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:23.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1388" for this suite. 12/14/22 16:25:23.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:23.568
Dec 14 16:25:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename endpointslicemirroring 12/14/22 16:25:23.572
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:23.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:23.599
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 12/14/22 16:25:23.617
Dec 14 16:25:23.633: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 12/14/22 16:25:25.643
Dec 14 16:25:25.666: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 12/14/22 16:25:27.672
Dec 14 16:25:27.688: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3595" for this suite. 12/14/22 16:25:29.702
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:23.568
    Dec 14 16:25:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename endpointslicemirroring 12/14/22 16:25:23.572
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:23.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:23.599
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 12/14/22 16:25:23.617
    Dec 14 16:25:23.633: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 12/14/22 16:25:25.643
    Dec 14 16:25:25.666: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 12/14/22 16:25:27.672
    Dec 14 16:25:27.688: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3595" for this suite. 12/14/22 16:25:29.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:29.715
Dec 14 16:25:29.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 16:25:29.716
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:29.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:29.744
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 12/14/22 16:25:29.748
STEP: Ensuring job reaches completions 12/14/22 16:25:29.756
STEP: Ensuring pods with index for job exist 12/14/22 16:25:39.765
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1352" for this suite. 12/14/22 16:25:39.782
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:29.715
    Dec 14 16:25:29.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 16:25:29.716
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:29.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:29.744
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 12/14/22 16:25:29.748
    STEP: Ensuring job reaches completions 12/14/22 16:25:29.756
    STEP: Ensuring pods with index for job exist 12/14/22 16:25:39.765
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1352" for this suite. 12/14/22 16:25:39.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:39.813
Dec 14 16:25:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename certificates 12/14/22 16:25:39.817
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:39.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:39.858
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 12/14/22 16:25:41.418
STEP: getting /apis/certificates.k8s.io 12/14/22 16:25:41.424
STEP: getting /apis/certificates.k8s.io/v1 12/14/22 16:25:41.426
STEP: creating 12/14/22 16:25:41.428
STEP: getting 12/14/22 16:25:41.458
STEP: listing 12/14/22 16:25:41.464
STEP: watching 12/14/22 16:25:41.469
Dec 14 16:25:41.469: INFO: starting watch
STEP: patching 12/14/22 16:25:41.471
STEP: updating 12/14/22 16:25:41.48
Dec 14 16:25:41.489: INFO: waiting for watch events with expected annotations
Dec 14 16:25:41.489: INFO: saw patched and updated annotations
STEP: getting /approval 12/14/22 16:25:41.489
STEP: patching /approval 12/14/22 16:25:41.493
STEP: updating /approval 12/14/22 16:25:41.502
STEP: getting /status 12/14/22 16:25:41.51
STEP: patching /status 12/14/22 16:25:41.514
STEP: updating /status 12/14/22 16:25:41.523
STEP: deleting 12/14/22 16:25:41.537
STEP: deleting a collection 12/14/22 16:25:41.555
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:41.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9588" for this suite. 12/14/22 16:25:41.583
------------------------------
• [1.781 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:39.813
    Dec 14 16:25:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename certificates 12/14/22 16:25:39.817
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:39.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:39.858
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 12/14/22 16:25:41.418
    STEP: getting /apis/certificates.k8s.io 12/14/22 16:25:41.424
    STEP: getting /apis/certificates.k8s.io/v1 12/14/22 16:25:41.426
    STEP: creating 12/14/22 16:25:41.428
    STEP: getting 12/14/22 16:25:41.458
    STEP: listing 12/14/22 16:25:41.464
    STEP: watching 12/14/22 16:25:41.469
    Dec 14 16:25:41.469: INFO: starting watch
    STEP: patching 12/14/22 16:25:41.471
    STEP: updating 12/14/22 16:25:41.48
    Dec 14 16:25:41.489: INFO: waiting for watch events with expected annotations
    Dec 14 16:25:41.489: INFO: saw patched and updated annotations
    STEP: getting /approval 12/14/22 16:25:41.489
    STEP: patching /approval 12/14/22 16:25:41.493
    STEP: updating /approval 12/14/22 16:25:41.502
    STEP: getting /status 12/14/22 16:25:41.51
    STEP: patching /status 12/14/22 16:25:41.514
    STEP: updating /status 12/14/22 16:25:41.523
    STEP: deleting 12/14/22 16:25:41.537
    STEP: deleting a collection 12/14/22 16:25:41.555
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:41.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9588" for this suite. 12/14/22 16:25:41.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:41.597
Dec 14 16:25:41.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:25:41.6
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:41.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:41.629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:25:41.656
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:25:42.646
STEP: Deploying the webhook pod 12/14/22 16:25:42.664
STEP: Wait for the deployment to be ready 12/14/22 16:25:42.688
Dec 14 16:25:42.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 16:25:44.735
STEP: Verifying the service has paired with the endpoint 12/14/22 16:25:44.752
Dec 14 16:25:45.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 16:25:45.759
STEP: create a pod 12/14/22 16:25:45.795
Dec 14 16:25:45.804: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4162" to be "running"
Dec 14 16:25:45.820: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.479614ms
Dec 14 16:25:47.826: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022311614s
Dec 14 16:25:47.826: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 12/14/22 16:25:47.827
Dec 14 16:25:47.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=webhook-4162 attach --namespace=webhook-4162 to-be-attached-pod -i -c=container1'
Dec 14 16:25:48.120: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4162" for this suite. 12/14/22 16:25:48.229
STEP: Destroying namespace "webhook-4162-markers" for this suite. 12/14/22 16:25:48.241
------------------------------
• [SLOW TEST] [6.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:41.597
    Dec 14 16:25:41.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:25:41.6
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:41.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:41.629
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:25:41.656
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:25:42.646
    STEP: Deploying the webhook pod 12/14/22 16:25:42.664
    STEP: Wait for the deployment to be ready 12/14/22 16:25:42.688
    Dec 14 16:25:42.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 16:25:44.735
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:25:44.752
    Dec 14 16:25:45.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 16:25:45.759
    STEP: create a pod 12/14/22 16:25:45.795
    Dec 14 16:25:45.804: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4162" to be "running"
    Dec 14 16:25:45.820: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.479614ms
    Dec 14 16:25:47.826: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022311614s
    Dec 14 16:25:47.826: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 12/14/22 16:25:47.827
    Dec 14 16:25:47.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=webhook-4162 attach --namespace=webhook-4162 to-be-attached-pod -i -c=container1'
    Dec 14 16:25:48.120: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:48.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4162" for this suite. 12/14/22 16:25:48.229
    STEP: Destroying namespace "webhook-4162-markers" for this suite. 12/14/22 16:25:48.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:48.259
Dec 14 16:25:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:25:48.263
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:48.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:48.327
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-6b3c6bb0-a6e6-4457-adb4-7b1e6c7c8316 12/14/22 16:25:48.334
STEP: Creating a pod to test consume configMaps 12/14/22 16:25:48.349
Dec 14 16:25:48.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e" in namespace "projected-983" to be "Succeeded or Failed"
Dec 14 16:25:48.376: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.99567ms
Dec 14 16:25:50.387: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021769025s
Dec 14 16:25:52.388: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023283285s
STEP: Saw pod success 12/14/22 16:25:52.388
Dec 14 16:25:52.389: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e" satisfied condition "Succeeded or Failed"
Dec 14 16:25:52.395: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:25:52.41
Dec 14 16:25:52.432: INFO: Waiting for pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e to disappear
Dec 14 16:25:52.436: INFO: Pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-983" for this suite. 12/14/22 16:25:52.447
------------------------------
• [4.203 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:48.259
    Dec 14 16:25:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:25:48.263
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:48.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:48.327
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-6b3c6bb0-a6e6-4457-adb4-7b1e6c7c8316 12/14/22 16:25:48.334
    STEP: Creating a pod to test consume configMaps 12/14/22 16:25:48.349
    Dec 14 16:25:48.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e" in namespace "projected-983" to be "Succeeded or Failed"
    Dec 14 16:25:48.376: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.99567ms
    Dec 14 16:25:50.387: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021769025s
    Dec 14 16:25:52.388: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023283285s
    STEP: Saw pod success 12/14/22 16:25:52.388
    Dec 14 16:25:52.389: INFO: Pod "pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e" satisfied condition "Succeeded or Failed"
    Dec 14 16:25:52.395: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:25:52.41
    Dec 14 16:25:52.432: INFO: Waiting for pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e to disappear
    Dec 14 16:25:52.436: INFO: Pod pod-projected-configmaps-7816c466-14ba-47ca-b058-d03985a01e6e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-983" for this suite. 12/14/22 16:25:52.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:52.469
Dec 14 16:25:52.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:25:52.471
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:52.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:52.506
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-763d7e7a-1fad-42db-93b7-5c1363b48e56 12/14/22 16:25:52.51
STEP: Creating a pod to test consume secrets 12/14/22 16:25:52.519
Dec 14 16:25:52.537: INFO: Waiting up to 5m0s for pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e" in namespace "secrets-8267" to be "Succeeded or Failed"
Dec 14 16:25:52.544: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299595ms
Dec 14 16:25:54.559: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021295765s
Dec 14 16:25:56.560: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022198939s
STEP: Saw pod success 12/14/22 16:25:56.56
Dec 14 16:25:56.561: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e" satisfied condition "Succeeded or Failed"
Dec 14 16:25:56.566: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:25:56.577
Dec 14 16:25:56.599: INFO: Waiting for pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e to disappear
Dec 14 16:25:56.603: INFO: Pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:56.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8267" for this suite. 12/14/22 16:25:56.612
------------------------------
• [4.153 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:52.469
    Dec 14 16:25:52.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:25:52.471
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:52.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:52.506
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-763d7e7a-1fad-42db-93b7-5c1363b48e56 12/14/22 16:25:52.51
    STEP: Creating a pod to test consume secrets 12/14/22 16:25:52.519
    Dec 14 16:25:52.537: INFO: Waiting up to 5m0s for pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e" in namespace "secrets-8267" to be "Succeeded or Failed"
    Dec 14 16:25:52.544: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299595ms
    Dec 14 16:25:54.559: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021295765s
    Dec 14 16:25:56.560: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022198939s
    STEP: Saw pod success 12/14/22 16:25:56.56
    Dec 14 16:25:56.561: INFO: Pod "pod-secrets-27486972-95e4-46ec-b909-5b98334d056e" satisfied condition "Succeeded or Failed"
    Dec 14 16:25:56.566: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:25:56.577
    Dec 14 16:25:56.599: INFO: Waiting for pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e to disappear
    Dec 14 16:25:56.603: INFO: Pod pod-secrets-27486972-95e4-46ec-b909-5b98334d056e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:56.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8267" for this suite. 12/14/22 16:25:56.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:56.631
Dec 14 16:25:56.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 16:25:56.634
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:56.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:56.66
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 12/14/22 16:25:56.664
Dec 14 16:25:56.670: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/14/22 16:25:56.671
Dec 14 16:25:56.680: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/14/22 16:25:56.681
Dec 14 16:25:56.695: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:25:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7644" for this suite. 12/14/22 16:25:56.706
------------------------------
• [0.084 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:56.631
    Dec 14 16:25:56.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 16:25:56.634
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:56.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:56.66
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 12/14/22 16:25:56.664
    Dec 14 16:25:56.670: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/14/22 16:25:56.671
    Dec 14 16:25:56.680: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/14/22 16:25:56.681
    Dec 14 16:25:56.695: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:25:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7644" for this suite. 12/14/22 16:25:56.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:25:56.716
Dec 14 16:25:56.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:25:56.719
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:56.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:56.743
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 12/14/22 16:25:56.746
STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:25:56.754
STEP: Creating a ResourceQuota with not terminating scope 12/14/22 16:25:58.76
STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:25:58.769
STEP: Creating a long running pod 12/14/22 16:26:00.778
STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/14/22 16:26:00.802
STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/14/22 16:26:02.811
STEP: Deleting the pod 12/14/22 16:26:04.819
STEP: Ensuring resource quota status released the pod usage 12/14/22 16:26:04.839
STEP: Creating a terminating pod 12/14/22 16:26:06.847
STEP: Ensuring resource quota with terminating scope captures the pod usage 12/14/22 16:26:06.864
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/14/22 16:26:08.872
STEP: Deleting the pod 12/14/22 16:26:10.882
STEP: Ensuring resource quota status released the pod usage 12/14/22 16:26:10.899
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:12.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3538" for this suite. 12/14/22 16:26:12.916
------------------------------
• [SLOW TEST] [16.212 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:25:56.716
    Dec 14 16:25:56.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:25:56.719
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:25:56.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:25:56.743
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 12/14/22 16:25:56.746
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:25:56.754
    STEP: Creating a ResourceQuota with not terminating scope 12/14/22 16:25:58.76
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:25:58.769
    STEP: Creating a long running pod 12/14/22 16:26:00.778
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/14/22 16:26:00.802
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/14/22 16:26:02.811
    STEP: Deleting the pod 12/14/22 16:26:04.819
    STEP: Ensuring resource quota status released the pod usage 12/14/22 16:26:04.839
    STEP: Creating a terminating pod 12/14/22 16:26:06.847
    STEP: Ensuring resource quota with terminating scope captures the pod usage 12/14/22 16:26:06.864
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/14/22 16:26:08.872
    STEP: Deleting the pod 12/14/22 16:26:10.882
    STEP: Ensuring resource quota status released the pod usage 12/14/22 16:26:10.899
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:12.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3538" for this suite. 12/14/22 16:26:12.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:12.933
Dec 14 16:26:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:26:12.936
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:12.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:12.961
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Dec 14 16:26:12.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 create -f -'
Dec 14 16:26:14.508: INFO: stderr: ""
Dec 14 16:26:14.508: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 14 16:26:14.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 create -f -'
Dec 14 16:26:14.915: INFO: stderr: ""
Dec 14 16:26:14.915: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 16:26:14.915
Dec 14 16:26:15.922: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:26:15.922: INFO: Found 1 / 1
Dec 14 16:26:15.922: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 16:26:15.927: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:26:15.927: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:26:15.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe pod agnhost-primary-b44gg'
Dec 14 16:26:16.153: INFO: stderr: ""
Dec 14 16:26:16.153: INFO: stdout: "Name:             agnhost-primary-b44gg\nNamespace:        kubectl-9426\nPriority:         0\nService Account:  default\nNode:             iet9eich7uhu-3/192.168.121.56\nStart Time:       Wed, 14 Dec 2022 16:26:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.67.168\nIPs:\n  IP:           10.233.67.168\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://d48359c4ec12986e1b36a05f411456c6fdd4bee7f7e1e7952784892d09345303\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Dec 2022 16:26:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9fmqq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9fmqq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9426/agnhost-primary-b44gg to iet9eich7uhu-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 14 16:26:16.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe rc agnhost-primary'
Dec 14 16:26:16.325: INFO: stderr: ""
Dec 14 16:26:16.325: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9426\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b44gg\n"
Dec 14 16:26:16.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe service agnhost-primary'
Dec 14 16:26:16.501: INFO: stderr: ""
Dec 14 16:26:16.501: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9426\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.36.194\nIPs:               10.233.36.194\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.67.168:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 16:26:16.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe node iet9eich7uhu-1'
Dec 14 16:26:16.724: INFO: stderr: ""
Dec 14 16:26:16.724: INFO: stdout: "Name:               iet9eich7uhu-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=iet9eich7uhu-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"42:16:8f:90:1f:bb\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.21\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Dec 2022 15:26:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  iet9eich7uhu-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 14 Dec 2022 16:26:14 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 14 Dec 2022 15:35:41 +0000   Wed, 14 Dec 2022 15:35:41 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.21\n  Hostname:    iet9eich7uhu-1\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      122749536Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8140752Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1600m\n  ephemeral-storage:      119410748528\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3291088Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 9ae5fc025929460e8761b1d79bf7771a\n  System UUID:                9ae5fc02-5929-460e-8761-b1d79bf7771a\n  Boot ID:                    e5805889-e08d-45a0-8971-33101e743b7e\n  Kernel Version:             5.15.0-56-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.0\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-plh4q                                      100m (6%)     100m (6%)   50Mi (1%)        50Mi (1%)      50m\n  kube-system                 coredns-787d4945fb-dthzx                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     19m\n  kube-system                 kube-addon-manager-iet9eich7uhu-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         51m\n  kube-system                 kube-apiserver-iet9eich7uhu-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-controller-manager-iet9eich7uhu-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-proxy-6lpvc                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-scheduler-iet9eich7uhu-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         51m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    755m (47%)  100m (6%)\n  memory                 170Mi (5%)  220Mi (6%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 59m                kube-proxy       \n  Normal  Starting                 50m                kube-proxy       \n  Normal  NodeHasSufficientPID     60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 59m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             59m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  59m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                59m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  RegisteredNode           59m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  NodeReady                58m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  58m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 58m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             58m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  Starting                 56m                kubelet          Starting kubelet.\n  Normal  NodeReady                56m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  56m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             56m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeHasSufficientPID     56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  56m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 56m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeNotReady             56m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeReady                56m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  RegisteredNode           52m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  RegisteredNode           51m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  Starting                 51m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m (x8 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m (x8 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m (x7 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeNotReady             51m                node-controller  Node iet9eich7uhu-1 status is now: NodeNotReady\n"
Dec 14 16:26:16.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe namespace kubectl-9426'
Dec 14 16:26:16.860: INFO: stderr: ""
Dec 14 16:26:16.860: INFO: stdout: "Name:         kubectl-9426\nLabels:       e2e-framework=kubectl\n              e2e-run=9a41bc61-18a6-47a0-82d3-e8ecc018d4c2\n              kubernetes.io/metadata.name=kubectl-9426\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9426" for this suite. 12/14/22 16:26:16.869
------------------------------
• [3.950 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:12.933
    Dec 14 16:26:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:26:12.936
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:12.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:12.961
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Dec 14 16:26:12.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 create -f -'
    Dec 14 16:26:14.508: INFO: stderr: ""
    Dec 14 16:26:14.508: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Dec 14 16:26:14.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 create -f -'
    Dec 14 16:26:14.915: INFO: stderr: ""
    Dec 14 16:26:14.915: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 16:26:14.915
    Dec 14 16:26:15.922: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:26:15.922: INFO: Found 1 / 1
    Dec 14 16:26:15.922: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 14 16:26:15.927: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:26:15.927: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 16:26:15.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe pod agnhost-primary-b44gg'
    Dec 14 16:26:16.153: INFO: stderr: ""
    Dec 14 16:26:16.153: INFO: stdout: "Name:             agnhost-primary-b44gg\nNamespace:        kubectl-9426\nPriority:         0\nService Account:  default\nNode:             iet9eich7uhu-3/192.168.121.56\nStart Time:       Wed, 14 Dec 2022 16:26:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.67.168\nIPs:\n  IP:           10.233.67.168\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://d48359c4ec12986e1b36a05f411456c6fdd4bee7f7e1e7952784892d09345303\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Dec 2022 16:26:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9fmqq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9fmqq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9426/agnhost-primary-b44gg to iet9eich7uhu-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Dec 14 16:26:16.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe rc agnhost-primary'
    Dec 14 16:26:16.325: INFO: stderr: ""
    Dec 14 16:26:16.325: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9426\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b44gg\n"
    Dec 14 16:26:16.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe service agnhost-primary'
    Dec 14 16:26:16.501: INFO: stderr: ""
    Dec 14 16:26:16.501: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9426\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.36.194\nIPs:               10.233.36.194\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.67.168:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Dec 14 16:26:16.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe node iet9eich7uhu-1'
    Dec 14 16:26:16.724: INFO: stderr: ""
    Dec 14 16:26:16.724: INFO: stdout: "Name:               iet9eich7uhu-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=iet9eich7uhu-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"42:16:8f:90:1f:bb\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.21\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Dec 2022 15:26:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  iet9eich7uhu-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 14 Dec 2022 16:26:14 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 14 Dec 2022 15:35:41 +0000   Wed, 14 Dec 2022 15:35:41 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 14 Dec 2022 16:24:45 +0000   Wed, 14 Dec 2022 15:35:12 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.21\n  Hostname:    iet9eich7uhu-1\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      122749536Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8140752Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1600m\n  ephemeral-storage:      119410748528\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3291088Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 9ae5fc025929460e8761b1d79bf7771a\n  System UUID:                9ae5fc02-5929-460e-8761-b1d79bf7771a\n  Boot ID:                    e5805889-e08d-45a0-8971-33101e743b7e\n  Kernel Version:             5.15.0-56-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.0\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-plh4q                                      100m (6%)     100m (6%)   50Mi (1%)        50Mi (1%)      50m\n  kube-system                 coredns-787d4945fb-dthzx                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     19m\n  kube-system                 kube-addon-manager-iet9eich7uhu-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         51m\n  kube-system                 kube-apiserver-iet9eich7uhu-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-controller-manager-iet9eich7uhu-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-proxy-6lpvc                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-scheduler-iet9eich7uhu-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         51m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    755m (47%)  100m (6%)\n  memory                 170Mi (5%)  220Mi (6%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 59m                kube-proxy       \n  Normal  Starting                 50m                kube-proxy       \n  Normal  NodeHasSufficientPID     60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    60m (x6 over 60m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 59m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     59m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             59m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  59m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                59m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  RegisteredNode           59m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  NodeReady                58m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  58m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 58m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     58m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             58m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  Starting                 56m                kubelet          Starting kubelet.\n  Normal  NodeReady                56m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  56m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             56m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeHasSufficientPID     56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  56m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 56m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m                kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeNotReady             56m                kubelet          Node iet9eich7uhu-1 status is now: NodeNotReady\n  Normal  NodeReady                56m                kubelet          Node iet9eich7uhu-1 status is now: NodeReady\n  Normal  RegisteredNode           52m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  RegisteredNode           51m                node-controller  Node iet9eich7uhu-1 event: Registered Node iet9eich7uhu-1 in Controller\n  Normal  Starting                 51m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m (x8 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m (x8 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m (x7 over 51m)  kubelet          Node iet9eich7uhu-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeNotReady             51m                node-controller  Node iet9eich7uhu-1 status is now: NodeNotReady\n"
    Dec 14 16:26:16.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9426 describe namespace kubectl-9426'
    Dec 14 16:26:16.860: INFO: stderr: ""
    Dec 14 16:26:16.860: INFO: stdout: "Name:         kubectl-9426\nLabels:       e2e-framework=kubectl\n              e2e-run=9a41bc61-18a6-47a0-82d3-e8ecc018d4c2\n              kubernetes.io/metadata.name=kubectl-9426\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9426" for this suite. 12/14/22 16:26:16.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:16.889
Dec 14 16:26:16.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:26:16.892
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:16.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:16.925
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Dec 14 16:26:16.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6511 version'
Dec 14 16:26:17.029: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec 14 16:26:17.029: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6511" for this suite. 12/14/22 16:26:17.041
------------------------------
• [0.162 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:16.889
    Dec 14 16:26:16.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:26:16.892
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:16.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:16.925
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Dec 14 16:26:16.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6511 version'
    Dec 14 16:26:17.029: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Dec 14 16:26:17.029: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6511" for this suite. 12/14/22 16:26:17.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:17.052
Dec 14 16:26:17.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:26:17.055
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:17.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:17.086
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:26:17.098
Dec 14 16:26:17.114: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3663" to be "running and ready"
Dec 14 16:26:17.120: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016967ms
Dec 14 16:26:17.121: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:26:19.131: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016761943s
Dec 14 16:26:19.131: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 16:26:19.132: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 12/14/22 16:26:19.137
Dec 14 16:26:19.147: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3663" to be "running and ready"
Dec 14 16:26:19.161: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220397ms
Dec 14 16:26:19.161: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:26:21.171: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023230333s
Dec 14 16:26:21.171: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Dec 14 16:26:21.171: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/14/22 16:26:21.179
Dec 14 16:26:21.188: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:26:21.195: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 16:26:23.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:26:23.204: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 16:26:25.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:26:25.209: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 12/14/22 16:26:25.209
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:25.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3663" for this suite. 12/14/22 16:26:25.277
------------------------------
• [SLOW TEST] [8.251 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:17.052
    Dec 14 16:26:17.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:26:17.055
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:17.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:17.086
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:26:17.098
    Dec 14 16:26:17.114: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3663" to be "running and ready"
    Dec 14 16:26:17.120: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016967ms
    Dec 14 16:26:17.121: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:26:19.131: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016761943s
    Dec 14 16:26:19.131: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 16:26:19.132: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 12/14/22 16:26:19.137
    Dec 14 16:26:19.147: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3663" to be "running and ready"
    Dec 14 16:26:19.161: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220397ms
    Dec 14 16:26:19.161: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:26:21.171: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023230333s
    Dec 14 16:26:21.171: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Dec 14 16:26:21.171: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/14/22 16:26:21.179
    Dec 14 16:26:21.188: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 16:26:21.195: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 14 16:26:23.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 16:26:23.204: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 14 16:26:25.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 14 16:26:25.209: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 12/14/22 16:26:25.209
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:25.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3663" for this suite. 12/14/22 16:26:25.277
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:25.304
Dec 14 16:26:25.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:26:25.307
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:25.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:25.336
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 12/14/22 16:26:25.339
Dec 14 16:26:25.361: INFO: Waiting up to 5m0s for pod "pod-8lzk6" in namespace "pods-5494" to be "running"
Dec 14 16:26:25.376: INFO: Pod "pod-8lzk6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.590072ms
Dec 14 16:26:27.385: INFO: Pod "pod-8lzk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.024057983s
Dec 14 16:26:27.385: INFO: Pod "pod-8lzk6" satisfied condition "running"
STEP: patching /status 12/14/22 16:26:27.385
Dec 14 16:26:27.399: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:27.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5494" for this suite. 12/14/22 16:26:27.407
------------------------------
• [2.114 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:25.304
    Dec 14 16:26:25.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:26:25.307
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:25.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:25.336
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 12/14/22 16:26:25.339
    Dec 14 16:26:25.361: INFO: Waiting up to 5m0s for pod "pod-8lzk6" in namespace "pods-5494" to be "running"
    Dec 14 16:26:25.376: INFO: Pod "pod-8lzk6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.590072ms
    Dec 14 16:26:27.385: INFO: Pod "pod-8lzk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.024057983s
    Dec 14 16:26:27.385: INFO: Pod "pod-8lzk6" satisfied condition "running"
    STEP: patching /status 12/14/22 16:26:27.385
    Dec 14 16:26:27.399: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:27.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5494" for this suite. 12/14/22 16:26:27.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:27.422
Dec 14 16:26:27.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:26:27.424
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:27.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:27.449
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-190/configmap-test-79579d6e-3f6b-4739-a9a9-68215eeaea47 12/14/22 16:26:27.455
STEP: Creating a pod to test consume configMaps 12/14/22 16:26:27.464
Dec 14 16:26:27.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee" in namespace "configmap-190" to be "Succeeded or Failed"
Dec 14 16:26:27.485: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.912922ms
Dec 14 16:26:29.497: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018744686s
Dec 14 16:26:31.496: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01745099s
STEP: Saw pod success 12/14/22 16:26:31.496
Dec 14 16:26:31.498: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee" satisfied condition "Succeeded or Failed"
Dec 14 16:26:31.506: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee container env-test: <nil>
STEP: delete the pod 12/14/22 16:26:31.525
Dec 14 16:26:31.542: INFO: Waiting for pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee to disappear
Dec 14 16:26:31.546: INFO: Pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-190" for this suite. 12/14/22 16:26:31.554
------------------------------
• [4.143 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:27.422
    Dec 14 16:26:27.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:26:27.424
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:27.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:27.449
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-190/configmap-test-79579d6e-3f6b-4739-a9a9-68215eeaea47 12/14/22 16:26:27.455
    STEP: Creating a pod to test consume configMaps 12/14/22 16:26:27.464
    Dec 14 16:26:27.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee" in namespace "configmap-190" to be "Succeeded or Failed"
    Dec 14 16:26:27.485: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.912922ms
    Dec 14 16:26:29.497: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018744686s
    Dec 14 16:26:31.496: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01745099s
    STEP: Saw pod success 12/14/22 16:26:31.496
    Dec 14 16:26:31.498: INFO: Pod "pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee" satisfied condition "Succeeded or Failed"
    Dec 14 16:26:31.506: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee container env-test: <nil>
    STEP: delete the pod 12/14/22 16:26:31.525
    Dec 14 16:26:31.542: INFO: Waiting for pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee to disappear
    Dec 14 16:26:31.546: INFO: Pod pod-configmaps-7a5424ad-3f43-4126-ad19-c3406de895ee no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-190" for this suite. 12/14/22 16:26:31.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:31.574
Dec 14 16:26:31.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:26:31.578
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:31.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:31.61
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:26:31.615
Dec 14 16:26:31.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8" in namespace "downward-api-7111" to be "Succeeded or Failed"
Dec 14 16:26:31.636: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828378ms
Dec 14 16:26:33.642: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010953328s
Dec 14 16:26:35.646: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Running", Reason="", readiness=false. Elapsed: 4.0143385s
Dec 14 16:26:37.645: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014031545s
STEP: Saw pod success 12/14/22 16:26:37.645
Dec 14 16:26:37.646: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8" satisfied condition "Succeeded or Failed"
Dec 14 16:26:37.660: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 container client-container: <nil>
STEP: delete the pod 12/14/22 16:26:37.67
Dec 14 16:26:37.686: INFO: Waiting for pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 to disappear
Dec 14 16:26:37.690: INFO: Pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:37.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7111" for this suite. 12/14/22 16:26:37.698
------------------------------
• [SLOW TEST] [6.137 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:31.574
    Dec 14 16:26:31.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:26:31.578
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:31.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:31.61
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:26:31.615
    Dec 14 16:26:31.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8" in namespace "downward-api-7111" to be "Succeeded or Failed"
    Dec 14 16:26:31.636: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828378ms
    Dec 14 16:26:33.642: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010953328s
    Dec 14 16:26:35.646: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Running", Reason="", readiness=false. Elapsed: 4.0143385s
    Dec 14 16:26:37.645: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014031545s
    STEP: Saw pod success 12/14/22 16:26:37.645
    Dec 14 16:26:37.646: INFO: Pod "downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8" satisfied condition "Succeeded or Failed"
    Dec 14 16:26:37.660: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:26:37.67
    Dec 14 16:26:37.686: INFO: Waiting for pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 to disappear
    Dec 14 16:26:37.690: INFO: Pod downwardapi-volume-f5ed14d3-f774-4b65-878f-3d3b559cdbe8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:37.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7111" for this suite. 12/14/22 16:26:37.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:37.713
Dec 14 16:26:37.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:26:37.717
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:37.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:37.751
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-269c6a5f-20b0-4aff-81e6-19a6646f7f22 12/14/22 16:26:37.758
STEP: Creating a pod to test consume configMaps 12/14/22 16:26:37.766
Dec 14 16:26:37.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2" in namespace "configmap-4990" to be "Succeeded or Failed"
Dec 14 16:26:37.801: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.882877ms
Dec 14 16:26:39.808: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Running", Reason="", readiness=false. Elapsed: 2.016720187s
Dec 14 16:26:41.809: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017473452s
STEP: Saw pod success 12/14/22 16:26:41.809
Dec 14 16:26:41.809: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2" satisfied condition "Succeeded or Failed"
Dec 14 16:26:41.813: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:26:41.929
Dec 14 16:26:41.955: INFO: Waiting for pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 to disappear
Dec 14 16:26:41.960: INFO: Pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:41.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4990" for this suite. 12/14/22 16:26:41.971
------------------------------
• [4.269 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:37.713
    Dec 14 16:26:37.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:26:37.717
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:37.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:37.751
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-269c6a5f-20b0-4aff-81e6-19a6646f7f22 12/14/22 16:26:37.758
    STEP: Creating a pod to test consume configMaps 12/14/22 16:26:37.766
    Dec 14 16:26:37.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2" in namespace "configmap-4990" to be "Succeeded or Failed"
    Dec 14 16:26:37.801: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.882877ms
    Dec 14 16:26:39.808: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Running", Reason="", readiness=false. Elapsed: 2.016720187s
    Dec 14 16:26:41.809: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017473452s
    STEP: Saw pod success 12/14/22 16:26:41.809
    Dec 14 16:26:41.809: INFO: Pod "pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2" satisfied condition "Succeeded or Failed"
    Dec 14 16:26:41.813: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:26:41.929
    Dec 14 16:26:41.955: INFO: Waiting for pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 to disappear
    Dec 14 16:26:41.960: INFO: Pod pod-configmaps-7c02474e-5e35-42ea-bff6-5405ff9e1ed2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:41.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4990" for this suite. 12/14/22 16:26:41.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:41.986
Dec 14 16:26:41.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename controllerrevisions 12/14/22 16:26:41.988
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:42.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:42.012
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-d8qkk-daemon-set" 12/14/22 16:26:42.043
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:26:42.052
Dec 14 16:26:42.066: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 0
Dec 14 16:26:42.066: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:26:43.079: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 2
Dec 14 16:26:43.079: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:26:44.084: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 3
Dec 14 16:26:44.084: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d8qkk-daemon-set
STEP: Confirm DaemonSet "e2e-d8qkk-daemon-set" successfully created with "daemonset-name=e2e-d8qkk-daemon-set" label 12/14/22 16:26:44.088
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d8qkk-daemon-set" 12/14/22 16:26:44.1
Dec 14 16:26:44.106: INFO: Located ControllerRevision: "e2e-d8qkk-daemon-set-6c5b4896b6"
STEP: Patching ControllerRevision "e2e-d8qkk-daemon-set-6c5b4896b6" 12/14/22 16:26:44.11
Dec 14 16:26:44.122: INFO: e2e-d8qkk-daemon-set-6c5b4896b6 has been patched
STEP: Create a new ControllerRevision 12/14/22 16:26:44.123
Dec 14 16:26:44.134: INFO: Created ControllerRevision: e2e-d8qkk-daemon-set-7f8cdf874d
STEP: Confirm that there are two ControllerRevisions 12/14/22 16:26:44.134
Dec 14 16:26:44.134: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 16:26:44.145: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-d8qkk-daemon-set-6c5b4896b6" 12/14/22 16:26:44.145
STEP: Confirm that there is only one ControllerRevision 12/14/22 16:26:44.156
Dec 14 16:26:44.156: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 16:26:44.160: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-d8qkk-daemon-set-7f8cdf874d" 12/14/22 16:26:44.164
Dec 14 16:26:44.178: INFO: e2e-d8qkk-daemon-set-7f8cdf874d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/14/22 16:26:44.178
W1214 16:26:44.196191      14 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/14/22 16:26:44.196
Dec 14 16:26:44.196: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 16:26:45.206: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 16:26:45.214: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d8qkk-daemon-set-7f8cdf874d=updated" 12/14/22 16:26:45.214
STEP: Confirm that there is only one ControllerRevision 12/14/22 16:26:45.226
Dec 14 16:26:45.226: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 14 16:26:45.232: INFO: Found 1 ControllerRevisions
Dec 14 16:26:45.236: INFO: ControllerRevision "e2e-d8qkk-daemon-set-7d454b4d49" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-d8qkk-daemon-set" 12/14/22 16:26:45.242
STEP: deleting DaemonSet.extensions e2e-d8qkk-daemon-set in namespace controllerrevisions-7111, will wait for the garbage collector to delete the pods 12/14/22 16:26:45.242
Dec 14 16:26:45.347: INFO: Deleting DaemonSet.extensions e2e-d8qkk-daemon-set took: 50.850631ms
Dec 14 16:26:45.448: INFO: Terminating DaemonSet.extensions e2e-d8qkk-daemon-set pods took: 100.29975ms
Dec 14 16:26:46.256: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 0
Dec 14 16:26:46.256: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d8qkk-daemon-set
Dec 14 16:26:46.260: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16556"},"items":null}

Dec 14 16:26:46.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16556"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:46.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7111" for this suite. 12/14/22 16:26:46.289
------------------------------
• [4.313 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:41.986
    Dec 14 16:26:41.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename controllerrevisions 12/14/22 16:26:41.988
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:42.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:42.012
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-d8qkk-daemon-set" 12/14/22 16:26:42.043
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:26:42.052
    Dec 14 16:26:42.066: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 0
    Dec 14 16:26:42.066: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:26:43.079: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 2
    Dec 14 16:26:43.079: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:26:44.084: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 3
    Dec 14 16:26:44.084: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d8qkk-daemon-set
    STEP: Confirm DaemonSet "e2e-d8qkk-daemon-set" successfully created with "daemonset-name=e2e-d8qkk-daemon-set" label 12/14/22 16:26:44.088
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d8qkk-daemon-set" 12/14/22 16:26:44.1
    Dec 14 16:26:44.106: INFO: Located ControllerRevision: "e2e-d8qkk-daemon-set-6c5b4896b6"
    STEP: Patching ControllerRevision "e2e-d8qkk-daemon-set-6c5b4896b6" 12/14/22 16:26:44.11
    Dec 14 16:26:44.122: INFO: e2e-d8qkk-daemon-set-6c5b4896b6 has been patched
    STEP: Create a new ControllerRevision 12/14/22 16:26:44.123
    Dec 14 16:26:44.134: INFO: Created ControllerRevision: e2e-d8qkk-daemon-set-7f8cdf874d
    STEP: Confirm that there are two ControllerRevisions 12/14/22 16:26:44.134
    Dec 14 16:26:44.134: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 16:26:44.145: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-d8qkk-daemon-set-6c5b4896b6" 12/14/22 16:26:44.145
    STEP: Confirm that there is only one ControllerRevision 12/14/22 16:26:44.156
    Dec 14 16:26:44.156: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 16:26:44.160: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-d8qkk-daemon-set-7f8cdf874d" 12/14/22 16:26:44.164
    Dec 14 16:26:44.178: INFO: e2e-d8qkk-daemon-set-7f8cdf874d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/14/22 16:26:44.178
    W1214 16:26:44.196191      14 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/14/22 16:26:44.196
    Dec 14 16:26:44.196: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 16:26:45.206: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 16:26:45.214: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d8qkk-daemon-set-7f8cdf874d=updated" 12/14/22 16:26:45.214
    STEP: Confirm that there is only one ControllerRevision 12/14/22 16:26:45.226
    Dec 14 16:26:45.226: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 14 16:26:45.232: INFO: Found 1 ControllerRevisions
    Dec 14 16:26:45.236: INFO: ControllerRevision "e2e-d8qkk-daemon-set-7d454b4d49" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-d8qkk-daemon-set" 12/14/22 16:26:45.242
    STEP: deleting DaemonSet.extensions e2e-d8qkk-daemon-set in namespace controllerrevisions-7111, will wait for the garbage collector to delete the pods 12/14/22 16:26:45.242
    Dec 14 16:26:45.347: INFO: Deleting DaemonSet.extensions e2e-d8qkk-daemon-set took: 50.850631ms
    Dec 14 16:26:45.448: INFO: Terminating DaemonSet.extensions e2e-d8qkk-daemon-set pods took: 100.29975ms
    Dec 14 16:26:46.256: INFO: Number of nodes with available pods controlled by daemonset e2e-d8qkk-daemon-set: 0
    Dec 14 16:26:46.256: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d8qkk-daemon-set
    Dec 14 16:26:46.260: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16556"},"items":null}

    Dec 14 16:26:46.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16556"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:46.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7111" for this suite. 12/14/22 16:26:46.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:46.303
Dec 14 16:26:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 16:26:46.305
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:46.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:46.342
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Dec 14 16:26:46.377: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:26:46.385
Dec 14 16:26:46.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:26:46.400: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:26:47.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:26:47.417: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:26:48.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:26:48.418: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 12/14/22 16:26:48.44
STEP: Check that daemon pods images are updated. 12/14/22 16:26:48.454
Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-gg5fq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:49.474: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:49.474: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:50.475: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:50.475: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:51.473: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:51.473: INFO: Pod daemon-set-r2lbx is not available
Dec 14 16:26:51.473: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:52.477: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:53.473: INFO: Pod daemon-set-9p2zc is not available
Dec 14 16:26:53.474: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec 14 16:26:55.473: INFO: Pod daemon-set-rpbrx is not available
STEP: Check that daemon pods are still running on every node of the cluster. 12/14/22 16:26:55.482
Dec 14 16:26:55.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:26:55.498: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 16:26:56.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:26:56.517: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:26:56.547
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6632, will wait for the garbage collector to delete the pods 12/14/22 16:26:56.548
Dec 14 16:26:56.618: INFO: Deleting DaemonSet.extensions daemon-set took: 13.095099ms
Dec 14 16:26:56.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.679619ms
Dec 14 16:26:59.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:26:59.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:26:59.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16725"},"items":null}

Dec 14 16:26:59.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16725"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:59.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6632" for this suite. 12/14/22 16:26:59.184
------------------------------
• [SLOW TEST] [12.892 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:46.303
    Dec 14 16:26:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 16:26:46.305
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:46.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:46.342
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Dec 14 16:26:46.377: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:26:46.385
    Dec 14 16:26:46.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:26:46.400: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:26:47.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:26:47.417: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:26:48.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:26:48.418: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 12/14/22 16:26:48.44
    STEP: Check that daemon pods images are updated. 12/14/22 16:26:48.454
    Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-gg5fq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:48.460: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:49.474: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:49.474: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:50.475: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:50.475: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:51.473: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:51.473: INFO: Pod daemon-set-r2lbx is not available
    Dec 14 16:26:51.473: INFO: Wrong image for pod: daemon-set-tt4k6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:52.477: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:53.473: INFO: Pod daemon-set-9p2zc is not available
    Dec 14 16:26:53.474: INFO: Wrong image for pod: daemon-set-bfqtl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec 14 16:26:55.473: INFO: Pod daemon-set-rpbrx is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 12/14/22 16:26:55.482
    Dec 14 16:26:55.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:26:55.498: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 16:26:56.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:26:56.517: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:26:56.547
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6632, will wait for the garbage collector to delete the pods 12/14/22 16:26:56.548
    Dec 14 16:26:56.618: INFO: Deleting DaemonSet.extensions daemon-set took: 13.095099ms
    Dec 14 16:26:56.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.679619ms
    Dec 14 16:26:59.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:26:59.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 16:26:59.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16725"},"items":null}

    Dec 14 16:26:59.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16725"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:59.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6632" for this suite. 12/14/22 16:26:59.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:59.202
Dec 14 16:26:59.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:26:59.204
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:59.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:59.228
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 12/14/22 16:26:59.231
STEP: getting /apis/node.k8s.io 12/14/22 16:26:59.238
STEP: getting /apis/node.k8s.io/v1 12/14/22 16:26:59.239
STEP: creating 12/14/22 16:26:59.241
STEP: watching 12/14/22 16:26:59.268
Dec 14 16:26:59.269: INFO: starting watch
STEP: getting 12/14/22 16:26:59.277
STEP: listing 12/14/22 16:26:59.286
STEP: patching 12/14/22 16:26:59.29
STEP: updating 12/14/22 16:26:59.3
Dec 14 16:26:59.308: INFO: waiting for watch events with expected annotations
STEP: deleting 12/14/22 16:26:59.308
STEP: deleting a collection 12/14/22 16:26:59.325
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 14 16:26:59.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7097" for this suite. 12/14/22 16:26:59.35
------------------------------
• [0.160 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:59.202
    Dec 14 16:26:59.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:26:59.204
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:59.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:59.228
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 12/14/22 16:26:59.231
    STEP: getting /apis/node.k8s.io 12/14/22 16:26:59.238
    STEP: getting /apis/node.k8s.io/v1 12/14/22 16:26:59.239
    STEP: creating 12/14/22 16:26:59.241
    STEP: watching 12/14/22 16:26:59.268
    Dec 14 16:26:59.269: INFO: starting watch
    STEP: getting 12/14/22 16:26:59.277
    STEP: listing 12/14/22 16:26:59.286
    STEP: patching 12/14/22 16:26:59.29
    STEP: updating 12/14/22 16:26:59.3
    Dec 14 16:26:59.308: INFO: waiting for watch events with expected annotations
    STEP: deleting 12/14/22 16:26:59.308
    STEP: deleting a collection 12/14/22 16:26:59.325
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:26:59.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7097" for this suite. 12/14/22 16:26:59.35
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:26:59.363
Dec 14 16:26:59.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:26:59.365
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:59.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:59.396
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 12/14/22 16:26:59.399
Dec 14 16:26:59.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: mark a version not serverd 12/14/22 16:27:04.608
STEP: check the unserved version gets removed 12/14/22 16:27:04.637
STEP: check the other version is not changed 12/14/22 16:27:06.638
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2101" for this suite. 12/14/22 16:27:10.505
------------------------------
• [SLOW TEST] [11.153 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:26:59.363
    Dec 14 16:26:59.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:26:59.365
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:26:59.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:26:59.396
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 12/14/22 16:26:59.399
    Dec 14 16:26:59.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: mark a version not serverd 12/14/22 16:27:04.608
    STEP: check the unserved version gets removed 12/14/22 16:27:04.637
    STEP: check the other version is not changed 12/14/22 16:27:06.638
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2101" for this suite. 12/14/22 16:27:10.505
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:10.519
Dec 14 16:27:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:27:10.526
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:10.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:10.556
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 12/14/22 16:27:10.561
STEP: listing secrets in all namespaces to ensure that there are more than zero 12/14/22 16:27:10.596
STEP: patching the secret 12/14/22 16:27:10.601
STEP: deleting the secret using a LabelSelector 12/14/22 16:27:10.611
STEP: listing secrets in all namespaces, searching for label name and value in patch 12/14/22 16:27:10.622
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:10.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8874" for this suite. 12/14/22 16:27:10.634
------------------------------
• [0.123 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:10.519
    Dec 14 16:27:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:27:10.526
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:10.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:10.556
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 12/14/22 16:27:10.561
    STEP: listing secrets in all namespaces to ensure that there are more than zero 12/14/22 16:27:10.596
    STEP: patching the secret 12/14/22 16:27:10.601
    STEP: deleting the secret using a LabelSelector 12/14/22 16:27:10.611
    STEP: listing secrets in all namespaces, searching for label name and value in patch 12/14/22 16:27:10.622
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:10.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8874" for this suite. 12/14/22 16:27:10.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:10.646
Dec 14 16:27:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:27:10.649
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:10.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:10.683
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-9533caa2-c4ea-4ba8-87a8-d69a342d26a5 12/14/22 16:27:10.687
STEP: Creating a pod to test consume secrets 12/14/22 16:27:10.694
Dec 14 16:27:10.708: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf" in namespace "projected-7383" to be "Succeeded or Failed"
Dec 14 16:27:10.712: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281949ms
Dec 14 16:27:12.722: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014241413s
Dec 14 16:27:14.723: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014789421s
STEP: Saw pod success 12/14/22 16:27:14.723
Dec 14 16:27:14.724: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf" satisfied condition "Succeeded or Failed"
Dec 14 16:27:14.728: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:27:14.751
Dec 14 16:27:14.772: INFO: Waiting for pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf to disappear
Dec 14 16:27:14.774: INFO: Pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:14.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7383" for this suite. 12/14/22 16:27:14.781
------------------------------
• [4.145 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:10.646
    Dec 14 16:27:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:27:10.649
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:10.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:10.683
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-9533caa2-c4ea-4ba8-87a8-d69a342d26a5 12/14/22 16:27:10.687
    STEP: Creating a pod to test consume secrets 12/14/22 16:27:10.694
    Dec 14 16:27:10.708: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf" in namespace "projected-7383" to be "Succeeded or Failed"
    Dec 14 16:27:10.712: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281949ms
    Dec 14 16:27:12.722: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014241413s
    Dec 14 16:27:14.723: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014789421s
    STEP: Saw pod success 12/14/22 16:27:14.723
    Dec 14 16:27:14.724: INFO: Pod "pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf" satisfied condition "Succeeded or Failed"
    Dec 14 16:27:14.728: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:27:14.751
    Dec 14 16:27:14.772: INFO: Waiting for pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf to disappear
    Dec 14 16:27:14.774: INFO: Pod pod-projected-secrets-d665d136-78cd-4c26-99e3-cd1f258970bf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:14.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7383" for this suite. 12/14/22 16:27:14.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:14.792
Dec 14 16:27:14.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename watch 12/14/22 16:27:14.794
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:14.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:14.822
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 12/14/22 16:27:14.828
STEP: modifying the configmap once 12/14/22 16:27:14.834
STEP: modifying the configmap a second time 12/14/22 16:27:14.847
STEP: deleting the configmap 12/14/22 16:27:14.857
STEP: creating a watch on configmaps from the resource version returned by the first update 12/14/22 16:27:14.866
STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/14/22 16:27:14.871
Dec 14 16:27:14.872: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321  e18bc1c5-1297-4040-ac6b-29a8c00c80cf 16859 0 2022-12-14 16:27:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 16:27:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:27:14.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321  e18bc1c5-1297-4040-ac6b-29a8c00c80cf 16860 0 2022-12-14 16:27:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 16:27:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:14.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7321" for this suite. 12/14/22 16:27:14.881
------------------------------
• [0.102 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:14.792
    Dec 14 16:27:14.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename watch 12/14/22 16:27:14.794
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:14.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:14.822
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 12/14/22 16:27:14.828
    STEP: modifying the configmap once 12/14/22 16:27:14.834
    STEP: modifying the configmap a second time 12/14/22 16:27:14.847
    STEP: deleting the configmap 12/14/22 16:27:14.857
    STEP: creating a watch on configmaps from the resource version returned by the first update 12/14/22 16:27:14.866
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/14/22 16:27:14.871
    Dec 14 16:27:14.872: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321  e18bc1c5-1297-4040-ac6b-29a8c00c80cf 16859 0 2022-12-14 16:27:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 16:27:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:27:14.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7321  e18bc1c5-1297-4040-ac6b-29a8c00c80cf 16860 0 2022-12-14 16:27:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-14 16:27:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:14.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7321" for this suite. 12/14/22 16:27:14.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:14.897
Dec 14 16:27:14.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:27:14.9
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:14.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:14.929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 12/14/22 16:27:14.933
Dec 14 16:27:14.933: INFO: namespace kubectl-5701
Dec 14 16:27:14.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 create -f -'
Dec 14 16:27:15.982: INFO: stderr: ""
Dec 14 16:27:15.982: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 16:27:15.982
Dec 14 16:27:16.992: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:27:16.992: INFO: Found 0 / 1
Dec 14 16:27:17.993: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:27:17.993: INFO: Found 1 / 1
Dec 14 16:27:17.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 16:27:17.999: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:27:17.999: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:27:17.999: INFO: wait on agnhost-primary startup in kubectl-5701 
Dec 14 16:27:17.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 logs agnhost-primary-tr2t2 agnhost-primary'
Dec 14 16:27:18.195: INFO: stderr: ""
Dec 14 16:27:18.195: INFO: stdout: "Paused\n"
STEP: exposing RC 12/14/22 16:27:18.195
Dec 14 16:27:18.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 14 16:27:18.452: INFO: stderr: ""
Dec 14 16:27:18.452: INFO: stdout: "service/rm2 exposed\n"
Dec 14 16:27:18.462: INFO: Service rm2 in namespace kubectl-5701 found.
STEP: exposing service 12/14/22 16:27:20.474
Dec 14 16:27:20.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 14 16:27:20.617: INFO: stderr: ""
Dec 14 16:27:20.617: INFO: stdout: "service/rm3 exposed\n"
Dec 14 16:27:20.627: INFO: Service rm3 in namespace kubectl-5701 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5701" for this suite. 12/14/22 16:27:22.649
------------------------------
• [SLOW TEST] [7.760 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:14.897
    Dec 14 16:27:14.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:27:14.9
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:14.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:14.929
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 12/14/22 16:27:14.933
    Dec 14 16:27:14.933: INFO: namespace kubectl-5701
    Dec 14 16:27:14.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 create -f -'
    Dec 14 16:27:15.982: INFO: stderr: ""
    Dec 14 16:27:15.982: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 16:27:15.982
    Dec 14 16:27:16.992: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:27:16.992: INFO: Found 0 / 1
    Dec 14 16:27:17.993: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:27:17.993: INFO: Found 1 / 1
    Dec 14 16:27:17.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 14 16:27:17.999: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:27:17.999: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 16:27:17.999: INFO: wait on agnhost-primary startup in kubectl-5701 
    Dec 14 16:27:17.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 logs agnhost-primary-tr2t2 agnhost-primary'
    Dec 14 16:27:18.195: INFO: stderr: ""
    Dec 14 16:27:18.195: INFO: stdout: "Paused\n"
    STEP: exposing RC 12/14/22 16:27:18.195
    Dec 14 16:27:18.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Dec 14 16:27:18.452: INFO: stderr: ""
    Dec 14 16:27:18.452: INFO: stdout: "service/rm2 exposed\n"
    Dec 14 16:27:18.462: INFO: Service rm2 in namespace kubectl-5701 found.
    STEP: exposing service 12/14/22 16:27:20.474
    Dec 14 16:27:20.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5701 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Dec 14 16:27:20.617: INFO: stderr: ""
    Dec 14 16:27:20.617: INFO: stdout: "service/rm3 exposed\n"
    Dec 14 16:27:20.627: INFO: Service rm3 in namespace kubectl-5701 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5701" for this suite. 12/14/22 16:27:22.649
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:22.661
Dec 14 16:27:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:27:22.665
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:22.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:22.696
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Dec 14 16:27:22.713: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 16:27:27.719: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:27:27.72
Dec 14 16:27:27.720: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/14/22 16:27:27.77
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:27:27.810: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9197  6c6a5e61-8bf6-461a-a9fb-1f7502ca6174 16953 1 2022-12-14 16:27:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-14 16:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ec8a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 16:27:27.826: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:27.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9197" for this suite. 12/14/22 16:27:27.859
------------------------------
• [SLOW TEST] [5.224 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:22.661
    Dec 14 16:27:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:27:22.665
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:22.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:22.696
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Dec 14 16:27:22.713: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Dec 14 16:27:27.719: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:27:27.72
    Dec 14 16:27:27.720: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/14/22 16:27:27.77
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:27:27.810: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9197  6c6a5e61-8bf6-461a-a9fb-1f7502ca6174 16953 1 2022-12-14 16:27:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-14 16:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ec8a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 14 16:27:27.826: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:27.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9197" for this suite. 12/14/22 16:27:27.859
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:27.885
Dec 14 16:27:27.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:27:27.889
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:28.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:28.014
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9373" for this suite. 12/14/22 16:27:28.059
------------------------------
• [0.193 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:27.885
    Dec 14 16:27:27.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:27:27.889
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:28.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:28.014
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9373" for this suite. 12/14/22 16:27:28.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:28.081
Dec 14 16:27:28.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:27:28.088
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:28.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:28.132
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:27:28.137
Dec 14 16:27:28.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310" in namespace "projected-5625" to be "Succeeded or Failed"
Dec 14 16:27:28.179: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990829ms
Dec 14 16:27:30.187: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014313445s
Dec 14 16:27:32.185: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012774054s
STEP: Saw pod success 12/14/22 16:27:32.185
Dec 14 16:27:32.186: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310" satisfied condition "Succeeded or Failed"
Dec 14 16:27:32.192: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 container client-container: <nil>
STEP: delete the pod 12/14/22 16:27:32.201
Dec 14 16:27:32.216: INFO: Waiting for pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 to disappear
Dec 14 16:27:32.220: INFO: Pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:32.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5625" for this suite. 12/14/22 16:27:32.229
------------------------------
• [4.161 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:28.081
    Dec 14 16:27:28.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:27:28.088
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:28.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:28.132
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:27:28.137
    Dec 14 16:27:28.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310" in namespace "projected-5625" to be "Succeeded or Failed"
    Dec 14 16:27:28.179: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990829ms
    Dec 14 16:27:30.187: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014313445s
    Dec 14 16:27:32.185: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012774054s
    STEP: Saw pod success 12/14/22 16:27:32.185
    Dec 14 16:27:32.186: INFO: Pod "downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310" satisfied condition "Succeeded or Failed"
    Dec 14 16:27:32.192: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:27:32.201
    Dec 14 16:27:32.216: INFO: Waiting for pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 to disappear
    Dec 14 16:27:32.220: INFO: Pod downwardapi-volume-a90dd2d7-3013-4676-93c9-d1089473c310 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:32.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5625" for this suite. 12/14/22 16:27:32.229
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:32.243
Dec 14 16:27:32.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:27:32.246
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:32.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:32.276
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-95eff504-c633-4662-859d-4946b511b8dc 12/14/22 16:27:32.281
STEP: Creating a pod to test consume configMaps 12/14/22 16:27:32.29
Dec 14 16:27:32.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0" in namespace "projected-6363" to be "Succeeded or Failed"
Dec 14 16:27:32.308: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.251647ms
Dec 14 16:27:34.318: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015392081s
Dec 14 16:27:36.322: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018930595s
STEP: Saw pod success 12/14/22 16:27:36.322
Dec 14 16:27:36.322: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0" satisfied condition "Succeeded or Failed"
Dec 14 16:27:36.334: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:27:36.345
Dec 14 16:27:36.362: INFO: Waiting for pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 to disappear
Dec 14 16:27:36.367: INFO: Pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:36.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6363" for this suite. 12/14/22 16:27:36.373
------------------------------
• [4.139 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:32.243
    Dec 14 16:27:32.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:27:32.246
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:32.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:32.276
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-95eff504-c633-4662-859d-4946b511b8dc 12/14/22 16:27:32.281
    STEP: Creating a pod to test consume configMaps 12/14/22 16:27:32.29
    Dec 14 16:27:32.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0" in namespace "projected-6363" to be "Succeeded or Failed"
    Dec 14 16:27:32.308: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.251647ms
    Dec 14 16:27:34.318: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015392081s
    Dec 14 16:27:36.322: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018930595s
    STEP: Saw pod success 12/14/22 16:27:36.322
    Dec 14 16:27:36.322: INFO: Pod "pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0" satisfied condition "Succeeded or Failed"
    Dec 14 16:27:36.334: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:27:36.345
    Dec 14 16:27:36.362: INFO: Waiting for pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 to disappear
    Dec 14 16:27:36.367: INFO: Pod pod-projected-configmaps-73d3466d-e5f5-4c9e-87f1-d7a272858cf0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:36.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6363" for this suite. 12/14/22 16:27:36.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:36.384
Dec 14 16:27:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 16:27:36.387
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:36.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:36.418
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 12/14/22 16:27:36.422
Dec 14 16:27:36.435: INFO: created test-pod-1
Dec 14 16:27:36.444: INFO: created test-pod-2
Dec 14 16:27:36.458: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 12/14/22 16:27:36.458
Dec 14 16:27:36.458: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7032' to be running and ready
Dec 14 16:27:36.507: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 16:27:36.508: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 16:27:36.508: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 16:27:36.508: INFO: 0 / 3 pods in namespace 'pods-7032' are running and ready (0 seconds elapsed)
Dec 14 16:27:36.508: INFO: expected 0 pod replicas in namespace 'pods-7032', 0 are Running and Ready.
Dec 14 16:27:36.508: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:27:36.508: INFO: test-pod-1  iet9eich7uhu-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
Dec 14 16:27:36.508: INFO: test-pod-2  iet9eich7uhu-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
Dec 14 16:27:36.508: INFO: test-pod-3  iet9eich7uhu-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
Dec 14 16:27:36.508: INFO: 
Dec 14 16:27:38.532: INFO: 3 / 3 pods in namespace 'pods-7032' are running and ready (2 seconds elapsed)
Dec 14 16:27:38.532: INFO: expected 0 pod replicas in namespace 'pods-7032', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 12/14/22 16:27:38.569
Dec 14 16:27:38.574: INFO: Pod quantity 3 is different from expected quantity 0
Dec 14 16:27:39.583: INFO: Pod quantity 3 is different from expected quantity 0
Dec 14 16:27:40.583: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:41.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7032" for this suite. 12/14/22 16:27:41.588
------------------------------
• [SLOW TEST] [5.214 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:36.384
    Dec 14 16:27:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 16:27:36.387
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:36.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:36.418
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 12/14/22 16:27:36.422
    Dec 14 16:27:36.435: INFO: created test-pod-1
    Dec 14 16:27:36.444: INFO: created test-pod-2
    Dec 14 16:27:36.458: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 12/14/22 16:27:36.458
    Dec 14 16:27:36.458: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7032' to be running and ready
    Dec 14 16:27:36.507: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 16:27:36.508: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 16:27:36.508: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 14 16:27:36.508: INFO: 0 / 3 pods in namespace 'pods-7032' are running and ready (0 seconds elapsed)
    Dec 14 16:27:36.508: INFO: expected 0 pod replicas in namespace 'pods-7032', 0 are Running and Ready.
    Dec 14 16:27:36.508: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Dec 14 16:27:36.508: INFO: test-pod-1  iet9eich7uhu-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
    Dec 14 16:27:36.508: INFO: test-pod-2  iet9eich7uhu-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
    Dec 14 16:27:36.508: INFO: test-pod-3  iet9eich7uhu-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:27:36 +0000 UTC  }]
    Dec 14 16:27:36.508: INFO: 
    Dec 14 16:27:38.532: INFO: 3 / 3 pods in namespace 'pods-7032' are running and ready (2 seconds elapsed)
    Dec 14 16:27:38.532: INFO: expected 0 pod replicas in namespace 'pods-7032', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 12/14/22 16:27:38.569
    Dec 14 16:27:38.574: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 14 16:27:39.583: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 14 16:27:40.583: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:41.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7032" for this suite. 12/14/22 16:27:41.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:41.63
Dec 14 16:27:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:27:41.635
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:41.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:41.68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/14/22 16:27:41.683
Dec 14 16:27:41.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:27:43.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:52.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1562" for this suite. 12/14/22 16:27:52.03
------------------------------
• [SLOW TEST] [10.410 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:41.63
    Dec 14 16:27:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:27:41.635
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:41.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:41.68
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/14/22 16:27:41.683
    Dec 14 16:27:41.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:27:43.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:52.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1562" for this suite. 12/14/22 16:27:52.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:52.044
Dec 14 16:27:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:27:52.046
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:52.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:52.075
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:27:52.095
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:27:53.161
STEP: Deploying the webhook pod 12/14/22 16:27:53.174
STEP: Wait for the deployment to be ready 12/14/22 16:27:53.191
Dec 14 16:27:53.204: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/14/22 16:27:55.224
STEP: Verifying the service has paired with the endpoint 12/14/22 16:27:55.245
Dec 14 16:27:56.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 16:27:56.255
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 16:27:56.287
STEP: Creating a dummy validating-webhook-configuration object 12/14/22 16:27:56.311
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/14/22 16:27:56.324
STEP: Creating a dummy mutating-webhook-configuration object 12/14/22 16:27:56.335
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/14/22 16:27:56.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:56.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8595" for this suite. 12/14/22 16:27:56.445
STEP: Destroying namespace "webhook-8595-markers" for this suite. 12/14/22 16:27:56.456
------------------------------
• [4.427 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:52.044
    Dec 14 16:27:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:27:52.046
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:52.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:52.075
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:27:52.095
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:27:53.161
    STEP: Deploying the webhook pod 12/14/22 16:27:53.174
    STEP: Wait for the deployment to be ready 12/14/22 16:27:53.191
    Dec 14 16:27:53.204: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/14/22 16:27:55.224
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:27:55.245
    Dec 14 16:27:56.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 16:27:56.255
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/14/22 16:27:56.287
    STEP: Creating a dummy validating-webhook-configuration object 12/14/22 16:27:56.311
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/14/22 16:27:56.324
    STEP: Creating a dummy mutating-webhook-configuration object 12/14/22 16:27:56.335
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/14/22 16:27:56.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:56.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8595" for this suite. 12/14/22 16:27:56.445
    STEP: Destroying namespace "webhook-8595-markers" for this suite. 12/14/22 16:27:56.456
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:56.471
Dec 14 16:27:56.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 16:27:56.475
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:56.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:56.504
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 12/14/22 16:27:56.507
STEP: Ensure pods equal to parallelism count is attached to the job 12/14/22 16:27:56.513
STEP: patching /status 12/14/22 16:27:58.521
STEP: updating /status 12/14/22 16:27:58.543
STEP: get /status 12/14/22 16:27:58.563
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 16:27:58.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9316" for this suite. 12/14/22 16:27:58.575
------------------------------
• [2.117 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:56.471
    Dec 14 16:27:56.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 16:27:56.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:56.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:56.504
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 12/14/22 16:27:56.507
    STEP: Ensure pods equal to parallelism count is attached to the job 12/14/22 16:27:56.513
    STEP: patching /status 12/14/22 16:27:58.521
    STEP: updating /status 12/14/22 16:27:58.543
    STEP: get /status 12/14/22 16:27:58.563
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:27:58.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9316" for this suite. 12/14/22 16:27:58.575
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:27:58.589
Dec 14 16:27:58.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:27:58.592
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:58.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:58.632
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Dec 14 16:27:58.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/14/22 16:28:00.368
Dec 14 16:28:00.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
Dec 14 16:28:00.919: INFO: stderr: ""
Dec 14 16:28:00.920: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 16:28:00.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 delete e2e-test-crd-publish-openapi-5700-crds test-foo'
Dec 14 16:28:01.136: INFO: stderr: ""
Dec 14 16:28:01.136: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 16:28:01.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
Dec 14 16:28:01.547: INFO: stderr: ""
Dec 14 16:28:01.547: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 16:28:01.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 delete e2e-test-crd-publish-openapi-5700-crds test-foo'
Dec 14 16:28:01.724: INFO: stderr: ""
Dec 14 16:28:01.724: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/14/22 16:28:01.724
Dec 14 16:28:01.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
Dec 14 16:28:02.716: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/14/22 16:28:02.716
Dec 14 16:28:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
Dec 14 16:28:03.030: INFO: rc: 1
Dec 14 16:28:03.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
Dec 14 16:28:03.372: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/14/22 16:28:03.372
Dec 14 16:28:03.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
Dec 14 16:28:03.708: INFO: rc: 1
Dec 14 16:28:03.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
Dec 14 16:28:04.286: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 12/14/22 16:28:04.286
Dec 14 16:28:04.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds'
Dec 14 16:28:04.728: INFO: stderr: ""
Dec 14 16:28:04.728: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 12/14/22 16:28:04.729
Dec 14 16:28:04.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.metadata'
Dec 14 16:28:05.066: INFO: stderr: ""
Dec 14 16:28:05.066: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 16:28:05.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec'
Dec 14 16:28:05.461: INFO: stderr: ""
Dec 14 16:28:05.461: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 16:28:05.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec.bars'
Dec 14 16:28:05.833: INFO: stderr: ""
Dec 14 16:28:05.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/14/22 16:28:05.833
Dec 14 16:28:05.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec.bars2'
Dec 14 16:28:06.298: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:28:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9257" for this suite. 12/14/22 16:28:08.432
------------------------------
• [SLOW TEST] [9.851 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:27:58.589
    Dec 14 16:27:58.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:27:58.592
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:27:58.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:27:58.632
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Dec 14 16:27:58.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/14/22 16:28:00.368
    Dec 14 16:28:00.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
    Dec 14 16:28:00.919: INFO: stderr: ""
    Dec 14 16:28:00.920: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 14 16:28:00.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 delete e2e-test-crd-publish-openapi-5700-crds test-foo'
    Dec 14 16:28:01.136: INFO: stderr: ""
    Dec 14 16:28:01.136: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Dec 14 16:28:01.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
    Dec 14 16:28:01.547: INFO: stderr: ""
    Dec 14 16:28:01.547: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 14 16:28:01.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 delete e2e-test-crd-publish-openapi-5700-crds test-foo'
    Dec 14 16:28:01.724: INFO: stderr: ""
    Dec 14 16:28:01.724: INFO: stdout: "e2e-test-crd-publish-openapi-5700-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/14/22 16:28:01.724
    Dec 14 16:28:01.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
    Dec 14 16:28:02.716: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/14/22 16:28:02.716
    Dec 14 16:28:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
    Dec 14 16:28:03.030: INFO: rc: 1
    Dec 14 16:28:03.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
    Dec 14 16:28:03.372: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/14/22 16:28:03.372
    Dec 14 16:28:03.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 create -f -'
    Dec 14 16:28:03.708: INFO: rc: 1
    Dec 14 16:28:03.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 --namespace=crd-publish-openapi-9257 apply -f -'
    Dec 14 16:28:04.286: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 12/14/22 16:28:04.286
    Dec 14 16:28:04.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds'
    Dec 14 16:28:04.728: INFO: stderr: ""
    Dec 14 16:28:04.728: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 12/14/22 16:28:04.729
    Dec 14 16:28:04.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.metadata'
    Dec 14 16:28:05.066: INFO: stderr: ""
    Dec 14 16:28:05.066: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Dec 14 16:28:05.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec'
    Dec 14 16:28:05.461: INFO: stderr: ""
    Dec 14 16:28:05.461: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Dec 14 16:28:05.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec.bars'
    Dec 14 16:28:05.833: INFO: stderr: ""
    Dec 14 16:28:05.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5700-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/14/22 16:28:05.833
    Dec 14 16:28:05.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-9257 explain e2e-test-crd-publish-openapi-5700-crds.spec.bars2'
    Dec 14 16:28:06.298: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:28:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9257" for this suite. 12/14/22 16:28:08.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:28:08.442
Dec 14 16:28:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:28:08.447
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:08.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:08.473
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 12/14/22 16:28:25.485
STEP: Creating a ResourceQuota 12/14/22 16:28:30.492
STEP: Ensuring resource quota status is calculated 12/14/22 16:28:30.505
STEP: Creating a ConfigMap 12/14/22 16:28:32.513
STEP: Ensuring resource quota status captures configMap creation 12/14/22 16:28:32.533
STEP: Deleting a ConfigMap 12/14/22 16:28:34.543
STEP: Ensuring resource quota status released usage 12/14/22 16:28:34.553
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:28:36.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-154" for this suite. 12/14/22 16:28:36.572
------------------------------
• [SLOW TEST] [28.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:28:08.442
    Dec 14 16:28:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:28:08.447
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:08.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:08.473
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 12/14/22 16:28:25.485
    STEP: Creating a ResourceQuota 12/14/22 16:28:30.492
    STEP: Ensuring resource quota status is calculated 12/14/22 16:28:30.505
    STEP: Creating a ConfigMap 12/14/22 16:28:32.513
    STEP: Ensuring resource quota status captures configMap creation 12/14/22 16:28:32.533
    STEP: Deleting a ConfigMap 12/14/22 16:28:34.543
    STEP: Ensuring resource quota status released usage 12/14/22 16:28:34.553
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:28:36.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-154" for this suite. 12/14/22 16:28:36.572
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:28:36.582
Dec 14 16:28:36.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 16:28:36.586
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:36.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:36.615
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Dec 14 16:28:36.656: INFO: Create a RollingUpdate DaemonSet
Dec 14 16:28:36.669: INFO: Check that daemon pods launch on every node of the cluster
Dec 14 16:28:36.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:28:36.683: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:28:37.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 16:28:37.708: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 16:28:38.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:28:38.701: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 16:28:39.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:28:39.697: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Dec 14 16:28:39.697: INFO: Update the DaemonSet to trigger a rollout
Dec 14 16:28:39.713: INFO: Updating DaemonSet daemon-set
Dec 14 16:28:41.738: INFO: Roll back the DaemonSet before rollout is complete
Dec 14 16:28:41.751: INFO: Updating DaemonSet daemon-set
Dec 14 16:28:41.751: INFO: Make sure DaemonSet rollback is complete
Dec 14 16:28:41.758: INFO: Wrong image for pod: daemon-set-ngnwj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Dec 14 16:28:41.759: INFO: Pod daemon-set-ngnwj is not available
Dec 14 16:28:47.778: INFO: Pod daemon-set-l6ntq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:28:47.805
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-734, will wait for the garbage collector to delete the pods 12/14/22 16:28:47.805
Dec 14 16:28:47.873: INFO: Deleting DaemonSet.extensions daemon-set took: 14.230025ms
Dec 14 16:28:47.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.907341ms
Dec 14 16:28:49.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:28:49.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:28:49.490: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17548"},"items":null}

Dec 14 16:28:49.494: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17548"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:28:49.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-734" for this suite. 12/14/22 16:28:49.519
------------------------------
• [SLOW TEST] [12.947 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:28:36.582
    Dec 14 16:28:36.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 16:28:36.586
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:36.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:36.615
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Dec 14 16:28:36.656: INFO: Create a RollingUpdate DaemonSet
    Dec 14 16:28:36.669: INFO: Check that daemon pods launch on every node of the cluster
    Dec 14 16:28:36.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:28:36.683: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:28:37.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 16:28:37.708: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 16:28:38.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:28:38.701: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 16:28:39.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:28:39.697: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Dec 14 16:28:39.697: INFO: Update the DaemonSet to trigger a rollout
    Dec 14 16:28:39.713: INFO: Updating DaemonSet daemon-set
    Dec 14 16:28:41.738: INFO: Roll back the DaemonSet before rollout is complete
    Dec 14 16:28:41.751: INFO: Updating DaemonSet daemon-set
    Dec 14 16:28:41.751: INFO: Make sure DaemonSet rollback is complete
    Dec 14 16:28:41.758: INFO: Wrong image for pod: daemon-set-ngnwj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Dec 14 16:28:41.759: INFO: Pod daemon-set-ngnwj is not available
    Dec 14 16:28:47.778: INFO: Pod daemon-set-l6ntq is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:28:47.805
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-734, will wait for the garbage collector to delete the pods 12/14/22 16:28:47.805
    Dec 14 16:28:47.873: INFO: Deleting DaemonSet.extensions daemon-set took: 14.230025ms
    Dec 14 16:28:47.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.907341ms
    Dec 14 16:28:49.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:28:49.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 16:28:49.490: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17548"},"items":null}

    Dec 14 16:28:49.494: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17548"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:28:49.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-734" for this suite. 12/14/22 16:28:49.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:28:49.535
Dec 14 16:28:49.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 16:28:49.536
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:49.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:49.577
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 12/14/22 16:28:49.581
Dec 14 16:28:49.596: INFO: Waiting up to 5m0s for pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde" in namespace "var-expansion-8492" to be "Succeeded or Failed"
Dec 14 16:28:49.600: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.843312ms
Dec 14 16:28:51.610: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013697367s
Dec 14 16:28:53.612: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015323899s
STEP: Saw pod success 12/14/22 16:28:53.612
Dec 14 16:28:53.612: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde" satisfied condition "Succeeded or Failed"
Dec 14 16:28:53.618: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:28:53.654
Dec 14 16:28:53.674: INFO: Waiting for pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde to disappear
Dec 14 16:28:53.679: INFO: Pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 16:28:53.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8492" for this suite. 12/14/22 16:28:53.685
------------------------------
• [4.158 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:28:49.535
    Dec 14 16:28:49.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 16:28:49.536
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:49.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:49.577
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 12/14/22 16:28:49.581
    Dec 14 16:28:49.596: INFO: Waiting up to 5m0s for pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde" in namespace "var-expansion-8492" to be "Succeeded or Failed"
    Dec 14 16:28:49.600: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.843312ms
    Dec 14 16:28:51.610: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013697367s
    Dec 14 16:28:53.612: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015323899s
    STEP: Saw pod success 12/14/22 16:28:53.612
    Dec 14 16:28:53.612: INFO: Pod "var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde" satisfied condition "Succeeded or Failed"
    Dec 14 16:28:53.618: INFO: Trying to get logs from node iet9eich7uhu-3 pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:28:53.654
    Dec 14 16:28:53.674: INFO: Waiting for pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde to disappear
    Dec 14 16:28:53.679: INFO: Pod var-expansion-188e850c-8ecf-49e4-9230-8b5806acabde no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:28:53.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8492" for this suite. 12/14/22 16:28:53.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:28:53.696
Dec 14 16:28:53.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:28:53.698
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:53.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:53.725
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 16:28:53.735
Dec 14 16:28:53.752: INFO: Waiting up to 5m0s for pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65" in namespace "emptydir-8199" to be "Succeeded or Failed"
Dec 14 16:28:53.765: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Pending", Reason="", readiness=false. Elapsed: 13.454877ms
Dec 14 16:28:55.774: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Running", Reason="", readiness=false. Elapsed: 2.021795425s
Dec 14 16:28:57.773: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020831331s
STEP: Saw pod success 12/14/22 16:28:57.773
Dec 14 16:28:57.773: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65" satisfied condition "Succeeded or Failed"
Dec 14 16:28:57.797: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 container test-container: <nil>
STEP: delete the pod 12/14/22 16:28:57.912
Dec 14 16:28:57.934: INFO: Waiting for pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 to disappear
Dec 14 16:28:57.939: INFO: Pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:28:57.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8199" for this suite. 12/14/22 16:28:57.947
------------------------------
• [4.259 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:28:53.696
    Dec 14 16:28:53.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:28:53.698
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:53.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:53.725
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/14/22 16:28:53.735
    Dec 14 16:28:53.752: INFO: Waiting up to 5m0s for pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65" in namespace "emptydir-8199" to be "Succeeded or Failed"
    Dec 14 16:28:53.765: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Pending", Reason="", readiness=false. Elapsed: 13.454877ms
    Dec 14 16:28:55.774: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Running", Reason="", readiness=false. Elapsed: 2.021795425s
    Dec 14 16:28:57.773: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020831331s
    STEP: Saw pod success 12/14/22 16:28:57.773
    Dec 14 16:28:57.773: INFO: Pod "pod-f016d606-e09c-43db-b0ad-04bd7cc7da65" satisfied condition "Succeeded or Failed"
    Dec 14 16:28:57.797: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:28:57.912
    Dec 14 16:28:57.934: INFO: Waiting for pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 to disappear
    Dec 14 16:28:57.939: INFO: Pod pod-f016d606-e09c-43db-b0ad-04bd7cc7da65 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:28:57.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8199" for this suite. 12/14/22 16:28:57.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:28:57.974
Dec 14 16:28:57.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:28:57.977
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:58.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:58.007
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-58e2c230-bb74-4fd8-b62d-132b6c9e6264 12/14/22 16:28:58.011
STEP: Creating a pod to test consume secrets 12/14/22 16:28:58.016
Dec 14 16:28:58.030: INFO: Waiting up to 5m0s for pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684" in namespace "secrets-7349" to be "Succeeded or Failed"
Dec 14 16:28:58.036: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985383ms
Dec 14 16:29:00.044: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Running", Reason="", readiness=false. Elapsed: 2.014118768s
Dec 14 16:29:02.045: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014808265s
STEP: Saw pod success 12/14/22 16:29:02.045
Dec 14 16:29:02.045: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684" satisfied condition "Succeeded or Failed"
Dec 14 16:29:02.049: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:29:02.061
Dec 14 16:29:02.082: INFO: Waiting for pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 to disappear
Dec 14 16:29:02.087: INFO: Pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:02.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7349" for this suite. 12/14/22 16:29:02.094
------------------------------
• [4.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:28:57.974
    Dec 14 16:28:57.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:28:57.977
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:28:58.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:28:58.007
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-58e2c230-bb74-4fd8-b62d-132b6c9e6264 12/14/22 16:28:58.011
    STEP: Creating a pod to test consume secrets 12/14/22 16:28:58.016
    Dec 14 16:28:58.030: INFO: Waiting up to 5m0s for pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684" in namespace "secrets-7349" to be "Succeeded or Failed"
    Dec 14 16:28:58.036: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985383ms
    Dec 14 16:29:00.044: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Running", Reason="", readiness=false. Elapsed: 2.014118768s
    Dec 14 16:29:02.045: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014808265s
    STEP: Saw pod success 12/14/22 16:29:02.045
    Dec 14 16:29:02.045: INFO: Pod "pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684" satisfied condition "Succeeded or Failed"
    Dec 14 16:29:02.049: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:29:02.061
    Dec 14 16:29:02.082: INFO: Waiting for pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 to disappear
    Dec 14 16:29:02.087: INFO: Pod pod-secrets-cd8d6270-9497-4847-9572-1d2f826a5684 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:02.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7349" for this suite. 12/14/22 16:29:02.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:02.107
Dec 14 16:29:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:29:02.111
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:02.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:02.134
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:02.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4731" for this suite. 12/14/22 16:29:02.199
------------------------------
• [0.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:02.107
    Dec 14 16:29:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:29:02.111
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:02.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:02.134
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:02.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4731" for this suite. 12/14/22 16:29:02.199
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:02.208
Dec 14 16:29:02.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename hostport 12/14/22 16:29:02.211
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:02.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:02.239
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/14/22 16:29:02.249
Dec 14 16:29:02.264: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4908" to be "running and ready"
Dec 14 16:29:02.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.125709ms
Dec 14 16:29:02.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:29:04.291: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026531203s
Dec 14 16:29:04.291: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 16:29:04.291: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.21 on the node which pod1 resides and expect scheduled 12/14/22 16:29:04.291
Dec 14 16:29:04.298: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4908" to be "running and ready"
Dec 14 16:29:04.307: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231242ms
Dec 14 16:29:04.307: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:29:06.316: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01780457s
Dec 14 16:29:06.316: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 16:29:06.316: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.21 but use UDP protocol on the node which pod2 resides 12/14/22 16:29:06.316
Dec 14 16:29:06.325: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4908" to be "running and ready"
Dec 14 16:29:06.339: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.863537ms
Dec 14 16:29:06.339: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:29:08.346: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.02171541s
Dec 14 16:29:08.347: INFO: The phase of Pod pod3 is Running (Ready = true)
Dec 14 16:29:08.347: INFO: Pod "pod3" satisfied condition "running and ready"
Dec 14 16:29:08.357: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4908" to be "running and ready"
Dec 14 16:29:08.370: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.03872ms
Dec 14 16:29:08.370: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:29:10.380: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.022528521s
Dec 14 16:29:10.380: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Dec 14 16:29:10.380: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/14/22 16:29:10.386
Dec 14 16:29:10.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.21 http://127.0.0.1:54323/hostname] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:29:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:29:10.390: INFO: ExecWithOptions: Clientset creation
Dec 14 16:29:10.390: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.21+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.21, port: 54323 12/14/22 16:29:10.531
Dec 14 16:29:10.531: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.21:54323/hostname] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:29:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:29:10.533: INFO: ExecWithOptions: Clientset creation
Dec 14 16:29:10.533: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.21%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.21, port: 54323 UDP 12/14/22 16:29:10.637
Dec 14 16:29:10.637: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.21 54323] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:29:10.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:29:10.639: INFO: ExecWithOptions: Clientset creation
Dec 14 16:29:10.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.21+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:15.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-4908" for this suite. 12/14/22 16:29:15.795
------------------------------
• [SLOW TEST] [13.599 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:02.208
    Dec 14 16:29:02.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename hostport 12/14/22 16:29:02.211
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:02.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:02.239
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/14/22 16:29:02.249
    Dec 14 16:29:02.264: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4908" to be "running and ready"
    Dec 14 16:29:02.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.125709ms
    Dec 14 16:29:02.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:29:04.291: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026531203s
    Dec 14 16:29:04.291: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 16:29:04.291: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.21 on the node which pod1 resides and expect scheduled 12/14/22 16:29:04.291
    Dec 14 16:29:04.298: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4908" to be "running and ready"
    Dec 14 16:29:04.307: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231242ms
    Dec 14 16:29:04.307: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:29:06.316: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01780457s
    Dec 14 16:29:06.316: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 16:29:06.316: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.21 but use UDP protocol on the node which pod2 resides 12/14/22 16:29:06.316
    Dec 14 16:29:06.325: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4908" to be "running and ready"
    Dec 14 16:29:06.339: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.863537ms
    Dec 14 16:29:06.339: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:29:08.346: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.02171541s
    Dec 14 16:29:08.347: INFO: The phase of Pod pod3 is Running (Ready = true)
    Dec 14 16:29:08.347: INFO: Pod "pod3" satisfied condition "running and ready"
    Dec 14 16:29:08.357: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4908" to be "running and ready"
    Dec 14 16:29:08.370: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.03872ms
    Dec 14 16:29:08.370: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:29:10.380: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.022528521s
    Dec 14 16:29:10.380: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Dec 14 16:29:10.380: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/14/22 16:29:10.386
    Dec 14 16:29:10.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.21 http://127.0.0.1:54323/hostname] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:29:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:29:10.390: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:29:10.390: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.21+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.21, port: 54323 12/14/22 16:29:10.531
    Dec 14 16:29:10.531: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.21:54323/hostname] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:29:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:29:10.533: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:29:10.533: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.21%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.21, port: 54323 UDP 12/14/22 16:29:10.637
    Dec 14 16:29:10.637: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.21 54323] Namespace:hostport-4908 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:29:10.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:29:10.639: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:29:10.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4908/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.21+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:15.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-4908" for this suite. 12/14/22 16:29:15.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:15.814
Dec 14 16:29:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:29:15.817
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:15.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:15.852
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 12/14/22 16:29:15.856
Dec 14 16:29:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 create -f -'
Dec 14 16:29:16.375: INFO: stderr: ""
Dec 14 16:29:16.375: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:16.375
Dec 14 16:29:16.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:16.534: INFO: stderr: ""
Dec 14 16:29:16.534: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
Dec 14 16:29:16.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:16.698: INFO: stderr: ""
Dec 14 16:29:16.698: INFO: stdout: ""
Dec 14 16:29:16.698: INFO: update-demo-nautilus-5q6bw is created but not running
Dec 14 16:29:21.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:21.857: INFO: stderr: ""
Dec 14 16:29:21.857: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
Dec 14 16:29:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:22.033: INFO: stderr: ""
Dec 14 16:29:22.033: INFO: stdout: ""
Dec 14 16:29:22.033: INFO: update-demo-nautilus-5q6bw is created but not running
Dec 14 16:29:27.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:27.181: INFO: stderr: ""
Dec 14 16:29:27.181: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
Dec 14 16:29:27.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:27.306: INFO: stderr: ""
Dec 14 16:29:27.306: INFO: stdout: "true"
Dec 14 16:29:27.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:29:27.436: INFO: stderr: ""
Dec 14 16:29:27.436: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:29:27.436: INFO: validating pod update-demo-nautilus-5q6bw
Dec 14 16:29:27.455: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:29:27.455: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:29:27.455: INFO: update-demo-nautilus-5q6bw is verified up and running
Dec 14 16:29:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:27.577: INFO: stderr: ""
Dec 14 16:29:27.577: INFO: stdout: "true"
Dec 14 16:29:27.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:29:27.683: INFO: stderr: ""
Dec 14 16:29:27.683: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:29:27.683: INFO: validating pod update-demo-nautilus-tc2sq
Dec 14 16:29:27.692: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:29:27.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:29:27.692: INFO: update-demo-nautilus-tc2sq is verified up and running
STEP: scaling down the replication controller 12/14/22 16:29:27.692
Dec 14 16:29:27.703: INFO: scanned /root for discovery docs: <nil>
Dec 14 16:29:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 14 16:29:28.866: INFO: stderr: ""
Dec 14 16:29:28.866: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:28.866
Dec 14 16:29:28.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:28.996: INFO: stderr: ""
Dec 14 16:29:28.996: INFO: stdout: "update-demo-nautilus-tc2sq "
Dec 14 16:29:28.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:29.108: INFO: stderr: ""
Dec 14 16:29:29.108: INFO: stdout: "true"
Dec 14 16:29:29.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:29:29.228: INFO: stderr: ""
Dec 14 16:29:29.228: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:29:29.228: INFO: validating pod update-demo-nautilus-tc2sq
Dec 14 16:29:29.235: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:29:29.235: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:29:29.237: INFO: update-demo-nautilus-tc2sq is verified up and running
STEP: scaling up the replication controller 12/14/22 16:29:29.237
Dec 14 16:29:29.245: INFO: scanned /root for discovery docs: <nil>
Dec 14 16:29:29.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 14 16:29:30.414: INFO: stderr: ""
Dec 14 16:29:30.414: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:30.414
Dec 14 16:29:30.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:30.544: INFO: stderr: ""
Dec 14 16:29:30.544: INFO: stdout: "update-demo-nautilus-4clnl update-demo-nautilus-tc2sq "
Dec 14 16:29:30.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:30.682: INFO: stderr: ""
Dec 14 16:29:30.682: INFO: stdout: ""
Dec 14 16:29:30.682: INFO: update-demo-nautilus-4clnl is created but not running
Dec 14 16:29:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 16:29:35.819: INFO: stderr: ""
Dec 14 16:29:35.819: INFO: stdout: "update-demo-nautilus-4clnl update-demo-nautilus-tc2sq "
Dec 14 16:29:35.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:35.990: INFO: stderr: ""
Dec 14 16:29:35.990: INFO: stdout: "true"
Dec 14 16:29:35.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:29:36.342: INFO: stderr: ""
Dec 14 16:29:36.342: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:29:36.342: INFO: validating pod update-demo-nautilus-4clnl
Dec 14 16:29:36.355: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:29:36.355: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:29:36.355: INFO: update-demo-nautilus-4clnl is verified up and running
Dec 14 16:29:36.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 16:29:36.525: INFO: stderr: ""
Dec 14 16:29:36.525: INFO: stdout: "true"
Dec 14 16:29:36.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 16:29:36.642: INFO: stderr: ""
Dec 14 16:29:36.642: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec 14 16:29:36.642: INFO: validating pod update-demo-nautilus-tc2sq
Dec 14 16:29:36.652: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 16:29:36.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 16:29:36.652: INFO: update-demo-nautilus-tc2sq is verified up and running
STEP: using delete to clean up resources 12/14/22 16:29:36.652
Dec 14 16:29:36.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 delete --grace-period=0 --force -f -'
Dec 14 16:29:36.775: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 16:29:36.775: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 16:29:36.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get rc,svc -l name=update-demo --no-headers'
Dec 14 16:29:36.943: INFO: stderr: "No resources found in kubectl-7169 namespace.\n"
Dec 14 16:29:36.943: INFO: stdout: ""
Dec 14 16:29:36.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 16:29:37.080: INFO: stderr: ""
Dec 14 16:29:37.080: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7169" for this suite. 12/14/22 16:29:37.094
------------------------------
• [SLOW TEST] [21.290 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:15.814
    Dec 14 16:29:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:29:15.817
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:15.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:15.852
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 12/14/22 16:29:15.856
    Dec 14 16:29:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 create -f -'
    Dec 14 16:29:16.375: INFO: stderr: ""
    Dec 14 16:29:16.375: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:16.375
    Dec 14 16:29:16.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:16.534: INFO: stderr: ""
    Dec 14 16:29:16.534: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
    Dec 14 16:29:16.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:16.698: INFO: stderr: ""
    Dec 14 16:29:16.698: INFO: stdout: ""
    Dec 14 16:29:16.698: INFO: update-demo-nautilus-5q6bw is created but not running
    Dec 14 16:29:21.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:21.857: INFO: stderr: ""
    Dec 14 16:29:21.857: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
    Dec 14 16:29:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:22.033: INFO: stderr: ""
    Dec 14 16:29:22.033: INFO: stdout: ""
    Dec 14 16:29:22.033: INFO: update-demo-nautilus-5q6bw is created but not running
    Dec 14 16:29:27.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:27.181: INFO: stderr: ""
    Dec 14 16:29:27.181: INFO: stdout: "update-demo-nautilus-5q6bw update-demo-nautilus-tc2sq "
    Dec 14 16:29:27.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:27.306: INFO: stderr: ""
    Dec 14 16:29:27.306: INFO: stdout: "true"
    Dec 14 16:29:27.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-5q6bw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:29:27.436: INFO: stderr: ""
    Dec 14 16:29:27.436: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:29:27.436: INFO: validating pod update-demo-nautilus-5q6bw
    Dec 14 16:29:27.455: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:29:27.455: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:29:27.455: INFO: update-demo-nautilus-5q6bw is verified up and running
    Dec 14 16:29:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:27.577: INFO: stderr: ""
    Dec 14 16:29:27.577: INFO: stdout: "true"
    Dec 14 16:29:27.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:29:27.683: INFO: stderr: ""
    Dec 14 16:29:27.683: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:29:27.683: INFO: validating pod update-demo-nautilus-tc2sq
    Dec 14 16:29:27.692: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:29:27.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:29:27.692: INFO: update-demo-nautilus-tc2sq is verified up and running
    STEP: scaling down the replication controller 12/14/22 16:29:27.692
    Dec 14 16:29:27.703: INFO: scanned /root for discovery docs: <nil>
    Dec 14 16:29:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Dec 14 16:29:28.866: INFO: stderr: ""
    Dec 14 16:29:28.866: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:28.866
    Dec 14 16:29:28.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:28.996: INFO: stderr: ""
    Dec 14 16:29:28.996: INFO: stdout: "update-demo-nautilus-tc2sq "
    Dec 14 16:29:28.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:29.108: INFO: stderr: ""
    Dec 14 16:29:29.108: INFO: stdout: "true"
    Dec 14 16:29:29.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:29:29.228: INFO: stderr: ""
    Dec 14 16:29:29.228: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:29:29.228: INFO: validating pod update-demo-nautilus-tc2sq
    Dec 14 16:29:29.235: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:29:29.235: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:29:29.237: INFO: update-demo-nautilus-tc2sq is verified up and running
    STEP: scaling up the replication controller 12/14/22 16:29:29.237
    Dec 14 16:29:29.245: INFO: scanned /root for discovery docs: <nil>
    Dec 14 16:29:29.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Dec 14 16:29:30.414: INFO: stderr: ""
    Dec 14 16:29:30.414: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/14/22 16:29:30.414
    Dec 14 16:29:30.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:30.544: INFO: stderr: ""
    Dec 14 16:29:30.544: INFO: stdout: "update-demo-nautilus-4clnl update-demo-nautilus-tc2sq "
    Dec 14 16:29:30.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:30.682: INFO: stderr: ""
    Dec 14 16:29:30.682: INFO: stdout: ""
    Dec 14 16:29:30.682: INFO: update-demo-nautilus-4clnl is created but not running
    Dec 14 16:29:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 14 16:29:35.819: INFO: stderr: ""
    Dec 14 16:29:35.819: INFO: stdout: "update-demo-nautilus-4clnl update-demo-nautilus-tc2sq "
    Dec 14 16:29:35.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:35.990: INFO: stderr: ""
    Dec 14 16:29:35.990: INFO: stdout: "true"
    Dec 14 16:29:35.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-4clnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:29:36.342: INFO: stderr: ""
    Dec 14 16:29:36.342: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:29:36.342: INFO: validating pod update-demo-nautilus-4clnl
    Dec 14 16:29:36.355: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:29:36.355: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:29:36.355: INFO: update-demo-nautilus-4clnl is verified up and running
    Dec 14 16:29:36.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 14 16:29:36.525: INFO: stderr: ""
    Dec 14 16:29:36.525: INFO: stdout: "true"
    Dec 14 16:29:36.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods update-demo-nautilus-tc2sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 14 16:29:36.642: INFO: stderr: ""
    Dec 14 16:29:36.642: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec 14 16:29:36.642: INFO: validating pod update-demo-nautilus-tc2sq
    Dec 14 16:29:36.652: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 14 16:29:36.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 14 16:29:36.652: INFO: update-demo-nautilus-tc2sq is verified up and running
    STEP: using delete to clean up resources 12/14/22 16:29:36.652
    Dec 14 16:29:36.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 delete --grace-period=0 --force -f -'
    Dec 14 16:29:36.775: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 16:29:36.775: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 14 16:29:36.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get rc,svc -l name=update-demo --no-headers'
    Dec 14 16:29:36.943: INFO: stderr: "No resources found in kubectl-7169 namespace.\n"
    Dec 14 16:29:36.943: INFO: stdout: ""
    Dec 14 16:29:36.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7169 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 16:29:37.080: INFO: stderr: ""
    Dec 14 16:29:37.080: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7169" for this suite. 12/14/22 16:29:37.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:37.107
Dec 14 16:29:37.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:29:37.111
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:37.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:37.142
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-bf234a3d-4aae-4472-8700-b9e32ca35a94 12/14/22 16:29:37.147
STEP: Creating a pod to test consume secrets 12/14/22 16:29:37.156
Dec 14 16:29:37.172: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c" in namespace "projected-6732" to be "Succeeded or Failed"
Dec 14 16:29:37.187: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.557523ms
Dec 14 16:29:39.197: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024931862s
Dec 14 16:29:41.195: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023077423s
STEP: Saw pod success 12/14/22 16:29:41.195
Dec 14 16:29:41.196: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c" satisfied condition "Succeeded or Failed"
Dec 14 16:29:41.206: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:29:41.216
Dec 14 16:29:41.230: INFO: Waiting for pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c to disappear
Dec 14 16:29:41.234: INFO: Pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6732" for this suite. 12/14/22 16:29:41.25
------------------------------
• [4.154 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:37.107
    Dec 14 16:29:37.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:29:37.111
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:37.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:37.142
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-bf234a3d-4aae-4472-8700-b9e32ca35a94 12/14/22 16:29:37.147
    STEP: Creating a pod to test consume secrets 12/14/22 16:29:37.156
    Dec 14 16:29:37.172: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c" in namespace "projected-6732" to be "Succeeded or Failed"
    Dec 14 16:29:37.187: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.557523ms
    Dec 14 16:29:39.197: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024931862s
    Dec 14 16:29:41.195: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023077423s
    STEP: Saw pod success 12/14/22 16:29:41.195
    Dec 14 16:29:41.196: INFO: Pod "pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c" satisfied condition "Succeeded or Failed"
    Dec 14 16:29:41.206: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:29:41.216
    Dec 14 16:29:41.230: INFO: Waiting for pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c to disappear
    Dec 14 16:29:41.234: INFO: Pod pod-projected-secrets-ef1521ef-d40d-49c6-a19f-03ca33d62b1c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6732" for this suite. 12/14/22 16:29:41.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:41.271
Dec 14 16:29:41.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:29:41.275
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:41.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:41.305
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 12/14/22 16:29:41.31
Dec 14 16:29:41.329: INFO: Waiting up to 5m0s for pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1" in namespace "downward-api-6277" to be "Succeeded or Failed"
Dec 14 16:29:41.336: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.497028ms
Dec 14 16:29:43.345: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016258878s
Dec 14 16:29:45.352: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022542716s
STEP: Saw pod success 12/14/22 16:29:45.352
Dec 14 16:29:45.352: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1" satisfied condition "Succeeded or Failed"
Dec 14 16:29:45.358: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:29:45.369
Dec 14 16:29:45.384: INFO: Waiting for pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 to disappear
Dec 14 16:29:45.389: INFO: Pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:29:45.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6277" for this suite. 12/14/22 16:29:45.396
------------------------------
• [4.137 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:41.271
    Dec 14 16:29:41.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:29:41.275
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:41.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:41.305
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 12/14/22 16:29:41.31
    Dec 14 16:29:41.329: INFO: Waiting up to 5m0s for pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1" in namespace "downward-api-6277" to be "Succeeded or Failed"
    Dec 14 16:29:41.336: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.497028ms
    Dec 14 16:29:43.345: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016258878s
    Dec 14 16:29:45.352: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022542716s
    STEP: Saw pod success 12/14/22 16:29:45.352
    Dec 14 16:29:45.352: INFO: Pod "downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1" satisfied condition "Succeeded or Failed"
    Dec 14 16:29:45.358: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:29:45.369
    Dec 14 16:29:45.384: INFO: Waiting for pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 to disappear
    Dec 14 16:29:45.389: INFO: Pod downward-api-238810c2-84ef-40fe-b794-35dfdd7da8b1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:29:45.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6277" for this suite. 12/14/22 16:29:45.396
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:29:45.407
Dec 14 16:29:45.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename var-expansion 12/14/22 16:29:45.41
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:45.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:45.434
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 12/14/22 16:29:45.439
Dec 14 16:29:45.454: INFO: Waiting up to 2m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625" to be "running"
Dec 14 16:29:45.458: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161259ms
Dec 14 16:29:47.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016174564s
Dec 14 16:29:49.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014825132s
Dec 14 16:29:51.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013187319s
Dec 14 16:29:53.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015987278s
Dec 14 16:29:55.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013074173s
Dec 14 16:29:57.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016833073s
Dec 14 16:29:59.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013686343s
Dec 14 16:30:01.477: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022858844s
Dec 14 16:30:03.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013623629s
Dec 14 16:30:05.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016023098s
Dec 14 16:30:07.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014477755s
Dec 14 16:30:09.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014266717s
Dec 14 16:30:11.464: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010410857s
Dec 14 16:30:13.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 28.016678561s
Dec 14 16:30:15.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016328324s
Dec 14 16:30:17.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012123103s
Dec 14 16:30:19.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014153225s
Dec 14 16:30:21.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012255513s
Dec 14 16:30:23.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016418457s
Dec 14 16:30:25.477: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023154925s
Dec 14 16:30:27.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014227325s
Dec 14 16:30:29.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015554077s
Dec 14 16:30:31.472: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018411574s
Dec 14 16:30:33.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013708929s
Dec 14 16:30:35.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016344232s
Dec 14 16:30:37.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012806147s
Dec 14 16:30:39.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014373553s
Dec 14 16:30:41.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015580926s
Dec 14 16:30:43.473: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018623846s
Dec 14 16:30:45.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014081718s
Dec 14 16:30:47.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014470957s
Dec 14 16:30:49.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013919826s
Dec 14 16:30:51.465: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011308127s
Dec 14 16:30:53.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013733203s
Dec 14 16:30:55.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01240905s
Dec 14 16:30:57.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015082694s
Dec 14 16:30:59.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014440478s
Dec 14 16:31:01.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016941283s
Dec 14 16:31:03.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01709332s
Dec 14 16:31:05.475: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020757706s
Dec 14 16:31:07.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01728768s
Dec 14 16:31:09.472: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017889951s
Dec 14 16:31:11.473: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01914616s
Dec 14 16:31:13.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013335292s
Dec 14 16:31:15.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012242578s
Dec 14 16:31:17.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017009276s
Dec 14 16:31:19.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017200384s
Dec 14 16:31:21.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.016989649s
Dec 14 16:31:23.484: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.030275803s
Dec 14 16:31:25.464: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010328269s
Dec 14 16:31:27.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012912639s
Dec 14 16:31:29.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014492395s
Dec 14 16:31:31.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016605287s
Dec 14 16:31:33.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014829894s
Dec 14 16:31:35.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015901687s
Dec 14 16:31:37.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015140775s
Dec 14 16:31:39.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015199121s
Dec 14 16:31:41.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016249972s
Dec 14 16:31:43.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013152365s
Dec 14 16:31:45.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016175719s
Dec 14 16:31:45.476: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022440051s
STEP: updating the pod 12/14/22 16:31:45.477
Dec 14 16:31:46.004: INFO: Successfully updated pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad"
STEP: waiting for pod running 12/14/22 16:31:46.004
Dec 14 16:31:46.004: INFO: Waiting up to 2m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625" to be "running"
Dec 14 16:31:46.011: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681484ms
Dec 14 16:31:48.019: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.015433247s
Dec 14 16:31:48.020: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" satisfied condition "running"
STEP: deleting the pod gracefully 12/14/22 16:31:48.02
Dec 14 16:31:48.020: INFO: Deleting pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625"
Dec 14 16:31:48.031: INFO: Wait up to 5m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5625" for this suite. 12/14/22 16:32:20.056
------------------------------
• [SLOW TEST] [154.664 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:29:45.407
    Dec 14 16:29:45.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename var-expansion 12/14/22 16:29:45.41
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:29:45.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:29:45.434
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 12/14/22 16:29:45.439
    Dec 14 16:29:45.454: INFO: Waiting up to 2m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625" to be "running"
    Dec 14 16:29:45.458: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161259ms
    Dec 14 16:29:47.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016174564s
    Dec 14 16:29:49.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014825132s
    Dec 14 16:29:51.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013187319s
    Dec 14 16:29:53.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015987278s
    Dec 14 16:29:55.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013074173s
    Dec 14 16:29:57.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016833073s
    Dec 14 16:29:59.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013686343s
    Dec 14 16:30:01.477: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022858844s
    Dec 14 16:30:03.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013623629s
    Dec 14 16:30:05.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016023098s
    Dec 14 16:30:07.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014477755s
    Dec 14 16:30:09.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014266717s
    Dec 14 16:30:11.464: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010410857s
    Dec 14 16:30:13.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 28.016678561s
    Dec 14 16:30:15.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016328324s
    Dec 14 16:30:17.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012123103s
    Dec 14 16:30:19.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014153225s
    Dec 14 16:30:21.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012255513s
    Dec 14 16:30:23.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016418457s
    Dec 14 16:30:25.477: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023154925s
    Dec 14 16:30:27.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014227325s
    Dec 14 16:30:29.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015554077s
    Dec 14 16:30:31.472: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018411574s
    Dec 14 16:30:33.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013708929s
    Dec 14 16:30:35.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016344232s
    Dec 14 16:30:37.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012806147s
    Dec 14 16:30:39.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014373553s
    Dec 14 16:30:41.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015580926s
    Dec 14 16:30:43.473: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018623846s
    Dec 14 16:30:45.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014081718s
    Dec 14 16:30:47.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014470957s
    Dec 14 16:30:49.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013919826s
    Dec 14 16:30:51.465: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011308127s
    Dec 14 16:30:53.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013733203s
    Dec 14 16:30:55.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01240905s
    Dec 14 16:30:57.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.015082694s
    Dec 14 16:30:59.468: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014440478s
    Dec 14 16:31:01.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016941283s
    Dec 14 16:31:03.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01709332s
    Dec 14 16:31:05.475: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020757706s
    Dec 14 16:31:07.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01728768s
    Dec 14 16:31:09.472: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017889951s
    Dec 14 16:31:11.473: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01914616s
    Dec 14 16:31:13.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013335292s
    Dec 14 16:31:15.466: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012242578s
    Dec 14 16:31:17.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017009276s
    Dec 14 16:31:19.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017200384s
    Dec 14 16:31:21.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.016989649s
    Dec 14 16:31:23.484: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.030275803s
    Dec 14 16:31:25.464: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010328269s
    Dec 14 16:31:27.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012912639s
    Dec 14 16:31:29.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014492395s
    Dec 14 16:31:31.471: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016605287s
    Dec 14 16:31:33.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014829894s
    Dec 14 16:31:35.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015901687s
    Dec 14 16:31:37.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015140775s
    Dec 14 16:31:39.469: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015199121s
    Dec 14 16:31:41.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016249972s
    Dec 14 16:31:43.467: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013152365s
    Dec 14 16:31:45.470: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016175719s
    Dec 14 16:31:45.476: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022440051s
    STEP: updating the pod 12/14/22 16:31:45.477
    Dec 14 16:31:46.004: INFO: Successfully updated pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad"
    STEP: waiting for pod running 12/14/22 16:31:46.004
    Dec 14 16:31:46.004: INFO: Waiting up to 2m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625" to be "running"
    Dec 14 16:31:46.011: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681484ms
    Dec 14 16:31:48.019: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.015433247s
    Dec 14 16:31:48.020: INFO: Pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" satisfied condition "running"
    STEP: deleting the pod gracefully 12/14/22 16:31:48.02
    Dec 14 16:31:48.020: INFO: Deleting pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" in namespace "var-expansion-5625"
    Dec 14 16:31:48.031: INFO: Wait up to 5m0s for pod "var-expansion-959cee68-2b0f-4292-8a8b-3ad5885c52ad" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5625" for this suite. 12/14/22 16:32:20.056
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:20.075
Dec 14 16:32:20.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename csiinlinevolumes 12/14/22 16:32:20.08
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:20.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:20.107
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 12/14/22 16:32:20.111
STEP: getting 12/14/22 16:32:20.134
STEP: listing 12/14/22 16:32:20.143
STEP: deleting 12/14/22 16:32:20.149
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4154" for this suite. 12/14/22 16:32:20.188
------------------------------
• [0.123 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:20.075
    Dec 14 16:32:20.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename csiinlinevolumes 12/14/22 16:32:20.08
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:20.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:20.107
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 12/14/22 16:32:20.111
    STEP: getting 12/14/22 16:32:20.134
    STEP: listing 12/14/22 16:32:20.143
    STEP: deleting 12/14/22 16:32:20.149
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4154" for this suite. 12/14/22 16:32:20.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:20.199
Dec 14 16:32:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-runtime 12/14/22 16:32:20.202
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:20.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:20.231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 12/14/22 16:32:20.234
STEP: wait for the container to reach Succeeded 12/14/22 16:32:20.249
STEP: get the container status 12/14/22 16:32:24.286
STEP: the container should be terminated 12/14/22 16:32:24.292
STEP: the termination message should be set 12/14/22 16:32:24.292
Dec 14 16:32:24.292: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/14/22 16:32:24.292
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:24.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1510" for this suite. 12/14/22 16:32:24.332
------------------------------
• [4.145 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:20.199
    Dec 14 16:32:20.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-runtime 12/14/22 16:32:20.202
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:20.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:20.231
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 12/14/22 16:32:20.234
    STEP: wait for the container to reach Succeeded 12/14/22 16:32:20.249
    STEP: get the container status 12/14/22 16:32:24.286
    STEP: the container should be terminated 12/14/22 16:32:24.292
    STEP: the termination message should be set 12/14/22 16:32:24.292
    Dec 14 16:32:24.292: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/14/22 16:32:24.292
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:24.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1510" for this suite. 12/14/22 16:32:24.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:24.35
Dec 14 16:32:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename proxy 12/14/22 16:32:24.356
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:24.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:24.382
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Dec 14 16:32:24.386: INFO: Creating pod...
Dec 14 16:32:24.406: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5227" to be "running"
Dec 14 16:32:24.410: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370455ms
Dec 14 16:32:26.420: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014524248s
Dec 14 16:32:26.421: INFO: Pod "agnhost" satisfied condition "running"
Dec 14 16:32:26.421: INFO: Creating service...
Dec 14 16:32:26.444: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=DELETE
Dec 14 16:32:26.456: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:32:26.456: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=OPTIONS
Dec 14 16:32:26.462: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:32:26.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=PATCH
Dec 14 16:32:26.468: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:32:26.468: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=POST
Dec 14 16:32:26.474: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:32:26.474: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=PUT
Dec 14 16:32:26.477: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 16:32:26.477: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=DELETE
Dec 14 16:32:26.483: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:32:26.483: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec 14 16:32:26.491: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:32:26.491: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=PATCH
Dec 14 16:32:26.499: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:32:26.499: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=POST
Dec 14 16:32:26.505: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:32:26.505: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=PUT
Dec 14 16:32:26.511: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 16:32:26.511: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=GET
Dec 14 16:32:26.515: INFO: http.Client request:GET StatusCode:301
Dec 14 16:32:26.515: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=GET
Dec 14 16:32:26.525: INFO: http.Client request:GET StatusCode:301
Dec 14 16:32:26.525: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=HEAD
Dec 14 16:32:26.538: INFO: http.Client request:HEAD StatusCode:301
Dec 14 16:32:26.538: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=HEAD
Dec 14 16:32:26.547: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5227" for this suite. 12/14/22 16:32:26.555
------------------------------
• [2.217 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:24.35
    Dec 14 16:32:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename proxy 12/14/22 16:32:24.356
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:24.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:24.382
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Dec 14 16:32:24.386: INFO: Creating pod...
    Dec 14 16:32:24.406: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5227" to be "running"
    Dec 14 16:32:24.410: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370455ms
    Dec 14 16:32:26.420: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014524248s
    Dec 14 16:32:26.421: INFO: Pod "agnhost" satisfied condition "running"
    Dec 14 16:32:26.421: INFO: Creating service...
    Dec 14 16:32:26.444: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=DELETE
    Dec 14 16:32:26.456: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 16:32:26.456: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=OPTIONS
    Dec 14 16:32:26.462: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 16:32:26.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=PATCH
    Dec 14 16:32:26.468: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 16:32:26.468: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=POST
    Dec 14 16:32:26.474: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 16:32:26.474: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=PUT
    Dec 14 16:32:26.477: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 16:32:26.477: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=DELETE
    Dec 14 16:32:26.483: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 14 16:32:26.483: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Dec 14 16:32:26.491: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 14 16:32:26.491: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=PATCH
    Dec 14 16:32:26.499: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 14 16:32:26.499: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=POST
    Dec 14 16:32:26.505: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 14 16:32:26.505: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=PUT
    Dec 14 16:32:26.511: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 14 16:32:26.511: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=GET
    Dec 14 16:32:26.515: INFO: http.Client request:GET StatusCode:301
    Dec 14 16:32:26.515: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=GET
    Dec 14 16:32:26.525: INFO: http.Client request:GET StatusCode:301
    Dec 14 16:32:26.525: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/pods/agnhost/proxy?method=HEAD
    Dec 14 16:32:26.538: INFO: http.Client request:HEAD StatusCode:301
    Dec 14 16:32:26.538: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5227/services/e2e-proxy-test-service/proxy?method=HEAD
    Dec 14 16:32:26.547: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5227" for this suite. 12/14/22 16:32:26.555
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:26.567
Dec 14 16:32:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:32:26.571
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:26.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:26.644
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3174 12/14/22 16:32:26.647
STEP: creating service affinity-clusterip-transition in namespace services-3174 12/14/22 16:32:26.647
STEP: creating replication controller affinity-clusterip-transition in namespace services-3174 12/14/22 16:32:26.661
I1214 16:32:26.673124      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3174, replica count: 3
I1214 16:32:29.725831      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:32:29.735: INFO: Creating new exec pod
Dec 14 16:32:29.746: INFO: Waiting up to 5m0s for pod "execpod-affinityz8nrb" in namespace "services-3174" to be "running"
Dec 14 16:32:29.751: INFO: Pod "execpod-affinityz8nrb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840389ms
Dec 14 16:32:31.760: INFO: Pod "execpod-affinityz8nrb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014732132s
Dec 14 16:32:31.760: INFO: Pod "execpod-affinityz8nrb" satisfied condition "running"
Dec 14 16:32:32.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Dec 14 16:32:33.036: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 14 16:32:33.037: INFO: stdout: ""
Dec 14 16:32:33.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c nc -v -z -w 2 10.233.8.24 80'
Dec 14 16:32:33.282: INFO: stderr: "+ nc -v -z -w 2 10.233.8.24 80\nConnection to 10.233.8.24 80 port [tcp/http] succeeded!\n"
Dec 14 16:32:33.282: INFO: stdout: ""
Dec 14 16:32:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.24:80/ ; done'
Dec 14 16:32:33.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n"
Dec 14 16:32:33.751: INFO: stdout: "\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-mf2w8"
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
Dec 14 16:32:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.24:80/ ; done'
Dec 14 16:32:34.519: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n"
Dec 14 16:32:34.519: INFO: stdout: "\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z"
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
Dec 14 16:32:34.519: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3174, will wait for the garbage collector to delete the pods 12/14/22 16:32:34.579
Dec 14 16:32:34.648: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.600013ms
Dec 14 16:32:34.749: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.67099ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:37.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3174" for this suite. 12/14/22 16:32:37.091
------------------------------
• [SLOW TEST] [10.534 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:26.567
    Dec 14 16:32:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:32:26.571
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:26.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:26.644
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3174 12/14/22 16:32:26.647
    STEP: creating service affinity-clusterip-transition in namespace services-3174 12/14/22 16:32:26.647
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3174 12/14/22 16:32:26.661
    I1214 16:32:26.673124      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3174, replica count: 3
    I1214 16:32:29.725831      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:32:29.735: INFO: Creating new exec pod
    Dec 14 16:32:29.746: INFO: Waiting up to 5m0s for pod "execpod-affinityz8nrb" in namespace "services-3174" to be "running"
    Dec 14 16:32:29.751: INFO: Pod "execpod-affinityz8nrb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840389ms
    Dec 14 16:32:31.760: INFO: Pod "execpod-affinityz8nrb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014732132s
    Dec 14 16:32:31.760: INFO: Pod "execpod-affinityz8nrb" satisfied condition "running"
    Dec 14 16:32:32.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Dec 14 16:32:33.036: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Dec 14 16:32:33.037: INFO: stdout: ""
    Dec 14 16:32:33.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c nc -v -z -w 2 10.233.8.24 80'
    Dec 14 16:32:33.282: INFO: stderr: "+ nc -v -z -w 2 10.233.8.24 80\nConnection to 10.233.8.24 80 port [tcp/http] succeeded!\n"
    Dec 14 16:32:33.282: INFO: stdout: ""
    Dec 14 16:32:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.24:80/ ; done'
    Dec 14 16:32:33.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n"
    Dec 14 16:32:33.751: INFO: stdout: "\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-5qxb2\naffinity-clusterip-transition-mf2w8\naffinity-clusterip-transition-mf2w8"
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-5qxb2
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.752: INFO: Received response from host: affinity-clusterip-transition-mf2w8
    Dec 14 16:32:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-3174 exec execpod-affinityz8nrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.24:80/ ; done'
    Dec 14 16:32:34.519: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.24:80/\n"
    Dec 14 16:32:34.519: INFO: stdout: "\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z\naffinity-clusterip-transition-gm62z"
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Received response from host: affinity-clusterip-transition-gm62z
    Dec 14 16:32:34.519: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3174, will wait for the garbage collector to delete the pods 12/14/22 16:32:34.579
    Dec 14 16:32:34.648: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.600013ms
    Dec 14 16:32:34.749: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.67099ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:37.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3174" for this suite. 12/14/22 16:32:37.091
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:37.103
Dec 14 16:32:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:32:37.106
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:37.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:37.129
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Dec 14 16:32:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:32:39.63
Dec 14 16:32:39.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 create -f -'
Dec 14 16:32:41.023: INFO: stderr: ""
Dec 14 16:32:41.023: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 16:32:41.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 delete e2e-test-crd-publish-openapi-5339-crds test-cr'
Dec 14 16:32:41.155: INFO: stderr: ""
Dec 14 16:32:41.155: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 16:32:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 apply -f -'
Dec 14 16:32:41.495: INFO: stderr: ""
Dec 14 16:32:41.495: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 16:32:41.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 delete e2e-test-crd-publish-openapi-5339-crds test-cr'
Dec 14 16:32:41.728: INFO: stderr: ""
Dec 14 16:32:41.728: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/14/22 16:32:41.728
Dec 14 16:32:41.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 explain e2e-test-crd-publish-openapi-5339-crds'
Dec 14 16:32:42.111: INFO: stderr: ""
Dec 14 16:32:42.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5339-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:44.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2964" for this suite. 12/14/22 16:32:44.668
------------------------------
• [SLOW TEST] [7.578 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:37.103
    Dec 14 16:32:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:32:37.106
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:37.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:37.129
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Dec 14 16:32:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:32:39.63
    Dec 14 16:32:39.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 create -f -'
    Dec 14 16:32:41.023: INFO: stderr: ""
    Dec 14 16:32:41.023: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 14 16:32:41.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 delete e2e-test-crd-publish-openapi-5339-crds test-cr'
    Dec 14 16:32:41.155: INFO: stderr: ""
    Dec 14 16:32:41.155: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Dec 14 16:32:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 apply -f -'
    Dec 14 16:32:41.495: INFO: stderr: ""
    Dec 14 16:32:41.495: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 14 16:32:41.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 --namespace=crd-publish-openapi-2964 delete e2e-test-crd-publish-openapi-5339-crds test-cr'
    Dec 14 16:32:41.728: INFO: stderr: ""
    Dec 14 16:32:41.728: INFO: stdout: "e2e-test-crd-publish-openapi-5339-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/14/22 16:32:41.728
    Dec 14 16:32:41.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-2964 explain e2e-test-crd-publish-openapi-5339-crds'
    Dec 14 16:32:42.111: INFO: stderr: ""
    Dec 14 16:32:42.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5339-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:44.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2964" for this suite. 12/14/22 16:32:44.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:44.692
Dec 14 16:32:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 16:32:44.693
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:44.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:44.737
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 12/14/22 16:32:44.769
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:32:44.776
Dec 14 16:32:44.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:32:44.787: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:32:45.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:32:45.800: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 16:32:46.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:32:46.806: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/14/22 16:32:46.811
Dec 14 16:32:46.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:32:46.848: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 16:32:47.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:32:47.859: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 16:32:48.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:32:48.863: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/14/22 16:32:48.864
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:32:48.875
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-295, will wait for the garbage collector to delete the pods 12/14/22 16:32:48.875
Dec 14 16:32:48.946: INFO: Deleting DaemonSet.extensions daemon-set took: 15.364215ms
Dec 14 16:32:49.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.611189ms
Dec 14 16:32:51.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:32:51.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:32:51.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18617"},"items":null}

Dec 14 16:32:51.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18617"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-295" for this suite. 12/14/22 16:32:51.39
------------------------------
• [SLOW TEST] [6.721 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:44.692
    Dec 14 16:32:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 16:32:44.693
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:44.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:44.737
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 12/14/22 16:32:44.769
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:32:44.776
    Dec 14 16:32:44.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:32:44.787: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:32:45.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:32:45.800: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 16:32:46.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:32:46.806: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/14/22 16:32:46.811
    Dec 14 16:32:46.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:32:46.848: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 16:32:47.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:32:47.859: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 16:32:48.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:32:48.863: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/14/22 16:32:48.864
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:32:48.875
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-295, will wait for the garbage collector to delete the pods 12/14/22 16:32:48.875
    Dec 14 16:32:48.946: INFO: Deleting DaemonSet.extensions daemon-set took: 15.364215ms
    Dec 14 16:32:49.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.611189ms
    Dec 14 16:32:51.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:32:51.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 16:32:51.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18617"},"items":null}

    Dec 14 16:32:51.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18617"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-295" for this suite. 12/14/22 16:32:51.39
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:51.416
Dec 14 16:32:51.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 16:32:51.42
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:51.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:51.45
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 12/14/22 16:32:51.455
Dec 14 16:32:51.471: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6703" to be "running and ready"
Dec 14 16:32:51.478: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994884ms
Dec 14 16:32:51.478: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:32:53.485: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.014338183s
Dec 14 16:32:53.485: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Dec 14 16:32:53.485: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 12/14/22 16:32:53.49
STEP: Then the orphan pod is adopted 12/14/22 16:32:53.508
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6703" for this suite. 12/14/22 16:32:54.543
------------------------------
• [3.165 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:51.416
    Dec 14 16:32:51.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 16:32:51.42
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:51.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:51.45
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 12/14/22 16:32:51.455
    Dec 14 16:32:51.471: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6703" to be "running and ready"
    Dec 14 16:32:51.478: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994884ms
    Dec 14 16:32:51.478: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:32:53.485: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.014338183s
    Dec 14 16:32:53.485: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Dec 14 16:32:53.485: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 12/14/22 16:32:53.49
    STEP: Then the orphan pod is adopted 12/14/22 16:32:53.508
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6703" for this suite. 12/14/22 16:32:54.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:54.591
Dec 14 16:32:54.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename server-version 12/14/22 16:32:54.598
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:54.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:54.632
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 12/14/22 16:32:54.642
STEP: Confirm major version 12/14/22 16:32:54.644
Dec 14 16:32:54.645: INFO: Major version: 1
STEP: Confirm minor version 12/14/22 16:32:54.645
Dec 14 16:32:54.645: INFO: cleanMinorVersion: 26
Dec 14 16:32:54.645: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5743" for this suite. 12/14/22 16:32:54.656
------------------------------
• [0.074 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:54.591
    Dec 14 16:32:54.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename server-version 12/14/22 16:32:54.598
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:54.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:54.632
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 12/14/22 16:32:54.642
    STEP: Confirm major version 12/14/22 16:32:54.644
    Dec 14 16:32:54.645: INFO: Major version: 1
    STEP: Confirm minor version 12/14/22 16:32:54.645
    Dec 14 16:32:54.645: INFO: cleanMinorVersion: 26
    Dec 14 16:32:54.645: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5743" for this suite. 12/14/22 16:32:54.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:54.671
Dec 14 16:32:54.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:32:54.673
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:54.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:54.706
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-ebd700e2-8632-48e9-b2c0-61195c05951c 12/14/22 16:32:54.728
STEP: Creating the pod 12/14/22 16:32:54.735
Dec 14 16:32:54.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670" in namespace "configmap-2595" to be "running"
Dec 14 16:32:54.749: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670": Phase="Pending", Reason="", readiness=false. Elapsed: 3.744529ms
Dec 14 16:32:56.755: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670": Phase="Running", Reason="", readiness=false. Elapsed: 2.009826995s
Dec 14 16:32:56.755: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670" satisfied condition "running"
STEP: Waiting for pod with text data 12/14/22 16:32:56.755
STEP: Waiting for pod with binary data 12/14/22 16:32:56.776
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:56.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2595" for this suite. 12/14/22 16:32:56.795
------------------------------
• [2.133 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:54.671
    Dec 14 16:32:54.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:32:54.673
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:54.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:54.706
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-ebd700e2-8632-48e9-b2c0-61195c05951c 12/14/22 16:32:54.728
    STEP: Creating the pod 12/14/22 16:32:54.735
    Dec 14 16:32:54.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670" in namespace "configmap-2595" to be "running"
    Dec 14 16:32:54.749: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670": Phase="Pending", Reason="", readiness=false. Elapsed: 3.744529ms
    Dec 14 16:32:56.755: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670": Phase="Running", Reason="", readiness=false. Elapsed: 2.009826995s
    Dec 14 16:32:56.755: INFO: Pod "pod-configmaps-702a32e6-5401-431a-9645-720ee1405670" satisfied condition "running"
    STEP: Waiting for pod with text data 12/14/22 16:32:56.755
    STEP: Waiting for pod with binary data 12/14/22 16:32:56.776
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:56.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2595" for this suite. 12/14/22 16:32:56.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:56.805
Dec 14 16:32:56.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename ingressclass 12/14/22 16:32:56.807
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:56.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:56.836
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 12/14/22 16:32:56.839
STEP: getting /apis/networking.k8s.io 12/14/22 16:32:56.842
STEP: getting /apis/networking.k8s.iov1 12/14/22 16:32:56.843
STEP: creating 12/14/22 16:32:56.844
STEP: getting 12/14/22 16:32:56.868
STEP: listing 12/14/22 16:32:56.871
STEP: watching 12/14/22 16:32:56.874
Dec 14 16:32:56.874: INFO: starting watch
STEP: patching 12/14/22 16:32:56.876
STEP: updating 12/14/22 16:32:56.889
Dec 14 16:32:56.900: INFO: waiting for watch events with expected annotations
Dec 14 16:32:56.900: INFO: saw patched and updated annotations
STEP: deleting 12/14/22 16:32:56.9
STEP: deleting a collection 12/14/22 16:32:56.916
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:32:56.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-6931" for this suite. 12/14/22 16:32:56.945
------------------------------
• [0.148 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:56.805
    Dec 14 16:32:56.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename ingressclass 12/14/22 16:32:56.807
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:56.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:56.836
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 12/14/22 16:32:56.839
    STEP: getting /apis/networking.k8s.io 12/14/22 16:32:56.842
    STEP: getting /apis/networking.k8s.iov1 12/14/22 16:32:56.843
    STEP: creating 12/14/22 16:32:56.844
    STEP: getting 12/14/22 16:32:56.868
    STEP: listing 12/14/22 16:32:56.871
    STEP: watching 12/14/22 16:32:56.874
    Dec 14 16:32:56.874: INFO: starting watch
    STEP: patching 12/14/22 16:32:56.876
    STEP: updating 12/14/22 16:32:56.889
    Dec 14 16:32:56.900: INFO: waiting for watch events with expected annotations
    Dec 14 16:32:56.900: INFO: saw patched and updated annotations
    STEP: deleting 12/14/22 16:32:56.9
    STEP: deleting a collection 12/14/22 16:32:56.916
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:32:56.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-6931" for this suite. 12/14/22 16:32:56.945
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:32:56.958
Dec 14 16:32:56.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:32:56.96
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:56.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:56.99
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 12/14/22 16:32:57.001
STEP: create the rc2 12/14/22 16:32:57.008
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/14/22 16:33:02.024
STEP: delete the rc simpletest-rc-to-be-deleted 12/14/22 16:33:03.736
STEP: wait for the rc to be deleted 12/14/22 16:33:03.759
Dec 14 16:33:08.805: INFO: 66 pods remaining
Dec 14 16:33:08.806: INFO: 66 pods has nil DeletionTimestamp
Dec 14 16:33:08.806: INFO: 
STEP: Gathering metrics 12/14/22 16:33:13.79
Dec 14 16:33:14.286: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 16:33:14.293: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.688537ms
Dec 14 16:33:14.293: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 16:33:14.293: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 16:33:14.826: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 16:33:14.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-226lm" in namespace "gc-4417"
Dec 14 16:33:14.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-27j2f" in namespace "gc-4417"
Dec 14 16:33:14.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g9wg" in namespace "gc-4417"
Dec 14 16:33:14.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m6dw" in namespace "gc-4417"
Dec 14 16:33:14.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-4764t" in namespace "gc-4417"
Dec 14 16:33:14.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spsc" in namespace "gc-4417"
Dec 14 16:33:14.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tffs" in namespace "gc-4417"
Dec 14 16:33:15.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vmf4" in namespace "gc-4417"
Dec 14 16:33:15.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-54szw" in namespace "gc-4417"
Dec 14 16:33:15.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c7qs" in namespace "gc-4417"
Dec 14 16:33:15.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d64t" in namespace "gc-4417"
Dec 14 16:33:15.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jst7" in namespace "gc-4417"
Dec 14 16:33:15.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k7jg" in namespace "gc-4417"
Dec 14 16:33:15.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m5s5" in namespace "gc-4417"
Dec 14 16:33:15.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nt5g" in namespace "gc-4417"
Dec 14 16:33:15.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-68fq5" in namespace "gc-4417"
Dec 14 16:33:15.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v9v9" in namespace "gc-4417"
Dec 14 16:33:15.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zhlr" in namespace "gc-4417"
Dec 14 16:33:15.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-7blpx" in namespace "gc-4417"
Dec 14 16:33:15.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gdcp" in namespace "gc-4417"
Dec 14 16:33:15.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-85hxx" in namespace "gc-4417"
Dec 14 16:33:15.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-89pnq" in namespace "gc-4417"
Dec 14 16:33:15.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kqfx" in namespace "gc-4417"
Dec 14 16:33:15.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mkg6" in namespace "gc-4417"
Dec 14 16:33:15.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rpdm" in namespace "gc-4417"
Dec 14 16:33:15.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v28v" in namespace "gc-4417"
Dec 14 16:33:16.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-94cgr" in namespace "gc-4417"
Dec 14 16:33:16.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-9526p" in namespace "gc-4417"
Dec 14 16:33:16.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-96rtn" in namespace "gc-4417"
Dec 14 16:33:16.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lwxg" in namespace "gc-4417"
Dec 14 16:33:16.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9br9" in namespace "gc-4417"
Dec 14 16:33:16.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt6ht" in namespace "gc-4417"
Dec 14 16:33:16.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-c58pk" in namespace "gc-4417"
Dec 14 16:33:16.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw7vx" in namespace "gc-4417"
Dec 14 16:33:16.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz2s5" in namespace "gc-4417"
Dec 14 16:33:16.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-czp4r" in namespace "gc-4417"
Dec 14 16:33:16.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5nd8" in namespace "gc-4417"
Dec 14 16:33:16.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddcvs" in namespace "gc-4417"
Dec 14 16:33:16.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh2lw" in namespace "gc-4417"
Dec 14 16:33:16.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlz8c" in namespace "gc-4417"
Dec 14 16:33:16.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvl65" in namespace "gc-4417"
Dec 14 16:33:16.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvnhz" in namespace "gc-4417"
Dec 14 16:33:16.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwztl" in namespace "gc-4417"
Dec 14 16:33:16.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxfks" in namespace "gc-4417"
Dec 14 16:33:16.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz8vd" in namespace "gc-4417"
Dec 14 16:33:16.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdr6z" in namespace "gc-4417"
Dec 14 16:33:16.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsff4" in namespace "gc-4417"
Dec 14 16:33:16.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5ldk" in namespace "gc-4417"
Dec 14 16:33:16.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6x7d" in namespace "gc-4417"
Dec 14 16:33:16.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdjcn" in namespace "gc-4417"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:33:16.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4417" for this suite. 12/14/22 16:33:16.895
------------------------------
• [SLOW TEST] [19.950 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:32:56.958
    Dec 14 16:32:56.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:32:56.96
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:32:56.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:32:56.99
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 12/14/22 16:32:57.001
    STEP: create the rc2 12/14/22 16:32:57.008
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/14/22 16:33:02.024
    STEP: delete the rc simpletest-rc-to-be-deleted 12/14/22 16:33:03.736
    STEP: wait for the rc to be deleted 12/14/22 16:33:03.759
    Dec 14 16:33:08.805: INFO: 66 pods remaining
    Dec 14 16:33:08.806: INFO: 66 pods has nil DeletionTimestamp
    Dec 14 16:33:08.806: INFO: 
    STEP: Gathering metrics 12/14/22 16:33:13.79
    Dec 14 16:33:14.286: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 16:33:14.293: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.688537ms
    Dec 14 16:33:14.293: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 16:33:14.293: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 16:33:14.826: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 14 16:33:14.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-226lm" in namespace "gc-4417"
    Dec 14 16:33:14.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-27j2f" in namespace "gc-4417"
    Dec 14 16:33:14.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g9wg" in namespace "gc-4417"
    Dec 14 16:33:14.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m6dw" in namespace "gc-4417"
    Dec 14 16:33:14.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-4764t" in namespace "gc-4417"
    Dec 14 16:33:14.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spsc" in namespace "gc-4417"
    Dec 14 16:33:14.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tffs" in namespace "gc-4417"
    Dec 14 16:33:15.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vmf4" in namespace "gc-4417"
    Dec 14 16:33:15.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-54szw" in namespace "gc-4417"
    Dec 14 16:33:15.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c7qs" in namespace "gc-4417"
    Dec 14 16:33:15.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d64t" in namespace "gc-4417"
    Dec 14 16:33:15.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jst7" in namespace "gc-4417"
    Dec 14 16:33:15.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k7jg" in namespace "gc-4417"
    Dec 14 16:33:15.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m5s5" in namespace "gc-4417"
    Dec 14 16:33:15.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nt5g" in namespace "gc-4417"
    Dec 14 16:33:15.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-68fq5" in namespace "gc-4417"
    Dec 14 16:33:15.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v9v9" in namespace "gc-4417"
    Dec 14 16:33:15.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zhlr" in namespace "gc-4417"
    Dec 14 16:33:15.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-7blpx" in namespace "gc-4417"
    Dec 14 16:33:15.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gdcp" in namespace "gc-4417"
    Dec 14 16:33:15.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-85hxx" in namespace "gc-4417"
    Dec 14 16:33:15.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-89pnq" in namespace "gc-4417"
    Dec 14 16:33:15.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kqfx" in namespace "gc-4417"
    Dec 14 16:33:15.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mkg6" in namespace "gc-4417"
    Dec 14 16:33:15.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rpdm" in namespace "gc-4417"
    Dec 14 16:33:15.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v28v" in namespace "gc-4417"
    Dec 14 16:33:16.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-94cgr" in namespace "gc-4417"
    Dec 14 16:33:16.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-9526p" in namespace "gc-4417"
    Dec 14 16:33:16.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-96rtn" in namespace "gc-4417"
    Dec 14 16:33:16.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lwxg" in namespace "gc-4417"
    Dec 14 16:33:16.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9br9" in namespace "gc-4417"
    Dec 14 16:33:16.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt6ht" in namespace "gc-4417"
    Dec 14 16:33:16.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-c58pk" in namespace "gc-4417"
    Dec 14 16:33:16.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw7vx" in namespace "gc-4417"
    Dec 14 16:33:16.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz2s5" in namespace "gc-4417"
    Dec 14 16:33:16.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-czp4r" in namespace "gc-4417"
    Dec 14 16:33:16.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5nd8" in namespace "gc-4417"
    Dec 14 16:33:16.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddcvs" in namespace "gc-4417"
    Dec 14 16:33:16.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh2lw" in namespace "gc-4417"
    Dec 14 16:33:16.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlz8c" in namespace "gc-4417"
    Dec 14 16:33:16.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvl65" in namespace "gc-4417"
    Dec 14 16:33:16.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvnhz" in namespace "gc-4417"
    Dec 14 16:33:16.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwztl" in namespace "gc-4417"
    Dec 14 16:33:16.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxfks" in namespace "gc-4417"
    Dec 14 16:33:16.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz8vd" in namespace "gc-4417"
    Dec 14 16:33:16.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdr6z" in namespace "gc-4417"
    Dec 14 16:33:16.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsff4" in namespace "gc-4417"
    Dec 14 16:33:16.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5ldk" in namespace "gc-4417"
    Dec 14 16:33:16.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6x7d" in namespace "gc-4417"
    Dec 14 16:33:16.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdjcn" in namespace "gc-4417"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:33:16.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4417" for this suite. 12/14/22 16:33:16.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:33:16.915
Dec 14 16:33:16.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:33:16.918
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:16.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:16.966
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-5604 12/14/22 16:33:16.969
STEP: creating replication controller nodeport-test in namespace services-5604 12/14/22 16:33:17.057
I1214 16:33:17.093186      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5604, replica count: 2
I1214 16:33:20.147140      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 16:33:23.147745      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:33:23.147: INFO: Creating new exec pod
Dec 14 16:33:23.167: INFO: Waiting up to 5m0s for pod "execpodfgrzc" in namespace "services-5604" to be "running"
Dec 14 16:33:23.174: INFO: Pod "execpodfgrzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.188621ms
Dec 14 16:33:25.187: INFO: Pod "execpodfgrzc": Phase="Running", Reason="", readiness=true. Elapsed: 2.020513921s
Dec 14 16:33:25.188: INFO: Pod "execpodfgrzc" satisfied condition "running"
Dec 14 16:33:26.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Dec 14 16:33:26.586: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 16:33:26.586: INFO: stdout: ""
Dec 14 16:33:26.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 10.233.3.130 80'
Dec 14 16:33:26.884: INFO: stderr: "+ nc -v -z -w 2 10.233.3.130 80\nConnection to 10.233.3.130 80 port [tcp/http] succeeded!\n"
Dec 14 16:33:26.884: INFO: stdout: ""
Dec 14 16:33:26.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31489'
Dec 14 16:33:27.103: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31489\nConnection to 192.168.121.16 31489 port [tcp/*] succeeded!\n"
Dec 14 16:33:27.103: INFO: stdout: ""
Dec 14 16:33:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 192.168.121.21 31489'
Dec 14 16:33:27.298: INFO: stderr: "+ nc -v -z -w 2 192.168.121.21 31489\nConnection to 192.168.121.21 31489 port [tcp/*] succeeded!\n"
Dec 14 16:33:27.298: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:33:27.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5604" for this suite. 12/14/22 16:33:27.305
------------------------------
• [SLOW TEST] [10.401 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:33:16.915
    Dec 14 16:33:16.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:33:16.918
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:16.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:16.966
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-5604 12/14/22 16:33:16.969
    STEP: creating replication controller nodeport-test in namespace services-5604 12/14/22 16:33:17.057
    I1214 16:33:17.093186      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5604, replica count: 2
    I1214 16:33:20.147140      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 16:33:23.147745      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:33:23.147: INFO: Creating new exec pod
    Dec 14 16:33:23.167: INFO: Waiting up to 5m0s for pod "execpodfgrzc" in namespace "services-5604" to be "running"
    Dec 14 16:33:23.174: INFO: Pod "execpodfgrzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.188621ms
    Dec 14 16:33:25.187: INFO: Pod "execpodfgrzc": Phase="Running", Reason="", readiness=true. Elapsed: 2.020513921s
    Dec 14 16:33:25.188: INFO: Pod "execpodfgrzc" satisfied condition "running"
    Dec 14 16:33:26.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Dec 14 16:33:26.586: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Dec 14 16:33:26.586: INFO: stdout: ""
    Dec 14 16:33:26.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 10.233.3.130 80'
    Dec 14 16:33:26.884: INFO: stderr: "+ nc -v -z -w 2 10.233.3.130 80\nConnection to 10.233.3.130 80 port [tcp/http] succeeded!\n"
    Dec 14 16:33:26.884: INFO: stdout: ""
    Dec 14 16:33:26.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31489'
    Dec 14 16:33:27.103: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31489\nConnection to 192.168.121.16 31489 port [tcp/*] succeeded!\n"
    Dec 14 16:33:27.103: INFO: stdout: ""
    Dec 14 16:33:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5604 exec execpodfgrzc -- /bin/sh -x -c nc -v -z -w 2 192.168.121.21 31489'
    Dec 14 16:33:27.298: INFO: stderr: "+ nc -v -z -w 2 192.168.121.21 31489\nConnection to 192.168.121.21 31489 port [tcp/*] succeeded!\n"
    Dec 14 16:33:27.298: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:33:27.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5604" for this suite. 12/14/22 16:33:27.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:33:27.317
Dec 14 16:33:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:33:27.32
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:27.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:27.348
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 12/14/22 16:33:27.352
Dec 14 16:33:27.366: INFO: Waiting up to 5m0s for pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862" in namespace "emptydir-3876" to be "Succeeded or Failed"
Dec 14 16:33:27.371: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342729ms
Dec 14 16:33:29.379: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012938568s
Dec 14 16:33:31.380: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013445612s
STEP: Saw pod success 12/14/22 16:33:31.38
Dec 14 16:33:31.381: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862" satisfied condition "Succeeded or Failed"
Dec 14 16:33:31.385: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 container test-container: <nil>
STEP: delete the pod 12/14/22 16:33:31.394
Dec 14 16:33:31.412: INFO: Waiting for pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 to disappear
Dec 14 16:33:31.417: INFO: Pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:33:31.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3876" for this suite. 12/14/22 16:33:31.426
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:33:27.317
    Dec 14 16:33:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:33:27.32
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:27.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:27.348
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 12/14/22 16:33:27.352
    Dec 14 16:33:27.366: INFO: Waiting up to 5m0s for pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862" in namespace "emptydir-3876" to be "Succeeded or Failed"
    Dec 14 16:33:27.371: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342729ms
    Dec 14 16:33:29.379: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012938568s
    Dec 14 16:33:31.380: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013445612s
    STEP: Saw pod success 12/14/22 16:33:31.38
    Dec 14 16:33:31.381: INFO: Pod "pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862" satisfied condition "Succeeded or Failed"
    Dec 14 16:33:31.385: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:33:31.394
    Dec 14 16:33:31.412: INFO: Waiting for pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 to disappear
    Dec 14 16:33:31.417: INFO: Pod pod-cccfd16f-546f-4ed6-8586-3c4a5f82d862 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:33:31.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3876" for this suite. 12/14/22 16:33:31.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:33:31.445
Dec 14 16:33:31.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename events 12/14/22 16:33:31.447
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:31.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:31.475
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 12/14/22 16:33:31.479
STEP: listing all events in all namespaces 12/14/22 16:33:31.49
STEP: patching the test event 12/14/22 16:33:31.509
STEP: fetching the test event 12/14/22 16:33:31.529
STEP: updating the test event 12/14/22 16:33:31.533
STEP: getting the test event 12/14/22 16:33:31.555
STEP: deleting the test event 12/14/22 16:33:31.562
STEP: listing all events in all namespaces 12/14/22 16:33:31.578
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec 14 16:33:31.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3139" for this suite. 12/14/22 16:33:31.612
------------------------------
• [0.179 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:33:31.445
    Dec 14 16:33:31.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename events 12/14/22 16:33:31.447
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:31.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:31.475
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 12/14/22 16:33:31.479
    STEP: listing all events in all namespaces 12/14/22 16:33:31.49
    STEP: patching the test event 12/14/22 16:33:31.509
    STEP: fetching the test event 12/14/22 16:33:31.529
    STEP: updating the test event 12/14/22 16:33:31.533
    STEP: getting the test event 12/14/22 16:33:31.555
    STEP: deleting the test event 12/14/22 16:33:31.562
    STEP: listing all events in all namespaces 12/14/22 16:33:31.578
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:33:31.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3139" for this suite. 12/14/22 16:33:31.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:33:31.633
Dec 14 16:33:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename init-container 12/14/22 16:33:31.635
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:31.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:31.682
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 12/14/22 16:33:31.686
Dec 14 16:33:31.686: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 16:34:15.258: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-99093955-2c76-4f4b-879a-60d1479cb1d8", GenerateName:"", Namespace:"init-container-7298", SelfLink:"", UID:"c37f1a27-fa06-4951-8088-c30735ef65d5", ResourceVersion:"20538", Generation:0, CreationTimestamp:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686855095"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00108d4e8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 34, 15, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00108db60), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jl2r2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0042ab580), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0055b3fd8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"iet9eich7uhu-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003d05650), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ece060)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ece080)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ece088), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ece08c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f41d20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.56", PodIP:"10.233.67.243", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.67.243"}}, StartTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d05730)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d057a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://32c892a8f5e819925b0e370d72d114ac32f6bc391896b3e33c1d13b28b209be9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0042ab600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0042ab5e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003ece104)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:34:15.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7298" for this suite. 12/14/22 16:34:15.269
------------------------------
• [SLOW TEST] [43.647 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:33:31.633
    Dec 14 16:33:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename init-container 12/14/22 16:33:31.635
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:33:31.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:33:31.682
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 12/14/22 16:33:31.686
    Dec 14 16:33:31.686: INFO: PodSpec: initContainers in spec.initContainers
    Dec 14 16:34:15.258: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-99093955-2c76-4f4b-879a-60d1479cb1d8", GenerateName:"", Namespace:"init-container-7298", SelfLink:"", UID:"c37f1a27-fa06-4951-8088-c30735ef65d5", ResourceVersion:"20538", Generation:0, CreationTimestamp:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686855095"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00108d4e8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 34, 15, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00108db60), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jl2r2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0042ab580), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jl2r2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0055b3fd8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"iet9eich7uhu-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003d05650), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ece060)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ece080)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ece088), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ece08c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f41d20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.56", PodIP:"10.233.67.243", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.67.243"}}, StartTime:time.Date(2022, time.December, 14, 16, 33, 31, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d05730)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d057a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://32c892a8f5e819925b0e370d72d114ac32f6bc391896b3e33c1d13b28b209be9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0042ab600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0042ab5e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003ece104)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:34:15.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7298" for this suite. 12/14/22 16:34:15.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:34:15.282
Dec 14 16:34:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 16:34:15.288
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:15.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:15.317
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 12/14/22 16:34:15.329
STEP: Verify that the required pods have come up. 12/14/22 16:34:15.337
Dec 14 16:34:15.340: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:34:20.346: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:34:20.346
STEP: Getting /status 12/14/22 16:34:20.347
Dec 14 16:34:20.354: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 12/14/22 16:34:20.354
Dec 14 16:34:20.369: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 12/14/22 16:34:20.369
Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: ADDED
Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.373: INFO: Found replicaset test-rs in namespace replicaset-4792 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 16:34:20.373: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 12/14/22 16:34:20.373
Dec 14 16:34:20.373: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 16:34:20.382: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 12/14/22 16:34:20.382
Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: ADDED
Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.386: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.386: INFO: Observed replicaset test-rs in namespace replicaset-4792 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 16:34:20.386: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 16:34:20.386: INFO: Found replicaset test-rs in namespace replicaset-4792 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 16:34:20.386: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:34:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4792" for this suite. 12/14/22 16:34:20.399
------------------------------
• [SLOW TEST] [5.138 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:34:15.282
    Dec 14 16:34:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 16:34:15.288
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:15.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:15.317
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 12/14/22 16:34:15.329
    STEP: Verify that the required pods have come up. 12/14/22 16:34:15.337
    Dec 14 16:34:15.340: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 16:34:20.346: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:34:20.346
    STEP: Getting /status 12/14/22 16:34:20.347
    Dec 14 16:34:20.354: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 12/14/22 16:34:20.354
    Dec 14 16:34:20.369: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 12/14/22 16:34:20.369
    Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: ADDED
    Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.372: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.373: INFO: Found replicaset test-rs in namespace replicaset-4792 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 16:34:20.373: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 12/14/22 16:34:20.373
    Dec 14 16:34:20.373: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 14 16:34:20.382: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 12/14/22 16:34:20.382
    Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: ADDED
    Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.385: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.386: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.386: INFO: Observed replicaset test-rs in namespace replicaset-4792 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 14 16:34:20.386: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 14 16:34:20.386: INFO: Found replicaset test-rs in namespace replicaset-4792 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Dec 14 16:34:20.386: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:34:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4792" for this suite. 12/14/22 16:34:20.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:34:20.424
Dec 14 16:34:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 16:34:20.426
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:20.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:20.461
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/14/22 16:34:20.465
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/14/22 16:34:20.465
STEP: creating a pod to probe DNS 12/14/22 16:34:20.465
STEP: submitting the pod to kubernetes 12/14/22 16:34:20.465
Dec 14 16:34:20.486: INFO: Waiting up to 15m0s for pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40" in namespace "dns-5175" to be "running"
Dec 14 16:34:20.500: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.302794ms
Dec 14 16:34:22.507: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40": Phase="Running", Reason="", readiness=true. Elapsed: 2.02078622s
Dec 14 16:34:22.507: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40" satisfied condition "running"
STEP: retrieving the pod 12/14/22 16:34:22.507
STEP: looking for the results for each expected name from probers 12/14/22 16:34:22.512
Dec 14 16:34:22.536: INFO: DNS probes using dns-5175/dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40 succeeded

STEP: deleting the pod 12/14/22 16:34:22.536
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 16:34:22.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5175" for this suite. 12/14/22 16:34:22.582
------------------------------
• [2.178 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:34:20.424
    Dec 14 16:34:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 16:34:20.426
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:20.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:20.461
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/14/22 16:34:20.465
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/14/22 16:34:20.465
    STEP: creating a pod to probe DNS 12/14/22 16:34:20.465
    STEP: submitting the pod to kubernetes 12/14/22 16:34:20.465
    Dec 14 16:34:20.486: INFO: Waiting up to 15m0s for pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40" in namespace "dns-5175" to be "running"
    Dec 14 16:34:20.500: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.302794ms
    Dec 14 16:34:22.507: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40": Phase="Running", Reason="", readiness=true. Elapsed: 2.02078622s
    Dec 14 16:34:22.507: INFO: Pod "dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 16:34:22.507
    STEP: looking for the results for each expected name from probers 12/14/22 16:34:22.512
    Dec 14 16:34:22.536: INFO: DNS probes using dns-5175/dns-test-9ae69f38-bbd5-4993-ae29-60b7221f8d40 succeeded

    STEP: deleting the pod 12/14/22 16:34:22.536
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:34:22.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5175" for this suite. 12/14/22 16:34:22.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:34:22.604
Dec 14 16:34:22.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:34:22.608
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:22.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:22.639
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-ec0041ac-6387-4ce6-b618-ba52d85bbca2 12/14/22 16:34:22.658
STEP: Creating secret with name s-test-opt-upd-230c411d-2db9-4a7e-b96f-07c535e64a4a 12/14/22 16:34:22.668
STEP: Creating the pod 12/14/22 16:34:22.673
Dec 14 16:34:22.684: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800" in namespace "projected-2450" to be "running and ready"
Dec 14 16:34:22.688: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643442ms
Dec 14 16:34:22.688: INFO: The phase of Pod pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:34:24.698: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800": Phase="Running", Reason="", readiness=true. Elapsed: 2.0140832s
Dec 14 16:34:24.698: INFO: The phase of Pod pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800 is Running (Ready = true)
Dec 14 16:34:24.698: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-ec0041ac-6387-4ce6-b618-ba52d85bbca2 12/14/22 16:34:24.743
STEP: Updating secret s-test-opt-upd-230c411d-2db9-4a7e-b96f-07c535e64a4a 12/14/22 16:34:24.752
STEP: Creating secret with name s-test-opt-create-ce29c418-5a8e-4146-ac4b-977f35e78a97 12/14/22 16:34:24.758
STEP: waiting to observe update in volume 12/14/22 16:34:24.765
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 16:34:28.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2450" for this suite. 12/14/22 16:34:28.831
------------------------------
• [SLOW TEST] [6.238 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:34:22.604
    Dec 14 16:34:22.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:34:22.608
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:22.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:22.639
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-ec0041ac-6387-4ce6-b618-ba52d85bbca2 12/14/22 16:34:22.658
    STEP: Creating secret with name s-test-opt-upd-230c411d-2db9-4a7e-b96f-07c535e64a4a 12/14/22 16:34:22.668
    STEP: Creating the pod 12/14/22 16:34:22.673
    Dec 14 16:34:22.684: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800" in namespace "projected-2450" to be "running and ready"
    Dec 14 16:34:22.688: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643442ms
    Dec 14 16:34:22.688: INFO: The phase of Pod pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:34:24.698: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800": Phase="Running", Reason="", readiness=true. Elapsed: 2.0140832s
    Dec 14 16:34:24.698: INFO: The phase of Pod pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800 is Running (Ready = true)
    Dec 14 16:34:24.698: INFO: Pod "pod-projected-secrets-519a405e-8d96-40f8-89dc-1ec8c1d89800" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-ec0041ac-6387-4ce6-b618-ba52d85bbca2 12/14/22 16:34:24.743
    STEP: Updating secret s-test-opt-upd-230c411d-2db9-4a7e-b96f-07c535e64a4a 12/14/22 16:34:24.752
    STEP: Creating secret with name s-test-opt-create-ce29c418-5a8e-4146-ac4b-977f35e78a97 12/14/22 16:34:24.758
    STEP: waiting to observe update in volume 12/14/22 16:34:24.765
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:34:28.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2450" for this suite. 12/14/22 16:34:28.831
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:34:28.845
Dec 14 16:34:28.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:34:28.847
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:28.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:28.874
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:34:28.877
Dec 14 16:34:28.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892" in namespace "projected-2642" to be "Succeeded or Failed"
Dec 14 16:34:28.899: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716968ms
Dec 14 16:34:30.914: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020868271s
Dec 14 16:34:32.907: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014333098s
STEP: Saw pod success 12/14/22 16:34:32.907
Dec 14 16:34:32.908: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892" satisfied condition "Succeeded or Failed"
Dec 14 16:34:32.913: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 container client-container: <nil>
STEP: delete the pod 12/14/22 16:34:32.921
Dec 14 16:34:32.936: INFO: Waiting for pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 to disappear
Dec 14 16:34:32.940: INFO: Pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:34:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2642" for this suite. 12/14/22 16:34:32.947
------------------------------
• [4.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:34:28.845
    Dec 14 16:34:28.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:34:28.847
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:28.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:28.874
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:34:28.877
    Dec 14 16:34:28.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892" in namespace "projected-2642" to be "Succeeded or Failed"
    Dec 14 16:34:28.899: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716968ms
    Dec 14 16:34:30.914: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020868271s
    Dec 14 16:34:32.907: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014333098s
    STEP: Saw pod success 12/14/22 16:34:32.907
    Dec 14 16:34:32.908: INFO: Pod "downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892" satisfied condition "Succeeded or Failed"
    Dec 14 16:34:32.913: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:34:32.921
    Dec 14 16:34:32.936: INFO: Waiting for pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 to disappear
    Dec 14 16:34:32.940: INFO: Pod downwardapi-volume-df944537-ca8c-40ee-9b99-b3c990735892 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:34:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2642" for this suite. 12/14/22 16:34:32.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:34:32.968
Dec 14 16:34:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename cronjob 12/14/22 16:34:32.972
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:32.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:32.996
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 12/14/22 16:34:33.005
STEP: Ensuring a job is scheduled 12/14/22 16:34:33.015
STEP: Ensuring exactly one is scheduled 12/14/22 16:35:01.026
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 16:35:01.033
STEP: Ensuring the job is replaced with a new one 12/14/22 16:35:01.039
STEP: Removing cronjob 12/14/22 16:36:01.052
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:01.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4165" for this suite. 12/14/22 16:36:01.079
------------------------------
• [SLOW TEST] [88.122 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:34:32.968
    Dec 14 16:34:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename cronjob 12/14/22 16:34:32.972
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:34:32.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:34:32.996
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 12/14/22 16:34:33.005
    STEP: Ensuring a job is scheduled 12/14/22 16:34:33.015
    STEP: Ensuring exactly one is scheduled 12/14/22 16:35:01.026
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/14/22 16:35:01.033
    STEP: Ensuring the job is replaced with a new one 12/14/22 16:35:01.039
    STEP: Removing cronjob 12/14/22 16:36:01.052
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:01.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4165" for this suite. 12/14/22 16:36:01.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:01.094
Dec 14 16:36:01.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:36:01.1
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:01.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:01.129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:36:01.134
Dec 14 16:36:01.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe" in namespace "projected-6983" to be "Succeeded or Failed"
Dec 14 16:36:01.152: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681323ms
Dec 14 16:36:03.174: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025835053s
Dec 14 16:36:05.162: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013625901s
STEP: Saw pod success 12/14/22 16:36:05.162
Dec 14 16:36:05.163: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe" satisfied condition "Succeeded or Failed"
Dec 14 16:36:05.170: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe container client-container: <nil>
STEP: delete the pod 12/14/22 16:36:05.197
Dec 14 16:36:05.219: INFO: Waiting for pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe to disappear
Dec 14 16:36:05.223: INFO: Pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:05.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6983" for this suite. 12/14/22 16:36:05.23
------------------------------
• [4.155 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:01.094
    Dec 14 16:36:01.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:36:01.1
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:01.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:01.129
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:36:01.134
    Dec 14 16:36:01.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe" in namespace "projected-6983" to be "Succeeded or Failed"
    Dec 14 16:36:01.152: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681323ms
    Dec 14 16:36:03.174: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025835053s
    Dec 14 16:36:05.162: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013625901s
    STEP: Saw pod success 12/14/22 16:36:05.162
    Dec 14 16:36:05.163: INFO: Pod "downwardapi-volume-0b832208-7905-4163-a444-826597602afe" satisfied condition "Succeeded or Failed"
    Dec 14 16:36:05.170: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe container client-container: <nil>
    STEP: delete the pod 12/14/22 16:36:05.197
    Dec 14 16:36:05.219: INFO: Waiting for pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe to disappear
    Dec 14 16:36:05.223: INFO: Pod downwardapi-volume-0b832208-7905-4163-a444-826597602afe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:05.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6983" for this suite. 12/14/22 16:36:05.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:05.254
Dec 14 16:36:05.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:36:05.256
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:05.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:05.284
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-2881/configmap-test-f62424c9-f506-4d27-b502-870fa31c2855 12/14/22 16:36:05.288
STEP: Creating a pod to test consume configMaps 12/14/22 16:36:05.297
Dec 14 16:36:05.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d" in namespace "configmap-2881" to be "Succeeded or Failed"
Dec 14 16:36:05.311: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38001ms
Dec 14 16:36:07.322: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014917541s
Dec 14 16:36:09.320: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012939713s
STEP: Saw pod success 12/14/22 16:36:09.32
Dec 14 16:36:09.321: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d" satisfied condition "Succeeded or Failed"
Dec 14 16:36:09.326: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d container env-test: <nil>
STEP: delete the pod 12/14/22 16:36:09.35
Dec 14 16:36:09.368: INFO: Waiting for pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d to disappear
Dec 14 16:36:09.371: INFO: Pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2881" for this suite. 12/14/22 16:36:09.38
------------------------------
• [4.143 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:05.254
    Dec 14 16:36:05.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:36:05.256
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:05.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:05.284
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-2881/configmap-test-f62424c9-f506-4d27-b502-870fa31c2855 12/14/22 16:36:05.288
    STEP: Creating a pod to test consume configMaps 12/14/22 16:36:05.297
    Dec 14 16:36:05.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d" in namespace "configmap-2881" to be "Succeeded or Failed"
    Dec 14 16:36:05.311: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38001ms
    Dec 14 16:36:07.322: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014917541s
    Dec 14 16:36:09.320: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012939713s
    STEP: Saw pod success 12/14/22 16:36:09.32
    Dec 14 16:36:09.321: INFO: Pod "pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d" satisfied condition "Succeeded or Failed"
    Dec 14 16:36:09.326: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d container env-test: <nil>
    STEP: delete the pod 12/14/22 16:36:09.35
    Dec 14 16:36:09.368: INFO: Waiting for pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d to disappear
    Dec 14 16:36:09.371: INFO: Pod pod-configmaps-3f91a514-f6c7-41f7-aa27-3c0bb99fa16d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2881" for this suite. 12/14/22 16:36:09.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:09.417
Dec 14 16:36:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:36:09.419
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:09.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:09.447
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Dec 14 16:36:09.469: INFO: created pod
Dec 14 16:36:09.469: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-764" to be "Succeeded or Failed"
Dec 14 16:36:09.473: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22192ms
Dec 14 16:36:11.479: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010662436s
Dec 14 16:36:13.482: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013177006s
STEP: Saw pod success 12/14/22 16:36:13.482
Dec 14 16:36:13.482: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 14 16:36:43.483: INFO: polling logs
Dec 14 16:36:43.500: INFO: Pod logs: 
I1214 16:36:10.528603       1 log.go:198] OK: Got token
I1214 16:36:10.528818       1 log.go:198] validating with in-cluster discovery
I1214 16:36:10.529622       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I1214 16:36:10.529656       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-764:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036369, NotBefore:1671035769, IssuedAt:1671035769, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-764", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d6c986ab-822e-435b-b49c-b0ebfd09d330"}}}
I1214 16:36:10.561502       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1214 16:36:10.578448       1 log.go:198] OK: Validated signature on JWT
I1214 16:36:10.578597       1 log.go:198] OK: Got valid claims from token!
I1214 16:36:10.578651       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-764:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036369, NotBefore:1671035769, IssuedAt:1671035769, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-764", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d6c986ab-822e-435b-b49c-b0ebfd09d330"}}}

Dec 14 16:36:43.500: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:43.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-764" for this suite. 12/14/22 16:36:43.521
------------------------------
• [SLOW TEST] [34.116 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:09.417
    Dec 14 16:36:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:36:09.419
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:09.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:09.447
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Dec 14 16:36:09.469: INFO: created pod
    Dec 14 16:36:09.469: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-764" to be "Succeeded or Failed"
    Dec 14 16:36:09.473: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22192ms
    Dec 14 16:36:11.479: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010662436s
    Dec 14 16:36:13.482: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013177006s
    STEP: Saw pod success 12/14/22 16:36:13.482
    Dec 14 16:36:13.482: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Dec 14 16:36:43.483: INFO: polling logs
    Dec 14 16:36:43.500: INFO: Pod logs: 
    I1214 16:36:10.528603       1 log.go:198] OK: Got token
    I1214 16:36:10.528818       1 log.go:198] validating with in-cluster discovery
    I1214 16:36:10.529622       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I1214 16:36:10.529656       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-764:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036369, NotBefore:1671035769, IssuedAt:1671035769, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-764", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d6c986ab-822e-435b-b49c-b0ebfd09d330"}}}
    I1214 16:36:10.561502       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I1214 16:36:10.578448       1 log.go:198] OK: Validated signature on JWT
    I1214 16:36:10.578597       1 log.go:198] OK: Got valid claims from token!
    I1214 16:36:10.578651       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-764:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036369, NotBefore:1671035769, IssuedAt:1671035769, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-764", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d6c986ab-822e-435b-b49c-b0ebfd09d330"}}}

    Dec 14 16:36:43.500: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:43.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-764" for this suite. 12/14/22 16:36:43.521
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:43.535
Dec 14 16:36:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 16:36:43.538
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:43.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:43.57
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6799" 12/14/22 16:36:43.576
Dec 14 16:36:43.587: INFO: Namespace "namespaces-6799" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"9a41bc61-18a6-47a0-82d3-e8ecc018d4c2", "kubernetes.io/metadata.name":"namespaces-6799", "namespaces-6799":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:43.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6799" for this suite. 12/14/22 16:36:43.593
------------------------------
• [0.067 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:43.535
    Dec 14 16:36:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 16:36:43.538
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:43.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:43.57
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6799" 12/14/22 16:36:43.576
    Dec 14 16:36:43.587: INFO: Namespace "namespaces-6799" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"9a41bc61-18a6-47a0-82d3-e8ecc018d4c2", "kubernetes.io/metadata.name":"namespaces-6799", "namespaces-6799":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:43.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6799" for this suite. 12/14/22 16:36:43.593
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:43.602
Dec 14 16:36:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:36:43.605
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:43.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:43.636
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 16:36:43.64
Dec 14 16:36:43.653: INFO: Waiting up to 5m0s for pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4" in namespace "emptydir-6663" to be "Succeeded or Failed"
Dec 14 16:36:43.657: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432643ms
Dec 14 16:36:45.666: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013479085s
Dec 14 16:36:47.666: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013499946s
STEP: Saw pod success 12/14/22 16:36:47.666
Dec 14 16:36:47.667: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4" satisfied condition "Succeeded or Failed"
Dec 14 16:36:47.673: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 container test-container: <nil>
STEP: delete the pod 12/14/22 16:36:47.682
Dec 14 16:36:47.700: INFO: Waiting for pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 to disappear
Dec 14 16:36:47.708: INFO: Pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6663" for this suite. 12/14/22 16:36:47.715
------------------------------
• [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:43.602
    Dec 14 16:36:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:36:43.605
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:43.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:43.636
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/14/22 16:36:43.64
    Dec 14 16:36:43.653: INFO: Waiting up to 5m0s for pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4" in namespace "emptydir-6663" to be "Succeeded or Failed"
    Dec 14 16:36:43.657: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432643ms
    Dec 14 16:36:45.666: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013479085s
    Dec 14 16:36:47.666: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013499946s
    STEP: Saw pod success 12/14/22 16:36:47.666
    Dec 14 16:36:47.667: INFO: Pod "pod-73861f51-83aa-461a-81e9-0e99dc39ffd4" satisfied condition "Succeeded or Failed"
    Dec 14 16:36:47.673: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:36:47.682
    Dec 14 16:36:47.700: INFO: Waiting for pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 to disappear
    Dec 14 16:36:47.708: INFO: Pod pod-73861f51-83aa-461a-81e9-0e99dc39ffd4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6663" for this suite. 12/14/22 16:36:47.715
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:47.731
Dec 14 16:36:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:36:47.735
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:47.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:47.761
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 12/14/22 16:36:47.773
Dec 14 16:36:47.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 create -f -'
Dec 14 16:36:49.060: INFO: stderr: ""
Dec 14 16:36:49.060: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 12/14/22 16:36:49.06
Dec 14 16:36:49.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 diff -f -'
Dec 14 16:36:49.522: INFO: rc: 1
Dec 14 16:36:49.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 delete -f -'
Dec 14 16:36:49.695: INFO: stderr: ""
Dec 14 16:36:49.695: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:49.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3541" for this suite. 12/14/22 16:36:49.708
------------------------------
• [1.987 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:47.731
    Dec 14 16:36:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:36:47.735
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:47.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:47.761
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 12/14/22 16:36:47.773
    Dec 14 16:36:47.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 create -f -'
    Dec 14 16:36:49.060: INFO: stderr: ""
    Dec 14 16:36:49.060: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 12/14/22 16:36:49.06
    Dec 14 16:36:49.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 diff -f -'
    Dec 14 16:36:49.522: INFO: rc: 1
    Dec 14 16:36:49.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3541 delete -f -'
    Dec 14 16:36:49.695: INFO: stderr: ""
    Dec 14 16:36:49.695: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:49.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3541" for this suite. 12/14/22 16:36:49.708
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:49.719
Dec 14 16:36:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:36:49.722
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:49.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:49.756
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 12/14/22 16:36:49.759
Dec 14 16:36:49.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7728 cluster-info'
Dec 14 16:36:49.958: INFO: stderr: ""
Dec 14 16:36:49.958: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:36:49.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7728" for this suite. 12/14/22 16:36:49.966
------------------------------
• [0.256 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:49.719
    Dec 14 16:36:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:36:49.722
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:49.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:49.756
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 12/14/22 16:36:49.759
    Dec 14 16:36:49.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7728 cluster-info'
    Dec 14 16:36:49.958: INFO: stderr: ""
    Dec 14 16:36:49.958: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:36:49.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7728" for this suite. 12/14/22 16:36:49.966
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:36:49.976
Dec 14 16:36:49.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:36:49.977
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:50.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:50.008
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/14/22 16:36:50.012
Dec 14 16:36:50.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/14/22 16:36:57.839
Dec 14 16:36:57.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:36:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:37:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7447" for this suite. 12/14/22 16:37:08.433
------------------------------
• [SLOW TEST] [18.468 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:36:49.976
    Dec 14 16:36:49.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:36:49.977
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:36:50.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:36:50.008
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/14/22 16:36:50.012
    Dec 14 16:36:50.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/14/22 16:36:57.839
    Dec 14 16:36:57.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:36:59.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:37:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7447" for this suite. 12/14/22 16:37:08.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:37:08.447
Dec 14 16:37:08.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 16:37:08.45
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:37:08.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:37:08.475
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 16:37:08.479
Dec 14 16:37:08.493: INFO: Waiting up to 5m0s for pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47" in namespace "emptydir-9930" to be "Succeeded or Failed"
Dec 14 16:37:08.499: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270679ms
Dec 14 16:37:10.505: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011839553s
Dec 14 16:37:12.506: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012269098s
STEP: Saw pod success 12/14/22 16:37:12.506
Dec 14 16:37:12.506: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47" satisfied condition "Succeeded or Failed"
Dec 14 16:37:12.511: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 container test-container: <nil>
STEP: delete the pod 12/14/22 16:37:12.545
Dec 14 16:37:12.565: INFO: Waiting for pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 to disappear
Dec 14 16:37:12.570: INFO: Pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 16:37:12.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9930" for this suite. 12/14/22 16:37:12.58
------------------------------
• [4.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:37:08.447
    Dec 14 16:37:08.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 16:37:08.45
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:37:08.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:37:08.475
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 16:37:08.479
    Dec 14 16:37:08.493: INFO: Waiting up to 5m0s for pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47" in namespace "emptydir-9930" to be "Succeeded or Failed"
    Dec 14 16:37:08.499: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270679ms
    Dec 14 16:37:10.505: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011839553s
    Dec 14 16:37:12.506: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012269098s
    STEP: Saw pod success 12/14/22 16:37:12.506
    Dec 14 16:37:12.506: INFO: Pod "pod-78a65e36-b57d-44cf-9e9b-282fd9610d47" satisfied condition "Succeeded or Failed"
    Dec 14 16:37:12.511: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 container test-container: <nil>
    STEP: delete the pod 12/14/22 16:37:12.545
    Dec 14 16:37:12.565: INFO: Waiting for pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 to disappear
    Dec 14 16:37:12.570: INFO: Pod pod-78a65e36-b57d-44cf-9e9b-282fd9610d47 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:37:12.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9930" for this suite. 12/14/22 16:37:12.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:37:12.597
Dec 14 16:37:12.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename cronjob 12/14/22 16:37:12.601
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:37:12.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:37:12.635
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 12/14/22 16:37:12.638
STEP: Ensuring more than one job is running at a time 12/14/22 16:37:12.646
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/14/22 16:39:00.655
STEP: Removing cronjob 12/14/22 16:39:00.664
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5568" for this suite. 12/14/22 16:39:00.683
------------------------------
• [SLOW TEST] [108.095 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:37:12.597
    Dec 14 16:37:12.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename cronjob 12/14/22 16:37:12.601
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:37:12.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:37:12.635
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 12/14/22 16:37:12.638
    STEP: Ensuring more than one job is running at a time 12/14/22 16:37:12.646
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/14/22 16:39:00.655
    STEP: Removing cronjob 12/14/22 16:39:00.664
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5568" for this suite. 12/14/22 16:39:00.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:00.696
Dec 14 16:39:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:39:00.703
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:00.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:00.75
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-bed4cfa1-6840-4f43-9823-612f350ea100 12/14/22 16:39:00.753
STEP: Creating a pod to test consume secrets 12/14/22 16:39:00.771
Dec 14 16:39:00.788: INFO: Waiting up to 5m0s for pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a" in namespace "secrets-824" to be "Succeeded or Failed"
Dec 14 16:39:00.794: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.970433ms
Dec 14 16:39:02.809: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020679014s
Dec 14 16:39:04.808: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020078595s
STEP: Saw pod success 12/14/22 16:39:04.808
Dec 14 16:39:04.810: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a" satisfied condition "Succeeded or Failed"
Dec 14 16:39:04.818: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:39:04.853
Dec 14 16:39:04.878: INFO: Waiting for pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a to disappear
Dec 14 16:39:04.883: INFO: Pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-824" for this suite. 12/14/22 16:39:04.89
------------------------------
• [4.204 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:00.696
    Dec 14 16:39:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:39:00.703
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:00.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:00.75
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-bed4cfa1-6840-4f43-9823-612f350ea100 12/14/22 16:39:00.753
    STEP: Creating a pod to test consume secrets 12/14/22 16:39:00.771
    Dec 14 16:39:00.788: INFO: Waiting up to 5m0s for pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a" in namespace "secrets-824" to be "Succeeded or Failed"
    Dec 14 16:39:00.794: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.970433ms
    Dec 14 16:39:02.809: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020679014s
    Dec 14 16:39:04.808: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020078595s
    STEP: Saw pod success 12/14/22 16:39:04.808
    Dec 14 16:39:04.810: INFO: Pod "pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a" satisfied condition "Succeeded or Failed"
    Dec 14 16:39:04.818: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:39:04.853
    Dec 14 16:39:04.878: INFO: Waiting for pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a to disappear
    Dec 14 16:39:04.883: INFO: Pod pod-secrets-8b4f8668-0afd-48c6-baad-c81d0d2f069a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-824" for this suite. 12/14/22 16:39:04.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:04.905
Dec 14 16:39:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:39:04.911
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:04.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:04.935
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:39:04.94
Dec 14 16:39:04.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203" in namespace "downward-api-9540" to be "Succeeded or Failed"
Dec 14 16:39:04.960: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605445ms
Dec 14 16:39:06.971: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016657147s
Dec 14 16:39:08.982: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027719411s
STEP: Saw pod success 12/14/22 16:39:08.983
Dec 14 16:39:08.984: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203" satisfied condition "Succeeded or Failed"
Dec 14 16:39:08.989: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 container client-container: <nil>
STEP: delete the pod 12/14/22 16:39:09.002
Dec 14 16:39:09.027: INFO: Waiting for pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 to disappear
Dec 14 16:39:09.031: INFO: Pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9540" for this suite. 12/14/22 16:39:09.038
------------------------------
• [4.147 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:04.905
    Dec 14 16:39:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:39:04.911
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:04.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:04.935
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:39:04.94
    Dec 14 16:39:04.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203" in namespace "downward-api-9540" to be "Succeeded or Failed"
    Dec 14 16:39:04.960: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605445ms
    Dec 14 16:39:06.971: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016657147s
    Dec 14 16:39:08.982: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027719411s
    STEP: Saw pod success 12/14/22 16:39:08.983
    Dec 14 16:39:08.984: INFO: Pod "downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203" satisfied condition "Succeeded or Failed"
    Dec 14 16:39:08.989: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:39:09.002
    Dec 14 16:39:09.027: INFO: Waiting for pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 to disappear
    Dec 14 16:39:09.031: INFO: Pod downwardapi-volume-75934c2b-10a6-4e19-8a23-abf34fc57203 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9540" for this suite. 12/14/22 16:39:09.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:09.06
Dec 14 16:39:09.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:39:09.062
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:09.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:09.088
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-c3ec8b76-69a6-4fc1-a493-6129622bf749 12/14/22 16:39:09.094
STEP: Creating a pod to test consume configMaps 12/14/22 16:39:09.101
Dec 14 16:39:09.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d" in namespace "configmap-3850" to be "Succeeded or Failed"
Dec 14 16:39:09.119: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877243ms
Dec 14 16:39:11.125: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011190332s
Dec 14 16:39:13.129: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015695149s
STEP: Saw pod success 12/14/22 16:39:13.13
Dec 14 16:39:13.130: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d" satisfied condition "Succeeded or Failed"
Dec 14 16:39:13.135: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d container configmap-volume-test: <nil>
STEP: delete the pod 12/14/22 16:39:13.145
Dec 14 16:39:13.162: INFO: Waiting for pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d to disappear
Dec 14 16:39:13.166: INFO: Pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:13.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3850" for this suite. 12/14/22 16:39:13.172
------------------------------
• [4.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:09.06
    Dec 14 16:39:09.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:39:09.062
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:09.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:09.088
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-c3ec8b76-69a6-4fc1-a493-6129622bf749 12/14/22 16:39:09.094
    STEP: Creating a pod to test consume configMaps 12/14/22 16:39:09.101
    Dec 14 16:39:09.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d" in namespace "configmap-3850" to be "Succeeded or Failed"
    Dec 14 16:39:09.119: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877243ms
    Dec 14 16:39:11.125: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011190332s
    Dec 14 16:39:13.129: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015695149s
    STEP: Saw pod success 12/14/22 16:39:13.13
    Dec 14 16:39:13.130: INFO: Pod "pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d" satisfied condition "Succeeded or Failed"
    Dec 14 16:39:13.135: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d container configmap-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:39:13.145
    Dec 14 16:39:13.162: INFO: Waiting for pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d to disappear
    Dec 14 16:39:13.166: INFO: Pod pod-configmaps-85e50975-6520-46f6-a1c3-4eb6acc3826d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:13.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3850" for this suite. 12/14/22 16:39:13.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:13.183
Dec 14 16:39:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename subpath 12/14/22 16:39:13.186
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:13.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:13.212
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 16:39:13.215
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-qccl 12/14/22 16:39:13.228
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:39:13.228
Dec 14 16:39:13.240: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qccl" in namespace "subpath-1518" to be "Succeeded or Failed"
Dec 14 16:39:13.246: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080543ms
Dec 14 16:39:15.253: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 2.013759596s
Dec 14 16:39:17.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 4.015022478s
Dec 14 16:39:19.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 6.014385416s
Dec 14 16:39:21.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 8.010965653s
Dec 14 16:39:23.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 10.014351948s
Dec 14 16:39:25.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 12.014232573s
Dec 14 16:39:27.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 14.015312378s
Dec 14 16:39:29.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 16.011347469s
Dec 14 16:39:31.252: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 18.012717399s
Dec 14 16:39:33.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 20.011430788s
Dec 14 16:39:35.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=false. Elapsed: 22.014475116s
Dec 14 16:39:37.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015324866s
STEP: Saw pod success 12/14/22 16:39:37.255
Dec 14 16:39:37.255: INFO: Pod "pod-subpath-test-secret-qccl" satisfied condition "Succeeded or Failed"
Dec 14 16:39:37.260: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-secret-qccl container test-container-subpath-secret-qccl: <nil>
STEP: delete the pod 12/14/22 16:39:37.275
Dec 14 16:39:37.291: INFO: Waiting for pod pod-subpath-test-secret-qccl to disappear
Dec 14 16:39:37.310: INFO: Pod pod-subpath-test-secret-qccl no longer exists
STEP: Deleting pod pod-subpath-test-secret-qccl 12/14/22 16:39:37.31
Dec 14 16:39:37.311: INFO: Deleting pod "pod-subpath-test-secret-qccl" in namespace "subpath-1518"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:37.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1518" for this suite. 12/14/22 16:39:37.32
------------------------------
• [SLOW TEST] [24.150 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:13.183
    Dec 14 16:39:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename subpath 12/14/22 16:39:13.186
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:13.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:13.212
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 16:39:13.215
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-qccl 12/14/22 16:39:13.228
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:39:13.228
    Dec 14 16:39:13.240: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qccl" in namespace "subpath-1518" to be "Succeeded or Failed"
    Dec 14 16:39:13.246: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080543ms
    Dec 14 16:39:15.253: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 2.013759596s
    Dec 14 16:39:17.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 4.015022478s
    Dec 14 16:39:19.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 6.014385416s
    Dec 14 16:39:21.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 8.010965653s
    Dec 14 16:39:23.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 10.014351948s
    Dec 14 16:39:25.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 12.014232573s
    Dec 14 16:39:27.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 14.015312378s
    Dec 14 16:39:29.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 16.011347469s
    Dec 14 16:39:31.252: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 18.012717399s
    Dec 14 16:39:33.251: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=true. Elapsed: 20.011430788s
    Dec 14 16:39:35.254: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Running", Reason="", readiness=false. Elapsed: 22.014475116s
    Dec 14 16:39:37.255: INFO: Pod "pod-subpath-test-secret-qccl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015324866s
    STEP: Saw pod success 12/14/22 16:39:37.255
    Dec 14 16:39:37.255: INFO: Pod "pod-subpath-test-secret-qccl" satisfied condition "Succeeded or Failed"
    Dec 14 16:39:37.260: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-secret-qccl container test-container-subpath-secret-qccl: <nil>
    STEP: delete the pod 12/14/22 16:39:37.275
    Dec 14 16:39:37.291: INFO: Waiting for pod pod-subpath-test-secret-qccl to disappear
    Dec 14 16:39:37.310: INFO: Pod pod-subpath-test-secret-qccl no longer exists
    STEP: Deleting pod pod-subpath-test-secret-qccl 12/14/22 16:39:37.31
    Dec 14 16:39:37.311: INFO: Deleting pod "pod-subpath-test-secret-qccl" in namespace "subpath-1518"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:37.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1518" for this suite. 12/14/22 16:39:37.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:37.345
Dec 14 16:39:37.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 16:39:37.346
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:37.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:37.372
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 12/14/22 16:39:37.377
STEP: When the matched label of one of its pods change 12/14/22 16:39:37.382
Dec 14 16:39:37.388: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 16:39:42.395: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 12/14/22 16:39:42.417
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:43.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7954" for this suite. 12/14/22 16:39:43.445
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:37.345
    Dec 14 16:39:37.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 16:39:37.346
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:37.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:37.372
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 12/14/22 16:39:37.377
    STEP: When the matched label of one of its pods change 12/14/22 16:39:37.382
    Dec 14 16:39:37.388: INFO: Pod name pod-release: Found 0 pods out of 1
    Dec 14 16:39:42.395: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/14/22 16:39:42.417
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:43.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7954" for this suite. 12/14/22 16:39:43.445
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:43.461
Dec 14 16:39:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename proxy 12/14/22 16:39:43.464
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:43.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:43.502
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 12/14/22 16:39:43.524
STEP: creating replication controller proxy-service-l7ktp in namespace proxy-4982 12/14/22 16:39:43.525
I1214 16:39:43.538534      14 runners.go:193] Created replication controller with name: proxy-service-l7ktp, namespace: proxy-4982, replica count: 1
I1214 16:39:44.591113      14 runners.go:193] proxy-service-l7ktp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 16:39:45.592104      14 runners.go:193] proxy-service-l7ktp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:39:45.600: INFO: setup took 2.093391656s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/14/22 16:39:45.6
Dec 14 16:39:45.615: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 14.593285ms)
Dec 14 16:39:45.616: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 15.241457ms)
Dec 14 16:39:45.624: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.771557ms)
Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 24.908126ms)
Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.995435ms)
Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 25.633019ms)
Dec 14 16:39:45.634: INFO: (0) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 33.392025ms)
Dec 14 16:39:45.635: INFO: (0) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 34.340188ms)
Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.977751ms)
Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 34.527943ms)
Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 34.455023ms)
Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 34.91416ms)
Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 41.110289ms)
Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 42.194779ms)
Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 41.602573ms)
Dec 14 16:39:45.643: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 41.847074ms)
Dec 14 16:39:45.653: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 9.006538ms)
Dec 14 16:39:45.658: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 15.3138ms)
Dec 14 16:39:45.676: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.247556ms)
Dec 14 16:39:45.676: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 31.881895ms)
Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 31.285488ms)
Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 31.989708ms)
Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 32.651657ms)
Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 32.947601ms)
Dec 14 16:39:45.679: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 34.528194ms)
Dec 14 16:39:45.679: INFO: (1) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 35.165751ms)
Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 36.938224ms)
Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 37.205539ms)
Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 38.095196ms)
Dec 14 16:39:45.683: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 37.928383ms)
Dec 14 16:39:45.684: INFO: (1) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 38.385019ms)
Dec 14 16:39:45.684: INFO: (1) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 38.939143ms)
Dec 14 16:39:45.697: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 12.912351ms)
Dec 14 16:39:45.701: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 15.071509ms)
Dec 14 16:39:45.703: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 18.179577ms)
Dec 14 16:39:45.704: INFO: (2) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 19.442309ms)
Dec 14 16:39:45.704: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.411131ms)
Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.31063ms)
Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 20.0858ms)
Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 19.118828ms)
Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.052142ms)
Dec 14 16:39:45.706: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 20.460976ms)
Dec 14 16:39:45.706: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.196435ms)
Dec 14 16:39:45.710: INFO: (2) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 24.16326ms)
Dec 14 16:39:45.710: INFO: (2) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.450593ms)
Dec 14 16:39:45.711: INFO: (2) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 25.258215ms)
Dec 14 16:39:45.712: INFO: (2) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 26.920398ms)
Dec 14 16:39:45.713: INFO: (2) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 27.513239ms)
Dec 14 16:39:45.730: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.024966ms)
Dec 14 16:39:45.732: INFO: (3) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 18.148803ms)
Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 18.85818ms)
Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.210873ms)
Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 19.563062ms)
Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 19.319404ms)
Dec 14 16:39:45.734: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.151355ms)
Dec 14 16:39:45.735: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 21.90673ms)
Dec 14 16:39:45.737: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 23.237281ms)
Dec 14 16:39:45.737: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.420718ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 24.528271ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 24.118734ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 24.253511ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 24.276159ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.818718ms)
Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 24.876552ms)
Dec 14 16:39:45.749: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 9.751694ms)
Dec 14 16:39:45.756: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 17.459544ms)
Dec 14 16:39:45.756: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 16.895153ms)
Dec 14 16:39:45.757: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.840542ms)
Dec 14 16:39:45.762: INFO: (4) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 23.110393ms)
Dec 14 16:39:45.762: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 22.868583ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 23.00203ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 23.742625ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 23.44883ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 23.565563ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 23.213105ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 23.389879ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 23.32771ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 23.695875ms)
Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 23.533586ms)
Dec 14 16:39:45.767: INFO: (4) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.823472ms)
Dec 14 16:39:45.780: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 12.771257ms)
Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 26.879581ms)
Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 25.986253ms)
Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.087797ms)
Dec 14 16:39:45.797: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 28.309048ms)
Dec 14 16:39:45.804: INFO: (5) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.334322ms)
Dec 14 16:39:45.804: INFO: (5) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 34.732736ms)
Dec 14 16:39:45.805: INFO: (5) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 36.445604ms)
Dec 14 16:39:45.805: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 36.476215ms)
Dec 14 16:39:45.806: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 35.536811ms)
Dec 14 16:39:45.807: INFO: (5) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 38.029662ms)
Dec 14 16:39:45.809: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 38.571407ms)
Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 39.837132ms)
Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 41.145394ms)
Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 41.487523ms)
Dec 14 16:39:45.818: INFO: (5) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.669387ms)
Dec 14 16:39:45.849: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 30.53579ms)
Dec 14 16:39:45.852: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 33.326866ms)
Dec 14 16:39:45.857: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 38.251377ms)
Dec 14 16:39:45.861: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 42.29438ms)
Dec 14 16:39:45.862: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 42.92092ms)
Dec 14 16:39:45.862: INFO: (6) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 43.020376ms)
Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 49.323989ms)
Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.166288ms)
Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 49.832299ms)
Dec 14 16:39:45.870: INFO: (6) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 51.540826ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 51.739564ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 51.458138ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 51.528768ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 51.761462ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 52.044499ms)
Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.001279ms)
Dec 14 16:39:45.890: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 18.719136ms)
Dec 14 16:39:45.892: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 21.602445ms)
Dec 14 16:39:45.892: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 21.087413ms)
Dec 14 16:39:45.900: INFO: (7) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 28.358547ms)
Dec 14 16:39:45.901: INFO: (7) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 24.73985ms)
Dec 14 16:39:45.901: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 25.261943ms)
Dec 14 16:39:45.908: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 30.2306ms)
Dec 14 16:39:45.908: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 31.872131ms)
Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 41.059441ms)
Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 42.721686ms)
Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 40.978244ms)
Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 42.581012ms)
Dec 14 16:39:45.922: INFO: (7) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 45.816332ms)
Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 45.456695ms)
Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 45.466002ms)
Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 45.411668ms)
Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 42.553046ms)
Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 44.060432ms)
Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.023466ms)
Dec 14 16:39:45.970: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 44.812284ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 47.9551ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.08899ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 49.673865ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 49.943648ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 48.38139ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 47.975922ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 49.40435ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 48.883411ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 50.282944ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 47.92131ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 48.420309ms)
Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 48.54372ms)
Dec 14 16:39:46.000: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.959518ms)
Dec 14 16:39:46.000: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.71402ms)
Dec 14 16:39:46.001: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 21.862174ms)
Dec 14 16:39:46.004: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.104501ms)
Dec 14 16:39:46.015: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 35.809152ms)
Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 41.286276ms)
Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 41.622103ms)
Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 42.389154ms)
Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 41.80619ms)
Dec 14 16:39:46.023: INFO: (9) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 43.74126ms)
Dec 14 16:39:46.023: INFO: (9) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.868046ms)
Dec 14 16:39:46.025: INFO: (9) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 45.64321ms)
Dec 14 16:39:46.025: INFO: (9) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 45.888737ms)
Dec 14 16:39:46.026: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 46.589538ms)
Dec 14 16:39:46.026: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 45.886378ms)
Dec 14 16:39:46.027: INFO: (9) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 47.862802ms)
Dec 14 16:39:46.040: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 12.297222ms)
Dec 14 16:39:46.042: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 13.932693ms)
Dec 14 16:39:46.042: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 13.317079ms)
Dec 14 16:39:46.048: INFO: (10) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 19.805645ms)
Dec 14 16:39:46.050: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.568399ms)
Dec 14 16:39:46.051: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 22.507581ms)
Dec 14 16:39:46.051: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 21.89346ms)
Dec 14 16:39:46.054: INFO: (10) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 25.427606ms)
Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 27.224733ms)
Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.883853ms)
Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 27.076772ms)
Dec 14 16:39:46.056: INFO: (10) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.96105ms)
Dec 14 16:39:46.057: INFO: (10) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.419108ms)
Dec 14 16:39:46.057: INFO: (10) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 29.808981ms)
Dec 14 16:39:46.058: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 27.700637ms)
Dec 14 16:39:46.063: INFO: (10) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 33.144094ms)
Dec 14 16:39:46.089: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.829056ms)
Dec 14 16:39:46.094: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 30.510497ms)
Dec 14 16:39:46.094: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.875929ms)
Dec 14 16:39:46.104: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 39.410801ms)
Dec 14 16:39:46.105: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 40.03391ms)
Dec 14 16:39:46.105: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 41.167407ms)
Dec 14 16:39:46.107: INFO: (11) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 43.207922ms)
Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 43.464371ms)
Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 43.228719ms)
Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 43.914605ms)
Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.8049ms)
Dec 14 16:39:46.116: INFO: (11) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 51.541619ms)
Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 52.044123ms)
Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 52.497167ms)
Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 53.175656ms)
Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.738011ms)
Dec 14 16:39:46.132: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 12.447453ms)
Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 15.67082ms)
Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 16.098176ms)
Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.703023ms)
Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.515339ms)
Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 17.836122ms)
Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 16.032628ms)
Dec 14 16:39:46.143: INFO: (12) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 24.145843ms)
Dec 14 16:39:46.143: INFO: (12) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.020906ms)
Dec 14 16:39:46.144: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.399552ms)
Dec 14 16:39:46.144: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 25.798872ms)
Dec 14 16:39:46.145: INFO: (12) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 25.836495ms)
Dec 14 16:39:46.153: INFO: (12) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 32.869103ms)
Dec 14 16:39:46.154: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 33.674595ms)
Dec 14 16:39:46.154: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 34.204091ms)
Dec 14 16:39:46.157: INFO: (12) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 36.57728ms)
Dec 14 16:39:46.175: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 17.851148ms)
Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 42.868638ms)
Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 43.236105ms)
Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.030134ms)
Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 42.846568ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 42.581202ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 44.305145ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 42.510879ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 43.128306ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 43.272747ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 43.249041ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 44.057794ms)
Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 44.526729ms)
Dec 14 16:39:46.203: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 45.447436ms)
Dec 14 16:39:46.203: INFO: (13) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 46.187133ms)
Dec 14 16:39:46.206: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 47.172669ms)
Dec 14 16:39:46.232: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 25.558529ms)
Dec 14 16:39:46.234: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 27.663247ms)
Dec 14 16:39:46.235: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 28.569265ms)
Dec 14 16:39:46.235: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 28.43071ms)
Dec 14 16:39:46.240: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 33.590715ms)
Dec 14 16:39:46.241: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 34.622737ms)
Dec 14 16:39:46.241: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 34.631696ms)
Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 45.560291ms)
Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 45.751511ms)
Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 45.597493ms)
Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 45.715857ms)
Dec 14 16:39:46.253: INFO: (14) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 45.959772ms)
Dec 14 16:39:46.258: INFO: (14) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.053675ms)
Dec 14 16:39:46.258: INFO: (14) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 51.554782ms)
Dec 14 16:39:46.259: INFO: (14) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 52.450569ms)
Dec 14 16:39:46.259: INFO: (14) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 52.221975ms)
Dec 14 16:39:46.271: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 11.813482ms)
Dec 14 16:39:46.272: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 11.682103ms)
Dec 14 16:39:46.276: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 14.877144ms)
Dec 14 16:39:46.276: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.850556ms)
Dec 14 16:39:46.278: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 17.642519ms)
Dec 14 16:39:46.278: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 17.86854ms)
Dec 14 16:39:46.283: INFO: (15) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 23.310608ms)
Dec 14 16:39:46.285: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 24.648928ms)
Dec 14 16:39:46.285: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.790899ms)
Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 25.221929ms)
Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 26.647495ms)
Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 25.733504ms)
Dec 14 16:39:46.287: INFO: (15) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 28.048718ms)
Dec 14 16:39:46.292: INFO: (15) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 33.209307ms)
Dec 14 16:39:46.292: INFO: (15) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 32.517708ms)
Dec 14 16:39:46.295: INFO: (15) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.745702ms)
Dec 14 16:39:46.307: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 11.479277ms)
Dec 14 16:39:46.308: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 11.379312ms)
Dec 14 16:39:46.309: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 13.184538ms)
Dec 14 16:39:46.311: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.269189ms)
Dec 14 16:39:46.312: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 15.895731ms)
Dec 14 16:39:46.313: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 15.745037ms)
Dec 14 16:39:46.319: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 21.275918ms)
Dec 14 16:39:46.320: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 22.943876ms)
Dec 14 16:39:46.321: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.164088ms)
Dec 14 16:39:46.323: INFO: (16) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 26.378235ms)
Dec 14 16:39:46.323: INFO: (16) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 26.920506ms)
Dec 14 16:39:46.324: INFO: (16) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 26.743248ms)
Dec 14 16:39:46.327: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.248057ms)
Dec 14 16:39:46.327: INFO: (16) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 30.044223ms)
Dec 14 16:39:46.330: INFO: (16) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 32.570632ms)
Dec 14 16:39:46.331: INFO: (16) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 34.22637ms)
Dec 14 16:39:46.345: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 13.685084ms)
Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 24.113696ms)
Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 24.331535ms)
Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 25.339602ms)
Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 25.04297ms)
Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 26.165402ms)
Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 26.186572ms)
Dec 14 16:39:46.359: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 26.134694ms)
Dec 14 16:39:46.368: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 36.137228ms)
Dec 14 16:39:46.372: INFO: (17) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 38.99711ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 41.011174ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 40.380769ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 42.29363ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 41.706327ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 41.871722ms)
Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 42.679649ms)
Dec 14 16:39:46.381: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 5.998859ms)
Dec 14 16:39:46.384: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 9.152437ms)
Dec 14 16:39:46.385: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 9.307139ms)
Dec 14 16:39:46.386: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 11.143086ms)
Dec 14 16:39:46.389: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 13.607669ms)
Dec 14 16:39:46.389: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 13.215021ms)
Dec 14 16:39:46.392: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.89149ms)
Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.339811ms)
Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 20.964392ms)
Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 19.387642ms)
Dec 14 16:39:46.399: INFO: (18) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 22.755086ms)
Dec 14 16:39:46.400: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.522782ms)
Dec 14 16:39:46.400: INFO: (18) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.086462ms)
Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 28.141525ms)
Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 27.710423ms)
Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.191337ms)
Dec 14 16:39:46.421: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 16.788235ms)
Dec 14 16:39:46.431: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 26.233298ms)
Dec 14 16:39:46.431: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 26.44979ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 26.267768ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 27.154691ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.671155ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 27.616924ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 27.259474ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 27.415641ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 27.943564ms)
Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 27.714107ms)
Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 30.933955ms)
Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 30.919988ms)
Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.17637ms)
Dec 14 16:39:46.436: INFO: (19) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 31.347574ms)
Dec 14 16:39:46.437: INFO: (19) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 32.583008ms)
STEP: deleting ReplicationController proxy-service-l7ktp in namespace proxy-4982, will wait for the garbage collector to delete the pods 12/14/22 16:39:46.437
Dec 14 16:39:46.505: INFO: Deleting ReplicationController proxy-service-l7ktp took: 10.878177ms
Dec 14 16:39:46.605: INFO: Terminating ReplicationController proxy-service-l7ktp pods took: 100.421774ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec 14 16:39:49.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4982" for this suite. 12/14/22 16:39:49.417
------------------------------
• [SLOW TEST] [5.973 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:43.461
    Dec 14 16:39:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename proxy 12/14/22 16:39:43.464
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:43.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:43.502
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 12/14/22 16:39:43.524
    STEP: creating replication controller proxy-service-l7ktp in namespace proxy-4982 12/14/22 16:39:43.525
    I1214 16:39:43.538534      14 runners.go:193] Created replication controller with name: proxy-service-l7ktp, namespace: proxy-4982, replica count: 1
    I1214 16:39:44.591113      14 runners.go:193] proxy-service-l7ktp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 16:39:45.592104      14 runners.go:193] proxy-service-l7ktp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:39:45.600: INFO: setup took 2.093391656s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/14/22 16:39:45.6
    Dec 14 16:39:45.615: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 14.593285ms)
    Dec 14 16:39:45.616: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 15.241457ms)
    Dec 14 16:39:45.624: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.771557ms)
    Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 24.908126ms)
    Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.995435ms)
    Dec 14 16:39:45.626: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 25.633019ms)
    Dec 14 16:39:45.634: INFO: (0) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 33.392025ms)
    Dec 14 16:39:45.635: INFO: (0) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 34.340188ms)
    Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.977751ms)
    Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 34.527943ms)
    Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 34.455023ms)
    Dec 14 16:39:45.636: INFO: (0) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 34.91416ms)
    Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 41.110289ms)
    Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 42.194779ms)
    Dec 14 16:39:45.642: INFO: (0) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 41.602573ms)
    Dec 14 16:39:45.643: INFO: (0) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 41.847074ms)
    Dec 14 16:39:45.653: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 9.006538ms)
    Dec 14 16:39:45.658: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 15.3138ms)
    Dec 14 16:39:45.676: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.247556ms)
    Dec 14 16:39:45.676: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 31.881895ms)
    Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 31.285488ms)
    Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 31.989708ms)
    Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 32.651657ms)
    Dec 14 16:39:45.677: INFO: (1) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 32.947601ms)
    Dec 14 16:39:45.679: INFO: (1) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 34.528194ms)
    Dec 14 16:39:45.679: INFO: (1) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 35.165751ms)
    Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 36.938224ms)
    Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 37.205539ms)
    Dec 14 16:39:45.682: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 38.095196ms)
    Dec 14 16:39:45.683: INFO: (1) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 37.928383ms)
    Dec 14 16:39:45.684: INFO: (1) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 38.385019ms)
    Dec 14 16:39:45.684: INFO: (1) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 38.939143ms)
    Dec 14 16:39:45.697: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 12.912351ms)
    Dec 14 16:39:45.701: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 15.071509ms)
    Dec 14 16:39:45.703: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 18.179577ms)
    Dec 14 16:39:45.704: INFO: (2) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 19.442309ms)
    Dec 14 16:39:45.704: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.411131ms)
    Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.31063ms)
    Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 20.0858ms)
    Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 19.118828ms)
    Dec 14 16:39:45.705: INFO: (2) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.052142ms)
    Dec 14 16:39:45.706: INFO: (2) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 20.460976ms)
    Dec 14 16:39:45.706: INFO: (2) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.196435ms)
    Dec 14 16:39:45.710: INFO: (2) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 24.16326ms)
    Dec 14 16:39:45.710: INFO: (2) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.450593ms)
    Dec 14 16:39:45.711: INFO: (2) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 25.258215ms)
    Dec 14 16:39:45.712: INFO: (2) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 26.920398ms)
    Dec 14 16:39:45.713: INFO: (2) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 27.513239ms)
    Dec 14 16:39:45.730: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.024966ms)
    Dec 14 16:39:45.732: INFO: (3) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 18.148803ms)
    Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 18.85818ms)
    Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 19.210873ms)
    Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 19.563062ms)
    Dec 14 16:39:45.733: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 19.319404ms)
    Dec 14 16:39:45.734: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.151355ms)
    Dec 14 16:39:45.735: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 21.90673ms)
    Dec 14 16:39:45.737: INFO: (3) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 23.237281ms)
    Dec 14 16:39:45.737: INFO: (3) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.420718ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 24.528271ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 24.118734ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 24.253511ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 24.276159ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.818718ms)
    Dec 14 16:39:45.738: INFO: (3) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 24.876552ms)
    Dec 14 16:39:45.749: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 9.751694ms)
    Dec 14 16:39:45.756: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 17.459544ms)
    Dec 14 16:39:45.756: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 16.895153ms)
    Dec 14 16:39:45.757: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.840542ms)
    Dec 14 16:39:45.762: INFO: (4) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 23.110393ms)
    Dec 14 16:39:45.762: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 22.868583ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 23.00203ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 23.742625ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 23.44883ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 23.565563ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 23.213105ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 23.389879ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 23.32771ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 23.695875ms)
    Dec 14 16:39:45.763: INFO: (4) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 23.533586ms)
    Dec 14 16:39:45.767: INFO: (4) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.823472ms)
    Dec 14 16:39:45.780: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 12.771257ms)
    Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 26.879581ms)
    Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 25.986253ms)
    Dec 14 16:39:45.795: INFO: (5) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.087797ms)
    Dec 14 16:39:45.797: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 28.309048ms)
    Dec 14 16:39:45.804: INFO: (5) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.334322ms)
    Dec 14 16:39:45.804: INFO: (5) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 34.732736ms)
    Dec 14 16:39:45.805: INFO: (5) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 36.445604ms)
    Dec 14 16:39:45.805: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 36.476215ms)
    Dec 14 16:39:45.806: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 35.536811ms)
    Dec 14 16:39:45.807: INFO: (5) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 38.029662ms)
    Dec 14 16:39:45.809: INFO: (5) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 38.571407ms)
    Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 39.837132ms)
    Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 41.145394ms)
    Dec 14 16:39:45.810: INFO: (5) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 41.487523ms)
    Dec 14 16:39:45.818: INFO: (5) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.669387ms)
    Dec 14 16:39:45.849: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 30.53579ms)
    Dec 14 16:39:45.852: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 33.326866ms)
    Dec 14 16:39:45.857: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 38.251377ms)
    Dec 14 16:39:45.861: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 42.29438ms)
    Dec 14 16:39:45.862: INFO: (6) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 42.92092ms)
    Dec 14 16:39:45.862: INFO: (6) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 43.020376ms)
    Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 49.323989ms)
    Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.166288ms)
    Dec 14 16:39:45.868: INFO: (6) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 49.832299ms)
    Dec 14 16:39:45.870: INFO: (6) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 51.540826ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 51.739564ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 51.458138ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 51.528768ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 51.761462ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 52.044499ms)
    Dec 14 16:39:45.871: INFO: (6) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.001279ms)
    Dec 14 16:39:45.890: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 18.719136ms)
    Dec 14 16:39:45.892: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 21.602445ms)
    Dec 14 16:39:45.892: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 21.087413ms)
    Dec 14 16:39:45.900: INFO: (7) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 28.358547ms)
    Dec 14 16:39:45.901: INFO: (7) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 24.73985ms)
    Dec 14 16:39:45.901: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 25.261943ms)
    Dec 14 16:39:45.908: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 30.2306ms)
    Dec 14 16:39:45.908: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 31.872131ms)
    Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 41.059441ms)
    Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 42.721686ms)
    Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 40.978244ms)
    Dec 14 16:39:45.919: INFO: (7) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 42.581012ms)
    Dec 14 16:39:45.922: INFO: (7) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 45.816332ms)
    Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 45.456695ms)
    Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 45.466002ms)
    Dec 14 16:39:45.923: INFO: (7) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 45.411668ms)
    Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 42.553046ms)
    Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 44.060432ms)
    Dec 14 16:39:45.968: INFO: (8) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.023466ms)
    Dec 14 16:39:45.970: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 44.812284ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 47.9551ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 49.08899ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 49.673865ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 49.943648ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 48.38139ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 47.975922ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 49.40435ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 48.883411ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 50.282944ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 47.92131ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 48.420309ms)
    Dec 14 16:39:45.974: INFO: (8) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 48.54372ms)
    Dec 14 16:39:46.000: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.959518ms)
    Dec 14 16:39:46.000: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 20.71402ms)
    Dec 14 16:39:46.001: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 21.862174ms)
    Dec 14 16:39:46.004: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.104501ms)
    Dec 14 16:39:46.015: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 35.809152ms)
    Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 41.286276ms)
    Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 41.622103ms)
    Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 42.389154ms)
    Dec 14 16:39:46.021: INFO: (9) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 41.80619ms)
    Dec 14 16:39:46.023: INFO: (9) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 43.74126ms)
    Dec 14 16:39:46.023: INFO: (9) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.868046ms)
    Dec 14 16:39:46.025: INFO: (9) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 45.64321ms)
    Dec 14 16:39:46.025: INFO: (9) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 45.888737ms)
    Dec 14 16:39:46.026: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 46.589538ms)
    Dec 14 16:39:46.026: INFO: (9) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 45.886378ms)
    Dec 14 16:39:46.027: INFO: (9) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 47.862802ms)
    Dec 14 16:39:46.040: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 12.297222ms)
    Dec 14 16:39:46.042: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 13.932693ms)
    Dec 14 16:39:46.042: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 13.317079ms)
    Dec 14 16:39:46.048: INFO: (10) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 19.805645ms)
    Dec 14 16:39:46.050: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.568399ms)
    Dec 14 16:39:46.051: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 22.507581ms)
    Dec 14 16:39:46.051: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 21.89346ms)
    Dec 14 16:39:46.054: INFO: (10) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 25.427606ms)
    Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 27.224733ms)
    Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.883853ms)
    Dec 14 16:39:46.055: INFO: (10) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 27.076772ms)
    Dec 14 16:39:46.056: INFO: (10) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.96105ms)
    Dec 14 16:39:46.057: INFO: (10) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.419108ms)
    Dec 14 16:39:46.057: INFO: (10) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 29.808981ms)
    Dec 14 16:39:46.058: INFO: (10) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 27.700637ms)
    Dec 14 16:39:46.063: INFO: (10) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 33.144094ms)
    Dec 14 16:39:46.089: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.829056ms)
    Dec 14 16:39:46.094: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 30.510497ms)
    Dec 14 16:39:46.094: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.875929ms)
    Dec 14 16:39:46.104: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 39.410801ms)
    Dec 14 16:39:46.105: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 40.03391ms)
    Dec 14 16:39:46.105: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 41.167407ms)
    Dec 14 16:39:46.107: INFO: (11) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 43.207922ms)
    Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 43.464371ms)
    Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 43.228719ms)
    Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 43.914605ms)
    Dec 14 16:39:46.108: INFO: (11) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.8049ms)
    Dec 14 16:39:46.116: INFO: (11) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 51.541619ms)
    Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 52.044123ms)
    Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 52.497167ms)
    Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 53.175656ms)
    Dec 14 16:39:46.117: INFO: (11) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.738011ms)
    Dec 14 16:39:46.132: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 12.447453ms)
    Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 15.67082ms)
    Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 16.098176ms)
    Dec 14 16:39:46.134: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.703023ms)
    Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 17.515339ms)
    Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 17.836122ms)
    Dec 14 16:39:46.136: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 16.032628ms)
    Dec 14 16:39:46.143: INFO: (12) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 24.145843ms)
    Dec 14 16:39:46.143: INFO: (12) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.020906ms)
    Dec 14 16:39:46.144: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 24.399552ms)
    Dec 14 16:39:46.144: INFO: (12) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 25.798872ms)
    Dec 14 16:39:46.145: INFO: (12) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 25.836495ms)
    Dec 14 16:39:46.153: INFO: (12) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 32.869103ms)
    Dec 14 16:39:46.154: INFO: (12) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 33.674595ms)
    Dec 14 16:39:46.154: INFO: (12) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 34.204091ms)
    Dec 14 16:39:46.157: INFO: (12) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 36.57728ms)
    Dec 14 16:39:46.175: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 17.851148ms)
    Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 42.868638ms)
    Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 43.236105ms)
    Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 43.030134ms)
    Dec 14 16:39:46.201: INFO: (13) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 42.846568ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 42.581202ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 44.305145ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 42.510879ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 43.128306ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 43.272747ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 43.249041ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 44.057794ms)
    Dec 14 16:39:46.202: INFO: (13) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 44.526729ms)
    Dec 14 16:39:46.203: INFO: (13) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 45.447436ms)
    Dec 14 16:39:46.203: INFO: (13) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 46.187133ms)
    Dec 14 16:39:46.206: INFO: (13) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 47.172669ms)
    Dec 14 16:39:46.232: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 25.558529ms)
    Dec 14 16:39:46.234: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 27.663247ms)
    Dec 14 16:39:46.235: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 28.569265ms)
    Dec 14 16:39:46.235: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 28.43071ms)
    Dec 14 16:39:46.240: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 33.590715ms)
    Dec 14 16:39:46.241: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 34.622737ms)
    Dec 14 16:39:46.241: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 34.631696ms)
    Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 45.560291ms)
    Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 45.751511ms)
    Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 45.597493ms)
    Dec 14 16:39:46.252: INFO: (14) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 45.715857ms)
    Dec 14 16:39:46.253: INFO: (14) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 45.959772ms)
    Dec 14 16:39:46.258: INFO: (14) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 52.053675ms)
    Dec 14 16:39:46.258: INFO: (14) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 51.554782ms)
    Dec 14 16:39:46.259: INFO: (14) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 52.450569ms)
    Dec 14 16:39:46.259: INFO: (14) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 52.221975ms)
    Dec 14 16:39:46.271: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 11.813482ms)
    Dec 14 16:39:46.272: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 11.682103ms)
    Dec 14 16:39:46.276: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 14.877144ms)
    Dec 14 16:39:46.276: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.850556ms)
    Dec 14 16:39:46.278: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 17.642519ms)
    Dec 14 16:39:46.278: INFO: (15) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 17.86854ms)
    Dec 14 16:39:46.283: INFO: (15) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 23.310608ms)
    Dec 14 16:39:46.285: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 24.648928ms)
    Dec 14 16:39:46.285: INFO: (15) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.790899ms)
    Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 25.221929ms)
    Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 26.647495ms)
    Dec 14 16:39:46.286: INFO: (15) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 25.733504ms)
    Dec 14 16:39:46.287: INFO: (15) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 28.048718ms)
    Dec 14 16:39:46.292: INFO: (15) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 33.209307ms)
    Dec 14 16:39:46.292: INFO: (15) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 32.517708ms)
    Dec 14 16:39:46.295: INFO: (15) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 34.745702ms)
    Dec 14 16:39:46.307: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 11.479277ms)
    Dec 14 16:39:46.308: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 11.379312ms)
    Dec 14 16:39:46.309: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 13.184538ms)
    Dec 14 16:39:46.311: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.269189ms)
    Dec 14 16:39:46.312: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 15.895731ms)
    Dec 14 16:39:46.313: INFO: (16) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 15.745037ms)
    Dec 14 16:39:46.319: INFO: (16) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 21.275918ms)
    Dec 14 16:39:46.320: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 22.943876ms)
    Dec 14 16:39:46.321: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 24.164088ms)
    Dec 14 16:39:46.323: INFO: (16) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 26.378235ms)
    Dec 14 16:39:46.323: INFO: (16) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 26.920506ms)
    Dec 14 16:39:46.324: INFO: (16) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 26.743248ms)
    Dec 14 16:39:46.327: INFO: (16) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.248057ms)
    Dec 14 16:39:46.327: INFO: (16) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 30.044223ms)
    Dec 14 16:39:46.330: INFO: (16) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 32.570632ms)
    Dec 14 16:39:46.331: INFO: (16) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 34.22637ms)
    Dec 14 16:39:46.345: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 13.685084ms)
    Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 24.113696ms)
    Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 24.331535ms)
    Dec 14 16:39:46.357: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 25.339602ms)
    Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 25.04297ms)
    Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 26.165402ms)
    Dec 14 16:39:46.358: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 26.186572ms)
    Dec 14 16:39:46.359: INFO: (17) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 26.134694ms)
    Dec 14 16:39:46.368: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 36.137228ms)
    Dec 14 16:39:46.372: INFO: (17) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 38.99711ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 41.011174ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 40.380769ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 42.29363ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 41.706327ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 41.871722ms)
    Dec 14 16:39:46.374: INFO: (17) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 42.679649ms)
    Dec 14 16:39:46.381: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 5.998859ms)
    Dec 14 16:39:46.384: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 9.152437ms)
    Dec 14 16:39:46.385: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 9.307139ms)
    Dec 14 16:39:46.386: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 11.143086ms)
    Dec 14 16:39:46.389: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 13.607669ms)
    Dec 14 16:39:46.389: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 13.215021ms)
    Dec 14 16:39:46.392: INFO: (18) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 15.89149ms)
    Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 20.339811ms)
    Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 20.964392ms)
    Dec 14 16:39:46.396: INFO: (18) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 19.387642ms)
    Dec 14 16:39:46.399: INFO: (18) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 22.755086ms)
    Dec 14 16:39:46.400: INFO: (18) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 23.522782ms)
    Dec 14 16:39:46.400: INFO: (18) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 24.086462ms)
    Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 28.141525ms)
    Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 27.710423ms)
    Dec 14 16:39:46.404: INFO: (18) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 27.191337ms)
    Dec 14 16:39:46.421: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:462/proxy/: tls qux (200; 16.788235ms)
    Dec 14 16:39:46.431: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 26.233298ms)
    Dec 14 16:39:46.431: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 26.44979ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname2/proxy/: tls qux (200; 26.267768ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname1/proxy/: foo (200; 27.154691ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/services/proxy-service-l7ktp:portname2/proxy/: bar (200; 27.671155ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:162/proxy/: bar (200; 27.616924ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/http:proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">... (200; 27.259474ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2/proxy/rewriteme">test</a> (200; 27.415641ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:460/proxy/: tls baz (200; 27.943564ms)
    Dec 14 16:39:46.432: INFO: (19) /api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/https:proxy-service-l7ktp-5fph2:443/proxy/tlsrewritem... (200; 27.714107ms)
    Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:160/proxy/: foo (200; 30.933955ms)
    Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/services/https:proxy-service-l7ktp:tlsportname1/proxy/: tls baz (200; 30.919988ms)
    Dec 14 16:39:46.435: INFO: (19) /api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/: <a href="/api/v1/namespaces/proxy-4982/pods/proxy-service-l7ktp-5fph2:1080/proxy/rewriteme">test<... (200; 30.17637ms)
    Dec 14 16:39:46.436: INFO: (19) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname1/proxy/: foo (200; 31.347574ms)
    Dec 14 16:39:46.437: INFO: (19) /api/v1/namespaces/proxy-4982/services/http:proxy-service-l7ktp:portname2/proxy/: bar (200; 32.583008ms)
    STEP: deleting ReplicationController proxy-service-l7ktp in namespace proxy-4982, will wait for the garbage collector to delete the pods 12/14/22 16:39:46.437
    Dec 14 16:39:46.505: INFO: Deleting ReplicationController proxy-service-l7ktp took: 10.878177ms
    Dec 14 16:39:46.605: INFO: Terminating ReplicationController proxy-service-l7ktp pods took: 100.421774ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:39:49.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4982" for this suite. 12/14/22 16:39:49.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:39:49.439
Dec 14 16:39:49.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 16:39:49.447
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:49.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:49.47
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5845 12/14/22 16:39:49.474
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Dec 14 16:39:49.513: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:39:59.522: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 12/14/22 16:39:59.534
W1214 16:39:59.562396      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 16:39:59.573: INFO: Found 1 stateful pods, waiting for 2
Dec 14 16:40:09.582: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:40:09.582: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 12/14/22 16:40:09.592
STEP: Delete all of the StatefulSets 12/14/22 16:40:09.596
STEP: Verify that StatefulSets have been deleted 12/14/22 16:40:09.609
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 16:40:09.615: INFO: Deleting all statefulset in ns statefulset-5845
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:40:09.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5845" for this suite. 12/14/22 16:40:09.698
------------------------------
• [SLOW TEST] [20.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:39:49.439
    Dec 14 16:39:49.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 16:39:49.447
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:39:49.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:39:49.47
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5845 12/14/22 16:39:49.474
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Dec 14 16:39:49.513: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 16:39:59.522: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 12/14/22 16:39:59.534
    W1214 16:39:59.562396      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 16:39:59.573: INFO: Found 1 stateful pods, waiting for 2
    Dec 14 16:40:09.582: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 16:40:09.582: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 12/14/22 16:40:09.592
    STEP: Delete all of the StatefulSets 12/14/22 16:40:09.596
    STEP: Verify that StatefulSets have been deleted 12/14/22 16:40:09.609
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 16:40:09.615: INFO: Deleting all statefulset in ns statefulset-5845
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:40:09.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5845" for this suite. 12/14/22 16:40:09.698
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:40:09.714
Dec 14 16:40:09.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 16:40:09.718
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:40:09.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:40:09.746
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Dec 14 16:40:09.750: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/14/22 16:40:09.765
STEP: Checking rc "condition-test" has the desired failure condition set 12/14/22 16:40:09.776
STEP: Scaling down rc "condition-test" to satisfy pod quota 12/14/22 16:40:10.797
Dec 14 16:40:10.808: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 12/14/22 16:40:10.808
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:40:10.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-406" for this suite. 12/14/22 16:40:10.833
------------------------------
• [1.129 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:40:09.714
    Dec 14 16:40:09.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 16:40:09.718
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:40:09.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:40:09.746
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Dec 14 16:40:09.750: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/14/22 16:40:09.765
    STEP: Checking rc "condition-test" has the desired failure condition set 12/14/22 16:40:09.776
    STEP: Scaling down rc "condition-test" to satisfy pod quota 12/14/22 16:40:10.797
    Dec 14 16:40:10.808: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 12/14/22 16:40:10.808
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:40:10.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-406" for this suite. 12/14/22 16:40:10.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:40:10.852
Dec 14 16:40:10.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 16:40:10.854
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:40:10.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:40:10.88
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 in namespace container-probe-8351 12/14/22 16:40:10.886
Dec 14 16:40:10.900: INFO: Waiting up to 5m0s for pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7" in namespace "container-probe-8351" to be "not pending"
Dec 14 16:40:10.904: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521687ms
Dec 14 16:40:12.914: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014424057s
Dec 14 16:40:12.914: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7" satisfied condition "not pending"
Dec 14 16:40:12.915: INFO: Started pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 in namespace container-probe-8351
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 16:40:12.915
Dec 14 16:40:12.921: INFO: Initial restart count of pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is 0
Dec 14 16:40:33.036: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 1 (20.114887743s elapsed)
Dec 14 16:40:53.137: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 2 (40.216109039s elapsed)
Dec 14 16:41:13.229: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 3 (1m0.307718373s elapsed)
Dec 14 16:41:33.347: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 4 (1m20.425688088s elapsed)
Dec 14 16:42:45.730: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 5 (2m32.809261204s elapsed)
STEP: deleting the pod 12/14/22 16:42:45.731
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 16:42:45.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8351" for this suite. 12/14/22 16:42:45.757
------------------------------
• [SLOW TEST] [154.914 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:40:10.852
    Dec 14 16:40:10.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 16:40:10.854
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:40:10.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:40:10.88
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 in namespace container-probe-8351 12/14/22 16:40:10.886
    Dec 14 16:40:10.900: INFO: Waiting up to 5m0s for pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7" in namespace "container-probe-8351" to be "not pending"
    Dec 14 16:40:10.904: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521687ms
    Dec 14 16:40:12.914: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014424057s
    Dec 14 16:40:12.914: INFO: Pod "liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7" satisfied condition "not pending"
    Dec 14 16:40:12.915: INFO: Started pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 in namespace container-probe-8351
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 16:40:12.915
    Dec 14 16:40:12.921: INFO: Initial restart count of pod liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is 0
    Dec 14 16:40:33.036: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 1 (20.114887743s elapsed)
    Dec 14 16:40:53.137: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 2 (40.216109039s elapsed)
    Dec 14 16:41:13.229: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 3 (1m0.307718373s elapsed)
    Dec 14 16:41:33.347: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 4 (1m20.425688088s elapsed)
    Dec 14 16:42:45.730: INFO: Restart count of pod container-probe-8351/liveness-e860deaf-080c-436f-9e62-a50a5cd8f2d7 is now 5 (2m32.809261204s elapsed)
    STEP: deleting the pod 12/14/22 16:42:45.731
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:42:45.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8351" for this suite. 12/14/22 16:42:45.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:42:45.775
Dec 14 16:42:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:42:45.781
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:42:45.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:42:45.812
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 16:42:45.816
Dec 14 16:42:45.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 16:42:45.985: INFO: stderr: ""
Dec 14 16:42:45.985: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 12/14/22 16:42:45.985
STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 16:42:51.037
Dec 14 16:42:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 get pod e2e-test-httpd-pod -o json'
Dec 14 16:42:51.159: INFO: stderr: ""
Dec 14 16:42:51.159: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-12-14T16:42:45Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5034\",\n        \"resourceVersion\": \"22346\",\n        \"uid\": \"0187544e-d42b-4159-872d-8d3265e81350\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gzskl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"iet9eich7uhu-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gzskl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9a140af13428d884ce3733c3f59870de936bd8e8618b0c20a3de1702de14504e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-14T16:42:47Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.56\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.67.18\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.67.18\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-14T16:42:45Z\"\n    }\n}\n"
STEP: replace the image in the pod 12/14/22 16:42:51.16
Dec 14 16:42:51.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 replace -f -'
Dec 14 16:42:52.406: INFO: stderr: ""
Dec 14 16:42:52.407: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/14/22 16:42:52.407
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Dec 14 16:42:52.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 delete pods e2e-test-httpd-pod'
Dec 14 16:42:54.560: INFO: stderr: ""
Dec 14 16:42:54.561: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:42:54.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5034" for this suite. 12/14/22 16:42:54.579
------------------------------
• [SLOW TEST] [8.829 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:42:45.775
    Dec 14 16:42:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:42:45.781
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:42:45.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:42:45.812
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 16:42:45.816
    Dec 14 16:42:45.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 14 16:42:45.985: INFO: stderr: ""
    Dec 14 16:42:45.985: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 12/14/22 16:42:45.985
    STEP: verifying the pod e2e-test-httpd-pod was created 12/14/22 16:42:51.037
    Dec 14 16:42:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 get pod e2e-test-httpd-pod -o json'
    Dec 14 16:42:51.159: INFO: stderr: ""
    Dec 14 16:42:51.159: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-12-14T16:42:45Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5034\",\n        \"resourceVersion\": \"22346\",\n        \"uid\": \"0187544e-d42b-4159-872d-8d3265e81350\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gzskl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"iet9eich7uhu-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gzskl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T16:42:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9a140af13428d884ce3733c3f59870de936bd8e8618b0c20a3de1702de14504e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-14T16:42:47Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.56\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.67.18\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.67.18\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-14T16:42:45Z\"\n    }\n}\n"
    STEP: replace the image in the pod 12/14/22 16:42:51.16
    Dec 14 16:42:51.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 replace -f -'
    Dec 14 16:42:52.406: INFO: stderr: ""
    Dec 14 16:42:52.407: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/14/22 16:42:52.407
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Dec 14 16:42:52.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-5034 delete pods e2e-test-httpd-pod'
    Dec 14 16:42:54.560: INFO: stderr: ""
    Dec 14 16:42:54.561: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:42:54.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5034" for this suite. 12/14/22 16:42:54.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:42:54.6
Dec 14 16:42:54.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption 12/14/22 16:42:54.605
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:42:54.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:42:54.637
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 12/14/22 16:42:54.64
STEP: Waiting for the pdb to be processed 12/14/22 16:42:54.649
STEP: First trying to evict a pod which shouldn't be evictable 12/14/22 16:42:56.677
STEP: Waiting for all pods to be running 12/14/22 16:42:56.678
Dec 14 16:42:56.682: INFO: pods: 0 < 3
STEP: locating a running pod 12/14/22 16:42:58.69
STEP: Updating the pdb to allow a pod to be evicted 12/14/22 16:42:58.709
STEP: Waiting for the pdb to be processed 12/14/22 16:42:58.724
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 16:43:00.74
STEP: Waiting for all pods to be running 12/14/22 16:43:00.74
STEP: Waiting for the pdb to observed all healthy pods 12/14/22 16:43:00.746
STEP: Patching the pdb to disallow a pod to be evicted 12/14/22 16:43:00.792
STEP: Waiting for the pdb to be processed 12/14/22 16:43:00.868
STEP: Waiting for all pods to be running 12/14/22 16:43:02.885
STEP: locating a running pod 12/14/22 16:43:02.895
STEP: Deleting the pdb to allow a pod to be evicted 12/14/22 16:43:02.913
STEP: Waiting for the pdb to be deleted 12/14/22 16:43:02.923
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 16:43:02.927
STEP: Waiting for all pods to be running 12/14/22 16:43:02.927
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:02.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4339" for this suite. 12/14/22 16:43:02.999
------------------------------
• [SLOW TEST] [8.451 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:42:54.6
    Dec 14 16:42:54.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption 12/14/22 16:42:54.605
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:42:54.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:42:54.637
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 12/14/22 16:42:54.64
    STEP: Waiting for the pdb to be processed 12/14/22 16:42:54.649
    STEP: First trying to evict a pod which shouldn't be evictable 12/14/22 16:42:56.677
    STEP: Waiting for all pods to be running 12/14/22 16:42:56.678
    Dec 14 16:42:56.682: INFO: pods: 0 < 3
    STEP: locating a running pod 12/14/22 16:42:58.69
    STEP: Updating the pdb to allow a pod to be evicted 12/14/22 16:42:58.709
    STEP: Waiting for the pdb to be processed 12/14/22 16:42:58.724
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 16:43:00.74
    STEP: Waiting for all pods to be running 12/14/22 16:43:00.74
    STEP: Waiting for the pdb to observed all healthy pods 12/14/22 16:43:00.746
    STEP: Patching the pdb to disallow a pod to be evicted 12/14/22 16:43:00.792
    STEP: Waiting for the pdb to be processed 12/14/22 16:43:00.868
    STEP: Waiting for all pods to be running 12/14/22 16:43:02.885
    STEP: locating a running pod 12/14/22 16:43:02.895
    STEP: Deleting the pdb to allow a pod to be evicted 12/14/22 16:43:02.913
    STEP: Waiting for the pdb to be deleted 12/14/22 16:43:02.923
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/14/22 16:43:02.927
    STEP: Waiting for all pods to be running 12/14/22 16:43:02.927
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:02.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4339" for this suite. 12/14/22 16:43:02.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:03.053
Dec 14 16:43:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:43:03.056
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:03.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:03.086
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 12/14/22 16:43:03.09
STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 16:43:03.099
STEP: delete the deployment 12/14/22 16:43:03.621
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/14/22 16:43:03.634
STEP: Gathering metrics 12/14/22 16:43:04.167
Dec 14 16:43:04.205: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 16:43:04.210: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.812906ms
Dec 14 16:43:04.210: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 16:43:04.210: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 16:43:04.332: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9692" for this suite. 12/14/22 16:43:04.343
------------------------------
• [1.302 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:03.053
    Dec 14 16:43:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:43:03.056
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:03.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:03.086
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 12/14/22 16:43:03.09
    STEP: Wait for the Deployment to create new ReplicaSet 12/14/22 16:43:03.099
    STEP: delete the deployment 12/14/22 16:43:03.621
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/14/22 16:43:03.634
    STEP: Gathering metrics 12/14/22 16:43:04.167
    Dec 14 16:43:04.205: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 16:43:04.210: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.812906ms
    Dec 14 16:43:04.210: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 16:43:04.210: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 16:43:04.332: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9692" for this suite. 12/14/22 16:43:04.343
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:04.359
Dec 14 16:43:04.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:43:04.362
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:04.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:04.401
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-45d04c91-c55f-4b37-91cc-3ab07c8375d8 12/14/22 16:43:04.405
STEP: Creating a pod to test consume configMaps 12/14/22 16:43:04.411
Dec 14 16:43:04.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a" in namespace "projected-9356" to be "Succeeded or Failed"
Dec 14 16:43:04.437: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.426484ms
Dec 14 16:43:06.444: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501925s
Dec 14 16:43:08.449: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023203372s
STEP: Saw pod success 12/14/22 16:43:08.449
Dec 14 16:43:08.449: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a" satisfied condition "Succeeded or Failed"
Dec 14 16:43:08.458: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a container projected-configmap-volume-test: <nil>
STEP: delete the pod 12/14/22 16:43:08.484
Dec 14 16:43:08.498: INFO: Waiting for pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a to disappear
Dec 14 16:43:08.502: INFO: Pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:08.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9356" for this suite. 12/14/22 16:43:08.512
------------------------------
• [4.163 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:04.359
    Dec 14 16:43:04.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:43:04.362
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:04.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:04.401
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-45d04c91-c55f-4b37-91cc-3ab07c8375d8 12/14/22 16:43:04.405
    STEP: Creating a pod to test consume configMaps 12/14/22 16:43:04.411
    Dec 14 16:43:04.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a" in namespace "projected-9356" to be "Succeeded or Failed"
    Dec 14 16:43:04.437: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.426484ms
    Dec 14 16:43:06.444: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501925s
    Dec 14 16:43:08.449: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023203372s
    STEP: Saw pod success 12/14/22 16:43:08.449
    Dec 14 16:43:08.449: INFO: Pod "pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a" satisfied condition "Succeeded or Failed"
    Dec 14 16:43:08.458: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a container projected-configmap-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:43:08.484
    Dec 14 16:43:08.498: INFO: Waiting for pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a to disappear
    Dec 14 16:43:08.502: INFO: Pod pod-projected-configmaps-bff872b7-70d6-4137-898b-26011bd0299a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:08.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9356" for this suite. 12/14/22 16:43:08.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:08.527
Dec 14 16:43:08.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:43:08.531
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:08.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:08.562
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Dec 14 16:43:08.608: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8622 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:08.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8622" for this suite. 12/14/22 16:43:08.65
------------------------------
• [0.186 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:08.527
    Dec 14 16:43:08.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:43:08.531
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:08.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:08.562
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Dec 14 16:43:08.608: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8622 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:08.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8622" for this suite. 12/14/22 16:43:08.65
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:08.716
Dec 14 16:43:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename watch 12/14/22 16:43:08.72
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:08.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:08.745
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 12/14/22 16:43:08.749
STEP: creating a watch on configmaps with label B 12/14/22 16:43:08.752
STEP: creating a watch on configmaps with label A or B 12/14/22 16:43:08.754
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.756
Dec 14 16:43:08.763: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22600 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:08.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22600 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.765
Dec 14 16:43:08.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22601 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:08.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22601 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/14/22 16:43:08.785
Dec 14 16:43:08.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22602 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:08.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22602 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.797
Dec 14 16:43:08.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22603 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:08.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22603 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/14/22 16:43:08.805
Dec 14 16:43:08.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22604 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:08.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22604 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/14/22 16:43:18.82
Dec 14 16:43:18.837: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22671 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:43:18.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22671 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:28.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2675" for this suite. 12/14/22 16:43:28.85
------------------------------
• [SLOW TEST] [20.147 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:08.716
    Dec 14 16:43:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename watch 12/14/22 16:43:08.72
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:08.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:08.745
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 12/14/22 16:43:08.749
    STEP: creating a watch on configmaps with label B 12/14/22 16:43:08.752
    STEP: creating a watch on configmaps with label A or B 12/14/22 16:43:08.754
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.756
    Dec 14 16:43:08.763: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22600 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:08.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22600 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.765
    Dec 14 16:43:08.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22601 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:08.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22601 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/14/22 16:43:08.785
    Dec 14 16:43:08.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22602 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:08.797: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22602 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/14/22 16:43:08.797
    Dec 14 16:43:08.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22603 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:08.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2675  72ac09f3-9624-471d-85df-7a817b961378 22603 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/14/22 16:43:08.805
    Dec 14 16:43:08.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22604 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:08.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22604 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/14/22 16:43:18.82
    Dec 14 16:43:18.837: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22671 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 14 16:43:18.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2675  26589f5f-215c-4d72-9dae-e987f81378d2 22671 0 2022-12-14 16:43:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-14 16:43:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:28.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2675" for this suite. 12/14/22 16:43:28.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:28.865
Dec 14 16:43:28.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:43:28.87
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:28.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:28.893
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6719 12/14/22 16:43:28.897
STEP: creating a selector 12/14/22 16:43:28.897
STEP: Creating the service pods in kubernetes 12/14/22 16:43:28.897
Dec 14 16:43:28.897: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 16:43:28.944: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6719" to be "running and ready"
Dec 14 16:43:28.975: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.602152ms
Dec 14 16:43:28.977: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:43:30.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.039704037s
Dec 14 16:43:30.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:43:32.987: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043237987s
Dec 14 16:43:32.987: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:43:34.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044024665s
Dec 14 16:43:34.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:43:36.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.038211788s
Dec 14 16:43:36.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:43:38.985: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.04107954s
Dec 14 16:43:38.985: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 16:43:40.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.037206382s
Dec 14 16:43:40.981: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 16:43:40.981: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 16:43:40.987: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6719" to be "running and ready"
Dec 14 16:43:40.992: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.641813ms
Dec 14 16:43:40.992: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 16:43:40.992: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 14 16:43:40.996: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6719" to be "running and ready"
Dec 14 16:43:41.001: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.680494ms
Dec 14 16:43:41.001: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 14 16:43:41.001: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 16:43:41.004
Dec 14 16:43:41.014: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6719" to be "running"
Dec 14 16:43:41.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00923ms
Dec 14 16:43:43.026: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012024232s
Dec 14 16:43:43.027: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 16:43:43.035: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 16:43:43.035: INFO: Breadth first check of 10.233.64.116 on host 192.168.121.21...
Dec 14 16:43:43.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.64.116&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:43:43.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:43:43.043: INFO: ExecWithOptions: Clientset creation
Dec 14 16:43:43.044: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:43:43.181: INFO: Waiting for responses: map[]
Dec 14 16:43:43.181: INFO: reached 10.233.64.116 after 0/1 tries
Dec 14 16:43:43.181: INFO: Breadth first check of 10.233.66.113 on host 192.168.121.16...
Dec 14 16:43:43.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.66.113&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:43:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:43:43.188: INFO: ExecWithOptions: Clientset creation
Dec 14 16:43:43.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.113%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:43:43.292: INFO: Waiting for responses: map[]
Dec 14 16:43:43.292: INFO: reached 10.233.66.113 after 0/1 tries
Dec 14 16:43:43.292: INFO: Breadth first check of 10.233.67.22 on host 192.168.121.56...
Dec 14 16:43:43.297: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.67.22&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:43:43.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:43:43.298: INFO: ExecWithOptions: Clientset creation
Dec 14 16:43:43.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.67.22%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:43:43.404: INFO: Waiting for responses: map[]
Dec 14 16:43:43.404: INFO: reached 10.233.67.22 after 0/1 tries
Dec 14 16:43:43.404: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:43.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6719" for this suite. 12/14/22 16:43:43.42
------------------------------
• [SLOW TEST] [14.571 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:28.865
    Dec 14 16:43:28.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 16:43:28.87
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:28.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:28.893
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6719 12/14/22 16:43:28.897
    STEP: creating a selector 12/14/22 16:43:28.897
    STEP: Creating the service pods in kubernetes 12/14/22 16:43:28.897
    Dec 14 16:43:28.897: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 16:43:28.944: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6719" to be "running and ready"
    Dec 14 16:43:28.975: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.602152ms
    Dec 14 16:43:28.977: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:43:30.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.039704037s
    Dec 14 16:43:30.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:43:32.987: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043237987s
    Dec 14 16:43:32.987: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:43:34.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044024665s
    Dec 14 16:43:34.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:43:36.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.038211788s
    Dec 14 16:43:36.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:43:38.985: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.04107954s
    Dec 14 16:43:38.985: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 16:43:40.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.037206382s
    Dec 14 16:43:40.981: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 16:43:40.981: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 16:43:40.987: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6719" to be "running and ready"
    Dec 14 16:43:40.992: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.641813ms
    Dec 14 16:43:40.992: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 16:43:40.992: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 14 16:43:40.996: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6719" to be "running and ready"
    Dec 14 16:43:41.001: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.680494ms
    Dec 14 16:43:41.001: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 14 16:43:41.001: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 16:43:41.004
    Dec 14 16:43:41.014: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6719" to be "running"
    Dec 14 16:43:41.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00923ms
    Dec 14 16:43:43.026: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012024232s
    Dec 14 16:43:43.027: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 16:43:43.035: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 14 16:43:43.035: INFO: Breadth first check of 10.233.64.116 on host 192.168.121.21...
    Dec 14 16:43:43.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.64.116&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:43:43.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:43:43.043: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:43:43.044: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 16:43:43.181: INFO: Waiting for responses: map[]
    Dec 14 16:43:43.181: INFO: reached 10.233.64.116 after 0/1 tries
    Dec 14 16:43:43.181: INFO: Breadth first check of 10.233.66.113 on host 192.168.121.16...
    Dec 14 16:43:43.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.66.113&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:43:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:43:43.188: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:43:43.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.113%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 16:43:43.292: INFO: Waiting for responses: map[]
    Dec 14 16:43:43.292: INFO: reached 10.233.66.113 after 0/1 tries
    Dec 14 16:43:43.292: INFO: Breadth first check of 10.233.67.22 on host 192.168.121.56...
    Dec 14 16:43:43.297: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.23:9080/dial?request=hostname&protocol=http&host=10.233.67.22&port=8083&tries=1'] Namespace:pod-network-test-6719 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:43:43.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:43:43.298: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:43:43.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6719/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.67.22%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 16:43:43.404: INFO: Waiting for responses: map[]
    Dec 14 16:43:43.404: INFO: reached 10.233.67.22 after 0/1 tries
    Dec 14 16:43:43.404: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:43.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6719" for this suite. 12/14/22 16:43:43.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:43.452
Dec 14 16:43:43.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:43:43.454
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:43.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:43.485
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  12/14/22 16:43:43.489
Dec 14 16:43:43.505: INFO: Waiting up to 5m0s for pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082" in namespace "svcaccounts-2863" to be "Succeeded or Failed"
Dec 14 16:43:43.509: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500445ms
Dec 14 16:43:45.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013805751s
Dec 14 16:43:47.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014119992s
STEP: Saw pod success 12/14/22 16:43:47.519
Dec 14 16:43:47.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082" satisfied condition "Succeeded or Failed"
Dec 14 16:43:47.525: INFO: Trying to get logs from node iet9eich7uhu-3 pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:43:47.536
Dec 14 16:43:47.562: INFO: Waiting for pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 to disappear
Dec 14 16:43:47.566: INFO: Pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:47.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2863" for this suite. 12/14/22 16:43:47.573
------------------------------
• [4.136 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:43.452
    Dec 14 16:43:43.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:43:43.454
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:43.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:43.485
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  12/14/22 16:43:43.489
    Dec 14 16:43:43.505: INFO: Waiting up to 5m0s for pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082" in namespace "svcaccounts-2863" to be "Succeeded or Failed"
    Dec 14 16:43:43.509: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500445ms
    Dec 14 16:43:45.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013805751s
    Dec 14 16:43:47.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014119992s
    STEP: Saw pod success 12/14/22 16:43:47.519
    Dec 14 16:43:47.519: INFO: Pod "test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082" satisfied condition "Succeeded or Failed"
    Dec 14 16:43:47.525: INFO: Trying to get logs from node iet9eich7uhu-3 pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:43:47.536
    Dec 14 16:43:47.562: INFO: Waiting for pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 to disappear
    Dec 14 16:43:47.566: INFO: Pod test-pod-833c833e-1ba9-450e-b7a8-5c28a1858082 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:47.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2863" for this suite. 12/14/22 16:43:47.573
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:47.589
Dec 14 16:43:47.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:43:47.592
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:47.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:47.619
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 12/14/22 16:43:47.623
Dec 14 16:43:47.645: INFO: Waiting up to 5m0s for pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67" in namespace "downward-api-4944" to be "Succeeded or Failed"
Dec 14 16:43:47.671: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Pending", Reason="", readiness=false. Elapsed: 25.334524ms
Dec 14 16:43:49.678: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032731781s
Dec 14 16:43:51.680: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034349015s
STEP: Saw pod success 12/14/22 16:43:51.68
Dec 14 16:43:51.680: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67" satisfied condition "Succeeded or Failed"
Dec 14 16:43:51.684: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 container dapi-container: <nil>
STEP: delete the pod 12/14/22 16:43:51.693
Dec 14 16:43:51.712: INFO: Waiting for pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 to disappear
Dec 14 16:43:51.715: INFO: Pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 14 16:43:51.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4944" for this suite. 12/14/22 16:43:51.721
------------------------------
• [4.140 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:47.589
    Dec 14 16:43:47.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:43:47.592
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:47.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:47.619
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 12/14/22 16:43:47.623
    Dec 14 16:43:47.645: INFO: Waiting up to 5m0s for pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67" in namespace "downward-api-4944" to be "Succeeded or Failed"
    Dec 14 16:43:47.671: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Pending", Reason="", readiness=false. Elapsed: 25.334524ms
    Dec 14 16:43:49.678: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032731781s
    Dec 14 16:43:51.680: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034349015s
    STEP: Saw pod success 12/14/22 16:43:51.68
    Dec 14 16:43:51.680: INFO: Pod "downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67" satisfied condition "Succeeded or Failed"
    Dec 14 16:43:51.684: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 16:43:51.693
    Dec 14 16:43:51.712: INFO: Waiting for pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 to disappear
    Dec 14 16:43:51.715: INFO: Pod downward-api-67935de4-ac8b-44f7-a51c-9cd338c6fc67 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:43:51.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4944" for this suite. 12/14/22 16:43:51.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:43:51.74
Dec 14 16:43:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:43:51.743
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:51.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:51.769
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7570 12/14/22 16:43:51.776
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[] 12/14/22 16:43:51.791
Dec 14 16:43:51.804: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 16:43:52.824: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7570 12/14/22 16:43:52.824
Dec 14 16:43:52.840: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7570" to be "running and ready"
Dec 14 16:43:52.865: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.481844ms
Dec 14 16:43:52.865: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:43:54.874: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033519939s
Dec 14 16:43:54.874: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 14 16:43:54.874: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod1:[80]] 12/14/22 16:43:54.88
Dec 14 16:43:54.901: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 12/14/22 16:43:54.901
Dec 14 16:43:54.902: INFO: Creating new exec pod
Dec 14 16:43:54.912: INFO: Waiting up to 5m0s for pod "execpodf94h6" in namespace "services-7570" to be "running"
Dec 14 16:43:54.916: INFO: Pod "execpodf94h6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447688ms
Dec 14 16:43:56.924: INFO: Pod "execpodf94h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011782178s
Dec 14 16:43:56.924: INFO: Pod "execpodf94h6" satisfied condition "running"
Dec 14 16:43:57.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:00.281: INFO: rc: 1
Dec 14 16:44:00.281: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Dec 14 16:44:01.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:03.505: INFO: rc: 1
Dec 14 16:44:03.505: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Dec 14 16:44:04.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:06.535: INFO: rc: 1
Dec 14 16:44:06.535: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Dec 14 16:44:07.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:07.527: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:07.527: INFO: stdout: ""
Dec 14 16:44:07.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
Dec 14 16:44:07.742: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:07.742: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7570 12/14/22 16:44:07.742
Dec 14 16:44:07.753: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7570" to be "running and ready"
Dec 14 16:44:07.759: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.362353ms
Dec 14 16:44:07.759: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:44:09.766: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013431789s
Dec 14 16:44:09.766: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 14 16:44:09.766: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod1:[80] pod2:[80]] 12/14/22 16:44:09.77
Dec 14 16:44:09.801: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 12/14/22 16:44:09.801
Dec 14 16:44:10.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:11.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:11.056: INFO: stdout: ""
Dec 14 16:44:11.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
Dec 14 16:44:11.304: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:11.304: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7570 12/14/22 16:44:11.304
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod2:[80]] 12/14/22 16:44:11.328
Dec 14 16:44:12.362: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 12/14/22 16:44:12.362
Dec 14 16:44:13.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec 14 16:44:13.595: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:13.595: INFO: stdout: ""
Dec 14 16:44:13.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
Dec 14 16:44:13.929: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:13.929: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7570 12/14/22 16:44:13.93
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[] 12/14/22 16:44:13.953
Dec 14 16:44:13.983: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:44:14.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7570" for this suite. 12/14/22 16:44:14.073
------------------------------
• [SLOW TEST] [22.345 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:43:51.74
    Dec 14 16:43:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:43:51.743
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:43:51.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:43:51.769
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7570 12/14/22 16:43:51.776
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[] 12/14/22 16:43:51.791
    Dec 14 16:43:51.804: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Dec 14 16:43:52.824: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7570 12/14/22 16:43:52.824
    Dec 14 16:43:52.840: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7570" to be "running and ready"
    Dec 14 16:43:52.865: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.481844ms
    Dec 14 16:43:52.865: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:43:54.874: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033519939s
    Dec 14 16:43:54.874: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 14 16:43:54.874: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod1:[80]] 12/14/22 16:43:54.88
    Dec 14 16:43:54.901: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 12/14/22 16:43:54.901
    Dec 14 16:43:54.902: INFO: Creating new exec pod
    Dec 14 16:43:54.912: INFO: Waiting up to 5m0s for pod "execpodf94h6" in namespace "services-7570" to be "running"
    Dec 14 16:43:54.916: INFO: Pod "execpodf94h6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447688ms
    Dec 14 16:43:56.924: INFO: Pod "execpodf94h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011782178s
    Dec 14 16:43:56.924: INFO: Pod "execpodf94h6" satisfied condition "running"
    Dec 14 16:43:57.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:00.281: INFO: rc: 1
    Dec 14 16:44:00.281: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Dec 14 16:44:01.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:03.505: INFO: rc: 1
    Dec 14 16:44:03.505: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Dec 14 16:44:04.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:06.535: INFO: rc: 1
    Dec 14 16:44:06.535: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Dec 14 16:44:07.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:07.527: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:07.527: INFO: stdout: ""
    Dec 14 16:44:07.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
    Dec 14 16:44:07.742: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:07.742: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7570 12/14/22 16:44:07.742
    Dec 14 16:44:07.753: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7570" to be "running and ready"
    Dec 14 16:44:07.759: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.362353ms
    Dec 14 16:44:07.759: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:44:09.766: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013431789s
    Dec 14 16:44:09.766: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 14 16:44:09.766: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod1:[80] pod2:[80]] 12/14/22 16:44:09.77
    Dec 14 16:44:09.801: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 12/14/22 16:44:09.801
    Dec 14 16:44:10.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:11.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:11.056: INFO: stdout: ""
    Dec 14 16:44:11.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
    Dec 14 16:44:11.304: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:11.304: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7570 12/14/22 16:44:11.304
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[pod2:[80]] 12/14/22 16:44:11.328
    Dec 14 16:44:12.362: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 12/14/22 16:44:12.362
    Dec 14 16:44:13.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec 14 16:44:13.595: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:13.595: INFO: stdout: ""
    Dec 14 16:44:13.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-7570 exec execpodf94h6 -- /bin/sh -x -c nc -v -z -w 2 10.233.20.38 80'
    Dec 14 16:44:13.929: INFO: stderr: "+ nc -v -z -w 2 10.233.20.38 80\nConnection to 10.233.20.38 80 port [tcp/http] succeeded!\n"
    Dec 14 16:44:13.929: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7570 12/14/22 16:44:13.93
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7570 to expose endpoints map[] 12/14/22 16:44:13.953
    Dec 14 16:44:13.983: INFO: successfully validated that service endpoint-test2 in namespace services-7570 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:44:14.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7570" for this suite. 12/14/22 16:44:14.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:44:14.104
Dec 14 16:44:14.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:44:14.107
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:14.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:14.135
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 12/14/22 16:44:14.144
STEP: waiting for Deployment to be created 12/14/22 16:44:14.152
STEP: waiting for all Replicas to be Ready 12/14/22 16:44:14.157
Dec 14 16:44:14.164: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.165: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.186: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.186: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.255: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.256: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.281: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.281: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 16:44:14.991: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 16:44:14.993: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 16:44:15.449: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 12/14/22 16:44:15.449
W1214 16:44:15.462354      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 14 16:44:15.466: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 12/14/22 16:44:15.466
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.485: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.485: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.532: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.532: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:15.558: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:15.558: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:17.476: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:17.476: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:17.538: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
STEP: listing Deployments 12/14/22 16:44:17.538
Dec 14 16:44:17.552: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 12/14/22 16:44:17.552
Dec 14 16:44:17.574: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 12/14/22 16:44:17.574
Dec 14 16:44:17.591: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:17.618: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:17.682: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:17.731: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:19.358: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:19.503: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:19.596: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:19.654: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 16:44:21.006: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 12/14/22 16:44:21.023
STEP: fetching the DeploymentStatus 12/14/22 16:44:21.035
Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3
Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:21.046: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
Dec 14 16:44:21.046: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3
STEP: deleting the Deployment 12/14/22 16:44:21.047
Dec 14 16:44:21.071: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
Dec 14 16:44:21.074: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:44:21.090: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 14 16:44:21.101: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9660  46bfc907-9d08-4d93-9f14-d8dd56541425 23116 2 2022-12-14 16:44:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053057 0xc005053058}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050530e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec 14 16:44:21.115: INFO: pod: "test-deployment-7b7876f9d6-8jw7g":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-8jw7g test-deployment-7b7876f9d6- deployment-9660  a8f3d0a7-03b3-4277-b382-3e550baf4d90 23083 0 2022-12-14 16:44:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 46bfc907-9d08-4d93-9f14-d8dd56541425 0xc005053557 0xc005053558}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46bfc907-9d08-4d93-9f14-d8dd56541425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwjrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwjrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.30,StartTime:2022-12-14 16:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://16fb50fd6420f21f1990ae6d469eed09ed158f7417e02266d68ac40fad8a3430,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 16:44:21.116: INFO: pod: "test-deployment-7b7876f9d6-bkgd6":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-bkgd6 test-deployment-7b7876f9d6- deployment-9660  f41ab634-496c-4b51-a104-ea9b18cc2081 23115 0 2022-12-14 16:44:19 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 46bfc907-9d08-4d93-9f14-d8dd56541425 0xc005053757 0xc005053758}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46bfc907-9d08-4d93-9f14-d8dd56541425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxp9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxp9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.115,StartTime:2022-12-14 16:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f8947fa62bc90ead31ee416c040aa6ddc2630b785f3672ec6f394196f203da1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 16:44:21.116: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9660  24c2b693-2034-4f00-b3d5-665b1e4c52a6 23123 4 2022-12-14 16:44:15 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053147 0xc005053148}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050531d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 14 16:44:21.122: INFO: pod: "test-deployment-7df74c55ff-ld6pq":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-ld6pq test-deployment-7df74c55ff- deployment-9660  b205e81f-47c9-4f45-aae2-bddc77f9a6b5 23119 0 2022-12-14 16:44:17 +0000 UTC 2022-12-14 16:44:22 +0000 UTC 0xc0034d62a8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 24c2b693-2034-4f00-b3d5-665b1e4c52a6 0xc0034d6a57 0xc0034d6a58}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24c2b693-2034-4f00-b3d5-665b1e4c52a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qrqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qrqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.118,StartTime:2022-12-14 16:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://21293878d40170330f8f168009124ba1249bc136e4045f7294bcecbefafc545c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 16:44:21.123: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9660  2ab5bea5-3cd1-42a5-be7d-27aa9ca71aea 23020 3 2022-12-14 16:44:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053237 0xc005053238}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050532c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:44:21.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9660" for this suite. 12/14/22 16:44:21.141
------------------------------
• [SLOW TEST] [7.050 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:44:14.104
    Dec 14 16:44:14.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:44:14.107
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:14.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:14.135
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 12/14/22 16:44:14.144
    STEP: waiting for Deployment to be created 12/14/22 16:44:14.152
    STEP: waiting for all Replicas to be Ready 12/14/22 16:44:14.157
    Dec 14 16:44:14.164: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.165: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.186: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.186: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.255: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.256: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.281: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.281: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.991: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 14 16:44:14.993: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 14 16:44:15.449: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 12/14/22 16:44:15.449
    W1214 16:44:15.462354      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 14 16:44:15.466: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 12/14/22 16:44:15.466
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 0
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.470: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.485: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.485: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.532: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.532: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:15.558: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:15.558: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:17.476: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:17.476: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:17.538: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    STEP: listing Deployments 12/14/22 16:44:17.538
    Dec 14 16:44:17.552: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 12/14/22 16:44:17.552
    Dec 14 16:44:17.574: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 12/14/22 16:44:17.574
    Dec 14 16:44:17.591: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:17.618: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:17.682: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:17.731: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:19.358: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:19.503: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:19.596: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:19.654: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 14 16:44:21.006: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 12/14/22 16:44:21.023
    STEP: fetching the DeploymentStatus 12/14/22 16:44:21.035
    Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:21.044: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 1
    Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3
    Dec 14 16:44:21.045: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:21.046: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 2
    Dec 14 16:44:21.046: INFO: observed Deployment test-deployment in namespace deployment-9660 with ReadyReplicas 3
    STEP: deleting the Deployment 12/14/22 16:44:21.047
    Dec 14 16:44:21.071: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    Dec 14 16:44:21.074: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:44:21.090: INFO: Log out all the ReplicaSets if there is no deployment created
    Dec 14 16:44:21.101: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9660  46bfc907-9d08-4d93-9f14-d8dd56541425 23116 2 2022-12-14 16:44:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053057 0xc005053058}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050530e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Dec 14 16:44:21.115: INFO: pod: "test-deployment-7b7876f9d6-8jw7g":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-8jw7g test-deployment-7b7876f9d6- deployment-9660  a8f3d0a7-03b3-4277-b382-3e550baf4d90 23083 0 2022-12-14 16:44:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 46bfc907-9d08-4d93-9f14-d8dd56541425 0xc005053557 0xc005053558}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46bfc907-9d08-4d93-9f14-d8dd56541425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwjrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwjrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.30,StartTime:2022-12-14 16:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://16fb50fd6420f21f1990ae6d469eed09ed158f7417e02266d68ac40fad8a3430,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 14 16:44:21.116: INFO: pod: "test-deployment-7b7876f9d6-bkgd6":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-bkgd6 test-deployment-7b7876f9d6- deployment-9660  f41ab634-496c-4b51-a104-ea9b18cc2081 23115 0 2022-12-14 16:44:19 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 46bfc907-9d08-4d93-9f14-d8dd56541425 0xc005053757 0xc005053758}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46bfc907-9d08-4d93-9f14-d8dd56541425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxp9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxp9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.16,PodIP:10.233.66.115,StartTime:2022-12-14 16:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f8947fa62bc90ead31ee416c040aa6ddc2630b785f3672ec6f394196f203da1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 14 16:44:21.116: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9660  24c2b693-2034-4f00-b3d5-665b1e4c52a6 23123 4 2022-12-14 16:44:15 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053147 0xc005053148}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050531d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Dec 14 16:44:21.122: INFO: pod: "test-deployment-7df74c55ff-ld6pq":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-ld6pq test-deployment-7df74c55ff- deployment-9660  b205e81f-47c9-4f45-aae2-bddc77f9a6b5 23119 0 2022-12-14 16:44:17 +0000 UTC 2022-12-14 16:44:22 +0000 UTC 0xc0034d62a8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 24c2b693-2034-4f00-b3d5-665b1e4c52a6 0xc0034d6a57 0xc0034d6a58}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24c2b693-2034-4f00-b3d5-665b1e4c52a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qrqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qrqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.21,PodIP:10.233.64.118,StartTime:2022-12-14 16:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://21293878d40170330f8f168009124ba1249bc136e4045f7294bcecbefafc545c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 14 16:44:21.123: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9660  2ab5bea5-3cd1-42a5-be7d-27aa9ca71aea 23020 3 2022-12-14 16:44:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b50475d4-b8b0-4670-a08b-bad23601bae1 0xc005053237 0xc005053238}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b50475d4-b8b0-4670-a08b-bad23601bae1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050532c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:44:21.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9660" for this suite. 12/14/22 16:44:21.141
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:44:21.155
Dec 14 16:44:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:44:21.16
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:21.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:21.181
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Dec 14 16:44:21.186: INFO: Creating deployment "test-recreate-deployment"
Dec 14 16:44:21.193: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 16:44:21.207: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 14 16:44:23.220: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 16:44:23.225: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 16:44:23.240: INFO: Updating deployment test-recreate-deployment
Dec 14 16:44:23.241: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:44:23.375: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6255  fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 23183 2 2022-12-14 16:44:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e22308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 16:44:23 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-14 16:44:23 +0000 UTC,LastTransitionTime:2022-12-14 16:44:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 16:44:23.382: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6255  827725b0-58e1-4a08-b245-c3c5709e9f92 23181 1 2022-12-14 16:44:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 0xc0035eec70 0xc0035eec71}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eed08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:44:23.382: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 16:44:23.382: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6255  cf34807c-3986-4093-a899-6bdb67f3f133 23171 2 2022-12-14 16:44:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 0xc0035eeb57 0xc0035eeb58}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eec08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:44:23.389: INFO: Pod "test-recreate-deployment-cff6dc657-wbll7" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-wbll7 test-recreate-deployment-cff6dc657- deployment-6255  a921bf05-b0d4-4025-8327-377579f3a500 23182 0 2022-12-14 16:44:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 827725b0-58e1-4a08-b245-c3c5709e9f92 0xc0035ef180 0xc0035ef181}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827725b0-58e1-4a08-b245-c3c5709e9f92\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh9r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh9r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:44:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:44:23.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6255" for this suite. 12/14/22 16:44:23.397
------------------------------
• [2.250 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:44:21.155
    Dec 14 16:44:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:44:21.16
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:21.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:21.181
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Dec 14 16:44:21.186: INFO: Creating deployment "test-recreate-deployment"
    Dec 14 16:44:21.193: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Dec 14 16:44:21.207: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Dec 14 16:44:23.220: INFO: Waiting deployment "test-recreate-deployment" to complete
    Dec 14 16:44:23.225: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Dec 14 16:44:23.240: INFO: Updating deployment test-recreate-deployment
    Dec 14 16:44:23.241: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:44:23.375: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6255  fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 23183 2 2022-12-14 16:44:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e22308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 16:44:23 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-14 16:44:23 +0000 UTC,LastTransitionTime:2022-12-14 16:44:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 14 16:44:23.382: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6255  827725b0-58e1-4a08-b245-c3c5709e9f92 23181 1 2022-12-14 16:44:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 0xc0035eec70 0xc0035eec71}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eed08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:44:23.382: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Dec 14 16:44:23.382: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6255  cf34807c-3986-4093-a899-6bdb67f3f133 23171 2 2022-12-14 16:44:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500 0xc0035eeb57 0xc0035eeb58}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3a5c19-ebdd-40f7-b7a3-6a5ea4193500\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035eec08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:44:23.389: INFO: Pod "test-recreate-deployment-cff6dc657-wbll7" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-wbll7 test-recreate-deployment-cff6dc657- deployment-6255  a921bf05-b0d4-4025-8327-377579f3a500 23182 0 2022-12-14 16:44:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 827725b0-58e1-4a08-b245-c3c5709e9f92 0xc0035ef180 0xc0035ef181}] [] [{kube-controller-manager Update v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"827725b0-58e1-4a08-b245-c3c5709e9f92\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:44:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh9r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh9r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:44:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:,StartTime:2022-12-14 16:44:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:44:23.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6255" for this suite. 12/14/22 16:44:23.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:44:23.412
Dec 14 16:44:23.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 16:44:23.414
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:23.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:23.437
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:44:23.44
Dec 14 16:44:23.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c" in namespace "downward-api-8483" to be "Succeeded or Failed"
Dec 14 16:44:23.460: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760444ms
Dec 14 16:44:25.469: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013192726s
Dec 14 16:44:27.468: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Running", Reason="", readiness=false. Elapsed: 4.012606907s
Dec 14 16:44:29.470: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014141263s
STEP: Saw pod success 12/14/22 16:44:29.47
Dec 14 16:44:29.470: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c" satisfied condition "Succeeded or Failed"
Dec 14 16:44:29.476: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c container client-container: <nil>
STEP: delete the pod 12/14/22 16:44:29.494
Dec 14 16:44:29.520: INFO: Waiting for pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c to disappear
Dec 14 16:44:29.526: INFO: Pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 16:44:29.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8483" for this suite. 12/14/22 16:44:29.534
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:44:23.412
    Dec 14 16:44:23.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 16:44:23.414
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:23.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:23.437
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:44:23.44
    Dec 14 16:44:23.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c" in namespace "downward-api-8483" to be "Succeeded or Failed"
    Dec 14 16:44:23.460: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760444ms
    Dec 14 16:44:25.469: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013192726s
    Dec 14 16:44:27.468: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Running", Reason="", readiness=false. Elapsed: 4.012606907s
    Dec 14 16:44:29.470: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014141263s
    STEP: Saw pod success 12/14/22 16:44:29.47
    Dec 14 16:44:29.470: INFO: Pod "downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c" satisfied condition "Succeeded or Failed"
    Dec 14 16:44:29.476: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c container client-container: <nil>
    STEP: delete the pod 12/14/22 16:44:29.494
    Dec 14 16:44:29.520: INFO: Waiting for pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c to disappear
    Dec 14 16:44:29.526: INFO: Pod downwardapi-volume-071c11c1-8a26-4e08-b70b-f58c5a638f3c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:44:29.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8483" for this suite. 12/14/22 16:44:29.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:44:29.552
Dec 14 16:44:29.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename subpath 12/14/22 16:44:29.555
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:29.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:29.585
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 16:44:29.589
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-f8rv 12/14/22 16:44:29.604
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:44:29.604
Dec 14 16:44:29.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f8rv" in namespace "subpath-3968" to be "Succeeded or Failed"
Dec 14 16:44:29.631: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468333ms
Dec 14 16:44:31.640: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 2.015939735s
Dec 14 16:44:33.649: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 4.024756071s
Dec 14 16:44:35.650: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 6.025650053s
Dec 14 16:44:37.642: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 8.017865769s
Dec 14 16:44:39.642: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 10.017971848s
Dec 14 16:44:41.641: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 12.016723192s
Dec 14 16:44:43.639: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 14.014704373s
Dec 14 16:44:45.650: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 16.025991705s
Dec 14 16:44:47.648: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 18.02434218s
Dec 14 16:44:49.640: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 20.015969452s
Dec 14 16:44:51.641: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 22.016788214s
Dec 14 16:44:53.644: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=false. Elapsed: 24.019950547s
Dec 14 16:44:55.644: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020174993s
STEP: Saw pod success 12/14/22 16:44:55.644
Dec 14 16:44:55.646: INFO: Pod "pod-subpath-test-configmap-f8rv" satisfied condition "Succeeded or Failed"
Dec 14 16:44:55.657: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-configmap-f8rv container test-container-subpath-configmap-f8rv: <nil>
STEP: delete the pod 12/14/22 16:44:55.672
Dec 14 16:44:55.696: INFO: Waiting for pod pod-subpath-test-configmap-f8rv to disappear
Dec 14 16:44:55.701: INFO: Pod pod-subpath-test-configmap-f8rv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f8rv 12/14/22 16:44:55.701
Dec 14 16:44:55.702: INFO: Deleting pod "pod-subpath-test-configmap-f8rv" in namespace "subpath-3968"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 14 16:44:55.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3968" for this suite. 12/14/22 16:44:55.715
------------------------------
• [SLOW TEST] [26.173 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:44:29.552
    Dec 14 16:44:29.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename subpath 12/14/22 16:44:29.555
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:29.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:29.585
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 16:44:29.589
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-f8rv 12/14/22 16:44:29.604
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:44:29.604
    Dec 14 16:44:29.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f8rv" in namespace "subpath-3968" to be "Succeeded or Failed"
    Dec 14 16:44:29.631: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468333ms
    Dec 14 16:44:31.640: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 2.015939735s
    Dec 14 16:44:33.649: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 4.024756071s
    Dec 14 16:44:35.650: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 6.025650053s
    Dec 14 16:44:37.642: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 8.017865769s
    Dec 14 16:44:39.642: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 10.017971848s
    Dec 14 16:44:41.641: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 12.016723192s
    Dec 14 16:44:43.639: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 14.014704373s
    Dec 14 16:44:45.650: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 16.025991705s
    Dec 14 16:44:47.648: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 18.02434218s
    Dec 14 16:44:49.640: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 20.015969452s
    Dec 14 16:44:51.641: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=true. Elapsed: 22.016788214s
    Dec 14 16:44:53.644: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Running", Reason="", readiness=false. Elapsed: 24.019950547s
    Dec 14 16:44:55.644: INFO: Pod "pod-subpath-test-configmap-f8rv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020174993s
    STEP: Saw pod success 12/14/22 16:44:55.644
    Dec 14 16:44:55.646: INFO: Pod "pod-subpath-test-configmap-f8rv" satisfied condition "Succeeded or Failed"
    Dec 14 16:44:55.657: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-configmap-f8rv container test-container-subpath-configmap-f8rv: <nil>
    STEP: delete the pod 12/14/22 16:44:55.672
    Dec 14 16:44:55.696: INFO: Waiting for pod pod-subpath-test-configmap-f8rv to disappear
    Dec 14 16:44:55.701: INFO: Pod pod-subpath-test-configmap-f8rv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-f8rv 12/14/22 16:44:55.701
    Dec 14 16:44:55.702: INFO: Deleting pod "pod-subpath-test-configmap-f8rv" in namespace "subpath-3968"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:44:55.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3968" for this suite. 12/14/22 16:44:55.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:44:55.734
Dec 14 16:44:55.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 16:44:55.737
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:55.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:55.769
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-c75ea86b-ced2-4fb5-9157-7a90df0408d1 12/14/22 16:44:55.773
STEP: Creating a pod to test consume secrets 12/14/22 16:44:55.787
Dec 14 16:44:55.798: INFO: Waiting up to 5m0s for pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155" in namespace "secrets-1212" to be "Succeeded or Failed"
Dec 14 16:44:55.809: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Pending", Reason="", readiness=false. Elapsed: 10.490134ms
Dec 14 16:44:57.814: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015652976s
Dec 14 16:44:59.815: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016438524s
STEP: Saw pod success 12/14/22 16:44:59.815
Dec 14 16:44:59.816: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155" satisfied condition "Succeeded or Failed"
Dec 14 16:44:59.820: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:44:59.997
Dec 14 16:45:00.018: INFO: Waiting for pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 to disappear
Dec 14 16:45:00.024: INFO: Pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 16:45:00.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1212" for this suite. 12/14/22 16:45:00.038
------------------------------
• [4.332 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:44:55.734
    Dec 14 16:44:55.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 16:44:55.737
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:44:55.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:44:55.769
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-c75ea86b-ced2-4fb5-9157-7a90df0408d1 12/14/22 16:44:55.773
    STEP: Creating a pod to test consume secrets 12/14/22 16:44:55.787
    Dec 14 16:44:55.798: INFO: Waiting up to 5m0s for pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155" in namespace "secrets-1212" to be "Succeeded or Failed"
    Dec 14 16:44:55.809: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Pending", Reason="", readiness=false. Elapsed: 10.490134ms
    Dec 14 16:44:57.814: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015652976s
    Dec 14 16:44:59.815: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016438524s
    STEP: Saw pod success 12/14/22 16:44:59.815
    Dec 14 16:44:59.816: INFO: Pod "pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155" satisfied condition "Succeeded or Failed"
    Dec 14 16:44:59.820: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:44:59.997
    Dec 14 16:45:00.018: INFO: Waiting for pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 to disappear
    Dec 14 16:45:00.024: INFO: Pod pod-secrets-4485166b-ad3f-4fe4-b0f4-2fc206bc6155 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:45:00.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1212" for this suite. 12/14/22 16:45:00.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:45:00.081
Dec 14 16:45:00.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 16:45:00.085
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:45:00.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:45:00.125
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8454 12/14/22 16:45:00.13
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-8454 12/14/22 16:45:00.137
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8454 12/14/22 16:45:00.153
Dec 14 16:45:00.164: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:45:10.172: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/14/22 16:45:10.172
Dec 14 16:45:10.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:45:10.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:45:10.502: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:45:10.502: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:45:10.510: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 16:45:20.522: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:45:20.522: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:45:20.544: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:45:20.544: INFO: ss-0  iet9eich7uhu-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  }]
Dec 14 16:45:20.544: INFO: 
Dec 14 16:45:20.544: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 16:45:21.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995486377s
Dec 14 16:45:22.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986258545s
Dec 14 16:45:23.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95188192s
Dec 14 16:45:24.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940412074s
Dec 14 16:45:25.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930189475s
Dec 14 16:45:26.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.918998797s
Dec 14 16:45:27.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900490443s
Dec 14 16:45:28.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.889281248s
Dec 14 16:45:29.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 876.055081ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8454 12/14/22 16:45:30.671
Dec 14 16:45:30.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:45:30.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:45:30.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:45:30.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:45:30.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:45:31.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 16:45:31.256: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:45:31.256: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:45:31.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:45:31.475: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 16:45:31.475: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:45:31.475: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:45:31.483: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 14 16:45:41.495: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:45:41.495: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:45:41.495: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 12/14/22 16:45:41.495
Dec 14 16:45:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:45:41.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:45:41.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:45:41.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:45:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:45:41.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:45:41.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:45:41.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:45:41.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:45:42.201: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:45:42.201: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:45:42.201: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:45:42.201: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:45:42.208: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 14 16:45:52.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:45:52.225: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:45:52.225: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:45:52.250: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:45:52.251: INFO: ss-0  iet9eich7uhu-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  }]
Dec 14 16:45:52.251: INFO: ss-1  iet9eich7uhu-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
Dec 14 16:45:52.251: INFO: ss-2  iet9eich7uhu-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
Dec 14 16:45:52.251: INFO: 
Dec 14 16:45:52.251: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 16:45:53.257: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:45:53.257: INFO: ss-2  iet9eich7uhu-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
Dec 14 16:45:53.257: INFO: 
Dec 14 16:45:53.257: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 14 16:45:54.265: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986414133s
Dec 14 16:45:55.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.977453448s
Dec 14 16:45:56.283: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969363721s
Dec 14 16:45:57.295: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960555529s
Dec 14 16:45:58.302: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.948646183s
Dec 14 16:45:59.312: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.941472956s
Dec 14 16:46:00.321: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.931918723s
Dec 14 16:46:01.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 922.354132ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8454 12/14/22 16:46:02.329
Dec 14 16:46:02.341: INFO: Scaling statefulset ss to 0
Dec 14 16:46:02.367: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 16:46:02.372: INFO: Deleting all statefulset in ns statefulset-8454
Dec 14 16:46:02.376: INFO: Scaling statefulset ss to 0
Dec 14 16:46:02.395: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:46:02.398: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:02.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8454" for this suite. 12/14/22 16:46:02.435
------------------------------
• [SLOW TEST] [62.364 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:45:00.081
    Dec 14 16:45:00.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 16:45:00.085
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:45:00.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:45:00.125
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8454 12/14/22 16:45:00.13
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-8454 12/14/22 16:45:00.137
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8454 12/14/22 16:45:00.153
    Dec 14 16:45:00.164: INFO: Found 0 stateful pods, waiting for 1
    Dec 14 16:45:10.172: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/14/22 16:45:10.172
    Dec 14 16:45:10.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:45:10.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:45:10.502: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:45:10.502: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:45:10.510: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 14 16:45:20.522: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:45:20.522: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:45:20.544: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec 14 16:45:20.544: INFO: ss-0  iet9eich7uhu-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  }]
    Dec 14 16:45:20.544: INFO: 
    Dec 14 16:45:20.544: INFO: StatefulSet ss has not reached scale 3, at 1
    Dec 14 16:45:21.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995486377s
    Dec 14 16:45:22.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986258545s
    Dec 14 16:45:23.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95188192s
    Dec 14 16:45:24.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940412074s
    Dec 14 16:45:25.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930189475s
    Dec 14 16:45:26.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.918998797s
    Dec 14 16:45:27.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900490443s
    Dec 14 16:45:28.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.889281248s
    Dec 14 16:45:29.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 876.055081ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8454 12/14/22 16:45:30.671
    Dec 14 16:45:30.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:45:30.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 16:45:30.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:45:30.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:45:30.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:45:31.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 14 16:45:31.256: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:45:31.256: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:45:31.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 16:45:31.475: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 14 16:45:31.475: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 16:45:31.475: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 14 16:45:31.483: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Dec 14 16:45:41.495: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 16:45:41.495: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 16:45:41.495: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 12/14/22 16:45:41.495
    Dec 14 16:45:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:45:41.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:45:41.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:45:41.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:45:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:45:41.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:45:41.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:45:41.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:45:41.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-8454 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 16:45:42.201: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 16:45:42.201: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 16:45:42.201: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 16:45:42.201: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:45:42.208: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Dec 14 16:45:52.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:45:52.225: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:45:52.225: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 14 16:45:52.250: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec 14 16:45:52.251: INFO: ss-0  iet9eich7uhu-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:00 +0000 UTC  }]
    Dec 14 16:45:52.251: INFO: ss-1  iet9eich7uhu-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
    Dec 14 16:45:52.251: INFO: ss-2  iet9eich7uhu-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
    Dec 14 16:45:52.251: INFO: 
    Dec 14 16:45:52.251: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec 14 16:45:53.257: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec 14 16:45:53.257: INFO: ss-2  iet9eich7uhu-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:45:20 +0000 UTC  }]
    Dec 14 16:45:53.257: INFO: 
    Dec 14 16:45:53.257: INFO: StatefulSet ss has not reached scale 0, at 1
    Dec 14 16:45:54.265: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986414133s
    Dec 14 16:45:55.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.977453448s
    Dec 14 16:45:56.283: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969363721s
    Dec 14 16:45:57.295: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960555529s
    Dec 14 16:45:58.302: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.948646183s
    Dec 14 16:45:59.312: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.941472956s
    Dec 14 16:46:00.321: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.931918723s
    Dec 14 16:46:01.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 922.354132ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8454 12/14/22 16:46:02.329
    Dec 14 16:46:02.341: INFO: Scaling statefulset ss to 0
    Dec 14 16:46:02.367: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 16:46:02.372: INFO: Deleting all statefulset in ns statefulset-8454
    Dec 14 16:46:02.376: INFO: Scaling statefulset ss to 0
    Dec 14 16:46:02.395: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 16:46:02.398: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:02.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8454" for this suite. 12/14/22 16:46:02.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:02.449
Dec 14 16:46:02.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:46:02.454
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:02.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:02.487
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:46:02.51
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:46:03.528
STEP: Deploying the webhook pod 12/14/22 16:46:03.563
STEP: Wait for the deployment to be ready 12/14/22 16:46:03.579
Dec 14 16:46:03.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 16:46:05.609
STEP: Verifying the service has paired with the endpoint 12/14/22 16:46:05.631
Dec 14 16:46:06.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 16:46:06.639
STEP: create a pod that should be denied by the webhook 12/14/22 16:46:06.672
STEP: create a pod that causes the webhook to hang 12/14/22 16:46:06.695
STEP: create a configmap that should be denied by the webhook 12/14/22 16:46:16.71
STEP: create a configmap that should be admitted by the webhook 12/14/22 16:46:16.724
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 16:46:16.737
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 16:46:16.754
STEP: create a namespace that bypass the webhook 12/14/22 16:46:16.76
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/14/22 16:46:16.769
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:16.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6618" for this suite. 12/14/22 16:46:16.871
STEP: Destroying namespace "webhook-6618-markers" for this suite. 12/14/22 16:46:16.88
------------------------------
• [SLOW TEST] [14.466 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:02.449
    Dec 14 16:46:02.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:46:02.454
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:02.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:02.487
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:46:02.51
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:46:03.528
    STEP: Deploying the webhook pod 12/14/22 16:46:03.563
    STEP: Wait for the deployment to be ready 12/14/22 16:46:03.579
    Dec 14 16:46:03.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 16:46:05.609
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:46:05.631
    Dec 14 16:46:06.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 12/14/22 16:46:06.639
    STEP: create a pod that should be denied by the webhook 12/14/22 16:46:06.672
    STEP: create a pod that causes the webhook to hang 12/14/22 16:46:06.695
    STEP: create a configmap that should be denied by the webhook 12/14/22 16:46:16.71
    STEP: create a configmap that should be admitted by the webhook 12/14/22 16:46:16.724
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 16:46:16.737
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/14/22 16:46:16.754
    STEP: create a namespace that bypass the webhook 12/14/22 16:46:16.76
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/14/22 16:46:16.769
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:16.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6618" for this suite. 12/14/22 16:46:16.871
    STEP: Destroying namespace "webhook-6618-markers" for this suite. 12/14/22 16:46:16.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:16.922
Dec 14 16:46:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 16:46:16.93
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:16.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:16.959
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Dec 14 16:46:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:18.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7905" for this suite. 12/14/22 16:46:18.035
------------------------------
• [1.127 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:16.922
    Dec 14 16:46:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 16:46:16.93
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:16.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:16.959
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Dec 14 16:46:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:18.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7905" for this suite. 12/14/22 16:46:18.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:18.057
Dec 14 16:46:18.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:46:18.059
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:18.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:18.095
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4581 12/14/22 16:46:18.102
STEP: creating service affinity-nodeport-transition in namespace services-4581 12/14/22 16:46:18.103
STEP: creating replication controller affinity-nodeport-transition in namespace services-4581 12/14/22 16:46:18.131
I1214 16:46:18.150010      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4581, replica count: 3
I1214 16:46:21.201188      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:46:21.224: INFO: Creating new exec pod
Dec 14 16:46:21.244: INFO: Waiting up to 5m0s for pod "execpod-affinity85cx2" in namespace "services-4581" to be "running"
Dec 14 16:46:21.258: INFO: Pod "execpod-affinity85cx2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.562651ms
Dec 14 16:46:23.263: INFO: Pod "execpod-affinity85cx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018499961s
Dec 14 16:46:23.263: INFO: Pod "execpod-affinity85cx2" satisfied condition "running"
Dec 14 16:46:24.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Dec 14 16:46:24.607: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 14 16:46:24.607: INFO: stdout: ""
Dec 14 16:46:24.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 10.233.50.227 80'
Dec 14 16:46:24.863: INFO: stderr: "+ nc -v -z -w 2 10.233.50.227 80\nConnection to 10.233.50.227 80 port [tcp/http] succeeded!\n"
Dec 14 16:46:24.863: INFO: stdout: ""
Dec 14 16:46:24.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31430'
Dec 14 16:46:25.067: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31430\nConnection to 192.168.121.56 31430 port [tcp/*] succeeded!\n"
Dec 14 16:46:25.067: INFO: stdout: ""
Dec 14 16:46:25.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31430'
Dec 14 16:46:25.257: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31430\nConnection to 192.168.121.16 31430 port [tcp/*] succeeded!\n"
Dec 14 16:46:25.257: INFO: stdout: ""
Dec 14 16:46:25.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31430/ ; done'
Dec 14 16:46:25.711: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n"
Dec 14 16:46:25.711: INFO: stdout: "\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-f7fmm"
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
Dec 14 16:46:25.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31430/ ; done'
Dec 14 16:46:26.311: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n"
Dec 14 16:46:26.313: INFO: stdout: "\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78"
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.314: INFO: Received response from host: affinity-nodeport-transition-n6p78
Dec 14 16:46:26.314: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4581, will wait for the garbage collector to delete the pods 12/14/22 16:46:26.336
Dec 14 16:46:26.405: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.742759ms
Dec 14 16:46:26.507: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.578038ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:28.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4581" for this suite. 12/14/22 16:46:28.954
------------------------------
• [SLOW TEST] [10.906 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:18.057
    Dec 14 16:46:18.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:46:18.059
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:18.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:18.095
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4581 12/14/22 16:46:18.102
    STEP: creating service affinity-nodeport-transition in namespace services-4581 12/14/22 16:46:18.103
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4581 12/14/22 16:46:18.131
    I1214 16:46:18.150010      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4581, replica count: 3
    I1214 16:46:21.201188      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:46:21.224: INFO: Creating new exec pod
    Dec 14 16:46:21.244: INFO: Waiting up to 5m0s for pod "execpod-affinity85cx2" in namespace "services-4581" to be "running"
    Dec 14 16:46:21.258: INFO: Pod "execpod-affinity85cx2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.562651ms
    Dec 14 16:46:23.263: INFO: Pod "execpod-affinity85cx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018499961s
    Dec 14 16:46:23.263: INFO: Pod "execpod-affinity85cx2" satisfied condition "running"
    Dec 14 16:46:24.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Dec 14 16:46:24.607: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Dec 14 16:46:24.607: INFO: stdout: ""
    Dec 14 16:46:24.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 10.233.50.227 80'
    Dec 14 16:46:24.863: INFO: stderr: "+ nc -v -z -w 2 10.233.50.227 80\nConnection to 10.233.50.227 80 port [tcp/http] succeeded!\n"
    Dec 14 16:46:24.863: INFO: stdout: ""
    Dec 14 16:46:24.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.56 31430'
    Dec 14 16:46:25.067: INFO: stderr: "+ nc -v -z -w 2 192.168.121.56 31430\nConnection to 192.168.121.56 31430 port [tcp/*] succeeded!\n"
    Dec 14 16:46:25.067: INFO: stdout: ""
    Dec 14 16:46:25.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.16 31430'
    Dec 14 16:46:25.257: INFO: stderr: "+ nc -v -z -w 2 192.168.121.16 31430\nConnection to 192.168.121.16 31430 port [tcp/*] succeeded!\n"
    Dec 14 16:46:25.257: INFO: stdout: ""
    Dec 14 16:46:25.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31430/ ; done'
    Dec 14 16:46:25.711: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n"
    Dec 14 16:46:25.711: INFO: stdout: "\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-f7fmm\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-f7fmm"
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:25.711: INFO: Received response from host: affinity-nodeport-transition-f7fmm
    Dec 14 16:46:25.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-4581 exec execpod-affinity85cx2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.21:31430/ ; done'
    Dec 14 16:46:26.311: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.21:31430/\n"
    Dec 14 16:46:26.313: INFO: stdout: "\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78\naffinity-nodeport-transition-n6p78"
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.313: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.314: INFO: Received response from host: affinity-nodeport-transition-n6p78
    Dec 14 16:46:26.314: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4581, will wait for the garbage collector to delete the pods 12/14/22 16:46:26.336
    Dec 14 16:46:26.405: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.742759ms
    Dec 14 16:46:26.507: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.578038ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:28.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4581" for this suite. 12/14/22 16:46:28.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:28.968
Dec 14 16:46:28.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:46:28.971
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:28.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:28.993
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:46:28.997
Dec 14 16:46:29.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d" in namespace "projected-9801" to be "Succeeded or Failed"
Dec 14 16:46:29.020: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.36051ms
Dec 14 16:46:31.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020413886s
Dec 14 16:46:33.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020319327s
STEP: Saw pod success 12/14/22 16:46:33.027
Dec 14 16:46:33.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d" satisfied condition "Succeeded or Failed"
Dec 14 16:46:33.033: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d container client-container: <nil>
STEP: delete the pod 12/14/22 16:46:33.053
Dec 14 16:46:33.065: INFO: Waiting for pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d to disappear
Dec 14 16:46:33.069: INFO: Pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:33.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9801" for this suite. 12/14/22 16:46:33.075
------------------------------
• [4.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:28.968
    Dec 14 16:46:28.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:46:28.971
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:28.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:28.993
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:46:28.997
    Dec 14 16:46:29.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d" in namespace "projected-9801" to be "Succeeded or Failed"
    Dec 14 16:46:29.020: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.36051ms
    Dec 14 16:46:31.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020413886s
    Dec 14 16:46:33.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020319327s
    STEP: Saw pod success 12/14/22 16:46:33.027
    Dec 14 16:46:33.027: INFO: Pod "downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d" satisfied condition "Succeeded or Failed"
    Dec 14 16:46:33.033: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d container client-container: <nil>
    STEP: delete the pod 12/14/22 16:46:33.053
    Dec 14 16:46:33.065: INFO: Waiting for pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d to disappear
    Dec 14 16:46:33.069: INFO: Pod downwardapi-volume-6519b8aa-5349-4dff-be53-4ac04300d29d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:33.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9801" for this suite. 12/14/22 16:46:33.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:33.086
Dec 14 16:46:33.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 16:46:33.088
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:33.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:33.164
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 12/14/22 16:46:33.173
STEP: Patching the Job 12/14/22 16:46:33.182
STEP: Watching for Job to be patched 12/14/22 16:46:33.216
Dec 14 16:46:33.219: INFO: Event ADDED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 14 16:46:33.220: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 14 16:46:33.220: INFO: Event MODIFIED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 12/14/22 16:46:33.22
STEP: Watching for Job to be updated 12/14/22 16:46:33.237
Dec 14 16:46:33.240: INFO: Event MODIFIED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:33.240: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 12/14/22 16:46:33.24
Dec 14 16:46:33.275: INFO: Job: e2e-fzqxn as labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn]
STEP: Waiting for job to complete 12/14/22 16:46:33.276
STEP: Delete a job collection with a labelselector 12/14/22 16:46:43.282
STEP: Watching for Job to be deleted 12/14/22 16:46:43.293
Dec 14 16:46:43.295: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:43.295: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 14 16:46:43.296: INFO: Event DELETED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 12/14/22 16:46:43.296
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5214" for this suite. 12/14/22 16:46:43.315
------------------------------
• [SLOW TEST] [10.248 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:33.086
    Dec 14 16:46:33.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 16:46:33.088
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:33.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:33.164
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 12/14/22 16:46:33.173
    STEP: Patching the Job 12/14/22 16:46:33.182
    STEP: Watching for Job to be patched 12/14/22 16:46:33.216
    Dec 14 16:46:33.219: INFO: Event ADDED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 14 16:46:33.220: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 14 16:46:33.220: INFO: Event MODIFIED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 12/14/22 16:46:33.22
    STEP: Watching for Job to be updated 12/14/22 16:46:33.237
    Dec 14 16:46:33.240: INFO: Event MODIFIED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:33.240: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 12/14/22 16:46:33.24
    Dec 14 16:46:33.275: INFO: Job: e2e-fzqxn as labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn]
    STEP: Waiting for job to complete 12/14/22 16:46:33.276
    STEP: Delete a job collection with a labelselector 12/14/22 16:46:43.282
    STEP: Watching for Job to be deleted 12/14/22 16:46:43.293
    Dec 14 16:46:43.295: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:43.295: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:43.296: INFO: Event MODIFIED observed for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 14 16:46:43.296: INFO: Event DELETED found for Job e2e-fzqxn in namespace job-5214 with labels: map[e2e-fzqxn:patched e2e-job-label:e2e-fzqxn] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 12/14/22 16:46:43.296
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5214" for this suite. 12/14/22 16:46:43.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:43.345
Dec 14 16:46:43.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 16:46:43.347
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:43.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:43.381
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/14/22 16:46:43.384
Dec 14 16:46:43.397: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1713  ca0f4331-c393-489b-bcaf-f362ffec66a6 24079 0 2022-12-14 16:46:43 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-14 16:46:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8468c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8468c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:46:43.399: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1713" to be "running and ready"
Dec 14 16:46:43.404: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383955ms
Dec 14 16:46:43.404: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:46:45.411: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012075832s
Dec 14 16:46:45.411: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Dec 14 16:46:45.411: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 12/14/22 16:46:45.411
Dec 14 16:46:45.411: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1713 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:46:45.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:46:45.413: INFO: ExecWithOptions: Clientset creation
Dec 14 16:46:45.413: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1713/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 12/14/22 16:46:45.54
Dec 14 16:46:45.540: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1713 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:46:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:46:45.542: INFO: ExecWithOptions: Clientset creation
Dec 14 16:46:45.542: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1713/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:46:45.670: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1713" for this suite. 12/14/22 16:46:45.692
------------------------------
• [2.357 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:43.345
    Dec 14 16:46:43.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 16:46:43.347
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:43.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:43.381
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/14/22 16:46:43.384
    Dec 14 16:46:43.397: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1713  ca0f4331-c393-489b-bcaf-f362ffec66a6 24079 0 2022-12-14 16:46:43 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-14 16:46:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8468c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8468c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:46:43.399: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1713" to be "running and ready"
    Dec 14 16:46:43.404: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383955ms
    Dec 14 16:46:43.404: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:46:45.411: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012075832s
    Dec 14 16:46:45.411: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Dec 14 16:46:45.411: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 12/14/22 16:46:45.411
    Dec 14 16:46:45.411: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1713 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:46:45.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:46:45.413: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:46:45.413: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1713/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 12/14/22 16:46:45.54
    Dec 14 16:46:45.540: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1713 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:46:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:46:45.542: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:46:45.542: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1713/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 14 16:46:45.670: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1713" for this suite. 12/14/22 16:46:45.692
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:45.709
Dec 14 16:46:45.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:46:45.712
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:45.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:45.741
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 12/14/22 16:46:45.744
Dec 14 16:46:45.755: INFO: Waiting up to 5m0s for pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d" in namespace "projected-320" to be "running and ready"
Dec 14 16:46:45.759: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826912ms
Dec 14 16:46:45.759: INFO: The phase of Pod annotationupdate03862247-9202-4ea9-8371-44662e56661d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:46:47.770: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014278426s
Dec 14 16:46:47.770: INFO: The phase of Pod annotationupdate03862247-9202-4ea9-8371-44662e56661d is Running (Ready = true)
Dec 14 16:46:47.770: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d" satisfied condition "running and ready"
Dec 14 16:46:48.578: INFO: Successfully updated pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:46:52.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-320" for this suite. 12/14/22 16:46:52.678
------------------------------
• [SLOW TEST] [6.979 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:45.709
    Dec 14 16:46:45.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:46:45.712
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:45.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:45.741
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 12/14/22 16:46:45.744
    Dec 14 16:46:45.755: INFO: Waiting up to 5m0s for pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d" in namespace "projected-320" to be "running and ready"
    Dec 14 16:46:45.759: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826912ms
    Dec 14 16:46:45.759: INFO: The phase of Pod annotationupdate03862247-9202-4ea9-8371-44662e56661d is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:46:47.770: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014278426s
    Dec 14 16:46:47.770: INFO: The phase of Pod annotationupdate03862247-9202-4ea9-8371-44662e56661d is Running (Ready = true)
    Dec 14 16:46:47.770: INFO: Pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d" satisfied condition "running and ready"
    Dec 14 16:46:48.578: INFO: Successfully updated pod "annotationupdate03862247-9202-4ea9-8371-44662e56661d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:46:52.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-320" for this suite. 12/14/22 16:46:52.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:46:52.696
Dec 14 16:46:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:46:52.699
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:52.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:52.728
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 12/14/22 16:46:52.733
STEP: Creating a ResourceQuota 12/14/22 16:46:57.739
STEP: Ensuring resource quota status is calculated 12/14/22 16:46:57.749
STEP: Creating a Service 12/14/22 16:46:59.78
STEP: Creating a NodePort Service 12/14/22 16:46:59.855
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/14/22 16:46:59.895
STEP: Ensuring resource quota status captures service creation 12/14/22 16:46:59.924
STEP: Deleting Services 12/14/22 16:47:01.932
STEP: Ensuring resource quota status released usage 12/14/22 16:47:02.025
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:47:04.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9933" for this suite. 12/14/22 16:47:04.041
------------------------------
• [SLOW TEST] [11.361 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:46:52.696
    Dec 14 16:46:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:46:52.699
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:46:52.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:46:52.728
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 12/14/22 16:46:52.733
    STEP: Creating a ResourceQuota 12/14/22 16:46:57.739
    STEP: Ensuring resource quota status is calculated 12/14/22 16:46:57.749
    STEP: Creating a Service 12/14/22 16:46:59.78
    STEP: Creating a NodePort Service 12/14/22 16:46:59.855
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/14/22 16:46:59.895
    STEP: Ensuring resource quota status captures service creation 12/14/22 16:46:59.924
    STEP: Deleting Services 12/14/22 16:47:01.932
    STEP: Ensuring resource quota status released usage 12/14/22 16:47:02.025
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:47:04.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9933" for this suite. 12/14/22 16:47:04.041
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:47:04.057
Dec 14 16:47:04.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:47:04.059
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:47:04.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:47:04.1
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 12/14/22 16:47:04.119
STEP: delete the rc 12/14/22 16:47:09.146
STEP: wait for the rc to be deleted 12/14/22 16:47:09.162
Dec 14 16:47:10.365: INFO: 81 pods remaining
Dec 14 16:47:10.365: INFO: 81 pods has nil DeletionTimestamp
Dec 14 16:47:10.365: INFO: 
Dec 14 16:47:11.323: INFO: 72 pods remaining
Dec 14 16:47:11.323: INFO: 71 pods has nil DeletionTimestamp
Dec 14 16:47:11.323: INFO: 
Dec 14 16:47:12.203: INFO: 58 pods remaining
Dec 14 16:47:12.203: INFO: 58 pods has nil DeletionTimestamp
Dec 14 16:47:12.203: INFO: 
Dec 14 16:47:13.190: INFO: 42 pods remaining
Dec 14 16:47:13.190: INFO: 42 pods has nil DeletionTimestamp
Dec 14 16:47:13.190: INFO: 
Dec 14 16:47:14.218: INFO: 31 pods remaining
Dec 14 16:47:14.218: INFO: 31 pods has nil DeletionTimestamp
Dec 14 16:47:14.219: INFO: 
Dec 14 16:47:15.179: INFO: 19 pods remaining
Dec 14 16:47:15.179: INFO: 19 pods has nil DeletionTimestamp
Dec 14 16:47:15.179: INFO: 
STEP: Gathering metrics 12/14/22 16:47:16.171
Dec 14 16:47:16.254: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
Dec 14 16:47:16.262: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.362711ms
Dec 14 16:47:16.262: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
Dec 14 16:47:16.262: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
Dec 14 16:47:16.606: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:47:16.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4949" for this suite. 12/14/22 16:47:16.621
------------------------------
• [SLOW TEST] [12.591 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:47:04.057
    Dec 14 16:47:04.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:47:04.059
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:47:04.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:47:04.1
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 12/14/22 16:47:04.119
    STEP: delete the rc 12/14/22 16:47:09.146
    STEP: wait for the rc to be deleted 12/14/22 16:47:09.162
    Dec 14 16:47:10.365: INFO: 81 pods remaining
    Dec 14 16:47:10.365: INFO: 81 pods has nil DeletionTimestamp
    Dec 14 16:47:10.365: INFO: 
    Dec 14 16:47:11.323: INFO: 72 pods remaining
    Dec 14 16:47:11.323: INFO: 71 pods has nil DeletionTimestamp
    Dec 14 16:47:11.323: INFO: 
    Dec 14 16:47:12.203: INFO: 58 pods remaining
    Dec 14 16:47:12.203: INFO: 58 pods has nil DeletionTimestamp
    Dec 14 16:47:12.203: INFO: 
    Dec 14 16:47:13.190: INFO: 42 pods remaining
    Dec 14 16:47:13.190: INFO: 42 pods has nil DeletionTimestamp
    Dec 14 16:47:13.190: INFO: 
    Dec 14 16:47:14.218: INFO: 31 pods remaining
    Dec 14 16:47:14.218: INFO: 31 pods has nil DeletionTimestamp
    Dec 14 16:47:14.219: INFO: 
    Dec 14 16:47:15.179: INFO: 19 pods remaining
    Dec 14 16:47:15.179: INFO: 19 pods has nil DeletionTimestamp
    Dec 14 16:47:15.179: INFO: 
    STEP: Gathering metrics 12/14/22 16:47:16.171
    Dec 14 16:47:16.254: INFO: Waiting up to 5m0s for pod "kube-controller-manager-iet9eich7uhu-2" in namespace "kube-system" to be "running and ready"
    Dec 14 16:47:16.262: INFO: Pod "kube-controller-manager-iet9eich7uhu-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.362711ms
    Dec 14 16:47:16.262: INFO: The phase of Pod kube-controller-manager-iet9eich7uhu-2 is Running (Ready = true)
    Dec 14 16:47:16.262: INFO: Pod "kube-controller-manager-iet9eich7uhu-2" satisfied condition "running and ready"
    Dec 14 16:47:16.606: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:47:16.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4949" for this suite. 12/14/22 16:47:16.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:47:16.653
Dec 14 16:47:16.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 16:47:16.657
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:47:16.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:47:16.707
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 12/14/22 16:47:16.711
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local;sleep 1; done
 12/14/22 16:47:16.742
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local;sleep 1; done
 12/14/22 16:47:16.743
STEP: creating a pod to probe DNS 12/14/22 16:47:16.743
STEP: submitting the pod to kubernetes 12/14/22 16:47:16.743
Dec 14 16:47:16.768: INFO: Waiting up to 15m0s for pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076" in namespace "dns-4280" to be "running"
Dec 14 16:47:16.810: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 42.556285ms
Dec 14 16:47:18.821: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053562226s
Dec 14 16:47:20.824: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056043046s
Dec 14 16:47:22.819: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051049575s
Dec 14 16:47:24.816: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048193932s
Dec 14 16:47:26.815: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047478171s
Dec 14 16:47:28.818: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Running", Reason="", readiness=true. Elapsed: 12.049950161s
Dec 14 16:47:28.818: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076" satisfied condition "running"
STEP: retrieving the pod 12/14/22 16:47:28.818
STEP: looking for the results for each expected name from probers 12/14/22 16:47:28.826
Dec 14 16:47:28.850: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.856: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.861: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.868: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.874: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.879: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.893: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.900: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:28.901: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:33.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.912: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.917: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.925: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.933: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.937: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:33.937: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:38.914: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.924: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.938: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.943: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.960: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.967: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.975: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:38.975: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:43.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.915: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.923: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.928: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.932: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.937: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.942: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.946: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:43.947: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:48.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.920: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.927: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.934: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.939: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.944: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.950: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.957: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:48.957: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:53.912: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.920: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.924: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.930: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.938: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.946: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.953: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.960: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:53.960: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:47:58.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:58.925: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
Dec 14 16:47:58.947: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

Dec 14 16:48:03.937: INFO: DNS probes using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 succeeded

STEP: deleting the pod 12/14/22 16:48:03.938
STEP: deleting the test headless service 12/14/22 16:48:03.96
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:04.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4280" for this suite. 12/14/22 16:48:04.07
------------------------------
• [SLOW TEST] [47.447 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:47:16.653
    Dec 14 16:47:16.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 16:47:16.657
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:47:16.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:47:16.707
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 12/14/22 16:47:16.711
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local;sleep 1; done
     12/14/22 16:47:16.742
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4280.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local;sleep 1; done
     12/14/22 16:47:16.743
    STEP: creating a pod to probe DNS 12/14/22 16:47:16.743
    STEP: submitting the pod to kubernetes 12/14/22 16:47:16.743
    Dec 14 16:47:16.768: INFO: Waiting up to 15m0s for pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076" in namespace "dns-4280" to be "running"
    Dec 14 16:47:16.810: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 42.556285ms
    Dec 14 16:47:18.821: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053562226s
    Dec 14 16:47:20.824: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056043046s
    Dec 14 16:47:22.819: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051049575s
    Dec 14 16:47:24.816: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048193932s
    Dec 14 16:47:26.815: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047478171s
    Dec 14 16:47:28.818: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076": Phase="Running", Reason="", readiness=true. Elapsed: 12.049950161s
    Dec 14 16:47:28.818: INFO: Pod "dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 16:47:28.818
    STEP: looking for the results for each expected name from probers 12/14/22 16:47:28.826
    Dec 14 16:47:28.850: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.856: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.861: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.868: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.874: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.879: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.893: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.900: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:28.901: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:33.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.912: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.917: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.925: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.933: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.937: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:33.937: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:38.914: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.924: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.931: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.938: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.943: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.960: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.967: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.975: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:38.975: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:43.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.915: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.923: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.928: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.932: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.937: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.942: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.946: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:43.947: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:48.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.920: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.927: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.934: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.939: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.944: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.950: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.957: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:48.957: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:53.912: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.920: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.924: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.930: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.938: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.946: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.953: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.960: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:53.960: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4280.svc.cluster.local jessie_udp@dns-test-service-2.dns-4280.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:47:58.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:58.925: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local from pod dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076: the server could not find the requested resource (get pods dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076)
    Dec 14 16:47:58.947: INFO: Lookups using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 failed for: [wheezy_udp@dns-test-service-2.dns-4280.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4280.svc.cluster.local]

    Dec 14 16:48:03.937: INFO: DNS probes using dns-4280/dns-test-944c55ea-e4a4-4347-b1bc-e5bbb6123076 succeeded

    STEP: deleting the pod 12/14/22 16:48:03.938
    STEP: deleting the test headless service 12/14/22 16:48:03.96
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:04.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4280" for this suite. 12/14/22 16:48:04.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:04.127
Dec 14 16:48:04.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:48:04.145
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:04.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:04.174
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-ab4b3167-ead3-4486-808a-fdb485ec9a0c 12/14/22 16:48:04.181
STEP: Creating a pod to test consume configMaps 12/14/22 16:48:04.188
Dec 14 16:48:04.199: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf" in namespace "projected-9420" to be "Succeeded or Failed"
Dec 14 16:48:04.207: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.365177ms
Dec 14 16:48:06.213: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607081s
Dec 14 16:48:08.232: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033506053s
STEP: Saw pod success 12/14/22 16:48:08.233
Dec 14 16:48:08.236: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf" satisfied condition "Succeeded or Failed"
Dec 14 16:48:08.245: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:48:08.262
Dec 14 16:48:08.285: INFO: Waiting for pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf to disappear
Dec 14 16:48:08.289: INFO: Pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9420" for this suite. 12/14/22 16:48:08.297
------------------------------
• [4.178 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:04.127
    Dec 14 16:48:04.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:48:04.145
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:04.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:04.174
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-ab4b3167-ead3-4486-808a-fdb485ec9a0c 12/14/22 16:48:04.181
    STEP: Creating a pod to test consume configMaps 12/14/22 16:48:04.188
    Dec 14 16:48:04.199: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf" in namespace "projected-9420" to be "Succeeded or Failed"
    Dec 14 16:48:04.207: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.365177ms
    Dec 14 16:48:06.213: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013607081s
    Dec 14 16:48:08.232: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033506053s
    STEP: Saw pod success 12/14/22 16:48:08.233
    Dec 14 16:48:08.236: INFO: Pod "pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf" satisfied condition "Succeeded or Failed"
    Dec 14 16:48:08.245: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:48:08.262
    Dec 14 16:48:08.285: INFO: Waiting for pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf to disappear
    Dec 14 16:48:08.289: INFO: Pod pod-projected-configmaps-9f5fa3b9-d8f5-4b44-9b2f-3fd6bf18dcaf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9420" for this suite. 12/14/22 16:48:08.297
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:08.306
Dec 14 16:48:08.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 16:48:08.312
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:08.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:08.342
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 16:48:08.379
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:48:08.389
Dec 14 16:48:08.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:48:08.418: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:48:09.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:48:09.444: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:48:10.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:48:10.433: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 12/14/22 16:48:10.438
STEP: DeleteCollection of the DaemonSets 12/14/22 16:48:10.443
STEP: Verify that ReplicaSets have been deleted 12/14/22 16:48:10.455
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Dec 14 16:48:10.482: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25586"},"items":null}

Dec 14 16:48:10.499: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25587"},"items":[{"metadata":{"name":"daemon-set-7j6j8","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"e4dd4f77-62b1-44a7-898b-33ad6a7fa63f","resourceVersion":"25586","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pcgn2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pcgn2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.16","podIP":"10.233.66.149","podIPs":[{"ip":"10.233.66.149"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://1bcc6920ba4f10b06807f4b7feeaebca69acdc30fbb77ba05dd7eb867e1ae8ce","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bcg2q","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"b5398838-8735-4d3f-94bd-249b941e8f85","resourceVersion":"25585","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fhmpx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fhmpx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.21","podIP":"10.233.64.149","podIPs":[{"ip":"10.233.64.149"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://6ca6aacb93dd8263c0589d8f018c8089498763b2889d9aa727a86e63df71ce51","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mtpqn","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"3aa02497-fe7d-42ac-8ea7-d6832677c90b","resourceVersion":"25587","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d6crs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d6crs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.56","podIP":"10.233.67.82","podIPs":[{"ip":"10.233.67.82"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://49b8a18daeeda44482a7148a132e3e1c221cc9ea1f1fced81053d2fb20021d27","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:10.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8856" for this suite. 12/14/22 16:48:10.53
------------------------------
• [2.241 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:08.306
    Dec 14 16:48:08.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 16:48:08.312
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:08.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:08.342
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 16:48:08.379
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:48:08.389
    Dec 14 16:48:08.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:48:08.418: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:48:09.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:48:09.444: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:48:10.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:48:10.433: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 12/14/22 16:48:10.438
    STEP: DeleteCollection of the DaemonSets 12/14/22 16:48:10.443
    STEP: Verify that ReplicaSets have been deleted 12/14/22 16:48:10.455
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Dec 14 16:48:10.482: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25586"},"items":null}

    Dec 14 16:48:10.499: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25587"},"items":[{"metadata":{"name":"daemon-set-7j6j8","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"e4dd4f77-62b1-44a7-898b-33ad6a7fa63f","resourceVersion":"25586","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pcgn2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pcgn2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.16","podIP":"10.233.66.149","podIPs":[{"ip":"10.233.66.149"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://1bcc6920ba4f10b06807f4b7feeaebca69acdc30fbb77ba05dd7eb867e1ae8ce","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bcg2q","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"b5398838-8735-4d3f-94bd-249b941e8f85","resourceVersion":"25585","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fhmpx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fhmpx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.21","podIP":"10.233.64.149","podIPs":[{"ip":"10.233.64.149"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://6ca6aacb93dd8263c0589d8f018c8089498763b2889d9aa727a86e63df71ce51","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mtpqn","generateName":"daemon-set-","namespace":"daemonsets-8856","uid":"3aa02497-fe7d-42ac-8ea7-d6832677c90b","resourceVersion":"25587","creationTimestamp":"2022-12-14T16:48:08Z","deletionTimestamp":"2022-12-14T16:48:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"09a1e3db-67b1-44aa-9871-57bd889f2386","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09a1e3db-67b1-44aa-9871-57bd889f2386\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T16:48:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d6crs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d6crs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"iet9eich7uhu-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["iet9eich7uhu-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T16:48:08Z"}],"hostIP":"192.168.121.56","podIP":"10.233.67.82","podIPs":[{"ip":"10.233.67.82"}],"startTime":"2022-12-14T16:48:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T16:48:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://49b8a18daeeda44482a7148a132e3e1c221cc9ea1f1fced81053d2fb20021d27","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:10.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8856" for this suite. 12/14/22 16:48:10.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:10.552
Dec 14 16:48:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:48:10.556
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:10.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:10.585
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-a8c01d0f-b17b-48d3-b0da-30fcccb16a59 12/14/22 16:48:10.591
STEP: Creating a pod to test consume configMaps 12/14/22 16:48:10.603
Dec 14 16:48:10.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860" in namespace "configmap-9459" to be "Succeeded or Failed"
Dec 14 16:48:10.631: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098242ms
Dec 14 16:48:12.647: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022437474s
Dec 14 16:48:14.638: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013361313s
STEP: Saw pod success 12/14/22 16:48:14.638
Dec 14 16:48:14.639: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860" satisfied condition "Succeeded or Failed"
Dec 14 16:48:14.645: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:48:14.658
Dec 14 16:48:14.681: INFO: Waiting for pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 to disappear
Dec 14 16:48:14.686: INFO: Pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:14.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9459" for this suite. 12/14/22 16:48:14.694
------------------------------
• [4.153 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:10.552
    Dec 14 16:48:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:48:10.556
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:10.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:10.585
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-a8c01d0f-b17b-48d3-b0da-30fcccb16a59 12/14/22 16:48:10.591
    STEP: Creating a pod to test consume configMaps 12/14/22 16:48:10.603
    Dec 14 16:48:10.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860" in namespace "configmap-9459" to be "Succeeded or Failed"
    Dec 14 16:48:10.631: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098242ms
    Dec 14 16:48:12.647: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022437474s
    Dec 14 16:48:14.638: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013361313s
    STEP: Saw pod success 12/14/22 16:48:14.638
    Dec 14 16:48:14.639: INFO: Pod "pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860" satisfied condition "Succeeded or Failed"
    Dec 14 16:48:14.645: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:48:14.658
    Dec 14 16:48:14.681: INFO: Waiting for pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 to disappear
    Dec 14 16:48:14.686: INFO: Pod pod-configmaps-0fc1acd9-10be-4bff-b221-37de8dc6e860 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:14.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9459" for this suite. 12/14/22 16:48:14.694
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:14.709
Dec 14 16:48:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:48:14.714
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:14.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:14.744
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-1808aad0-62a2-4cc8-869b-c898c68d98a0 12/14/22 16:48:14.748
STEP: Creating a pod to test consume secrets 12/14/22 16:48:14.754
Dec 14 16:48:14.767: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a" in namespace "projected-7219" to be "Succeeded or Failed"
Dec 14 16:48:14.789: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.651635ms
Dec 14 16:48:16.806: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Running", Reason="", readiness=false. Elapsed: 2.039579786s
Dec 14 16:48:18.800: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032710448s
STEP: Saw pod success 12/14/22 16:48:18.8
Dec 14 16:48:18.800: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a" satisfied condition "Succeeded or Failed"
Dec 14 16:48:18.803: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:48:18.813
Dec 14 16:48:18.836: INFO: Waiting for pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a to disappear
Dec 14 16:48:18.848: INFO: Pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:18.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7219" for this suite. 12/14/22 16:48:18.855
------------------------------
• [4.159 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:14.709
    Dec 14 16:48:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:48:14.714
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:14.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:14.744
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-1808aad0-62a2-4cc8-869b-c898c68d98a0 12/14/22 16:48:14.748
    STEP: Creating a pod to test consume secrets 12/14/22 16:48:14.754
    Dec 14 16:48:14.767: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a" in namespace "projected-7219" to be "Succeeded or Failed"
    Dec 14 16:48:14.789: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.651635ms
    Dec 14 16:48:16.806: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Running", Reason="", readiness=false. Elapsed: 2.039579786s
    Dec 14 16:48:18.800: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032710448s
    STEP: Saw pod success 12/14/22 16:48:18.8
    Dec 14 16:48:18.800: INFO: Pod "pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a" satisfied condition "Succeeded or Failed"
    Dec 14 16:48:18.803: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:48:18.813
    Dec 14 16:48:18.836: INFO: Waiting for pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a to disappear
    Dec 14 16:48:18.848: INFO: Pod pod-projected-secrets-85ef24dd-d2a8-4ead-acfd-56f3730ae86a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:18.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7219" for this suite. 12/14/22 16:48:18.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:18.868
Dec 14 16:48:18.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 16:48:18.871
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:18.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:18.897
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 12/14/22 16:48:18.901
STEP: Ensuring active pods == parallelism 12/14/22 16:48:18.91
STEP: Orphaning one of the Job's Pods 12/14/22 16:48:20.917
Dec 14 16:48:21.449: INFO: Successfully updated pod "adopt-release-9nkn4"
STEP: Checking that the Job readopts the Pod 12/14/22 16:48:21.449
Dec 14 16:48:21.449: INFO: Waiting up to 15m0s for pod "adopt-release-9nkn4" in namespace "job-4738" to be "adopted"
Dec 14 16:48:21.457: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 7.640331ms
Dec 14 16:48:23.468: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018939401s
Dec 14 16:48:23.469: INFO: Pod "adopt-release-9nkn4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 12/14/22 16:48:23.469
Dec 14 16:48:23.987: INFO: Successfully updated pod "adopt-release-9nkn4"
STEP: Checking that the Job releases the Pod 12/14/22 16:48:23.987
Dec 14 16:48:23.987: INFO: Waiting up to 15m0s for pod "adopt-release-9nkn4" in namespace "job-4738" to be "released"
Dec 14 16:48:23.995: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 7.486788ms
Dec 14 16:48:26.001: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013631778s
Dec 14 16:48:26.001: INFO: Pod "adopt-release-9nkn4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4738" for this suite. 12/14/22 16:48:26.009
------------------------------
• [SLOW TEST] [7.151 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:18.868
    Dec 14 16:48:18.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 16:48:18.871
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:18.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:18.897
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 12/14/22 16:48:18.901
    STEP: Ensuring active pods == parallelism 12/14/22 16:48:18.91
    STEP: Orphaning one of the Job's Pods 12/14/22 16:48:20.917
    Dec 14 16:48:21.449: INFO: Successfully updated pod "adopt-release-9nkn4"
    STEP: Checking that the Job readopts the Pod 12/14/22 16:48:21.449
    Dec 14 16:48:21.449: INFO: Waiting up to 15m0s for pod "adopt-release-9nkn4" in namespace "job-4738" to be "adopted"
    Dec 14 16:48:21.457: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 7.640331ms
    Dec 14 16:48:23.468: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018939401s
    Dec 14 16:48:23.469: INFO: Pod "adopt-release-9nkn4" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 12/14/22 16:48:23.469
    Dec 14 16:48:23.987: INFO: Successfully updated pod "adopt-release-9nkn4"
    STEP: Checking that the Job releases the Pod 12/14/22 16:48:23.987
    Dec 14 16:48:23.987: INFO: Waiting up to 15m0s for pod "adopt-release-9nkn4" in namespace "job-4738" to be "released"
    Dec 14 16:48:23.995: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 7.486788ms
    Dec 14 16:48:26.001: INFO: Pod "adopt-release-9nkn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013631778s
    Dec 14 16:48:26.001: INFO: Pod "adopt-release-9nkn4" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4738" for this suite. 12/14/22 16:48:26.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:26.02
Dec 14 16:48:26.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:48:26.024
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:26.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:26.045
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-e61ca83e-3640-4da4-8b07-fdf05278a1e2 12/14/22 16:48:26.048
STEP: Creating a pod to test consume configMaps 12/14/22 16:48:26.07
Dec 14 16:48:26.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0" in namespace "configmap-8446" to be "Succeeded or Failed"
Dec 14 16:48:26.102: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004442ms
Dec 14 16:48:28.111: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024963946s
Dec 14 16:48:30.109: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022708373s
STEP: Saw pod success 12/14/22 16:48:30.109
Dec 14 16:48:30.110: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0" satisfied condition "Succeeded or Failed"
Dec 14 16:48:30.120: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 16:48:30.145
Dec 14 16:48:30.181: INFO: Waiting for pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 to disappear
Dec 14 16:48:30.188: INFO: Pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:48:30.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8446" for this suite. 12/14/22 16:48:30.197
------------------------------
• [4.189 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:26.02
    Dec 14 16:48:26.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:48:26.024
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:26.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:26.045
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-e61ca83e-3640-4da4-8b07-fdf05278a1e2 12/14/22 16:48:26.048
    STEP: Creating a pod to test consume configMaps 12/14/22 16:48:26.07
    Dec 14 16:48:26.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0" in namespace "configmap-8446" to be "Succeeded or Failed"
    Dec 14 16:48:26.102: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004442ms
    Dec 14 16:48:28.111: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024963946s
    Dec 14 16:48:30.109: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022708373s
    STEP: Saw pod success 12/14/22 16:48:30.109
    Dec 14 16:48:30.110: INFO: Pod "pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0" satisfied condition "Succeeded or Failed"
    Dec 14 16:48:30.120: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 16:48:30.145
    Dec 14 16:48:30.181: INFO: Waiting for pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 to disappear
    Dec 14 16:48:30.188: INFO: Pod pod-configmaps-78b18a2d-892c-442f-b623-78920802d9a0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:48:30.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8446" for this suite. 12/14/22 16:48:30.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:48:30.21
Dec 14 16:48:30.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:48:30.214
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:30.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:30.237
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 14 16:48:30.264: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:49:30.304: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:49:30.309
Dec 14 16:49:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 16:49:30.312
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:49:30.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:49:30.346
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 12/14/22 16:49:30.351
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 16:49:30.351
Dec 14 16:49:30.366: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1088" to be "running"
Dec 14 16:49:30.372: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655459ms
Dec 14 16:49:32.381: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015458035s
Dec 14 16:49:32.381: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 16:49:32.386
Dec 14 16:49:32.403: INFO: found a healthy node: iet9eich7uhu-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Dec 14 16:49:44.580: INFO: pods created so far: [1 1 1]
Dec 14 16:49:44.581: INFO: length of pods created so far: 3
Dec 14 16:49:46.603: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Dec 14 16:49:53.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:49:53.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1088" for this suite. 12/14/22 16:49:53.781
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3507" for this suite. 12/14/22 16:49:53.795
------------------------------
• [SLOW TEST] [83.593 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:48:30.21
    Dec 14 16:48:30.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:48:30.214
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:48:30.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:48:30.237
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 14 16:48:30.264: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 16:49:30.304: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:49:30.309
    Dec 14 16:49:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 16:49:30.312
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:49:30.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:49:30.346
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 12/14/22 16:49:30.351
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 16:49:30.351
    Dec 14 16:49:30.366: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1088" to be "running"
    Dec 14 16:49:30.372: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655459ms
    Dec 14 16:49:32.381: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015458035s
    Dec 14 16:49:32.381: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 16:49:32.386
    Dec 14 16:49:32.403: INFO: found a healthy node: iet9eich7uhu-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Dec 14 16:49:44.580: INFO: pods created so far: [1 1 1]
    Dec 14 16:49:44.581: INFO: length of pods created so far: 3
    Dec 14 16:49:46.603: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:49:53.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:49:53.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1088" for this suite. 12/14/22 16:49:53.781
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3507" for this suite. 12/14/22 16:49:53.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:49:53.804
Dec 14 16:49:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:49:53.814
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:49:53.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:49:53.841
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:49:53.86
Dec 14 16:49:53.889: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4942" to be "running and ready"
Dec 14 16:49:53.896: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6044ms
Dec 14 16:49:53.896: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:49:55.903: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014057787s
Dec 14 16:49:55.903: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 16:49:55.903: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 12/14/22 16:49:55.907
Dec 14 16:49:55.915: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4942" to be "running and ready"
Dec 14 16:49:55.919: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068188ms
Dec 14 16:49:55.919: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:49:57.926: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010835683s
Dec 14 16:49:57.926: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:49:59.927: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011751295s
Dec 14 16:49:59.927: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Dec 14 16:49:59.927: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/14/22 16:49:59.935
STEP: delete the pod with lifecycle hook 12/14/22 16:49:59.961
Dec 14 16:49:59.994: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 16:50:00.005: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 16:50:02.006: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 16:50:02.012: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:02.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4942" for this suite. 12/14/22 16:50:02.018
------------------------------
• [SLOW TEST] [8.222 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:49:53.804
    Dec 14 16:49:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:49:53.814
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:49:53.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:49:53.841
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:49:53.86
    Dec 14 16:49:53.889: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4942" to be "running and ready"
    Dec 14 16:49:53.896: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6044ms
    Dec 14 16:49:53.896: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:49:55.903: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014057787s
    Dec 14 16:49:55.903: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 16:49:55.903: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 12/14/22 16:49:55.907
    Dec 14 16:49:55.915: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4942" to be "running and ready"
    Dec 14 16:49:55.919: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068188ms
    Dec 14 16:49:55.919: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:49:57.926: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010835683s
    Dec 14 16:49:57.926: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:49:59.927: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011751295s
    Dec 14 16:49:59.927: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Dec 14 16:49:59.927: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/14/22 16:49:59.935
    STEP: delete the pod with lifecycle hook 12/14/22 16:49:59.961
    Dec 14 16:49:59.994: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 14 16:50:00.005: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 14 16:50:02.006: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 14 16:50:02.012: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:02.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4942" for this suite. 12/14/22 16:50:02.018
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:02.029
Dec 14 16:50:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-runtime 12/14/22 16:50:02.032
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:02.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:02.05
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 12/14/22 16:50:02.054
STEP: wait for the container to reach Succeeded 12/14/22 16:50:02.066
STEP: get the container status 12/14/22 16:50:06.118
STEP: the container should be terminated 12/14/22 16:50:06.123
STEP: the termination message should be set 12/14/22 16:50:06.123
Dec 14 16:50:06.123: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 12/14/22 16:50:06.123
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:06.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8201" for this suite. 12/14/22 16:50:06.159
------------------------------
• [4.141 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:02.029
    Dec 14 16:50:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-runtime 12/14/22 16:50:02.032
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:02.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:02.05
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 12/14/22 16:50:02.054
    STEP: wait for the container to reach Succeeded 12/14/22 16:50:02.066
    STEP: get the container status 12/14/22 16:50:06.118
    STEP: the container should be terminated 12/14/22 16:50:06.123
    STEP: the termination message should be set 12/14/22 16:50:06.123
    Dec 14 16:50:06.123: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 12/14/22 16:50:06.123
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:06.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8201" for this suite. 12/14/22 16:50:06.159
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:06.173
Dec 14 16:50:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename subpath 12/14/22 16:50:06.175
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:06.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:06.21
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 16:50:06.214
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-ss8d 12/14/22 16:50:06.23
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:50:06.23
Dec 14 16:50:06.244: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ss8d" in namespace "subpath-5774" to be "Succeeded or Failed"
Dec 14 16:50:06.249: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686838ms
Dec 14 16:50:08.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011947089s
Dec 14 16:50:10.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011256127s
Dec 14 16:50:12.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 6.011506531s
Dec 14 16:50:14.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 8.011934265s
Dec 14 16:50:16.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.011724123s
Dec 14 16:50:18.255: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.011052262s
Dec 14 16:50:20.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 14.013216043s
Dec 14 16:50:22.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 16.0116961s
Dec 14 16:50:24.257: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 18.0124001s
Dec 14 16:50:26.259: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 20.014596228s
Dec 14 16:50:28.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=false. Elapsed: 22.013092959s
Dec 14 16:50:30.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013547436s
STEP: Saw pod success 12/14/22 16:50:30.258
Dec 14 16:50:30.259: INFO: Pod "pod-subpath-test-downwardapi-ss8d" satisfied condition "Succeeded or Failed"
Dec 14 16:50:30.267: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-downwardapi-ss8d container test-container-subpath-downwardapi-ss8d: <nil>
STEP: delete the pod 12/14/22 16:50:30.307
Dec 14 16:50:30.325: INFO: Waiting for pod pod-subpath-test-downwardapi-ss8d to disappear
Dec 14 16:50:30.332: INFO: Pod pod-subpath-test-downwardapi-ss8d no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ss8d 12/14/22 16:50:30.332
Dec 14 16:50:30.332: INFO: Deleting pod "pod-subpath-test-downwardapi-ss8d" in namespace "subpath-5774"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:30.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5774" for this suite. 12/14/22 16:50:30.343
------------------------------
• [SLOW TEST] [24.183 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:06.173
    Dec 14 16:50:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename subpath 12/14/22 16:50:06.175
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:06.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:06.21
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 16:50:06.214
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-ss8d 12/14/22 16:50:06.23
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:50:06.23
    Dec 14 16:50:06.244: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ss8d" in namespace "subpath-5774" to be "Succeeded or Failed"
    Dec 14 16:50:06.249: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686838ms
    Dec 14 16:50:08.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011947089s
    Dec 14 16:50:10.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.011256127s
    Dec 14 16:50:12.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 6.011506531s
    Dec 14 16:50:14.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 8.011934265s
    Dec 14 16:50:16.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.011724123s
    Dec 14 16:50:18.255: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.011052262s
    Dec 14 16:50:20.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 14.013216043s
    Dec 14 16:50:22.256: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 16.0116961s
    Dec 14 16:50:24.257: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 18.0124001s
    Dec 14 16:50:26.259: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=true. Elapsed: 20.014596228s
    Dec 14 16:50:28.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Running", Reason="", readiness=false. Elapsed: 22.013092959s
    Dec 14 16:50:30.258: INFO: Pod "pod-subpath-test-downwardapi-ss8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013547436s
    STEP: Saw pod success 12/14/22 16:50:30.258
    Dec 14 16:50:30.259: INFO: Pod "pod-subpath-test-downwardapi-ss8d" satisfied condition "Succeeded or Failed"
    Dec 14 16:50:30.267: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-downwardapi-ss8d container test-container-subpath-downwardapi-ss8d: <nil>
    STEP: delete the pod 12/14/22 16:50:30.307
    Dec 14 16:50:30.325: INFO: Waiting for pod pod-subpath-test-downwardapi-ss8d to disappear
    Dec 14 16:50:30.332: INFO: Pod pod-subpath-test-downwardapi-ss8d no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-ss8d 12/14/22 16:50:30.332
    Dec 14 16:50:30.332: INFO: Deleting pod "pod-subpath-test-downwardapi-ss8d" in namespace "subpath-5774"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:30.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5774" for this suite. 12/14/22 16:50:30.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:30.36
Dec 14 16:50:30.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:50:30.364
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:30.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:30.391
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 12/14/22 16:50:30.395
Dec 14 16:50:30.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 16:50:30.603: INFO: stderr: ""
Dec 14 16:50:30.603: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 12/14/22 16:50:30.603
Dec 14 16:50:30.603: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 16:50:30.603: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7672" to be "running and ready, or succeeded"
Dec 14 16:50:30.611: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014959ms
Dec 14 16:50:30.611: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'iet9eich7uhu-3' to be 'Running' but was 'Pending'
Dec 14 16:50:32.620: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017271469s
Dec 14 16:50:32.620: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 16:50:32.621: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 12/14/22 16:50:32.621
Dec 14 16:50:32.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator'
Dec 14 16:50:32.849: INFO: stderr: ""
Dec 14 16:50:32.849: INFO: stdout: "I1214 16:50:31.390294       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d5cd 590\nI1214 16:50:31.590470       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/nppk 446\nI1214 16:50:31.791114       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/27r 399\nI1214 16:50:31.990388       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/lg5 442\nI1214 16:50:32.190777       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/c7fs 346\nI1214 16:50:32.391250       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wr4j 579\nI1214 16:50:32.590684       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rdg 549\nI1214 16:50:32.790597       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wrq 395\n"
STEP: limiting log lines 12/14/22 16:50:32.849
Dec 14 16:50:32.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --tail=1'
Dec 14 16:50:33.023: INFO: stderr: ""
Dec 14 16:50:33.023: INFO: stdout: "I1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\n"
Dec 14 16:50:33.023: INFO: got output "I1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\n"
STEP: limiting log bytes 12/14/22 16:50:33.024
Dec 14 16:50:33.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --limit-bytes=1'
Dec 14 16:50:33.183: INFO: stderr: ""
Dec 14 16:50:33.183: INFO: stdout: "I"
Dec 14 16:50:33.183: INFO: got output "I"
STEP: exposing timestamps 12/14/22 16:50:33.183
Dec 14 16:50:33.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 14 16:50:33.327: INFO: stderr: ""
Dec 14 16:50:33.327: INFO: stdout: "2022-12-14T16:50:33.190828207Z I1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\n"
Dec 14 16:50:33.327: INFO: got output "2022-12-14T16:50:33.190828207Z I1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\n"
STEP: restricting to a time range 12/14/22 16:50:33.327
Dec 14 16:50:35.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --since=1s'
Dec 14 16:50:36.183: INFO: stderr: ""
Dec 14 16:50:36.183: INFO: stdout: "I1214 16:50:35.190777       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/l5jn 479\nI1214 16:50:35.391243       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dxg 355\nI1214 16:50:35.590533       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/pgx 392\nI1214 16:50:35.791018       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/m877 434\nI1214 16:50:35.990374       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/9j78 233\n"
Dec 14 16:50:36.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --since=24h'
Dec 14 16:50:36.367: INFO: stderr: ""
Dec 14 16:50:36.367: INFO: stdout: "I1214 16:50:31.390294       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d5cd 590\nI1214 16:50:31.590470       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/nppk 446\nI1214 16:50:31.791114       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/27r 399\nI1214 16:50:31.990388       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/lg5 442\nI1214 16:50:32.190777       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/c7fs 346\nI1214 16:50:32.391250       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wr4j 579\nI1214 16:50:32.590684       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rdg 549\nI1214 16:50:32.790597       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wrq 395\nI1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\nI1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\nI1214 16:50:33.391128       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/r76k 219\nI1214 16:50:33.590492       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/kqrt 374\nI1214 16:50:33.790857       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/9nkv 486\nI1214 16:50:33.991145       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/7c2 302\nI1214 16:50:34.190856       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/h47 236\nI1214 16:50:34.390898       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/ldwx 412\nI1214 16:50:34.591535       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/qtz 455\nI1214 16:50:34.790790       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/rp4k 559\nI1214 16:50:34.991229       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/w2f 200\nI1214 16:50:35.190777       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/l5jn 479\nI1214 16:50:35.391243       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dxg 355\nI1214 16:50:35.590533       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/pgx 392\nI1214 16:50:35.791018       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/m877 434\nI1214 16:50:35.990374       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/9j78 233\nI1214 16:50:36.191857       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/nlz 377\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Dec 14 16:50:36.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 delete pod logs-generator'
Dec 14 16:50:37.563: INFO: stderr: ""
Dec 14 16:50:37.563: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7672" for this suite. 12/14/22 16:50:37.572
------------------------------
• [SLOW TEST] [7.227 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:30.36
    Dec 14 16:50:30.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:50:30.364
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:30.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:30.391
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 12/14/22 16:50:30.395
    Dec 14 16:50:30.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Dec 14 16:50:30.603: INFO: stderr: ""
    Dec 14 16:50:30.603: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 12/14/22 16:50:30.603
    Dec 14 16:50:30.603: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Dec 14 16:50:30.603: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7672" to be "running and ready, or succeeded"
    Dec 14 16:50:30.611: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014959ms
    Dec 14 16:50:30.611: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'iet9eich7uhu-3' to be 'Running' but was 'Pending'
    Dec 14 16:50:32.620: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017271469s
    Dec 14 16:50:32.620: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Dec 14 16:50:32.621: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 12/14/22 16:50:32.621
    Dec 14 16:50:32.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator'
    Dec 14 16:50:32.849: INFO: stderr: ""
    Dec 14 16:50:32.849: INFO: stdout: "I1214 16:50:31.390294       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d5cd 590\nI1214 16:50:31.590470       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/nppk 446\nI1214 16:50:31.791114       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/27r 399\nI1214 16:50:31.990388       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/lg5 442\nI1214 16:50:32.190777       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/c7fs 346\nI1214 16:50:32.391250       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wr4j 579\nI1214 16:50:32.590684       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rdg 549\nI1214 16:50:32.790597       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wrq 395\n"
    STEP: limiting log lines 12/14/22 16:50:32.849
    Dec 14 16:50:32.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --tail=1'
    Dec 14 16:50:33.023: INFO: stderr: ""
    Dec 14 16:50:33.023: INFO: stdout: "I1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\n"
    Dec 14 16:50:33.023: INFO: got output "I1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\n"
    STEP: limiting log bytes 12/14/22 16:50:33.024
    Dec 14 16:50:33.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --limit-bytes=1'
    Dec 14 16:50:33.183: INFO: stderr: ""
    Dec 14 16:50:33.183: INFO: stdout: "I"
    Dec 14 16:50:33.183: INFO: got output "I"
    STEP: exposing timestamps 12/14/22 16:50:33.183
    Dec 14 16:50:33.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --tail=1 --timestamps'
    Dec 14 16:50:33.327: INFO: stderr: ""
    Dec 14 16:50:33.327: INFO: stdout: "2022-12-14T16:50:33.190828207Z I1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\n"
    Dec 14 16:50:33.327: INFO: got output "2022-12-14T16:50:33.190828207Z I1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\n"
    STEP: restricting to a time range 12/14/22 16:50:33.327
    Dec 14 16:50:35.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --since=1s'
    Dec 14 16:50:36.183: INFO: stderr: ""
    Dec 14 16:50:36.183: INFO: stdout: "I1214 16:50:35.190777       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/l5jn 479\nI1214 16:50:35.391243       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dxg 355\nI1214 16:50:35.590533       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/pgx 392\nI1214 16:50:35.791018       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/m877 434\nI1214 16:50:35.990374       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/9j78 233\n"
    Dec 14 16:50:36.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 logs logs-generator logs-generator --since=24h'
    Dec 14 16:50:36.367: INFO: stderr: ""
    Dec 14 16:50:36.367: INFO: stdout: "I1214 16:50:31.390294       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/d5cd 590\nI1214 16:50:31.590470       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/nppk 446\nI1214 16:50:31.791114       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/27r 399\nI1214 16:50:31.990388       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/lg5 442\nI1214 16:50:32.190777       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/c7fs 346\nI1214 16:50:32.391250       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wr4j 579\nI1214 16:50:32.590684       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rdg 549\nI1214 16:50:32.790597       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wrq 395\nI1214 16:50:32.990382       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zxr 579\nI1214 16:50:33.190771       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rp46 574\nI1214 16:50:33.391128       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/r76k 219\nI1214 16:50:33.590492       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/kqrt 374\nI1214 16:50:33.790857       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/9nkv 486\nI1214 16:50:33.991145       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/7c2 302\nI1214 16:50:34.190856       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/h47 236\nI1214 16:50:34.390898       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/ldwx 412\nI1214 16:50:34.591535       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/qtz 455\nI1214 16:50:34.790790       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/rp4k 559\nI1214 16:50:34.991229       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/w2f 200\nI1214 16:50:35.190777       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/l5jn 479\nI1214 16:50:35.391243       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/dxg 355\nI1214 16:50:35.590533       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/pgx 392\nI1214 16:50:35.791018       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/m877 434\nI1214 16:50:35.990374       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/9j78 233\nI1214 16:50:36.191857       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/nlz 377\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Dec 14 16:50:36.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-7672 delete pod logs-generator'
    Dec 14 16:50:37.563: INFO: stderr: ""
    Dec 14 16:50:37.563: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7672" for this suite. 12/14/22 16:50:37.572
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:37.587
Dec 14 16:50:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:50:37.595
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:37.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:37.622
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:50:37.633
Dec 14 16:50:37.653: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9173" to be "running and ready"
Dec 14 16:50:37.662: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423219ms
Dec 14 16:50:37.662: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:50:39.668: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014571827s
Dec 14 16:50:39.668: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 14 16:50:39.668: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 12/14/22 16:50:39.675
Dec 14 16:50:39.686: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9173" to be "running and ready"
Dec 14 16:50:39.690: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282096ms
Dec 14 16:50:39.691: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:50:41.700: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01368777s
Dec 14 16:50:41.700: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Dec 14 16:50:41.700: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/14/22 16:50:41.705
Dec 14 16:50:41.719: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:50:41.724: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 16:50:43.724: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:50:43.739: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 16:50:45.725: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:50:45.731: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 12/14/22 16:50:45.732
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:45.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9173" for this suite. 12/14/22 16:50:45.769
------------------------------
• [SLOW TEST] [8.201 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:37.587
    Dec 14 16:50:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/14/22 16:50:37.595
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:37.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:37.622
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/14/22 16:50:37.633
    Dec 14 16:50:37.653: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9173" to be "running and ready"
    Dec 14 16:50:37.662: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423219ms
    Dec 14 16:50:37.662: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:50:39.668: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014571827s
    Dec 14 16:50:39.668: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 14 16:50:39.668: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 12/14/22 16:50:39.675
    Dec 14 16:50:39.686: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9173" to be "running and ready"
    Dec 14 16:50:39.690: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282096ms
    Dec 14 16:50:39.691: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:50:41.700: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01368777s
    Dec 14 16:50:41.700: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Dec 14 16:50:41.700: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/14/22 16:50:41.705
    Dec 14 16:50:41.719: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 14 16:50:41.724: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 14 16:50:43.724: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 14 16:50:43.739: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 14 16:50:45.725: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 14 16:50:45.731: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 12/14/22 16:50:45.732
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:45.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9173" for this suite. 12/14/22 16:50:45.769
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:45.791
Dec 14 16:50:45.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:50:45.795
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.817
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-95f37e92-ab7a-4a6d-a084-97010dde7810 12/14/22 16:50:45.82
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:45.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8094" for this suite. 12/14/22 16:50:45.832
------------------------------
• [0.051 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:45.791
    Dec 14 16:50:45.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:50:45.795
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.817
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-95f37e92-ab7a-4a6d-a084-97010dde7810 12/14/22 16:50:45.82
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:45.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8094" for this suite. 12/14/22 16:50:45.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:45.849
Dec 14 16:50:45.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:50:45.85
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.872
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:50:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2387" for this suite. 12/14/22 16:50:45.936
------------------------------
• [0.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:45.849
    Dec 14 16:50:45.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:50:45.85
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.872
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:50:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2387" for this suite. 12/14/22 16:50:45.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:50:45.958
Dec 14 16:50:45.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:50:45.961
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.985
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Dec 14 16:50:46.000: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 16:50:51.007: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:50:51.008
Dec 14 16:50:51.008: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 16:50:53.018: INFO: Creating deployment "test-rollover-deployment"
Dec 14 16:50:53.033: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 16:50:55.047: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 16:50:55.066: INFO: Ensure that both replica sets have 1 created replica
Dec 14 16:50:55.076: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 16:50:55.101: INFO: Updating deployment test-rollover-deployment
Dec 14 16:50:55.102: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 16:50:57.115: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 16:50:57.131: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 16:50:57.146: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 16:50:57.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:50:59.158: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 16:50:59.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:51:01.166: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 16:51:01.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:51:03.160: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 16:51:03.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:51:05.165: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 16:51:05.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:51:07.163: INFO: 
Dec 14 16:51:07.163: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:51:07.181: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1220  9e8b73ed-8833-4747-aa96-423fade94a1a 26533 2 2022-12-14 16:50:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a71b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:50:53 +0000 UTC,LastTransitionTime:2022-12-14 16:50:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-14 16:51:06 +0000 UTC,LastTransitionTime:2022-12-14 16:50:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:51:07.186: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-1220  4984c06d-ac4b-4160-a32b-d438bfa1d79d 26523 2 2022-12-14 16:50:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a7687 0xc0041a7688}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a7738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:51:07.186: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 16:51:07.186: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1220  019a9613-5683-43e0-abe1-cb6cd431b778 26532 2 2022-12-14 16:50:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a7557 0xc0041a7558}] [] [{e2e.test Update apps/v1 2022-12-14 16:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041a7618 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:51:07.187: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-1220  139981e9-145b-4562-a87a-033d4c960d3d 26487 2 2022-12-14 16:50:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a77a7 0xc0041a77a8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a7858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:51:07.193: INFO: Pod "test-rollover-deployment-6c6df9974f-vpb8h" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-vpb8h test-rollover-deployment-6c6df9974f- deployment-1220  c3a99e05-ee21-4c57-802e-18fbf5906619 26503 0 2022-12-14 16:50:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 4984c06d-ac4b-4160-a32b-d438bfa1d79d 0xc0041a7da7 0xc0041a7da8}] [] [{kube-controller-manager Update v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4984c06d-ac4b-4160-a32b-d438bfa1d79d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfzr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfzr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.100,StartTime:2022-12-14 16:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:50:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://15e290e5b39b2f9148bfe1120a4de424e871c78a93f4ed5d5edbd943f5337776,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:51:07.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1220" for this suite. 12/14/22 16:51:07.2
------------------------------
• [SLOW TEST] [21.253 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:50:45.958
    Dec 14 16:50:45.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:50:45.961
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:50:45.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:50:45.985
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Dec 14 16:50:46.000: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Dec 14 16:50:51.007: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:50:51.008
    Dec 14 16:50:51.008: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Dec 14 16:50:53.018: INFO: Creating deployment "test-rollover-deployment"
    Dec 14 16:50:53.033: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Dec 14 16:50:55.047: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Dec 14 16:50:55.066: INFO: Ensure that both replica sets have 1 created replica
    Dec 14 16:50:55.076: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Dec 14 16:50:55.101: INFO: Updating deployment test-rollover-deployment
    Dec 14 16:50:55.102: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Dec 14 16:50:57.115: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Dec 14 16:50:57.131: INFO: Make sure deployment "test-rollover-deployment" is complete
    Dec 14 16:50:57.146: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 16:50:57.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:50:59.158: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 16:50:59.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:51:01.166: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 16:51:01.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:51:03.160: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 16:51:03.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:51:05.165: INFO: all replica sets need to contain the pod-template-hash label
    Dec 14 16:51:05.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 50, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 50, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:51:07.163: INFO: 
    Dec 14 16:51:07.163: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:51:07.181: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1220  9e8b73ed-8833-4747-aa96-423fade94a1a 26533 2 2022-12-14 16:50:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a71b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:50:53 +0000 UTC,LastTransitionTime:2022-12-14 16:50:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-14 16:51:06 +0000 UTC,LastTransitionTime:2022-12-14 16:50:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 16:51:07.186: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-1220  4984c06d-ac4b-4160-a32b-d438bfa1d79d 26523 2 2022-12-14 16:50:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a7687 0xc0041a7688}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a7738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:51:07.186: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Dec 14 16:51:07.186: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1220  019a9613-5683-43e0-abe1-cb6cd431b778 26532 2 2022-12-14 16:50:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a7557 0xc0041a7558}] [] [{e2e.test Update apps/v1 2022-12-14 16:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041a7618 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:51:07.187: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-1220  139981e9-145b-4562-a87a-033d4c960d3d 26487 2 2022-12-14 16:50:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9e8b73ed-8833-4747-aa96-423fade94a1a 0xc0041a77a7 0xc0041a77a8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e8b73ed-8833-4747-aa96-423fade94a1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a7858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:51:07.193: INFO: Pod "test-rollover-deployment-6c6df9974f-vpb8h" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-vpb8h test-rollover-deployment-6c6df9974f- deployment-1220  c3a99e05-ee21-4c57-802e-18fbf5906619 26503 0 2022-12-14 16:50:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 4984c06d-ac4b-4160-a32b-d438bfa1d79d 0xc0041a7da7 0xc0041a7da8}] [] [{kube-controller-manager Update v1 2022-12-14 16:50:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4984c06d-ac4b-4160-a32b-d438bfa1d79d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfzr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfzr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:50:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.100,StartTime:2022-12-14 16:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:50:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://15e290e5b39b2f9148bfe1120a4de424e871c78a93f4ed5d5edbd943f5337776,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:51:07.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1220" for this suite. 12/14/22 16:51:07.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:51:07.221
Dec 14 16:51:07.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename deployment 12/14/22 16:51:07.223
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:07.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:07.253
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Dec 14 16:51:07.258: INFO: Creating simple deployment test-new-deployment
Dec 14 16:51:07.276: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 12/14/22 16:51:09.3
STEP: updating a scale subresource 12/14/22 16:51:09.305
STEP: verifying the deployment Spec.Replicas was modified 12/14/22 16:51:09.32
STEP: Patch a scale subresource 12/14/22 16:51:09.328
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:51:09.377: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-55  df8e7774-ef6d-418b-b258-dbf477a23fee 26565 3 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-14 16:51:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c921c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-14 16:51:08 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 16:51:09 +0000 UTC,LastTransitionTime:2022-12-14 16:51:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:51:09.395: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-55  c26a7c51-2de1-4c31-9af3-62a6b6614b2f 26569 3 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment df8e7774-ef6d-418b-b258-dbf477a23fee 0xc000c936d7 0xc000c936d8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df8e7774-ef6d-418b-b258-dbf477a23fee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c93968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:51:09.415: INFO: Pod "test-new-deployment-7f5969cbc7-6khkd" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6khkd test-new-deployment-7f5969cbc7- deployment-55  d73ef4fb-5bcc-47d1-953a-46fc295b7e22 26566 0 2022-12-14 16:51:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c26a7c51-2de1-4c31-9af3-62a6b6614b2f 0xc0042fe977 0xc0042fe978}] [] [{kube-controller-manager Update v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c26a7c51-2de1-4c31-9af3-62a6b6614b2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd7d8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd7d8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:51:09.416: INFO: Pod "test-new-deployment-7f5969cbc7-dsmhj" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dsmhj test-new-deployment-7f5969cbc7- deployment-55  658f9348-2375-41ba-b6d6-81f5df6cf5ed 26556 0 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c26a7c51-2de1-4c31-9af3-62a6b6614b2f 0xc0042feaf0 0xc0042feaf1}] [] [{kube-controller-manager Update v1 2022-12-14 16:51:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c26a7c51-2de1-4c31-9af3-62a6b6614b2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:51:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhzwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhzwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.101,StartTime:2022-12-14 16:51:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:51:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://698b448fd9f251373c134f0ff754993cfaf02f7a0dd4217757ad357e9bf9f890,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec 14 16:51:09.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-55" for this suite. 12/14/22 16:51:09.425
------------------------------
• [2.223 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:51:07.221
    Dec 14 16:51:07.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename deployment 12/14/22 16:51:07.223
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:07.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:07.253
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Dec 14 16:51:07.258: INFO: Creating simple deployment test-new-deployment
    Dec 14 16:51:07.276: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 12/14/22 16:51:09.3
    STEP: updating a scale subresource 12/14/22 16:51:09.305
    STEP: verifying the deployment Spec.Replicas was modified 12/14/22 16:51:09.32
    STEP: Patch a scale subresource 12/14/22 16:51:09.328
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 14 16:51:09.377: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-55  df8e7774-ef6d-418b-b258-dbf477a23fee 26565 3 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-14 16:51:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c921c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-14 16:51:08 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 16:51:09 +0000 UTC,LastTransitionTime:2022-12-14 16:51:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 14 16:51:09.395: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-55  c26a7c51-2de1-4c31-9af3-62a6b6614b2f 26569 3 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment df8e7774-ef6d-418b-b258-dbf477a23fee 0xc000c936d7 0xc000c936d8}] [] [{kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df8e7774-ef6d-418b-b258-dbf477a23fee\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c93968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 14 16:51:09.415: INFO: Pod "test-new-deployment-7f5969cbc7-6khkd" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6khkd test-new-deployment-7f5969cbc7- deployment-55  d73ef4fb-5bcc-47d1-953a-46fc295b7e22 26566 0 2022-12-14 16:51:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c26a7c51-2de1-4c31-9af3-62a6b6614b2f 0xc0042fe977 0xc0042fe978}] [] [{kube-controller-manager Update v1 2022-12-14 16:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c26a7c51-2de1-4c31-9af3-62a6b6614b2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd7d8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd7d8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 14 16:51:09.416: INFO: Pod "test-new-deployment-7f5969cbc7-dsmhj" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dsmhj test-new-deployment-7f5969cbc7- deployment-55  658f9348-2375-41ba-b6d6-81f5df6cf5ed 26556 0 2022-12-14 16:51:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 c26a7c51-2de1-4c31-9af3-62a6b6614b2f 0xc0042feaf0 0xc0042feaf1}] [] [{kube-controller-manager Update v1 2022-12-14 16:51:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c26a7c51-2de1-4c31-9af3-62a6b6614b2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:51:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhzwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhzwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:iet9eich7uhu-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:51:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.56,PodIP:10.233.67.101,StartTime:2022-12-14 16:51:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:51:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://698b448fd9f251373c134f0ff754993cfaf02f7a0dd4217757ad357e9bf9f890,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:51:09.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-55" for this suite. 12/14/22 16:51:09.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:51:09.448
Dec 14 16:51:09.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:51:09.452
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:09.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:09.484
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Dec 14 16:51:09.492: INFO: Got root ca configmap in namespace "svcaccounts-953"
Dec 14 16:51:09.500: INFO: Deleted root ca configmap in namespace "svcaccounts-953"
STEP: waiting for a new root ca configmap created 12/14/22 16:51:10.001
Dec 14 16:51:10.007: INFO: Recreated root ca configmap in namespace "svcaccounts-953"
Dec 14 16:51:10.020: INFO: Updated root ca configmap in namespace "svcaccounts-953"
STEP: waiting for the root ca configmap reconciled 12/14/22 16:51:10.521
Dec 14 16:51:10.528: INFO: Reconciled root ca configmap in namespace "svcaccounts-953"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec 14 16:51:10.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-953" for this suite. 12/14/22 16:51:10.535
------------------------------
• [1.105 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:51:09.448
    Dec 14 16:51:09.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svcaccounts 12/14/22 16:51:09.452
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:09.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:09.484
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Dec 14 16:51:09.492: INFO: Got root ca configmap in namespace "svcaccounts-953"
    Dec 14 16:51:09.500: INFO: Deleted root ca configmap in namespace "svcaccounts-953"
    STEP: waiting for a new root ca configmap created 12/14/22 16:51:10.001
    Dec 14 16:51:10.007: INFO: Recreated root ca configmap in namespace "svcaccounts-953"
    Dec 14 16:51:10.020: INFO: Updated root ca configmap in namespace "svcaccounts-953"
    STEP: waiting for the root ca configmap reconciled 12/14/22 16:51:10.521
    Dec 14 16:51:10.528: INFO: Reconciled root ca configmap in namespace "svcaccounts-953"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:51:10.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-953" for this suite. 12/14/22 16:51:10.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:51:10.557
Dec 14 16:51:10.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename gc 12/14/22 16:51:10.559
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:10.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:10.584
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Dec 14 16:51:10.666: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"32348ade-199f-4ba9-8e4c-569806de695a", Controller:(*bool)(0xc000e23d46), BlockOwnerDeletion:(*bool)(0xc000e23d47)}}
Dec 14 16:51:10.683: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"085baad9-8adc-4e0e-9f06-6144cfa2be0f", Controller:(*bool)(0xc005052c5e), BlockOwnerDeletion:(*bool)(0xc005052c5f)}}
Dec 14 16:51:10.703: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"09a04f32-cd0f-42ec-98a6-bf1db1e672df", Controller:(*bool)(0xc005052ef6), BlockOwnerDeletion:(*bool)(0xc005052ef7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec 14 16:51:15.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8368" for this suite. 12/14/22 16:51:15.747
------------------------------
• [SLOW TEST] [5.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:51:10.557
    Dec 14 16:51:10.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename gc 12/14/22 16:51:10.559
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:10.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:10.584
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Dec 14 16:51:10.666: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"32348ade-199f-4ba9-8e4c-569806de695a", Controller:(*bool)(0xc000e23d46), BlockOwnerDeletion:(*bool)(0xc000e23d47)}}
    Dec 14 16:51:10.683: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"085baad9-8adc-4e0e-9f06-6144cfa2be0f", Controller:(*bool)(0xc005052c5e), BlockOwnerDeletion:(*bool)(0xc005052c5f)}}
    Dec 14 16:51:10.703: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"09a04f32-cd0f-42ec-98a6-bf1db1e672df", Controller:(*bool)(0xc005052ef6), BlockOwnerDeletion:(*bool)(0xc005052ef7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:51:15.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8368" for this suite. 12/14/22 16:51:15.747
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:51:15.759
Dec 14 16:51:15.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 16:51:15.763
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:15.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:15.787
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/14/22 16:51:15.791
Dec 14 16:51:15.830: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:51:20.848: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/14/22 16:51:20.848
STEP: getting scale subresource 12/14/22 16:51:20.848
STEP: updating a scale subresource 12/14/22 16:51:20.855
STEP: verifying the replicaset Spec.Replicas was modified 12/14/22 16:51:20.868
STEP: Patch a scale subresource 12/14/22 16:51:20.875
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:51:20.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1280" for this suite. 12/14/22 16:51:20.904
------------------------------
• [SLOW TEST] [5.161 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:51:15.759
    Dec 14 16:51:15.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 16:51:15.763
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:15.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:15.787
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/14/22 16:51:15.791
    Dec 14 16:51:15.830: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 14 16:51:20.848: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/14/22 16:51:20.848
    STEP: getting scale subresource 12/14/22 16:51:20.848
    STEP: updating a scale subresource 12/14/22 16:51:20.855
    STEP: verifying the replicaset Spec.Replicas was modified 12/14/22 16:51:20.868
    STEP: Patch a scale subresource 12/14/22 16:51:20.875
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:51:20.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1280" for this suite. 12/14/22 16:51:20.904
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:51:20.921
Dec 14 16:51:20.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:51:20.924
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:20.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:20.964
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 14 16:51:20.990: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:52:21.031: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:21.036
Dec 14 16:52:21.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 16:52:21.041
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:21.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:21.069
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Dec 14 16:52:21.095: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec 14 16:52:21.100: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:21.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4985" for this suite. 12/14/22 16:52:21.212
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5691" for this suite. 12/14/22 16:52:21.221
------------------------------
• [SLOW TEST] [60.308 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:51:20.921
    Dec 14 16:51:20.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 16:51:20.924
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:51:20.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:51:20.964
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 14 16:51:20.990: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 16:52:21.031: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:21.036
    Dec 14 16:52:21.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption-path 12/14/22 16:52:21.041
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:21.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:21.069
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Dec 14 16:52:21.095: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec 14 16:52:21.100: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:21.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4985" for this suite. 12/14/22 16:52:21.212
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5691" for this suite. 12/14/22 16:52:21.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:21.237
Dec 14 16:52:21.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename init-container 12/14/22 16:52:21.24
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:21.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:21.266
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 12/14/22 16:52:21.27
Dec 14 16:52:21.271: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:27.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-562" for this suite. 12/14/22 16:52:27.087
------------------------------
• [SLOW TEST] [5.860 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:21.237
    Dec 14 16:52:21.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename init-container 12/14/22 16:52:21.24
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:21.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:21.266
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 12/14/22 16:52:21.27
    Dec 14 16:52:21.271: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:27.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-562" for this suite. 12/14/22 16:52:27.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:27.114
Dec 14 16:52:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:52:27.116
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:27.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:27.141
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 12/14/22 16:52:27.144
STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:52:27.15
STEP: Creating a ResourceQuota with not best effort scope 12/14/22 16:52:29.158
STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:52:29.165
STEP: Creating a best-effort pod 12/14/22 16:52:31.173
STEP: Ensuring resource quota with best effort scope captures the pod usage 12/14/22 16:52:31.196
STEP: Ensuring resource quota with not best effort ignored the pod usage 12/14/22 16:52:33.205
STEP: Deleting the pod 12/14/22 16:52:35.214
STEP: Ensuring resource quota status released the pod usage 12/14/22 16:52:35.235
STEP: Creating a not best-effort pod 12/14/22 16:52:37.256
STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/14/22 16:52:37.275
STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/14/22 16:52:39.285
STEP: Deleting the pod 12/14/22 16:52:41.298
STEP: Ensuring resource quota status released the pod usage 12/14/22 16:52:41.329
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:43.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5709" for this suite. 12/14/22 16:52:43.349
------------------------------
• [SLOW TEST] [16.246 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:27.114
    Dec 14 16:52:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:52:27.116
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:27.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:27.141
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 12/14/22 16:52:27.144
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:52:27.15
    STEP: Creating a ResourceQuota with not best effort scope 12/14/22 16:52:29.158
    STEP: Ensuring ResourceQuota status is calculated 12/14/22 16:52:29.165
    STEP: Creating a best-effort pod 12/14/22 16:52:31.173
    STEP: Ensuring resource quota with best effort scope captures the pod usage 12/14/22 16:52:31.196
    STEP: Ensuring resource quota with not best effort ignored the pod usage 12/14/22 16:52:33.205
    STEP: Deleting the pod 12/14/22 16:52:35.214
    STEP: Ensuring resource quota status released the pod usage 12/14/22 16:52:35.235
    STEP: Creating a not best-effort pod 12/14/22 16:52:37.256
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/14/22 16:52:37.275
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/14/22 16:52:39.285
    STEP: Deleting the pod 12/14/22 16:52:41.298
    STEP: Ensuring resource quota status released the pod usage 12/14/22 16:52:41.329
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:43.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5709" for this suite. 12/14/22 16:52:43.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:43.365
Dec 14 16:52:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:52:43.369
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:43.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:43.397
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:52:43.418
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:52:43.995
STEP: Deploying the webhook pod 12/14/22 16:52:44.007
STEP: Wait for the deployment to be ready 12/14/22 16:52:44.026
Dec 14 16:52:44.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 16:52:46.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 16:52:48.059
STEP: Verifying the service has paired with the endpoint 12/14/22 16:52:48.077
Dec 14 16:52:49.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 12/14/22 16:52:49.086
STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/14/22 16:52:49.117
STEP: Creating a configMap that should not be mutated 12/14/22 16:52:49.128
STEP: Patching a mutating webhook configuration's rules to include the create operation 12/14/22 16:52:49.141
STEP: Creating a configMap that should be mutated 12/14/22 16:52:49.151
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:49.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9522" for this suite. 12/14/22 16:52:49.327
STEP: Destroying namespace "webhook-9522-markers" for this suite. 12/14/22 16:52:49.339
------------------------------
• [SLOW TEST] [5.985 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:43.365
    Dec 14 16:52:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:52:43.369
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:43.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:43.397
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:52:43.418
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:52:43.995
    STEP: Deploying the webhook pod 12/14/22 16:52:44.007
    STEP: Wait for the deployment to be ready 12/14/22 16:52:44.026
    Dec 14 16:52:44.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec 14 16:52:46.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 16:52:48.059
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:52:48.077
    Dec 14 16:52:49.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 12/14/22 16:52:49.086
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/14/22 16:52:49.117
    STEP: Creating a configMap that should not be mutated 12/14/22 16:52:49.128
    STEP: Patching a mutating webhook configuration's rules to include the create operation 12/14/22 16:52:49.141
    STEP: Creating a configMap that should be mutated 12/14/22 16:52:49.151
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:49.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9522" for this suite. 12/14/22 16:52:49.327
    STEP: Destroying namespace "webhook-9522-markers" for this suite. 12/14/22 16:52:49.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:49.353
Dec 14 16:52:49.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:52:49.358
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:49.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:49.383
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-64831ab9-dea1-4be1-b63c-30498eb46199 12/14/22 16:52:49.387
STEP: Creating a pod to test consume secrets 12/14/22 16:52:49.392
Dec 14 16:52:49.409: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc" in namespace "projected-4536" to be "Succeeded or Failed"
Dec 14 16:52:49.414: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466403ms
Dec 14 16:52:51.423: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01356385s
Dec 14 16:52:53.422: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012371364s
Dec 14 16:52:55.421: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012156343s
STEP: Saw pod success 12/14/22 16:52:55.422
Dec 14 16:52:55.422: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc" satisfied condition "Succeeded or Failed"
Dec 14 16:52:55.427: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc container projected-secret-volume-test: <nil>
STEP: delete the pod 12/14/22 16:52:55.45
Dec 14 16:52:55.464: INFO: Waiting for pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc to disappear
Dec 14 16:52:55.469: INFO: Pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 16:52:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4536" for this suite. 12/14/22 16:52:55.475
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:49.353
    Dec 14 16:52:49.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:52:49.358
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:49.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:49.383
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-64831ab9-dea1-4be1-b63c-30498eb46199 12/14/22 16:52:49.387
    STEP: Creating a pod to test consume secrets 12/14/22 16:52:49.392
    Dec 14 16:52:49.409: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc" in namespace "projected-4536" to be "Succeeded or Failed"
    Dec 14 16:52:49.414: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466403ms
    Dec 14 16:52:51.423: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01356385s
    Dec 14 16:52:53.422: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012371364s
    Dec 14 16:52:55.421: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012156343s
    STEP: Saw pod success 12/14/22 16:52:55.422
    Dec 14 16:52:55.422: INFO: Pod "pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc" satisfied condition "Succeeded or Failed"
    Dec 14 16:52:55.427: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 16:52:55.45
    Dec 14 16:52:55.464: INFO: Waiting for pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc to disappear
    Dec 14 16:52:55.469: INFO: Pod pod-projected-secrets-94da55d8-6ea9-4777-b6ef-37a52df61cdc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:52:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4536" for this suite. 12/14/22 16:52:55.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:52:55.491
Dec 14 16:52:55.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename subpath 12/14/22 16:52:55.493
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:55.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:55.515
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/14/22 16:52:55.518
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-qrqg 12/14/22 16:52:55.529
STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:52:55.529
Dec 14 16:52:55.543: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qrqg" in namespace "subpath-8212" to be "Succeeded or Failed"
Dec 14 16:52:55.548: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45215ms
Dec 14 16:52:57.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 2.013339719s
Dec 14 16:52:59.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 4.013478032s
Dec 14 16:53:01.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 6.013681951s
Dec 14 16:53:03.558: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 8.015098613s
Dec 14 16:53:05.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 10.013687389s
Dec 14 16:53:07.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 12.013108875s
Dec 14 16:53:09.558: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 14.014790261s
Dec 14 16:53:11.555: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 16.012282895s
Dec 14 16:53:13.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 18.014249967s
Dec 14 16:53:15.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 20.012987164s
Dec 14 16:53:17.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 22.013156625s
Dec 14 16:53:19.568: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=false. Elapsed: 24.025306278s
Dec 14 16:53:21.568: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024972021s
STEP: Saw pod success 12/14/22 16:53:21.568
Dec 14 16:53:21.569: INFO: Pod "pod-subpath-test-projected-qrqg" satisfied condition "Succeeded or Failed"
Dec 14 16:53:21.574: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-projected-qrqg container test-container-subpath-projected-qrqg: <nil>
STEP: delete the pod 12/14/22 16:53:21.586
Dec 14 16:53:21.601: INFO: Waiting for pod pod-subpath-test-projected-qrqg to disappear
Dec 14 16:53:21.607: INFO: Pod pod-subpath-test-projected-qrqg no longer exists
STEP: Deleting pod pod-subpath-test-projected-qrqg 12/14/22 16:53:21.607
Dec 14 16:53:21.607: INFO: Deleting pod "pod-subpath-test-projected-qrqg" in namespace "subpath-8212"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:21.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8212" for this suite. 12/14/22 16:53:21.627
------------------------------
• [SLOW TEST] [26.162 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:52:55.491
    Dec 14 16:52:55.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename subpath 12/14/22 16:52:55.493
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:52:55.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:52:55.515
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/14/22 16:52:55.518
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-qrqg 12/14/22 16:52:55.529
    STEP: Creating a pod to test atomic-volume-subpath 12/14/22 16:52:55.529
    Dec 14 16:52:55.543: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qrqg" in namespace "subpath-8212" to be "Succeeded or Failed"
    Dec 14 16:52:55.548: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45215ms
    Dec 14 16:52:57.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 2.013339719s
    Dec 14 16:52:59.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 4.013478032s
    Dec 14 16:53:01.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 6.013681951s
    Dec 14 16:53:03.558: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 8.015098613s
    Dec 14 16:53:05.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 10.013687389s
    Dec 14 16:53:07.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 12.013108875s
    Dec 14 16:53:09.558: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 14.014790261s
    Dec 14 16:53:11.555: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 16.012282895s
    Dec 14 16:53:13.557: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 18.014249967s
    Dec 14 16:53:15.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 20.012987164s
    Dec 14 16:53:17.556: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=true. Elapsed: 22.013156625s
    Dec 14 16:53:19.568: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Running", Reason="", readiness=false. Elapsed: 24.025306278s
    Dec 14 16:53:21.568: INFO: Pod "pod-subpath-test-projected-qrqg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024972021s
    STEP: Saw pod success 12/14/22 16:53:21.568
    Dec 14 16:53:21.569: INFO: Pod "pod-subpath-test-projected-qrqg" satisfied condition "Succeeded or Failed"
    Dec 14 16:53:21.574: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-subpath-test-projected-qrqg container test-container-subpath-projected-qrqg: <nil>
    STEP: delete the pod 12/14/22 16:53:21.586
    Dec 14 16:53:21.601: INFO: Waiting for pod pod-subpath-test-projected-qrqg to disappear
    Dec 14 16:53:21.607: INFO: Pod pod-subpath-test-projected-qrqg no longer exists
    STEP: Deleting pod pod-subpath-test-projected-qrqg 12/14/22 16:53:21.607
    Dec 14 16:53:21.607: INFO: Deleting pod "pod-subpath-test-projected-qrqg" in namespace "subpath-8212"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:21.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8212" for this suite. 12/14/22 16:53:21.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:21.655
Dec 14 16:53:21.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:53:21.658
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:21.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:21.695
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:53:21.698
Dec 14 16:53:21.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954" in namespace "projected-1688" to be "Succeeded or Failed"
Dec 14 16:53:21.720: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Pending", Reason="", readiness=false. Elapsed: 9.381202ms
Dec 14 16:53:23.729: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017954368s
Dec 14 16:53:25.731: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020527888s
STEP: Saw pod success 12/14/22 16:53:25.731
Dec 14 16:53:25.732: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954" satisfied condition "Succeeded or Failed"
Dec 14 16:53:25.736: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 container client-container: <nil>
STEP: delete the pod 12/14/22 16:53:25.745
Dec 14 16:53:25.761: INFO: Waiting for pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 to disappear
Dec 14 16:53:25.766: INFO: Pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:25.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1688" for this suite. 12/14/22 16:53:25.775
------------------------------
• [4.133 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:21.655
    Dec 14 16:53:21.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:53:21.658
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:21.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:21.695
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:53:21.698
    Dec 14 16:53:21.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954" in namespace "projected-1688" to be "Succeeded or Failed"
    Dec 14 16:53:21.720: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Pending", Reason="", readiness=false. Elapsed: 9.381202ms
    Dec 14 16:53:23.729: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017954368s
    Dec 14 16:53:25.731: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020527888s
    STEP: Saw pod success 12/14/22 16:53:25.731
    Dec 14 16:53:25.732: INFO: Pod "downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954" satisfied condition "Succeeded or Failed"
    Dec 14 16:53:25.736: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:53:25.745
    Dec 14 16:53:25.761: INFO: Waiting for pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 to disappear
    Dec 14 16:53:25.766: INFO: Pod downwardapi-volume-25b9c055-847a-4ade-b33c-0f8356201954 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:25.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1688" for this suite. 12/14/22 16:53:25.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:25.798
Dec 14 16:53:25.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename conformance-tests 12/14/22 16:53:25.8
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:25.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:25.832
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 12/14/22 16:53:25.844
Dec 14 16:53:25.844: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:25.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-1819" for this suite. 12/14/22 16:53:25.878
------------------------------
• [0.094 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:25.798
    Dec 14 16:53:25.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename conformance-tests 12/14/22 16:53:25.8
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:25.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:25.832
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 12/14/22 16:53:25.844
    Dec 14 16:53:25.844: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:25.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-1819" for this suite. 12/14/22 16:53:25.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:25.9
Dec 14 16:53:25.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-pred 12/14/22 16:53:25.902
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:25.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:25.932
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 14 16:53:25.936: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 16:53:25.948: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 16:53:25.953: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
Dec 14 16:53:25.963: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:53:25.963: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:53:25.963: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:53:25.963: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:53:25.963: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 16:53:25.963: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:53:25.963: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 16:53:25.963: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:53:25.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:53:25.963: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:53:25.963: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
Dec 14 16:53:25.975: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.975: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:53:25.975: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.975: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:53:25.976: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.976: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:53:25.976: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.976: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:53:25.976: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.976: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 16:53:25.976: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.976: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:53:25.976: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.976: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 16:53:25.976: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:53:25.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:53:25.977: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:53:25.977: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
Dec 14 16:53:25.999: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:25.999: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:53:26.000: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:26.000: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:53:26.000: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
Dec 14 16:53:26.000: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 16:53:26.000: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:53:26.000: INFO: 	Container e2e ready: true, restart count 0
Dec 14 16:53:26.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:53:26.000: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 16:53:26.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:53:26.000: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node iet9eich7uhu-1 12/14/22 16:53:26.034
STEP: verifying the node has the label node iet9eich7uhu-2 12/14/22 16:53:26.051
STEP: verifying the node has the label node iet9eich7uhu-3 12/14/22 16:53:26.089
Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-flfrk requesting resource cpu=100m on Node iet9eich7uhu-2
Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-plh4q requesting resource cpu=100m on Node iet9eich7uhu-1
Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-vfpcb requesting resource cpu=100m on Node iet9eich7uhu-3
Dec 14 16:53:26.111: INFO: Pod coredns-787d4945fb-c79wd requesting resource cpu=100m on Node iet9eich7uhu-2
Dec 14 16:53:26.112: INFO: Pod coredns-787d4945fb-dthzx requesting resource cpu=100m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-addon-manager-iet9eich7uhu-1 requesting resource cpu=5m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-addon-manager-iet9eich7uhu-2 requesting resource cpu=5m on Node iet9eich7uhu-2
Dec 14 16:53:26.112: INFO: Pod kube-apiserver-iet9eich7uhu-1 requesting resource cpu=250m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-apiserver-iet9eich7uhu-2 requesting resource cpu=250m on Node iet9eich7uhu-2
Dec 14 16:53:26.112: INFO: Pod kube-controller-manager-iet9eich7uhu-1 requesting resource cpu=200m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-controller-manager-iet9eich7uhu-2 requesting resource cpu=200m on Node iet9eich7uhu-2
Dec 14 16:53:26.112: INFO: Pod kube-proxy-6lpvc requesting resource cpu=0m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-proxy-b4ld4 requesting resource cpu=0m on Node iet9eich7uhu-3
Dec 14 16:53:26.112: INFO: Pod kube-proxy-xtp9v requesting resource cpu=0m on Node iet9eich7uhu-2
Dec 14 16:53:26.112: INFO: Pod kube-scheduler-iet9eich7uhu-1 requesting resource cpu=100m on Node iet9eich7uhu-1
Dec 14 16:53:26.112: INFO: Pod kube-scheduler-iet9eich7uhu-2 requesting resource cpu=100m on Node iet9eich7uhu-2
Dec 14 16:53:26.113: INFO: Pod sonobuoy requesting resource cpu=0m on Node iet9eich7uhu-3
Dec 14 16:53:26.113: INFO: Pod sonobuoy-e2e-job-d217a1a041484246 requesting resource cpu=0m on Node iet9eich7uhu-3
Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh requesting resource cpu=0m on Node iet9eich7uhu-1
Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx requesting resource cpu=0m on Node iet9eich7uhu-3
Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k requesting resource cpu=0m on Node iet9eich7uhu-2
STEP: Starting Pods to consume most of the cluster CPU. 12/14/22 16:53:26.113
Dec 14 16:53:26.116: INFO: Creating a pod which consumes cpu=591m on Node iet9eich7uhu-1
Dec 14 16:53:26.146: INFO: Creating a pod which consumes cpu=591m on Node iet9eich7uhu-2
Dec 14 16:53:26.156: INFO: Creating a pod which consumes cpu=1050m on Node iet9eich7uhu-3
Dec 14 16:53:26.172: INFO: Waiting up to 5m0s for pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df" in namespace "sched-pred-835" to be "running"
Dec 14 16:53:26.198: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Pending", Reason="", readiness=false. Elapsed: 25.676076ms
Dec 14 16:53:28.207: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034481323s
Dec 14 16:53:30.205: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Running", Reason="", readiness=true. Elapsed: 4.032561472s
Dec 14 16:53:30.205: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df" satisfied condition "running"
Dec 14 16:53:30.205: INFO: Waiting up to 5m0s for pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf" in namespace "sched-pred-835" to be "running"
Dec 14 16:53:30.210: INFO: Pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.613252ms
Dec 14 16:53:30.210: INFO: Pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf" satisfied condition "running"
Dec 14 16:53:30.210: INFO: Waiting up to 5m0s for pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d" in namespace "sched-pred-835" to be "running"
Dec 14 16:53:30.216: INFO: Pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d": Phase="Running", Reason="", readiness=true. Elapsed: 5.610403ms
Dec 14 16:53:30.216: INFO: Pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/14/22 16:53:30.216
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f29f8dda1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf to iet9eich7uhu-2] 12/14/22 16:53:30.223
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f560f78af], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.223
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f61ea49ab], Reason = [Created], Message = [Created container filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf] 12/14/22 16:53:30.223
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f64c5fa3d], Reason = [Started], Message = [Started container filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf] 12/14/22 16:53:30.224
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f27d7cf20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df to iet9eich7uhu-1] 12/14/22 16:53:30.224
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f6e0aadc8], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-wm7c9" : failed to sync configmap cache: timed out waiting for the condition] 12/14/22 16:53:30.224
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f9adce23b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.225
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71fa2704467], Reason = [Created], Message = [Created container filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df] 12/14/22 16:53:30.225
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71fa3d08fa4], Reason = [Started], Message = [Started container filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df] 12/14/22 16:53:30.225
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f2a123d82], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d to iet9eich7uhu-3] 12/14/22 16:53:30.225
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f752a69c9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.225
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f7ccffc5f], Reason = [Created], Message = [Created container filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d] 12/14/22 16:53:30.226
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f7e068ad4], Reason = [Started], Message = [Started container filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d] 12/14/22 16:53:30.226
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1730b7201ab0fe02], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 12/14/22 16:53:30.243
STEP: removing the label node off the node iet9eich7uhu-3 12/14/22 16:53:31.244
STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.262
STEP: removing the label node off the node iet9eich7uhu-1 12/14/22 16:53:31.276
STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.318
STEP: removing the label node off the node iet9eich7uhu-2 12/14/22 16:53:31.324
STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.347
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:31.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-835" for this suite. 12/14/22 16:53:31.36
------------------------------
• [SLOW TEST] [5.475 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:25.9
    Dec 14 16:53:25.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-pred 12/14/22 16:53:25.902
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:25.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:25.932
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 14 16:53:25.936: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 16:53:25.948: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 16:53:25.953: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
    Dec 14 16:53:25.963: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:53:25.963: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 16:53:25.963: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 16:53:25.963: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 16:53:25.963: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Dec 14 16:53:25.963: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:53:25.963: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container kube-scheduler ready: true, restart count 2
    Dec 14 16:53:25.963: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:53:25.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:53:25.963: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 16:53:25.963: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
    Dec 14 16:53:25.975: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.975: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:53:25.975: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.975: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 16:53:25.976: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.976: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 16:53:25.976: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.976: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 16:53:25.976: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.976: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 14 16:53:25.976: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.976: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:53:25.976: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.976: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 14 16:53:25.976: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:53:25.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:53:25.977: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 16:53:25.977: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
    Dec 14 16:53:25.999: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:25.999: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:26.000: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
    Dec 14 16:53:26.000: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:53:26.000: INFO: 	Container e2e ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 16:53:26.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 16:53:26.000: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node iet9eich7uhu-1 12/14/22 16:53:26.034
    STEP: verifying the node has the label node iet9eich7uhu-2 12/14/22 16:53:26.051
    STEP: verifying the node has the label node iet9eich7uhu-3 12/14/22 16:53:26.089
    Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-flfrk requesting resource cpu=100m on Node iet9eich7uhu-2
    Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-plh4q requesting resource cpu=100m on Node iet9eich7uhu-1
    Dec 14 16:53:26.111: INFO: Pod kube-flannel-ds-vfpcb requesting resource cpu=100m on Node iet9eich7uhu-3
    Dec 14 16:53:26.111: INFO: Pod coredns-787d4945fb-c79wd requesting resource cpu=100m on Node iet9eich7uhu-2
    Dec 14 16:53:26.112: INFO: Pod coredns-787d4945fb-dthzx requesting resource cpu=100m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-addon-manager-iet9eich7uhu-1 requesting resource cpu=5m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-addon-manager-iet9eich7uhu-2 requesting resource cpu=5m on Node iet9eich7uhu-2
    Dec 14 16:53:26.112: INFO: Pod kube-apiserver-iet9eich7uhu-1 requesting resource cpu=250m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-apiserver-iet9eich7uhu-2 requesting resource cpu=250m on Node iet9eich7uhu-2
    Dec 14 16:53:26.112: INFO: Pod kube-controller-manager-iet9eich7uhu-1 requesting resource cpu=200m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-controller-manager-iet9eich7uhu-2 requesting resource cpu=200m on Node iet9eich7uhu-2
    Dec 14 16:53:26.112: INFO: Pod kube-proxy-6lpvc requesting resource cpu=0m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-proxy-b4ld4 requesting resource cpu=0m on Node iet9eich7uhu-3
    Dec 14 16:53:26.112: INFO: Pod kube-proxy-xtp9v requesting resource cpu=0m on Node iet9eich7uhu-2
    Dec 14 16:53:26.112: INFO: Pod kube-scheduler-iet9eich7uhu-1 requesting resource cpu=100m on Node iet9eich7uhu-1
    Dec 14 16:53:26.112: INFO: Pod kube-scheduler-iet9eich7uhu-2 requesting resource cpu=100m on Node iet9eich7uhu-2
    Dec 14 16:53:26.113: INFO: Pod sonobuoy requesting resource cpu=0m on Node iet9eich7uhu-3
    Dec 14 16:53:26.113: INFO: Pod sonobuoy-e2e-job-d217a1a041484246 requesting resource cpu=0m on Node iet9eich7uhu-3
    Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh requesting resource cpu=0m on Node iet9eich7uhu-1
    Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx requesting resource cpu=0m on Node iet9eich7uhu-3
    Dec 14 16:53:26.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k requesting resource cpu=0m on Node iet9eich7uhu-2
    STEP: Starting Pods to consume most of the cluster CPU. 12/14/22 16:53:26.113
    Dec 14 16:53:26.116: INFO: Creating a pod which consumes cpu=591m on Node iet9eich7uhu-1
    Dec 14 16:53:26.146: INFO: Creating a pod which consumes cpu=591m on Node iet9eich7uhu-2
    Dec 14 16:53:26.156: INFO: Creating a pod which consumes cpu=1050m on Node iet9eich7uhu-3
    Dec 14 16:53:26.172: INFO: Waiting up to 5m0s for pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df" in namespace "sched-pred-835" to be "running"
    Dec 14 16:53:26.198: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Pending", Reason="", readiness=false. Elapsed: 25.676076ms
    Dec 14 16:53:28.207: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034481323s
    Dec 14 16:53:30.205: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df": Phase="Running", Reason="", readiness=true. Elapsed: 4.032561472s
    Dec 14 16:53:30.205: INFO: Pod "filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df" satisfied condition "running"
    Dec 14 16:53:30.205: INFO: Waiting up to 5m0s for pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf" in namespace "sched-pred-835" to be "running"
    Dec 14 16:53:30.210: INFO: Pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.613252ms
    Dec 14 16:53:30.210: INFO: Pod "filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf" satisfied condition "running"
    Dec 14 16:53:30.210: INFO: Waiting up to 5m0s for pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d" in namespace "sched-pred-835" to be "running"
    Dec 14 16:53:30.216: INFO: Pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d": Phase="Running", Reason="", readiness=true. Elapsed: 5.610403ms
    Dec 14 16:53:30.216: INFO: Pod "filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/14/22 16:53:30.216
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f29f8dda1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf to iet9eich7uhu-2] 12/14/22 16:53:30.223
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f560f78af], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.223
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f61ea49ab], Reason = [Created], Message = [Created container filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf] 12/14/22 16:53:30.223
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf.1730b71f64c5fa3d], Reason = [Started], Message = [Started container filler-pod-5e5234df-1368-40f5-a8b5-05bf80e103bf] 12/14/22 16:53:30.224
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f27d7cf20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df to iet9eich7uhu-1] 12/14/22 16:53:30.224
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f6e0aadc8], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-wm7c9" : failed to sync configmap cache: timed out waiting for the condition] 12/14/22 16:53:30.224
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71f9adce23b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.225
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71fa2704467], Reason = [Created], Message = [Created container filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df] 12/14/22 16:53:30.225
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df.1730b71fa3d08fa4], Reason = [Started], Message = [Started container filler-pod-7265e4fe-5b8a-4528-9b41-6b4bf8cc16df] 12/14/22 16:53:30.225
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f2a123d82], Reason = [Scheduled], Message = [Successfully assigned sched-pred-835/filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d to iet9eich7uhu-3] 12/14/22 16:53:30.225
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f752a69c9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/14/22 16:53:30.225
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f7ccffc5f], Reason = [Created], Message = [Created container filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d] 12/14/22 16:53:30.226
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d.1730b71f7e068ad4], Reason = [Started], Message = [Started container filler-pod-b7d38492-bd47-4b9e-8182-779a8dbc3f1d] 12/14/22 16:53:30.226
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1730b7201ab0fe02], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 12/14/22 16:53:30.243
    STEP: removing the label node off the node iet9eich7uhu-3 12/14/22 16:53:31.244
    STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.262
    STEP: removing the label node off the node iet9eich7uhu-1 12/14/22 16:53:31.276
    STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.318
    STEP: removing the label node off the node iet9eich7uhu-2 12/14/22 16:53:31.324
    STEP: verifying the node doesn't have the label node 12/14/22 16:53:31.347
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:31.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-835" for this suite. 12/14/22 16:53:31.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:31.377
Dec 14 16:53:31.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 16:53:31.385
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:31.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:31.413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 12/14/22 16:53:31.419
Dec 14 16:53:31.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9787 create -f -'
Dec 14 16:53:33.059: INFO: stderr: ""
Dec 14 16:53:33.059: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/14/22 16:53:33.059
Dec 14 16:53:34.066: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:53:34.066: INFO: Found 0 / 1
Dec 14 16:53:35.070: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:53:35.070: INFO: Found 1 / 1
Dec 14 16:53:35.070: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 12/14/22 16:53:35.07
Dec 14 16:53:35.078: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:53:35.078: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:53:35.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9787 patch pod agnhost-primary-l859j -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 16:53:35.218: INFO: stderr: ""
Dec 14 16:53:35.218: INFO: stdout: "pod/agnhost-primary-l859j patched\n"
STEP: checking annotations 12/14/22 16:53:35.218
Dec 14 16:53:35.223: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:53:35.223: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9787" for this suite. 12/14/22 16:53:35.23
------------------------------
• [3.862 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:31.377
    Dec 14 16:53:31.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 16:53:31.385
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:31.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:31.413
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 12/14/22 16:53:31.419
    Dec 14 16:53:31.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9787 create -f -'
    Dec 14 16:53:33.059: INFO: stderr: ""
    Dec 14 16:53:33.059: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/14/22 16:53:33.059
    Dec 14 16:53:34.066: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:53:34.066: INFO: Found 0 / 1
    Dec 14 16:53:35.070: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:53:35.070: INFO: Found 1 / 1
    Dec 14 16:53:35.070: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 12/14/22 16:53:35.07
    Dec 14 16:53:35.078: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:53:35.078: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 14 16:53:35.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-9787 patch pod agnhost-primary-l859j -p {"metadata":{"annotations":{"x":"y"}}}'
    Dec 14 16:53:35.218: INFO: stderr: ""
    Dec 14 16:53:35.218: INFO: stdout: "pod/agnhost-primary-l859j patched\n"
    STEP: checking annotations 12/14/22 16:53:35.218
    Dec 14 16:53:35.223: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 14 16:53:35.223: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9787" for this suite. 12/14/22 16:53:35.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:35.24
Dec 14 16:53:35.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 16:53:35.244
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:35.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:35.271
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 12/14/22 16:53:35.275
STEP: Creating a ResourceQuota 12/14/22 16:53:40.282
STEP: Ensuring resource quota status is calculated 12/14/22 16:53:40.29
STEP: Creating a ReplicaSet 12/14/22 16:53:42.3
STEP: Ensuring resource quota status captures replicaset creation 12/14/22 16:53:42.315
STEP: Deleting a ReplicaSet 12/14/22 16:53:44.323
STEP: Ensuring resource quota status released usage 12/14/22 16:53:44.335
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:46.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9296" for this suite. 12/14/22 16:53:46.351
------------------------------
• [SLOW TEST] [11.121 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:35.24
    Dec 14 16:53:35.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 16:53:35.244
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:35.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:35.271
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 12/14/22 16:53:35.275
    STEP: Creating a ResourceQuota 12/14/22 16:53:40.282
    STEP: Ensuring resource quota status is calculated 12/14/22 16:53:40.29
    STEP: Creating a ReplicaSet 12/14/22 16:53:42.3
    STEP: Ensuring resource quota status captures replicaset creation 12/14/22 16:53:42.315
    STEP: Deleting a ReplicaSet 12/14/22 16:53:44.323
    STEP: Ensuring resource quota status released usage 12/14/22 16:53:44.335
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:46.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9296" for this suite. 12/14/22 16:53:46.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:46.368
Dec 14 16:53:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 16:53:46.376
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:46.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:46.409
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 16:53:46.465
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:53:46.475
Dec 14 16:53:46.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:53:46.490: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:53:47.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:53:47.506: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 16:53:48.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:53:48.504: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 16:53:49.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:53:49.511: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 12/14/22 16:53:49.517
Dec 14 16:53:49.524: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/14/22 16:53:49.524
Dec 14 16:53:49.538: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/14/22 16:53:49.538
Dec 14 16:53:49.542: INFO: Observed &DaemonSet event: ADDED
Dec 14 16:53:49.542: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.543: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.543: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.544: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.544: INFO: Found daemon set daemon-set in namespace daemonsets-773 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 16:53:49.544: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/14/22 16:53:49.544
STEP: watching for the daemon set status to be patched 12/14/22 16:53:49.553
Dec 14 16:53:49.557: INFO: Observed &DaemonSet event: ADDED
Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.559: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.559: INFO: Observed daemon set daemon-set in namespace daemonsets-773 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 16:53:49.559: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 16:53:49.560: INFO: Found daemon set daemon-set in namespace daemonsets-773 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec 14 16:53:49.560: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:53:49.565
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-773, will wait for the garbage collector to delete the pods 12/14/22 16:53:49.565
Dec 14 16:53:49.631: INFO: Deleting DaemonSet.extensions daemon-set took: 12.90052ms
Dec 14 16:53:49.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.915452ms
Dec 14 16:53:52.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:53:52.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:53:52.247: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27461"},"items":null}

Dec 14 16:53:52.251: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27461"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:52.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-773" for this suite. 12/14/22 16:53:52.276
------------------------------
• [SLOW TEST] [5.918 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:46.368
    Dec 14 16:53:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 16:53:46.376
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:46.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:46.409
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 16:53:46.465
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 16:53:46.475
    Dec 14 16:53:46.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:53:46.490: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:53:47.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:53:47.506: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 16:53:48.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 16:53:48.504: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 16:53:49.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 16:53:49.511: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 12/14/22 16:53:49.517
    Dec 14 16:53:49.524: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/14/22 16:53:49.524
    Dec 14 16:53:49.538: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/14/22 16:53:49.538
    Dec 14 16:53:49.542: INFO: Observed &DaemonSet event: ADDED
    Dec 14 16:53:49.542: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.543: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.543: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.544: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.544: INFO: Found daemon set daemon-set in namespace daemonsets-773 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 16:53:49.544: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/14/22 16:53:49.544
    STEP: watching for the daemon set status to be patched 12/14/22 16:53:49.553
    Dec 14 16:53:49.557: INFO: Observed &DaemonSet event: ADDED
    Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.558: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.559: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.559: INFO: Observed daemon set daemon-set in namespace daemonsets-773 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 14 16:53:49.559: INFO: Observed &DaemonSet event: MODIFIED
    Dec 14 16:53:49.560: INFO: Found daemon set daemon-set in namespace daemonsets-773 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec 14 16:53:49.560: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 16:53:49.565
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-773, will wait for the garbage collector to delete the pods 12/14/22 16:53:49.565
    Dec 14 16:53:49.631: INFO: Deleting DaemonSet.extensions daemon-set took: 12.90052ms
    Dec 14 16:53:49.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.915452ms
    Dec 14 16:53:52.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 16:53:52.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 16:53:52.247: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27461"},"items":null}

    Dec 14 16:53:52.251: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27461"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:52.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-773" for this suite. 12/14/22 16:53:52.276
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:52.287
Dec 14 16:53:52.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 16:53:52.292
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:52.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:52.316
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 12/14/22 16:53:52.321
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:52.345
STEP: Creating a service in the namespace 12/14/22 16:53:52.349
STEP: Deleting the namespace 12/14/22 16:53:52.373
STEP: Waiting for the namespace to be removed. 12/14/22 16:53:52.419
STEP: Recreating the namespace 12/14/22 16:53:58.445
STEP: Verifying there is no service in the namespace 12/14/22 16:53:58.472
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:58.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6883" for this suite. 12/14/22 16:53:58.486
STEP: Destroying namespace "nsdeletetest-6766" for this suite. 12/14/22 16:53:58.5
Dec 14 16:53:58.505: INFO: Namespace nsdeletetest-6766 was already deleted
STEP: Destroying namespace "nsdeletetest-1750" for this suite. 12/14/22 16:53:58.505
------------------------------
• [SLOW TEST] [6.227 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:52.287
    Dec 14 16:53:52.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 16:53:52.292
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:52.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:52.316
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 12/14/22 16:53:52.321
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:52.345
    STEP: Creating a service in the namespace 12/14/22 16:53:52.349
    STEP: Deleting the namespace 12/14/22 16:53:52.373
    STEP: Waiting for the namespace to be removed. 12/14/22 16:53:52.419
    STEP: Recreating the namespace 12/14/22 16:53:58.445
    STEP: Verifying there is no service in the namespace 12/14/22 16:53:58.472
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:58.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6883" for this suite. 12/14/22 16:53:58.486
    STEP: Destroying namespace "nsdeletetest-6766" for this suite. 12/14/22 16:53:58.5
    Dec 14 16:53:58.505: INFO: Namespace nsdeletetest-6766 was already deleted
    STEP: Destroying namespace "nsdeletetest-1750" for this suite. 12/14/22 16:53:58.505
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:58.516
Dec 14 16:53:58.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename namespaces 12/14/22 16:53:58.521
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:58.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:58.571
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 12/14/22 16:53:58.574
STEP: patching the Namespace 12/14/22 16:53:58.613
STEP: get the Namespace and ensuring it has the label 12/14/22 16:53:58.632
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:53:58.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7674" for this suite. 12/14/22 16:53:58.643
STEP: Destroying namespace "nspatchtest-3ccdebdc-bcea-46b2-ad6c-ccda851de405-4651" for this suite. 12/14/22 16:53:58.658
------------------------------
• [0.152 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:58.516
    Dec 14 16:53:58.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename namespaces 12/14/22 16:53:58.521
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:58.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:58.571
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 12/14/22 16:53:58.574
    STEP: patching the Namespace 12/14/22 16:53:58.613
    STEP: get the Namespace and ensuring it has the label 12/14/22 16:53:58.632
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:53:58.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7674" for this suite. 12/14/22 16:53:58.643
    STEP: Destroying namespace "nspatchtest-3ccdebdc-bcea-46b2-ad6c-ccda851de405-4651" for this suite. 12/14/22 16:53:58.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:53:58.678
Dec 14 16:53:58.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 16:53:58.684
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:58.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:58.714
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 12/14/22 16:53:58.717
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 12/14/22 16:53:58.725
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 12/14/22 16:53:58.725
STEP: creating a pod to probe DNS 12/14/22 16:53:58.725
STEP: submitting the pod to kubernetes 12/14/22 16:53:58.725
Dec 14 16:53:58.746: INFO: Waiting up to 15m0s for pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098" in namespace "dns-5929" to be "running"
Dec 14 16:53:58.756: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Pending", Reason="", readiness=false. Elapsed: 9.066981ms
Dec 14 16:54:00.763: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016414204s
Dec 14 16:54:02.764: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Running", Reason="", readiness=true. Elapsed: 4.017831831s
Dec 14 16:54:02.764: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098" satisfied condition "running"
STEP: retrieving the pod 12/14/22 16:54:02.764
STEP: looking for the results for each expected name from probers 12/14/22 16:54:02.771
Dec 14 16:54:02.801: INFO: DNS probes using dns-5929/dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098 succeeded

STEP: deleting the pod 12/14/22 16:54:02.801
STEP: deleting the test headless service 12/14/22 16:54:02.84
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5929" for this suite. 12/14/22 16:54:02.898
------------------------------
• [4.233 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:53:58.678
    Dec 14 16:53:58.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 16:53:58.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:53:58.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:53:58.714
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 12/14/22 16:53:58.717
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     12/14/22 16:53:58.725
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     12/14/22 16:53:58.725
    STEP: creating a pod to probe DNS 12/14/22 16:53:58.725
    STEP: submitting the pod to kubernetes 12/14/22 16:53:58.725
    Dec 14 16:53:58.746: INFO: Waiting up to 15m0s for pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098" in namespace "dns-5929" to be "running"
    Dec 14 16:53:58.756: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Pending", Reason="", readiness=false. Elapsed: 9.066981ms
    Dec 14 16:54:00.763: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016414204s
    Dec 14 16:54:02.764: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098": Phase="Running", Reason="", readiness=true. Elapsed: 4.017831831s
    Dec 14 16:54:02.764: INFO: Pod "dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 16:54:02.764
    STEP: looking for the results for each expected name from probers 12/14/22 16:54:02.771
    Dec 14 16:54:02.801: INFO: DNS probes using dns-5929/dns-test-f8b4703b-8a06-48f7-afa2-607fbd217098 succeeded

    STEP: deleting the pod 12/14/22 16:54:02.801
    STEP: deleting the test headless service 12/14/22 16:54:02.84
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5929" for this suite. 12/14/22 16:54:02.898
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:02.912
Dec 14 16:54:02.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:54:02.915
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:02.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:02.95
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Dec 14 16:54:02.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:54:05.759
Dec 14 16:54:05.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 create -f -'
Dec 14 16:54:07.011: INFO: stderr: ""
Dec 14 16:54:07.011: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 16:54:07.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
Dec 14 16:54:07.307: INFO: stderr: ""
Dec 14 16:54:07.307: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 16:54:07.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 apply -f -'
Dec 14 16:54:07.663: INFO: stderr: ""
Dec 14 16:54:07.663: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 16:54:07.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
Dec 14 16:54:07.794: INFO: stderr: ""
Dec 14 16:54:07.794: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/14/22 16:54:07.794
Dec 14 16:54:07.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 explain e2e-test-crd-publish-openapi-5296-crds'
Dec 14 16:54:08.555: INFO: stderr: ""
Dec 14 16:54:08.555: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5296-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:10.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8345" for this suite. 12/14/22 16:54:10.659
------------------------------
• [SLOW TEST] [7.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:02.912
    Dec 14 16:54:02.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename crd-publish-openapi 12/14/22 16:54:02.915
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:02.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:02.95
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Dec 14 16:54:02.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/14/22 16:54:05.759
    Dec 14 16:54:05.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 create -f -'
    Dec 14 16:54:07.011: INFO: stderr: ""
    Dec 14 16:54:07.011: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 14 16:54:07.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
    Dec 14 16:54:07.307: INFO: stderr: ""
    Dec 14 16:54:07.307: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Dec 14 16:54:07.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 apply -f -'
    Dec 14 16:54:07.663: INFO: stderr: ""
    Dec 14 16:54:07.663: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 14 16:54:07.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 --namespace=crd-publish-openapi-8345 delete e2e-test-crd-publish-openapi-5296-crds test-cr'
    Dec 14 16:54:07.794: INFO: stderr: ""
    Dec 14 16:54:07.794: INFO: stdout: "e2e-test-crd-publish-openapi-5296-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/14/22 16:54:07.794
    Dec 14 16:54:07.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=crd-publish-openapi-8345 explain e2e-test-crd-publish-openapi-5296-crds'
    Dec 14 16:54:08.555: INFO: stderr: ""
    Dec 14 16:54:08.555: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5296-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:10.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8345" for this suite. 12/14/22 16:54:10.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:10.675
Dec 14 16:54:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:54:10.678
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:10.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:10.707
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Dec 14 16:54:10.726: INFO: Waiting up to 5m0s for pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663" in namespace "kubelet-test-7502" to be "running and ready"
Dec 14 16:54:10.732: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663": Phase="Pending", Reason="", readiness=false. Elapsed: 5.94183ms
Dec 14 16:54:10.732: INFO: The phase of Pod busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:54:12.739: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663": Phase="Running", Reason="", readiness=true. Elapsed: 2.013022081s
Dec 14 16:54:12.739: INFO: The phase of Pod busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663 is Running (Ready = true)
Dec 14 16:54:12.739: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7502" for this suite. 12/14/22 16:54:12.764
------------------------------
• [2.114 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:10.675
    Dec 14 16:54:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:54:10.678
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:10.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:10.707
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Dec 14 16:54:10.726: INFO: Waiting up to 5m0s for pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663" in namespace "kubelet-test-7502" to be "running and ready"
    Dec 14 16:54:10.732: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663": Phase="Pending", Reason="", readiness=false. Elapsed: 5.94183ms
    Dec 14 16:54:10.732: INFO: The phase of Pod busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:54:12.739: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663": Phase="Running", Reason="", readiness=true. Elapsed: 2.013022081s
    Dec 14 16:54:12.739: INFO: The phase of Pod busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663 is Running (Ready = true)
    Dec 14 16:54:12.739: INFO: Pod "busybox-scheduling-518dde28-7d5f-4860-9e04-1e8002046663" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7502" for this suite. 12/14/22 16:54:12.764
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:12.789
Dec 14 16:54:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename ephemeral-containers-test 12/14/22 16:54:12.794
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:12.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:12.823
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 12/14/22 16:54:12.829
Dec 14 16:54:12.847: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4990" to be "running and ready"
Dec 14 16:54:12.852: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296483ms
Dec 14 16:54:12.852: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:54:14.860: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012873715s
Dec 14 16:54:14.860: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Dec 14 16:54:14.860: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 12/14/22 16:54:14.865
Dec 14 16:54:14.886: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4990" to be "container debugger running"
Dec 14 16:54:14.891: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.811839ms
Dec 14 16:54:16.897: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010920625s
Dec 14 16:54:18.899: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012920603s
Dec 14 16:54:18.899: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 12/14/22 16:54:18.899
Dec 14 16:54:18.899: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4990 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:54:18.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 16:54:18.900: INFO: ExecWithOptions: Clientset creation
Dec 14 16:54:18.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4990/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Dec 14 16:54:19.105: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:19.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4990" for this suite. 12/14/22 16:54:19.119
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:12.789
    Dec 14 16:54:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename ephemeral-containers-test 12/14/22 16:54:12.794
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:12.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:12.823
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 12/14/22 16:54:12.829
    Dec 14 16:54:12.847: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4990" to be "running and ready"
    Dec 14 16:54:12.852: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296483ms
    Dec 14 16:54:12.852: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:54:14.860: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012873715s
    Dec 14 16:54:14.860: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Dec 14 16:54:14.860: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 12/14/22 16:54:14.865
    Dec 14 16:54:14.886: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4990" to be "container debugger running"
    Dec 14 16:54:14.891: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.811839ms
    Dec 14 16:54:16.897: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010920625s
    Dec 14 16:54:18.899: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012920603s
    Dec 14 16:54:18.899: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 12/14/22 16:54:18.899
    Dec 14 16:54:18.899: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4990 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 16:54:18.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 16:54:18.900: INFO: ExecWithOptions: Clientset creation
    Dec 14 16:54:18.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4990/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Dec 14 16:54:19.105: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:19.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4990" for this suite. 12/14/22 16:54:19.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:19.131
Dec 14 16:54:19.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:54:19.133
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:19.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:19.162
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5829 12/14/22 16:54:19.165
STEP: creating service affinity-clusterip in namespace services-5829 12/14/22 16:54:19.165
STEP: creating replication controller affinity-clusterip in namespace services-5829 12/14/22 16:54:19.19
I1214 16:54:19.204646      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5829, replica count: 3
I1214 16:54:22.256889      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:54:22.271: INFO: Creating new exec pod
Dec 14 16:54:22.280: INFO: Waiting up to 5m0s for pod "execpod-affinityqn7q7" in namespace "services-5829" to be "running"
Dec 14 16:54:22.291: INFO: Pod "execpod-affinityqn7q7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.699518ms
Dec 14 16:54:24.297: INFO: Pod "execpod-affinityqn7q7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01670541s
Dec 14 16:54:24.297: INFO: Pod "execpod-affinityqn7q7" satisfied condition "running"
Dec 14 16:54:25.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Dec 14 16:54:25.569: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 14 16:54:25.569: INFO: stdout: ""
Dec 14 16:54:25.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c nc -v -z -w 2 10.233.12.167 80'
Dec 14 16:54:25.782: INFO: stderr: "+ nc -v -z -w 2 10.233.12.167 80\nConnection to 10.233.12.167 80 port [tcp/http] succeeded!\n"
Dec 14 16:54:25.782: INFO: stdout: ""
Dec 14 16:54:25.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.12.167:80/ ; done'
Dec 14 16:54:26.516: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n"
Dec 14 16:54:26.516: INFO: stdout: "\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8"
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
Dec 14 16:54:26.517: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5829, will wait for the garbage collector to delete the pods 12/14/22 16:54:26.534
Dec 14 16:54:26.613: INFO: Deleting ReplicationController affinity-clusterip took: 17.888328ms
Dec 14 16:54:26.714: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.112069ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5829" for this suite. 12/14/22 16:54:29.285
------------------------------
• [SLOW TEST] [10.191 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:19.131
    Dec 14 16:54:19.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:54:19.133
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:19.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:19.162
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5829 12/14/22 16:54:19.165
    STEP: creating service affinity-clusterip in namespace services-5829 12/14/22 16:54:19.165
    STEP: creating replication controller affinity-clusterip in namespace services-5829 12/14/22 16:54:19.19
    I1214 16:54:19.204646      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5829, replica count: 3
    I1214 16:54:22.256889      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:54:22.271: INFO: Creating new exec pod
    Dec 14 16:54:22.280: INFO: Waiting up to 5m0s for pod "execpod-affinityqn7q7" in namespace "services-5829" to be "running"
    Dec 14 16:54:22.291: INFO: Pod "execpod-affinityqn7q7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.699518ms
    Dec 14 16:54:24.297: INFO: Pod "execpod-affinityqn7q7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01670541s
    Dec 14 16:54:24.297: INFO: Pod "execpod-affinityqn7q7" satisfied condition "running"
    Dec 14 16:54:25.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Dec 14 16:54:25.569: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Dec 14 16:54:25.569: INFO: stdout: ""
    Dec 14 16:54:25.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c nc -v -z -w 2 10.233.12.167 80'
    Dec 14 16:54:25.782: INFO: stderr: "+ nc -v -z -w 2 10.233.12.167 80\nConnection to 10.233.12.167 80 port [tcp/http] succeeded!\n"
    Dec 14 16:54:25.782: INFO: stdout: ""
    Dec 14 16:54:25.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-5829 exec execpod-affinityqn7q7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.12.167:80/ ; done'
    Dec 14 16:54:26.516: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.167:80/\n"
    Dec 14 16:54:26.516: INFO: stdout: "\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8\naffinity-clusterip-tpvc8"
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.516: INFO: Received response from host: affinity-clusterip-tpvc8
    Dec 14 16:54:26.517: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5829, will wait for the garbage collector to delete the pods 12/14/22 16:54:26.534
    Dec 14 16:54:26.613: INFO: Deleting ReplicationController affinity-clusterip took: 17.888328ms
    Dec 14 16:54:26.714: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.112069ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5829" for this suite. 12/14/22 16:54:29.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:29.332
Dec 14 16:54:29.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:54:29.335
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:29.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:29.367
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2832 12/14/22 16:54:29.372
STEP: changing the ExternalName service to type=ClusterIP 12/14/22 16:54:29.38
STEP: creating replication controller externalname-service in namespace services-2832 12/14/22 16:54:29.405
I1214 16:54:29.412732      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2832, replica count: 2
I1214 16:54:32.464022      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:54:32.464: INFO: Creating new exec pod
Dec 14 16:54:32.474: INFO: Waiting up to 5m0s for pod "execpodm4zsh" in namespace "services-2832" to be "running"
Dec 14 16:54:32.481: INFO: Pod "execpodm4zsh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176584ms
Dec 14 16:54:34.492: INFO: Pod "execpodm4zsh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01781821s
Dec 14 16:54:34.492: INFO: Pod "execpodm4zsh" satisfied condition "running"
Dec 14 16:54:35.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-2832 exec execpodm4zsh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec 14 16:54:35.745: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 16:54:35.745: INFO: stdout: ""
Dec 14 16:54:35.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-2832 exec execpodm4zsh -- /bin/sh -x -c nc -v -z -w 2 10.233.48.244 80'
Dec 14 16:54:36.267: INFO: stderr: "+ nc -v -z -w 2 10.233.48.244 80\nConnection to 10.233.48.244 80 port [tcp/http] succeeded!\n"
Dec 14 16:54:36.267: INFO: stdout: ""
Dec 14 16:54:36.267: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:36.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2832" for this suite. 12/14/22 16:54:36.325
------------------------------
• [SLOW TEST] [7.009 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:29.332
    Dec 14 16:54:29.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:54:29.335
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:29.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:29.367
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2832 12/14/22 16:54:29.372
    STEP: changing the ExternalName service to type=ClusterIP 12/14/22 16:54:29.38
    STEP: creating replication controller externalname-service in namespace services-2832 12/14/22 16:54:29.405
    I1214 16:54:29.412732      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2832, replica count: 2
    I1214 16:54:32.464022      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 16:54:32.464: INFO: Creating new exec pod
    Dec 14 16:54:32.474: INFO: Waiting up to 5m0s for pod "execpodm4zsh" in namespace "services-2832" to be "running"
    Dec 14 16:54:32.481: INFO: Pod "execpodm4zsh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176584ms
    Dec 14 16:54:34.492: INFO: Pod "execpodm4zsh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01781821s
    Dec 14 16:54:34.492: INFO: Pod "execpodm4zsh" satisfied condition "running"
    Dec 14 16:54:35.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-2832 exec execpodm4zsh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec 14 16:54:35.745: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 14 16:54:35.745: INFO: stdout: ""
    Dec 14 16:54:35.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-2832 exec execpodm4zsh -- /bin/sh -x -c nc -v -z -w 2 10.233.48.244 80'
    Dec 14 16:54:36.267: INFO: stderr: "+ nc -v -z -w 2 10.233.48.244 80\nConnection to 10.233.48.244 80 port [tcp/http] succeeded!\n"
    Dec 14 16:54:36.267: INFO: stdout: ""
    Dec 14 16:54:36.267: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:36.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2832" for this suite. 12/14/22 16:54:36.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:36.341
Dec 14 16:54:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename configmap 12/14/22 16:54:36.344
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:36.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:36.373
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-66912b35-c7cb-453d-b875-3c1ed78a2755 12/14/22 16:54:36.399
STEP: Creating the pod 12/14/22 16:54:36.408
Dec 14 16:54:36.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b" in namespace "configmap-1900" to be "running and ready"
Dec 14 16:54:36.446: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.934042ms
Dec 14 16:54:36.446: INFO: The phase of Pod pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:54:38.454: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025506808s
Dec 14 16:54:38.454: INFO: The phase of Pod pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b is Running (Ready = true)
Dec 14 16:54:38.454: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-66912b35-c7cb-453d-b875-3c1ed78a2755 12/14/22 16:54:38.467
STEP: waiting to observe update in volume 12/14/22 16:54:38.476
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1900" for this suite. 12/14/22 16:54:40.504
------------------------------
• [4.179 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:36.341
    Dec 14 16:54:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename configmap 12/14/22 16:54:36.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:36.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:36.373
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-66912b35-c7cb-453d-b875-3c1ed78a2755 12/14/22 16:54:36.399
    STEP: Creating the pod 12/14/22 16:54:36.408
    Dec 14 16:54:36.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b" in namespace "configmap-1900" to be "running and ready"
    Dec 14 16:54:36.446: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.934042ms
    Dec 14 16:54:36.446: INFO: The phase of Pod pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 16:54:38.454: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025506808s
    Dec 14 16:54:38.454: INFO: The phase of Pod pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b is Running (Ready = true)
    Dec 14 16:54:38.454: INFO: Pod "pod-configmaps-6de1c3a7-1c7d-4710-8fa3-602b856e133b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-66912b35-c7cb-453d-b875-3c1ed78a2755 12/14/22 16:54:38.467
    STEP: waiting to observe update in volume 12/14/22 16:54:38.476
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1900" for this suite. 12/14/22 16:54:40.504
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:40.522
Dec 14 16:54:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:54:40.526
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:40.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:40.554
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Dec 14 16:54:40.572: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6033 to be scheduled
Dec 14 16:54:40.577: INFO: 1 pods are not scheduled: [runtimeclass-6033/test-runtimeclass-runtimeclass-6033-preconfigured-handler-4gb87(9ca3f29e-dcef-4f34-8c3c-9c91bc327200)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:42.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6033" for this suite. 12/14/22 16:54:42.606
------------------------------
• [2.128 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:40.522
    Dec 14 16:54:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename runtimeclass 12/14/22 16:54:40.526
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:40.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:40.554
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Dec 14 16:54:40.572: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6033 to be scheduled
    Dec 14 16:54:40.577: INFO: 1 pods are not scheduled: [runtimeclass-6033/test-runtimeclass-runtimeclass-6033-preconfigured-handler-4gb87(9ca3f29e-dcef-4f34-8c3c-9c91bc327200)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:42.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6033" for this suite. 12/14/22 16:54:42.606
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:42.651
Dec 14 16:54:42.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:54:42.658
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:42.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:42.72
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:54:42.747
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:54:43.465
STEP: Deploying the webhook pod 12/14/22 16:54:43.48
STEP: Wait for the deployment to be ready 12/14/22 16:54:43.498
Dec 14 16:54:43.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 16:54:45.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 16:54:47.568
STEP: Verifying the service has paired with the endpoint 12/14/22 16:54:47.584
Dec 14 16:54:48.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Dec 14 16:54:48.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1489-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:54:49.125
STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 16:54:49.158
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4414" for this suite. 12/14/22 16:54:51.87
STEP: Destroying namespace "webhook-4414-markers" for this suite. 12/14/22 16:54:51.886
------------------------------
• [SLOW TEST] [9.249 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:42.651
    Dec 14 16:54:42.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:54:42.658
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:42.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:42.72
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:54:42.747
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:54:43.465
    STEP: Deploying the webhook pod 12/14/22 16:54:43.48
    STEP: Wait for the deployment to be ready 12/14/22 16:54:43.498
    Dec 14 16:54:43.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec 14 16:54:45.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 16:54:47.568
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:54:47.584
    Dec 14 16:54:48.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Dec 14 16:54:48.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1489-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:54:49.125
    STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 16:54:49.158
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4414" for this suite. 12/14/22 16:54:51.87
    STEP: Destroying namespace "webhook-4414-markers" for this suite. 12/14/22 16:54:51.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:51.905
Dec 14 16:54:51.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sysctl 12/14/22 16:54:51.908
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:51.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:51.976
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/14/22 16:54:51.98
STEP: Watching for error events or started pod 12/14/22 16:54:51.996
STEP: Waiting for pod completion 12/14/22 16:54:54.006
Dec 14 16:54:54.007: INFO: Waiting up to 3m0s for pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be" in namespace "sysctl-435" to be "completed"
Dec 14 16:54:54.013: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6957ms
Dec 14 16:54:56.023: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016068034s
Dec 14 16:54:56.023: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be" satisfied condition "completed"
STEP: Checking that the pod succeeded 12/14/22 16:54:56.028
STEP: Getting logs from the pod 12/14/22 16:54:56.029
STEP: Checking that the sysctl is actually updated 12/14/22 16:54:56.155
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:54:56.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-435" for this suite. 12/14/22 16:54:56.166
------------------------------
• [4.280 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:51.905
    Dec 14 16:54:51.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sysctl 12/14/22 16:54:51.908
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:51.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:51.976
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/14/22 16:54:51.98
    STEP: Watching for error events or started pod 12/14/22 16:54:51.996
    STEP: Waiting for pod completion 12/14/22 16:54:54.006
    Dec 14 16:54:54.007: INFO: Waiting up to 3m0s for pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be" in namespace "sysctl-435" to be "completed"
    Dec 14 16:54:54.013: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6957ms
    Dec 14 16:54:56.023: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016068034s
    Dec 14 16:54:56.023: INFO: Pod "sysctl-648d7c81-5212-4cd6-9cd1-b01daf4ae9be" satisfied condition "completed"
    STEP: Checking that the pod succeeded 12/14/22 16:54:56.028
    STEP: Getting logs from the pod 12/14/22 16:54:56.029
    STEP: Checking that the sysctl is actually updated 12/14/22 16:54:56.155
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:54:56.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-435" for this suite. 12/14/22 16:54:56.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:54:56.19
Dec 14 16:54:56.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename aggregator 12/14/22 16:54:56.196
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:56.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:56.233
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Dec 14 16:54:56.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 12/14/22 16:54:56.237
Dec 14 16:54:57.072: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 14 16:54:59.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:01.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:03.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:05.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:07.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:09.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:11.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:13.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:15.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:17.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:19.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:21.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:55:23.298: INFO: Waited 140.901821ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 12/14/22 16:55:23.377
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/14/22 16:55:23.384
STEP: List APIServices 12/14/22 16:55:23.393
Dec 14 16:55:23.407: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Dec 14 16:55:23.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7732" for this suite. 12/14/22 16:55:23.704
------------------------------
• [SLOW TEST] [27.530 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:54:56.19
    Dec 14 16:54:56.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename aggregator 12/14/22 16:54:56.196
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:54:56.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:54:56.233
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Dec 14 16:54:56.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 12/14/22 16:54:56.237
    Dec 14 16:54:57.072: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Dec 14 16:54:59.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:01.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:03.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:05.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:07.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:09.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:11.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:13.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:15.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:17.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:19.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:21.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 14 16:55:23.298: INFO: Waited 140.901821ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 12/14/22 16:55:23.377
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/14/22 16:55:23.384
    STEP: List APIServices 12/14/22 16:55:23.393
    Dec 14 16:55:23.407: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:55:23.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7732" for this suite. 12/14/22 16:55:23.704
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:55:23.722
Dec 14 16:55:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename init-container 12/14/22 16:55:23.726
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:55:23.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:55:23.762
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 12/14/22 16:55:23.767
Dec 14 16:55:23.768: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:55:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7558" for this suite. 12/14/22 16:55:27.645
------------------------------
• [3.933 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:55:23.722
    Dec 14 16:55:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename init-container 12/14/22 16:55:23.726
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:55:23.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:55:23.762
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 12/14/22 16:55:23.767
    Dec 14 16:55:23.768: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:55:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7558" for this suite. 12/14/22 16:55:27.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:55:27.659
Dec 14 16:55:27.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 16:55:27.66
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:55:27.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:55:27.683
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 in namespace container-probe-9806 12/14/22 16:55:27.686
Dec 14 16:55:27.696: INFO: Waiting up to 5m0s for pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206" in namespace "container-probe-9806" to be "not pending"
Dec 14 16:55:27.712: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206": Phase="Pending", Reason="", readiness=false. Elapsed: 16.270809ms
Dec 14 16:55:29.722: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206": Phase="Running", Reason="", readiness=true. Elapsed: 2.025998846s
Dec 14 16:55:29.722: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206" satisfied condition "not pending"
Dec 14 16:55:29.722: INFO: Started pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 in namespace container-probe-9806
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 16:55:29.722
Dec 14 16:55:29.727: INFO: Initial restart count of pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 is 0
STEP: deleting the pod 12/14/22 16:59:30.717
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:30.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9806" for this suite. 12/14/22 16:59:30.75
------------------------------
• [SLOW TEST] [243.115 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:55:27.659
    Dec 14 16:55:27.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 16:55:27.66
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:55:27.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:55:27.683
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 in namespace container-probe-9806 12/14/22 16:55:27.686
    Dec 14 16:55:27.696: INFO: Waiting up to 5m0s for pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206" in namespace "container-probe-9806" to be "not pending"
    Dec 14 16:55:27.712: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206": Phase="Pending", Reason="", readiness=false. Elapsed: 16.270809ms
    Dec 14 16:55:29.722: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206": Phase="Running", Reason="", readiness=true. Elapsed: 2.025998846s
    Dec 14 16:55:29.722: INFO: Pod "test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206" satisfied condition "not pending"
    Dec 14 16:55:29.722: INFO: Started pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 in namespace container-probe-9806
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 16:55:29.722
    Dec 14 16:55:29.727: INFO: Initial restart count of pod test-webserver-26c10317-0a69-4ffc-a6c9-30e781d41206 is 0
    STEP: deleting the pod 12/14/22 16:59:30.717
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:30.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9806" for this suite. 12/14/22 16:59:30.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:30.775
Dec 14 16:59:30.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 16:59:30.779
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:30.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:30.836
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 12/14/22 16:59:30.841
Dec 14 16:59:30.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345" in namespace "projected-6634" to be "Succeeded or Failed"
Dec 14 16:59:30.859: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118333ms
Dec 14 16:59:32.866: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010646665s
Dec 14 16:59:34.865: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010275999s
Dec 14 16:59:36.866: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011070077s
STEP: Saw pod success 12/14/22 16:59:36.866
Dec 14 16:59:36.867: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345" satisfied condition "Succeeded or Failed"
Dec 14 16:59:36.871: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 container client-container: <nil>
STEP: delete the pod 12/14/22 16:59:36.903
Dec 14 16:59:36.918: INFO: Waiting for pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 to disappear
Dec 14 16:59:36.921: INFO: Pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6634" for this suite. 12/14/22 16:59:36.93
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:30.775
    Dec 14 16:59:30.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 16:59:30.779
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:30.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:30.836
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 12/14/22 16:59:30.841
    Dec 14 16:59:30.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345" in namespace "projected-6634" to be "Succeeded or Failed"
    Dec 14 16:59:30.859: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118333ms
    Dec 14 16:59:32.866: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010646665s
    Dec 14 16:59:34.865: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010275999s
    Dec 14 16:59:36.866: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011070077s
    STEP: Saw pod success 12/14/22 16:59:36.866
    Dec 14 16:59:36.867: INFO: Pod "downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345" satisfied condition "Succeeded or Failed"
    Dec 14 16:59:36.871: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 container client-container: <nil>
    STEP: delete the pod 12/14/22 16:59:36.903
    Dec 14 16:59:36.918: INFO: Waiting for pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 to disappear
    Dec 14 16:59:36.921: INFO: Pod downwardapi-volume-db06629f-7b98-471a-8f88-035e53d38345 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6634" for this suite. 12/14/22 16:59:36.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:36.949
Dec 14 16:59:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename watch 12/14/22 16:59:36.951
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:36.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:36.978
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 12/14/22 16:59:36.981
STEP: starting a background goroutine to produce watch events 12/14/22 16:59:36.985
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/14/22 16:59:36.986
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4064" for this suite. 12/14/22 16:59:39.809
------------------------------
• [2.913 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:36.949
    Dec 14 16:59:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename watch 12/14/22 16:59:36.951
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:36.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:36.978
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 12/14/22 16:59:36.981
    STEP: starting a background goroutine to produce watch events 12/14/22 16:59:36.985
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/14/22 16:59:36.986
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4064" for this suite. 12/14/22 16:59:39.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:39.865
Dec 14 16:59:39.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:59:39.867
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:39.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:39.888
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:59:39.915
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:59:40.649
STEP: Deploying the webhook pod 12/14/22 16:59:40.66
STEP: Wait for the deployment to be ready 12/14/22 16:59:40.676
Dec 14 16:59:40.693: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 16:59:42.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 16:59:44.717
STEP: Verifying the service has paired with the endpoint 12/14/22 16:59:44.734
Dec 14 16:59:45.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Dec 14 16:59:45.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8985-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:59:46.258
STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 16:59:46.286
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-480" for this suite. 12/14/22 16:59:49.039
STEP: Destroying namespace "webhook-480-markers" for this suite. 12/14/22 16:59:49.067
------------------------------
• [SLOW TEST] [9.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:39.865
    Dec 14 16:59:39.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:59:39.867
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:39.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:39.888
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:59:39.915
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:59:40.649
    STEP: Deploying the webhook pod 12/14/22 16:59:40.66
    STEP: Wait for the deployment to be ready 12/14/22 16:59:40.676
    Dec 14 16:59:40.693: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec 14 16:59:42.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 16:59:44.717
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:59:44.734
    Dec 14 16:59:45.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Dec 14 16:59:45.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8985-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:59:46.258
    STEP: Creating a custom resource that should be mutated by the webhook 12/14/22 16:59:46.286
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-480" for this suite. 12/14/22 16:59:49.039
    STEP: Destroying namespace "webhook-480-markers" for this suite. 12/14/22 16:59:49.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:49.1
Dec 14 16:59:49.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 16:59:49.113
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:49.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:49.181
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 16:59:49.231
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:59:50.138
STEP: Deploying the webhook pod 12/14/22 16:59:50.151
STEP: Wait for the deployment to be ready 12/14/22 16:59:50.169
Dec 14 16:59:50.190: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 16:59:52.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 16:59:54.221
STEP: Verifying the service has paired with the endpoint 12/14/22 16:59:54.243
Dec 14 16:59:55.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Dec 14 16:59:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6962-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:59:55.77
STEP: Creating a custom resource while v1 is storage version 12/14/22 16:59:55.803
STEP: Patching Custom Resource Definition to set v2 as storage 12/14/22 16:59:58.077
STEP: Patching the custom resource while v2 is storage version 12/14/22 16:59:58.106
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:58.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1061" for this suite. 12/14/22 16:59:59.017
STEP: Destroying namespace "webhook-1061-markers" for this suite. 12/14/22 16:59:59.034
------------------------------
• [SLOW TEST] [9.994 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:49.1
    Dec 14 16:59:49.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 16:59:49.113
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:49.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:49.181
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 16:59:49.231
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 16:59:50.138
    STEP: Deploying the webhook pod 12/14/22 16:59:50.151
    STEP: Wait for the deployment to be ready 12/14/22 16:59:50.169
    Dec 14 16:59:50.190: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec 14 16:59:52.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 59, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 16:59:54.221
    STEP: Verifying the service has paired with the endpoint 12/14/22 16:59:54.243
    Dec 14 16:59:55.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Dec 14 16:59:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6962-crds.webhook.example.com via the AdmissionRegistration API 12/14/22 16:59:55.77
    STEP: Creating a custom resource while v1 is storage version 12/14/22 16:59:55.803
    STEP: Patching Custom Resource Definition to set v2 as storage 12/14/22 16:59:58.077
    STEP: Patching the custom resource while v2 is storage version 12/14/22 16:59:58.106
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:58.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1061" for this suite. 12/14/22 16:59:59.017
    STEP: Destroying namespace "webhook-1061-markers" for this suite. 12/14/22 16:59:59.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:59.096
Dec 14 16:59:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:59:59.103
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:59.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:59.221
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec 14 16:59:59.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3063" for this suite. 12/14/22 16:59:59.347
------------------------------
• [0.266 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:59.096
    Dec 14 16:59:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubelet-test 12/14/22 16:59:59.103
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:59.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:59.221
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec 14 16:59:59.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3063" for this suite. 12/14/22 16:59:59.347
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 16:59:59.364
Dec 14 16:59:59.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename services 12/14/22 16:59:59.367
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:59.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:59.394
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9503 12/14/22 16:59:59.398
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 16:59:59.414
STEP: creating service externalsvc in namespace services-9503 12/14/22 16:59:59.414
STEP: creating replication controller externalsvc in namespace services-9503 12/14/22 16:59:59.439
I1214 16:59:59.448082      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9503, replica count: 2
I1214 17:00:02.507037      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 12/14/22 17:00:02.52
Dec 14 17:00:02.545: INFO: Creating new exec pod
Dec 14 17:00:02.557: INFO: Waiting up to 5m0s for pod "execpodt44mf" in namespace "services-9503" to be "running"
Dec 14 17:00:02.566: INFO: Pod "execpodt44mf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905343ms
Dec 14 17:00:04.575: INFO: Pod "execpodt44mf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017838519s
Dec 14 17:00:04.575: INFO: Pod "execpodt44mf" satisfied condition "running"
Dec 14 17:00:04.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9503 exec execpodt44mf -- /bin/sh -x -c nslookup clusterip-service.services-9503.svc.cluster.local'
Dec 14 17:00:05.028: INFO: stderr: "+ nslookup clusterip-service.services-9503.svc.cluster.local\n"
Dec 14 17:00:05.028: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-9503.svc.cluster.local\tcanonical name = externalsvc.services-9503.svc.cluster.local.\nName:\texternalsvc.services-9503.svc.cluster.local\nAddress: 10.233.26.193\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9503, will wait for the garbage collector to delete the pods 12/14/22 17:00:05.028
Dec 14 17:00:05.094: INFO: Deleting ReplicationController externalsvc took: 9.034617ms
Dec 14 17:00:05.195: INFO: Terminating ReplicationController externalsvc pods took: 100.646987ms
Dec 14 17:00:07.422: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:07.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9503" for this suite. 12/14/22 17:00:07.458
------------------------------
• [SLOW TEST] [8.102 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 16:59:59.364
    Dec 14 16:59:59.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename services 12/14/22 16:59:59.367
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 16:59:59.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 16:59:59.394
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9503 12/14/22 16:59:59.398
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/14/22 16:59:59.414
    STEP: creating service externalsvc in namespace services-9503 12/14/22 16:59:59.414
    STEP: creating replication controller externalsvc in namespace services-9503 12/14/22 16:59:59.439
    I1214 16:59:59.448082      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9503, replica count: 2
    I1214 17:00:02.507037      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 12/14/22 17:00:02.52
    Dec 14 17:00:02.545: INFO: Creating new exec pod
    Dec 14 17:00:02.557: INFO: Waiting up to 5m0s for pod "execpodt44mf" in namespace "services-9503" to be "running"
    Dec 14 17:00:02.566: INFO: Pod "execpodt44mf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905343ms
    Dec 14 17:00:04.575: INFO: Pod "execpodt44mf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017838519s
    Dec 14 17:00:04.575: INFO: Pod "execpodt44mf" satisfied condition "running"
    Dec 14 17:00:04.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=services-9503 exec execpodt44mf -- /bin/sh -x -c nslookup clusterip-service.services-9503.svc.cluster.local'
    Dec 14 17:00:05.028: INFO: stderr: "+ nslookup clusterip-service.services-9503.svc.cluster.local\n"
    Dec 14 17:00:05.028: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-9503.svc.cluster.local\tcanonical name = externalsvc.services-9503.svc.cluster.local.\nName:\texternalsvc.services-9503.svc.cluster.local\nAddress: 10.233.26.193\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9503, will wait for the garbage collector to delete the pods 12/14/22 17:00:05.028
    Dec 14 17:00:05.094: INFO: Deleting ReplicationController externalsvc took: 9.034617ms
    Dec 14 17:00:05.195: INFO: Terminating ReplicationController externalsvc pods took: 100.646987ms
    Dec 14 17:00:07.422: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:07.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9503" for this suite. 12/14/22 17:00:07.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:07.468
Dec 14 17:00:07.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 17:00:07.475
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:07.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:07.503
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Dec 14 17:00:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2288" for this suite. 12/14/22 17:00:08.113
------------------------------
• [0.667 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:07.468
    Dec 14 17:00:07.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 17:00:07.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:07.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:07.503
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Dec 14 17:00:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2288" for this suite. 12/14/22 17:00:08.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:08.139
Dec 14 17:00:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 17:00:08.142
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:08.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:08.19
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-27da902c-a270-4ede-9a50-21ae9f4f9ee6 12/14/22 17:00:08.193
STEP: Creating a pod to test consume secrets 12/14/22 17:00:08.204
Dec 14 17:00:08.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568" in namespace "projected-4445" to be "Succeeded or Failed"
Dec 14 17:00:08.221: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Pending", Reason="", readiness=false. Elapsed: 4.897091ms
Dec 14 17:00:10.230: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Running", Reason="", readiness=true. Elapsed: 2.014747092s
Dec 14 17:00:12.228: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Running", Reason="", readiness=false. Elapsed: 4.012811098s
Dec 14 17:00:14.229: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013440483s
STEP: Saw pod success 12/14/22 17:00:14.229
Dec 14 17:00:14.230: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568" satisfied condition "Succeeded or Failed"
Dec 14 17:00:14.235: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 17:00:14.275
Dec 14 17:00:14.290: INFO: Waiting for pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 to disappear
Dec 14 17:00:14.293: INFO: Pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:14.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4445" for this suite. 12/14/22 17:00:14.3
------------------------------
• [SLOW TEST] [6.171 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:08.139
    Dec 14 17:00:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 17:00:08.142
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:08.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:08.19
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-27da902c-a270-4ede-9a50-21ae9f4f9ee6 12/14/22 17:00:08.193
    STEP: Creating a pod to test consume secrets 12/14/22 17:00:08.204
    Dec 14 17:00:08.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568" in namespace "projected-4445" to be "Succeeded or Failed"
    Dec 14 17:00:08.221: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Pending", Reason="", readiness=false. Elapsed: 4.897091ms
    Dec 14 17:00:10.230: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Running", Reason="", readiness=true. Elapsed: 2.014747092s
    Dec 14 17:00:12.228: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Running", Reason="", readiness=false. Elapsed: 4.012811098s
    Dec 14 17:00:14.229: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013440483s
    STEP: Saw pod success 12/14/22 17:00:14.229
    Dec 14 17:00:14.230: INFO: Pod "pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568" satisfied condition "Succeeded or Failed"
    Dec 14 17:00:14.235: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 17:00:14.275
    Dec 14 17:00:14.290: INFO: Waiting for pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 to disappear
    Dec 14 17:00:14.293: INFO: Pod pod-projected-secrets-84a66d56-12c1-4923-bd5a-9f1c53355568 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:14.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4445" for this suite. 12/14/22 17:00:14.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:14.314
Dec 14 17:00:14.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 17:00:14.316
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:14.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:14.353
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 17:00:14.357
Dec 14 17:00:14.369: INFO: Waiting up to 5m0s for pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9" in namespace "emptydir-9112" to be "Succeeded or Failed"
Dec 14 17:00:14.376: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.554244ms
Dec 14 17:00:16.382: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013338781s
Dec 14 17:00:18.382: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01319091s
STEP: Saw pod success 12/14/22 17:00:18.382
Dec 14 17:00:18.383: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9" satisfied condition "Succeeded or Failed"
Dec 14 17:00:18.387: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 container test-container: <nil>
STEP: delete the pod 12/14/22 17:00:18.401
Dec 14 17:00:18.418: INFO: Waiting for pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 to disappear
Dec 14 17:00:18.422: INFO: Pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:18.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9112" for this suite. 12/14/22 17:00:18.432
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:14.314
    Dec 14 17:00:14.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 17:00:14.316
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:14.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:14.353
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/14/22 17:00:14.357
    Dec 14 17:00:14.369: INFO: Waiting up to 5m0s for pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9" in namespace "emptydir-9112" to be "Succeeded or Failed"
    Dec 14 17:00:14.376: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.554244ms
    Dec 14 17:00:16.382: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013338781s
    Dec 14 17:00:18.382: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01319091s
    STEP: Saw pod success 12/14/22 17:00:18.382
    Dec 14 17:00:18.383: INFO: Pod "pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9" satisfied condition "Succeeded or Failed"
    Dec 14 17:00:18.387: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 container test-container: <nil>
    STEP: delete the pod 12/14/22 17:00:18.401
    Dec 14 17:00:18.418: INFO: Waiting for pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 to disappear
    Dec 14 17:00:18.422: INFO: Pod pod-cb5c4497-0b44-4dc5-89c1-366bb88e89e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:18.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9112" for this suite. 12/14/22 17:00:18.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:18.443
Dec 14 17:00:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 17:00:18.446
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:18.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:18.477
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-bf156f2e-c3f0-459b-b43e-4d2701d0d109 12/14/22 17:00:18.48
STEP: Creating secret with name secret-projected-all-test-volume-cbcffe4a-c8cb-4e8c-8cd7-c5191dcc3cfe 12/14/22 17:00:18.49
STEP: Creating a pod to test Check all projections for projected volume plugin 12/14/22 17:00:18.496
Dec 14 17:00:18.510: INFO: Waiting up to 5m0s for pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0" in namespace "projected-2881" to be "Succeeded or Failed"
Dec 14 17:00:18.519: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.157981ms
Dec 14 17:00:20.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016170145s
Dec 14 17:00:22.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01582081s
STEP: Saw pod success 12/14/22 17:00:22.526
Dec 14 17:00:22.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0" satisfied condition "Succeeded or Failed"
Dec 14 17:00:22.530: INFO: Trying to get logs from node iet9eich7uhu-3 pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 container projected-all-volume-test: <nil>
STEP: delete the pod 12/14/22 17:00:22.541
Dec 14 17:00:22.559: INFO: Waiting for pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 to disappear
Dec 14 17:00:22.563: INFO: Pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:22.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2881" for this suite. 12/14/22 17:00:22.568
------------------------------
• [4.134 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:18.443
    Dec 14 17:00:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 17:00:18.446
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:18.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:18.477
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-bf156f2e-c3f0-459b-b43e-4d2701d0d109 12/14/22 17:00:18.48
    STEP: Creating secret with name secret-projected-all-test-volume-cbcffe4a-c8cb-4e8c-8cd7-c5191dcc3cfe 12/14/22 17:00:18.49
    STEP: Creating a pod to test Check all projections for projected volume plugin 12/14/22 17:00:18.496
    Dec 14 17:00:18.510: INFO: Waiting up to 5m0s for pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0" in namespace "projected-2881" to be "Succeeded or Failed"
    Dec 14 17:00:18.519: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.157981ms
    Dec 14 17:00:20.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016170145s
    Dec 14 17:00:22.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01582081s
    STEP: Saw pod success 12/14/22 17:00:22.526
    Dec 14 17:00:22.526: INFO: Pod "projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0" satisfied condition "Succeeded or Failed"
    Dec 14 17:00:22.530: INFO: Trying to get logs from node iet9eich7uhu-3 pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 container projected-all-volume-test: <nil>
    STEP: delete the pod 12/14/22 17:00:22.541
    Dec 14 17:00:22.559: INFO: Waiting for pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 to disappear
    Dec 14 17:00:22.563: INFO: Pod projected-volume-bfeb954e-310a-45cc-918e-14a0b88837b0 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:22.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2881" for this suite. 12/14/22 17:00:22.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:22.592
Dec 14 17:00:22.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 17:00:22.595
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:22.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:22.619
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 12/14/22 17:00:22.622
Dec 14 17:00:22.636: INFO: Waiting up to 5m0s for pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3" in namespace "pods-3687" to be "running and ready"
Dec 14 17:00:22.643: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680731ms
Dec 14 17:00:22.643: INFO: The phase of Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:00:24.650: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014494748s
Dec 14 17:00:24.651: INFO: The phase of Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 is Running (Ready = true)
Dec 14 17:00:24.651: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3" satisfied condition "running and ready"
Dec 14 17:00:24.659: INFO: Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 has hostIP: 192.168.121.56
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:24.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3687" for this suite. 12/14/22 17:00:24.666
------------------------------
• [2.081 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:22.592
    Dec 14 17:00:22.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 17:00:22.595
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:22.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:22.619
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 12/14/22 17:00:22.622
    Dec 14 17:00:22.636: INFO: Waiting up to 5m0s for pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3" in namespace "pods-3687" to be "running and ready"
    Dec 14 17:00:22.643: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680731ms
    Dec 14 17:00:22.643: INFO: The phase of Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:00:24.650: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014494748s
    Dec 14 17:00:24.651: INFO: The phase of Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 is Running (Ready = true)
    Dec 14 17:00:24.651: INFO: Pod "pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3" satisfied condition "running and ready"
    Dec 14 17:00:24.659: INFO: Pod pod-hostip-7bdca8d4-62f7-4627-8d7e-37daaed2efb3 has hostIP: 192.168.121.56
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:24.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3687" for this suite. 12/14/22 17:00:24.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:24.677
Dec 14 17:00:24.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 17:00:24.68
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:24.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:24.709
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Dec 14 17:00:24.725: INFO: Waiting up to 5m0s for pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345" in namespace "pods-968" to be "running and ready"
Dec 14 17:00:24.733: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345": Phase="Pending", Reason="", readiness=false. Elapsed: 7.816273ms
Dec 14 17:00:24.733: INFO: The phase of Pod server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:00:26.738: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345": Phase="Running", Reason="", readiness=true. Elapsed: 2.012683291s
Dec 14 17:00:26.738: INFO: The phase of Pod server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345 is Running (Ready = true)
Dec 14 17:00:26.738: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345" satisfied condition "running and ready"
Dec 14 17:00:26.773: INFO: Waiting up to 5m0s for pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88" in namespace "pods-968" to be "Succeeded or Failed"
Dec 14 17:00:26.782: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Pending", Reason="", readiness=false. Elapsed: 9.114917ms
Dec 14 17:00:28.790: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016989271s
Dec 14 17:00:30.789: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016352615s
STEP: Saw pod success 12/14/22 17:00:30.79
Dec 14 17:00:30.790: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88" satisfied condition "Succeeded or Failed"
Dec 14 17:00:30.794: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 container env3cont: <nil>
STEP: delete the pod 12/14/22 17:00:30.806
Dec 14 17:00:30.824: INFO: Waiting for pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 to disappear
Dec 14 17:00:30.827: INFO: Pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:30.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-968" for this suite. 12/14/22 17:00:30.832
------------------------------
• [SLOW TEST] [6.166 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:24.677
    Dec 14 17:00:24.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 17:00:24.68
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:24.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:24.709
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Dec 14 17:00:24.725: INFO: Waiting up to 5m0s for pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345" in namespace "pods-968" to be "running and ready"
    Dec 14 17:00:24.733: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345": Phase="Pending", Reason="", readiness=false. Elapsed: 7.816273ms
    Dec 14 17:00:24.733: INFO: The phase of Pod server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:00:26.738: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345": Phase="Running", Reason="", readiness=true. Elapsed: 2.012683291s
    Dec 14 17:00:26.738: INFO: The phase of Pod server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345 is Running (Ready = true)
    Dec 14 17:00:26.738: INFO: Pod "server-envvars-84f522b4-1d4f-4ad4-8745-37ffe5f42345" satisfied condition "running and ready"
    Dec 14 17:00:26.773: INFO: Waiting up to 5m0s for pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88" in namespace "pods-968" to be "Succeeded or Failed"
    Dec 14 17:00:26.782: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Pending", Reason="", readiness=false. Elapsed: 9.114917ms
    Dec 14 17:00:28.790: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016989271s
    Dec 14 17:00:30.789: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016352615s
    STEP: Saw pod success 12/14/22 17:00:30.79
    Dec 14 17:00:30.790: INFO: Pod "client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88" satisfied condition "Succeeded or Failed"
    Dec 14 17:00:30.794: INFO: Trying to get logs from node iet9eich7uhu-3 pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 container env3cont: <nil>
    STEP: delete the pod 12/14/22 17:00:30.806
    Dec 14 17:00:30.824: INFO: Waiting for pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 to disappear
    Dec 14 17:00:30.827: INFO: Pod client-envvars-40fbfb71-ab54-42db-82d4-f2581b8d9b88 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:30.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-968" for this suite. 12/14/22 17:00:30.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:30.843
Dec 14 17:00:30.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-runtime 12/14/22 17:00:30.847
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:30.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:30.876
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/14/22 17:00:30.899
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/14/22 17:00:48.055
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/14/22 17:00:48.061
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/14/22 17:00:48.073
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/14/22 17:00:48.074
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/14/22 17:00:48.1
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/14/22 17:00:52.137
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/14/22 17:00:54.16
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/14/22 17:00:54.171
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/14/22 17:00:54.171
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/14/22 17:00:54.198
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/14/22 17:00:55.21
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/14/22 17:00:58.241
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/14/22 17:00:58.252
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/14/22 17:00:58.252
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 14 17:00:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1578" for this suite. 12/14/22 17:00:58.289
------------------------------
• [SLOW TEST] [27.463 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:30.843
    Dec 14 17:00:30.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-runtime 12/14/22 17:00:30.847
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:30.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:30.876
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/14/22 17:00:30.899
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/14/22 17:00:48.055
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/14/22 17:00:48.061
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/14/22 17:00:48.073
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/14/22 17:00:48.074
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/14/22 17:00:48.1
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/14/22 17:00:52.137
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/14/22 17:00:54.16
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/14/22 17:00:54.171
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/14/22 17:00:54.171
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/14/22 17:00:54.198
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/14/22 17:00:55.21
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/14/22 17:00:58.241
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/14/22 17:00:58.252
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/14/22 17:00:58.252
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:00:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1578" for this suite. 12/14/22 17:00:58.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:00:58.322
Dec 14 17:00:58.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 17:00:58.329
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:58.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:58.357
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 17:00:58.383
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:00:58.94
STEP: Deploying the webhook pod 12/14/22 17:00:58.961
STEP: Wait for the deployment to be ready 12/14/22 17:00:58.979
Dec 14 17:00:59.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 17:01:01.026
STEP: Verifying the service has paired with the endpoint 12/14/22 17:01:01.043
Dec 14 17:01:02.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/14/22 17:01:02.049
STEP: create a configmap that should be updated by the webhook 12/14/22 17:01:02.074
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:02.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-896" for this suite. 12/14/22 17:01:02.205
STEP: Destroying namespace "webhook-896-markers" for this suite. 12/14/22 17:01:02.223
------------------------------
• [3.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:00:58.322
    Dec 14 17:00:58.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 17:00:58.329
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:00:58.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:00:58.357
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 17:00:58.383
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:00:58.94
    STEP: Deploying the webhook pod 12/14/22 17:00:58.961
    STEP: Wait for the deployment to be ready 12/14/22 17:00:58.979
    Dec 14 17:00:59.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 17:01:01.026
    STEP: Verifying the service has paired with the endpoint 12/14/22 17:01:01.043
    Dec 14 17:01:02.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/14/22 17:01:02.049
    STEP: create a configmap that should be updated by the webhook 12/14/22 17:01:02.074
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:02.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-896" for this suite. 12/14/22 17:01:02.205
    STEP: Destroying namespace "webhook-896-markers" for this suite. 12/14/22 17:01:02.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:02.24
Dec 14 17:01:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-runtime 12/14/22 17:01:02.242
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:02.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:02.27
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 12/14/22 17:01:02.274
STEP: wait for the container to reach Succeeded 12/14/22 17:01:02.293
STEP: get the container status 12/14/22 17:01:07.337
STEP: the container should be terminated 12/14/22 17:01:07.346
STEP: the termination message should be set 12/14/22 17:01:07.346
Dec 14 17:01:07.346: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 12/14/22 17:01:07.346
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:07.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5080" for this suite. 12/14/22 17:01:07.376
------------------------------
• [SLOW TEST] [5.155 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:02.24
    Dec 14 17:01:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-runtime 12/14/22 17:01:02.242
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:02.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:02.27
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 12/14/22 17:01:02.274
    STEP: wait for the container to reach Succeeded 12/14/22 17:01:02.293
    STEP: get the container status 12/14/22 17:01:07.337
    STEP: the container should be terminated 12/14/22 17:01:07.346
    STEP: the termination message should be set 12/14/22 17:01:07.346
    Dec 14 17:01:07.346: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 12/14/22 17:01:07.346
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:07.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5080" for this suite. 12/14/22 17:01:07.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:07.398
Dec 14 17:01:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 17:01:07.401
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:07.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:07.44
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-afa6c5c0-28b3-48c4-8c11-85f94b120280 12/14/22 17:01:07.45
STEP: Creating a pod to test consume secrets 12/14/22 17:01:07.458
Dec 14 17:01:07.471: INFO: Waiting up to 5m0s for pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a" in namespace "secrets-7571" to be "Succeeded or Failed"
Dec 14 17:01:07.481: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.448706ms
Dec 14 17:01:09.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018592436s
Dec 14 17:01:11.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018825762s
STEP: Saw pod success 12/14/22 17:01:11.49
Dec 14 17:01:11.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a" satisfied condition "Succeeded or Failed"
Dec 14 17:01:11.495: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 17:01:11.505
Dec 14 17:01:11.532: INFO: Waiting for pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a to disappear
Dec 14 17:01:11.536: INFO: Pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:11.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7571" for this suite. 12/14/22 17:01:11.543
------------------------------
• [4.154 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:07.398
    Dec 14 17:01:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 17:01:07.401
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:07.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:07.44
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-afa6c5c0-28b3-48c4-8c11-85f94b120280 12/14/22 17:01:07.45
    STEP: Creating a pod to test consume secrets 12/14/22 17:01:07.458
    Dec 14 17:01:07.471: INFO: Waiting up to 5m0s for pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a" in namespace "secrets-7571" to be "Succeeded or Failed"
    Dec 14 17:01:07.481: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.448706ms
    Dec 14 17:01:09.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018592436s
    Dec 14 17:01:11.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018825762s
    STEP: Saw pod success 12/14/22 17:01:11.49
    Dec 14 17:01:11.490: INFO: Pod "pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a" satisfied condition "Succeeded or Failed"
    Dec 14 17:01:11.495: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 17:01:11.505
    Dec 14 17:01:11.532: INFO: Waiting for pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a to disappear
    Dec 14 17:01:11.536: INFO: Pod pod-secrets-15384fa8-575e-4189-88ac-54ac3deee62a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:11.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7571" for this suite. 12/14/22 17:01:11.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:11.558
Dec 14 17:01:11.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption 12/14/22 17:01:11.56
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:11.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:11.591
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 12/14/22 17:01:11.594
STEP: Waiting for the pdb to be processed 12/14/22 17:01:11.599
STEP: updating the pdb 12/14/22 17:01:13.614
STEP: Waiting for the pdb to be processed 12/14/22 17:01:13.637
STEP: patching the pdb 12/14/22 17:01:15.652
STEP: Waiting for the pdb to be processed 12/14/22 17:01:15.669
STEP: Waiting for the pdb to be deleted 12/14/22 17:01:17.691
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:17.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8221" for this suite. 12/14/22 17:01:17.703
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:11.558
    Dec 14 17:01:11.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption 12/14/22 17:01:11.56
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:11.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:11.591
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 12/14/22 17:01:11.594
    STEP: Waiting for the pdb to be processed 12/14/22 17:01:11.599
    STEP: updating the pdb 12/14/22 17:01:13.614
    STEP: Waiting for the pdb to be processed 12/14/22 17:01:13.637
    STEP: patching the pdb 12/14/22 17:01:15.652
    STEP: Waiting for the pdb to be processed 12/14/22 17:01:15.669
    STEP: Waiting for the pdb to be deleted 12/14/22 17:01:17.691
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:17.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8221" for this suite. 12/14/22 17:01:17.703
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:17.711
Dec 14 17:01:17.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename endpointslice 12/14/22 17:01:17.715
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:17.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:17.739
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Dec 14 17:01:17.773: INFO: Endpoints addresses: [192.168.121.16 192.168.121.21] , ports: [6443]
Dec 14 17:01:17.773: INFO: EndpointSlices addresses: [192.168.121.16 192.168.121.21] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:17.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5269" for this suite. 12/14/22 17:01:17.791
------------------------------
• [0.087 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:17.711
    Dec 14 17:01:17.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename endpointslice 12/14/22 17:01:17.715
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:17.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:17.739
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Dec 14 17:01:17.773: INFO: Endpoints addresses: [192.168.121.16 192.168.121.21] , ports: [6443]
    Dec 14 17:01:17.773: INFO: EndpointSlices addresses: [192.168.121.16 192.168.121.21] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:17.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5269" for this suite. 12/14/22 17:01:17.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:17.801
Dec 14 17:01:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 17:01:17.803
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:17.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:17.825
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/14/22 17:01:17.828
Dec 14 17:01:17.838: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-657" to be "running and ready"
Dec 14 17:01:17.845: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.865368ms
Dec 14 17:01:17.845: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:01:19.851: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013726749s
Dec 14 17:01:19.851: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:01:21.854: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.015921438s
Dec 14 17:01:21.854: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Dec 14 17:01:21.854: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 12/14/22 17:01:21.857
STEP: Then the orphan pod is adopted 12/14/22 17:01:21.864
STEP: When the matched label of one of its pods change 12/14/22 17:01:22.874
Dec 14 17:01:22.880: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 12/14/22 17:01:22.893
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:23.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-657" for this suite. 12/14/22 17:01:23.91
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:17.801
    Dec 14 17:01:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 17:01:17.803
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:17.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:17.825
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/14/22 17:01:17.828
    Dec 14 17:01:17.838: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-657" to be "running and ready"
    Dec 14 17:01:17.845: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.865368ms
    Dec 14 17:01:17.845: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:01:19.851: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013726749s
    Dec 14 17:01:19.851: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:01:21.854: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.015921438s
    Dec 14 17:01:21.854: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Dec 14 17:01:21.854: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 12/14/22 17:01:21.857
    STEP: Then the orphan pod is adopted 12/14/22 17:01:21.864
    STEP: When the matched label of one of its pods change 12/14/22 17:01:22.874
    Dec 14 17:01:22.880: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/14/22 17:01:22.893
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:23.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-657" for this suite. 12/14/22 17:01:23.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:23.919
Dec 14 17:01:23.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context-test 12/14/22 17:01:23.923
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:23.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:23.945
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Dec 14 17:01:23.959: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3" in namespace "security-context-test-1728" to be "Succeeded or Failed"
Dec 14 17:01:23.972: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.186355ms
Dec 14 17:01:25.977: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018740289s
Dec 14 17:01:27.979: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020156482s
Dec 14 17:01:27.979: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1728" for this suite. 12/14/22 17:01:27.985
------------------------------
• [4.075 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:23.919
    Dec 14 17:01:23.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context-test 12/14/22 17:01:23.923
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:23.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:23.945
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Dec 14 17:01:23.959: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3" in namespace "security-context-test-1728" to be "Succeeded or Failed"
    Dec 14 17:01:23.972: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.186355ms
    Dec 14 17:01:25.977: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018740289s
    Dec 14 17:01:27.979: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020156482s
    Dec 14 17:01:27.979: INFO: Pod "busybox-readonly-false-0a6c6515-69e8-41df-92a7-9d0ad7f281b3" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1728" for this suite. 12/14/22 17:01:27.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:27.996
Dec 14 17:01:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 17:01:27.998
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:28.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:28.021
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 12/14/22 17:01:28.024
STEP: Creating a ResourceQuota 12/14/22 17:01:33.03
STEP: Ensuring resource quota status is calculated 12/14/22 17:01:33.041
STEP: Creating a Pod that fits quota 12/14/22 17:01:35.049
STEP: Ensuring ResourceQuota status captures the pod usage 12/14/22 17:01:35.075
STEP: Not allowing a pod to be created that exceeds remaining quota 12/14/22 17:01:37.088
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/14/22 17:01:37.093
STEP: Ensuring a pod cannot update its resource requirements 12/14/22 17:01:37.096
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/14/22 17:01:37.104
STEP: Deleting the pod 12/14/22 17:01:39.113
STEP: Ensuring resource quota status released the pod usage 12/14/22 17:01:39.129
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:41.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3816" for this suite. 12/14/22 17:01:41.146
------------------------------
• [SLOW TEST] [13.160 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:27.996
    Dec 14 17:01:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 17:01:27.998
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:28.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:28.021
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 12/14/22 17:01:28.024
    STEP: Creating a ResourceQuota 12/14/22 17:01:33.03
    STEP: Ensuring resource quota status is calculated 12/14/22 17:01:33.041
    STEP: Creating a Pod that fits quota 12/14/22 17:01:35.049
    STEP: Ensuring ResourceQuota status captures the pod usage 12/14/22 17:01:35.075
    STEP: Not allowing a pod to be created that exceeds remaining quota 12/14/22 17:01:37.088
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/14/22 17:01:37.093
    STEP: Ensuring a pod cannot update its resource requirements 12/14/22 17:01:37.096
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/14/22 17:01:37.104
    STEP: Deleting the pod 12/14/22 17:01:39.113
    STEP: Ensuring resource quota status released the pod usage 12/14/22 17:01:39.129
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:41.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3816" for this suite. 12/14/22 17:01:41.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:41.159
Dec 14 17:01:41.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replicaset 12/14/22 17:01:41.162
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:41.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:41.2
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Dec 14 17:01:41.203: INFO: Creating ReplicaSet my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0
Dec 14 17:01:41.216: INFO: Pod name my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Found 0 pods out of 1
Dec 14 17:01:46.222: INFO: Pod name my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Found 1 pods out of 1
Dec 14 17:01:46.222: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0" is running
Dec 14 17:01:46.222: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" in namespace "replicaset-563" to be "running"
Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv": Phase="Running", Reason="", readiness=true. Elapsed: 5.522832ms
Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" satisfied condition "running"
Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:41 +0000 UTC Reason: Message:}])
Dec 14 17:01:46.228: INFO: Trying to dial the pod
Dec 14 17:01:51.256: INFO: Controller my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Got expected result from replica 1 [my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv]: "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:51.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-563" for this suite. 12/14/22 17:01:51.267
------------------------------
• [SLOW TEST] [10.119 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:41.159
    Dec 14 17:01:41.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replicaset 12/14/22 17:01:41.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:41.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:41.2
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Dec 14 17:01:41.203: INFO: Creating ReplicaSet my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0
    Dec 14 17:01:41.216: INFO: Pod name my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Found 0 pods out of 1
    Dec 14 17:01:46.222: INFO: Pod name my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Found 1 pods out of 1
    Dec 14 17:01:46.222: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0" is running
    Dec 14 17:01:46.222: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" in namespace "replicaset-563" to be "running"
    Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv": Phase="Running", Reason="", readiness=true. Elapsed: 5.522832ms
    Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" satisfied condition "running"
    Dec 14 17:01:46.228: INFO: Pod "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 17:01:41 +0000 UTC Reason: Message:}])
    Dec 14 17:01:46.228: INFO: Trying to dial the pod
    Dec 14 17:01:51.256: INFO: Controller my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0: Got expected result from replica 1 [my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv]: "my-hostname-basic-2541e9a5-6f1e-4ec2-8cb0-330bf3384fe0-r7bjv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:51.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-563" for this suite. 12/14/22 17:01:51.267
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:51.284
Dec 14 17:01:51.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 17:01:51.286
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:51.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:51.311
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 12/14/22 17:01:51.315
Dec 14 17:01:51.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f" in namespace "downward-api-1529" to be "Succeeded or Failed"
Dec 14 17:01:51.332: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89127ms
Dec 14 17:01:53.341: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015727346s
Dec 14 17:01:55.341: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015786743s
STEP: Saw pod success 12/14/22 17:01:55.342
Dec 14 17:01:55.342: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f" satisfied condition "Succeeded or Failed"
Dec 14 17:01:55.348: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f container client-container: <nil>
STEP: delete the pod 12/14/22 17:01:55.361
Dec 14 17:01:55.378: INFO: Waiting for pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f to disappear
Dec 14 17:01:55.381: INFO: Pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 17:01:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1529" for this suite. 12/14/22 17:01:55.388
------------------------------
• [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:51.284
    Dec 14 17:01:51.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 17:01:51.286
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:51.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:51.311
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 12/14/22 17:01:51.315
    Dec 14 17:01:51.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f" in namespace "downward-api-1529" to be "Succeeded or Failed"
    Dec 14 17:01:51.332: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89127ms
    Dec 14 17:01:53.341: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015727346s
    Dec 14 17:01:55.341: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015786743s
    STEP: Saw pod success 12/14/22 17:01:55.342
    Dec 14 17:01:55.342: INFO: Pod "downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f" satisfied condition "Succeeded or Failed"
    Dec 14 17:01:55.348: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f container client-container: <nil>
    STEP: delete the pod 12/14/22 17:01:55.361
    Dec 14 17:01:55.378: INFO: Waiting for pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f to disappear
    Dec 14 17:01:55.381: INFO: Pod downwardapi-volume-f5b95a89-9069-4575-8563-91110b4e4b7f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:01:55.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1529" for this suite. 12/14/22 17:01:55.388
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:01:55.399
Dec 14 17:01:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 17:01:55.402
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:55.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:55.423
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 12/14/22 17:01:55.427
Dec 14 17:01:55.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda" in namespace "downward-api-7009" to be "Succeeded or Failed"
Dec 14 17:01:55.444: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691835ms
Dec 14 17:01:57.458: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Running", Reason="", readiness=true. Elapsed: 2.018365517s
Dec 14 17:01:59.456: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Running", Reason="", readiness=false. Elapsed: 4.016753374s
Dec 14 17:02:01.452: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012841933s
STEP: Saw pod success 12/14/22 17:02:01.452
Dec 14 17:02:01.453: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda" satisfied condition "Succeeded or Failed"
Dec 14 17:02:01.460: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda container client-container: <nil>
STEP: delete the pod 12/14/22 17:02:01.468
Dec 14 17:02:01.489: INFO: Waiting for pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda to disappear
Dec 14 17:02:01.494: INFO: Pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:01.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7009" for this suite. 12/14/22 17:02:01.503
------------------------------
• [SLOW TEST] [6.114 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:01:55.399
    Dec 14 17:01:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 17:01:55.402
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:01:55.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:01:55.423
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 12/14/22 17:01:55.427
    Dec 14 17:01:55.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda" in namespace "downward-api-7009" to be "Succeeded or Failed"
    Dec 14 17:01:55.444: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691835ms
    Dec 14 17:01:57.458: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Running", Reason="", readiness=true. Elapsed: 2.018365517s
    Dec 14 17:01:59.456: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Running", Reason="", readiness=false. Elapsed: 4.016753374s
    Dec 14 17:02:01.452: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012841933s
    STEP: Saw pod success 12/14/22 17:02:01.452
    Dec 14 17:02:01.453: INFO: Pod "downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda" satisfied condition "Succeeded or Failed"
    Dec 14 17:02:01.460: INFO: Trying to get logs from node iet9eich7uhu-3 pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda container client-container: <nil>
    STEP: delete the pod 12/14/22 17:02:01.468
    Dec 14 17:02:01.489: INFO: Waiting for pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda to disappear
    Dec 14 17:02:01.494: INFO: Pod downwardapi-volume-54181946-b0a2-47b8-a0e7-9a105faf8fda no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:01.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7009" for this suite. 12/14/22 17:02:01.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:01.515
Dec 14 17:02:01.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename podtemplate 12/14/22 17:02:01.519
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:01.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:01.547
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:01.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5383" for this suite. 12/14/22 17:02:01.606
------------------------------
• [0.112 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:01.515
    Dec 14 17:02:01.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename podtemplate 12/14/22 17:02:01.519
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:01.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:01.547
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:01.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5383" for this suite. 12/14/22 17:02:01.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:01.629
Dec 14 17:02:01.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename resourcequota 12/14/22 17:02:01.631
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:01.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:01.667
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 12/14/22 17:02:01.674
STEP: Creating a ResourceQuota 12/14/22 17:02:06.693
STEP: Ensuring resource quota status is calculated 12/14/22 17:02:06.706
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4986" for this suite. 12/14/22 17:02:08.722
------------------------------
• [SLOW TEST] [7.103 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:01.629
    Dec 14 17:02:01.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename resourcequota 12/14/22 17:02:01.631
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:01.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:01.667
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 12/14/22 17:02:01.674
    STEP: Creating a ResourceQuota 12/14/22 17:02:06.693
    STEP: Ensuring resource quota status is calculated 12/14/22 17:02:06.706
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4986" for this suite. 12/14/22 17:02:08.722
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:08.734
Dec 14 17:02:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 17:02:08.736
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:08.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:08.761
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 12/14/22 17:02:08.765
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:08.772
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:08.772
STEP: creating a pod to probe DNS 12/14/22 17:02:08.772
STEP: submitting the pod to kubernetes 12/14/22 17:02:08.773
Dec 14 17:02:08.784: INFO: Waiting up to 15m0s for pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863" in namespace "dns-6309" to be "running"
Dec 14 17:02:08.795: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863": Phase="Pending", Reason="", readiness=false. Elapsed: 9.844569ms
Dec 14 17:02:10.802: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863": Phase="Running", Reason="", readiness=true. Elapsed: 2.017156516s
Dec 14 17:02:10.802: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863" satisfied condition "running"
STEP: retrieving the pod 12/14/22 17:02:10.802
STEP: looking for the results for each expected name from probers 12/14/22 17:02:10.807
Dec 14 17:02:10.827: INFO: DNS probes using dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863 succeeded

STEP: deleting the pod 12/14/22 17:02:10.827
STEP: changing the externalName to bar.example.com 12/14/22 17:02:10.845
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:10.862
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:10.863
STEP: creating a second pod to probe DNS 12/14/22 17:02:10.863
STEP: submitting the pod to kubernetes 12/14/22 17:02:10.863
Dec 14 17:02:10.873: INFO: Waiting up to 15m0s for pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5" in namespace "dns-6309" to be "running"
Dec 14 17:02:10.883: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.24036ms
Dec 14 17:02:12.898: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.024065774s
Dec 14 17:02:12.898: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5" satisfied condition "running"
STEP: retrieving the pod 12/14/22 17:02:12.898
STEP: looking for the results for each expected name from probers 12/14/22 17:02:12.911
Dec 14 17:02:12.940: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:12.954: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:12.954: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:17.965: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:17.975: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:17.976: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:22.961: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:22.967: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:22.967: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:27.963: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:27.968: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:27.968: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:32.974: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:32.983: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:32.983: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:37.961: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:37.966: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 17:02:37.966: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

Dec 14 17:02:42.972: INFO: DNS probes using dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 succeeded

STEP: deleting the pod 12/14/22 17:02:42.972
STEP: changing the service to type=ClusterIP 12/14/22 17:02:43.041
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:43.076
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
 12/14/22 17:02:43.077
STEP: creating a third pod to probe DNS 12/14/22 17:02:43.077
STEP: submitting the pod to kubernetes 12/14/22 17:02:43.094
Dec 14 17:02:43.141: INFO: Waiting up to 15m0s for pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b" in namespace "dns-6309" to be "running"
Dec 14 17:02:43.149: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487672ms
Dec 14 17:02:45.156: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014517568s
Dec 14 17:02:47.157: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Running", Reason="", readiness=true. Elapsed: 4.016088593s
Dec 14 17:02:47.157: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b" satisfied condition "running"
STEP: retrieving the pod 12/14/22 17:02:47.157
STEP: looking for the results for each expected name from probers 12/14/22 17:02:47.171
Dec 14 17:02:47.187: INFO: DNS probes using dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b succeeded

STEP: deleting the pod 12/14/22 17:02:47.188
STEP: deleting the test externalName service 12/14/22 17:02:47.206
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6309" for this suite. 12/14/22 17:02:47.236
------------------------------
• [SLOW TEST] [38.513 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:08.734
    Dec 14 17:02:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 17:02:08.736
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:08.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:08.761
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 12/14/22 17:02:08.765
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:08.772
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:08.772
    STEP: creating a pod to probe DNS 12/14/22 17:02:08.772
    STEP: submitting the pod to kubernetes 12/14/22 17:02:08.773
    Dec 14 17:02:08.784: INFO: Waiting up to 15m0s for pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863" in namespace "dns-6309" to be "running"
    Dec 14 17:02:08.795: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863": Phase="Pending", Reason="", readiness=false. Elapsed: 9.844569ms
    Dec 14 17:02:10.802: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863": Phase="Running", Reason="", readiness=true. Elapsed: 2.017156516s
    Dec 14 17:02:10.802: INFO: Pod "dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 17:02:10.802
    STEP: looking for the results for each expected name from probers 12/14/22 17:02:10.807
    Dec 14 17:02:10.827: INFO: DNS probes using dns-test-8a19e394-9f0c-49dc-b5cf-6f12e8779863 succeeded

    STEP: deleting the pod 12/14/22 17:02:10.827
    STEP: changing the externalName to bar.example.com 12/14/22 17:02:10.845
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:10.862
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:10.863
    STEP: creating a second pod to probe DNS 12/14/22 17:02:10.863
    STEP: submitting the pod to kubernetes 12/14/22 17:02:10.863
    Dec 14 17:02:10.873: INFO: Waiting up to 15m0s for pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5" in namespace "dns-6309" to be "running"
    Dec 14 17:02:10.883: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.24036ms
    Dec 14 17:02:12.898: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.024065774s
    Dec 14 17:02:12.898: INFO: Pod "dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 17:02:12.898
    STEP: looking for the results for each expected name from probers 12/14/22 17:02:12.911
    Dec 14 17:02:12.940: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:12.954: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:12.954: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:17.965: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:17.975: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:17.976: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:22.961: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:22.967: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:22.967: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:27.963: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:27.968: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:27.968: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:32.974: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:32.983: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:32.983: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:37.961: INFO: File wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:37.966: INFO: File jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local from pod  dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec 14 17:02:37.966: INFO: Lookups using dns-6309/dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 failed for: [wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local]

    Dec 14 17:02:42.972: INFO: DNS probes using dns-test-55c35427-0c5f-4dab-9af0-43ff9a28fdc5 succeeded

    STEP: deleting the pod 12/14/22 17:02:42.972
    STEP: changing the service to type=ClusterIP 12/14/22 17:02:43.041
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:43.076
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6309.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6309.svc.cluster.local; sleep 1; done
     12/14/22 17:02:43.077
    STEP: creating a third pod to probe DNS 12/14/22 17:02:43.077
    STEP: submitting the pod to kubernetes 12/14/22 17:02:43.094
    Dec 14 17:02:43.141: INFO: Waiting up to 15m0s for pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b" in namespace "dns-6309" to be "running"
    Dec 14 17:02:43.149: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487672ms
    Dec 14 17:02:45.156: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014517568s
    Dec 14 17:02:47.157: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b": Phase="Running", Reason="", readiness=true. Elapsed: 4.016088593s
    Dec 14 17:02:47.157: INFO: Pod "dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 17:02:47.157
    STEP: looking for the results for each expected name from probers 12/14/22 17:02:47.171
    Dec 14 17:02:47.187: INFO: DNS probes using dns-test-bc6bc7b9-de69-458a-a7f8-43245872a99b succeeded

    STEP: deleting the pod 12/14/22 17:02:47.188
    STEP: deleting the test externalName service 12/14/22 17:02:47.206
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6309" for this suite. 12/14/22 17:02:47.236
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:47.249
Dec 14 17:02:47.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 17:02:47.256
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:47.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:47.284
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 17:02:47.305
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:02:47.911
STEP: Deploying the webhook pod 12/14/22 17:02:47.923
STEP: Wait for the deployment to be ready 12/14/22 17:02:47.939
Dec 14 17:02:47.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/14/22 17:02:50.002
STEP: Verifying the service has paired with the endpoint 12/14/22 17:02:50.017
Dec 14 17:02:51.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 12/14/22 17:02:51.095
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 17:02:51.168
STEP: Deleting the collection of validation webhooks 12/14/22 17:02:51.225
STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 17:02:51.309
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:51.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3990" for this suite. 12/14/22 17:02:51.4
STEP: Destroying namespace "webhook-3990-markers" for this suite. 12/14/22 17:02:51.415
------------------------------
• [4.180 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:47.249
    Dec 14 17:02:47.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 17:02:47.256
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:47.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:47.284
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 17:02:47.305
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:02:47.911
    STEP: Deploying the webhook pod 12/14/22 17:02:47.923
    STEP: Wait for the deployment to be ready 12/14/22 17:02:47.939
    Dec 14 17:02:47.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/14/22 17:02:50.002
    STEP: Verifying the service has paired with the endpoint 12/14/22 17:02:50.017
    Dec 14 17:02:51.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 12/14/22 17:02:51.095
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 17:02:51.168
    STEP: Deleting the collection of validation webhooks 12/14/22 17:02:51.225
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/14/22 17:02:51.309
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:51.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3990" for this suite. 12/14/22 17:02:51.4
    STEP: Destroying namespace "webhook-3990-markers" for this suite. 12/14/22 17:02:51.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:51.43
Dec 14 17:02:51.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context-test 12/14/22 17:02:51.433
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:51.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:51.459
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Dec 14 17:02:51.477: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9" in namespace "security-context-test-9651" to be "Succeeded or Failed"
Dec 14 17:02:51.484: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.527912ms
Dec 14 17:02:53.490: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013391682s
Dec 14 17:02:55.491: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013789752s
Dec 14 17:02:57.492: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014762023s
Dec 14 17:02:59.496: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019021809s
Dec 14 17:02:59.497: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 17:02:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9651" for this suite. 12/14/22 17:02:59.516
------------------------------
• [SLOW TEST] [8.097 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:51.43
    Dec 14 17:02:51.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context-test 12/14/22 17:02:51.433
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:51.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:51.459
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Dec 14 17:02:51.477: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9" in namespace "security-context-test-9651" to be "Succeeded or Failed"
    Dec 14 17:02:51.484: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.527912ms
    Dec 14 17:02:53.490: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013391682s
    Dec 14 17:02:55.491: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013789752s
    Dec 14 17:02:57.492: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014762023s
    Dec 14 17:02:59.496: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019021809s
    Dec 14 17:02:59.497: INFO: Pod "alpine-nnp-false-41b69c1e-1f44-4583-9ff0-0e31748567a9" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:02:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9651" for this suite. 12/14/22 17:02:59.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:02:59.538
Dec 14 17:02:59.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 17:02:59.54
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:59.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:59.566
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-0e621f87-1785-4047-9097-d36e18a9729f 12/14/22 17:02:59.599
STEP: Creating a pod to test consume secrets 12/14/22 17:02:59.604
Dec 14 17:02:59.617: INFO: Waiting up to 5m0s for pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e" in namespace "secrets-8823" to be "Succeeded or Failed"
Dec 14 17:02:59.624: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.405427ms
Dec 14 17:03:01.633: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016489468s
Dec 14 17:03:03.643: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026529571s
Dec 14 17:03:05.641: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024417203s
STEP: Saw pod success 12/14/22 17:03:05.641
Dec 14 17:03:05.642: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e" satisfied condition "Succeeded or Failed"
Dec 14 17:03:05.649: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 17:03:05.661
Dec 14 17:03:05.676: INFO: Waiting for pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e to disappear
Dec 14 17:03:05.682: INFO: Pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:05.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8823" for this suite. 12/14/22 17:03:05.69
STEP: Destroying namespace "secret-namespace-5663" for this suite. 12/14/22 17:03:05.702
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:02:59.538
    Dec 14 17:02:59.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 17:02:59.54
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:02:59.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:02:59.566
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-0e621f87-1785-4047-9097-d36e18a9729f 12/14/22 17:02:59.599
    STEP: Creating a pod to test consume secrets 12/14/22 17:02:59.604
    Dec 14 17:02:59.617: INFO: Waiting up to 5m0s for pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e" in namespace "secrets-8823" to be "Succeeded or Failed"
    Dec 14 17:02:59.624: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.405427ms
    Dec 14 17:03:01.633: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016489468s
    Dec 14 17:03:03.643: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026529571s
    Dec 14 17:03:05.641: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024417203s
    STEP: Saw pod success 12/14/22 17:03:05.641
    Dec 14 17:03:05.642: INFO: Pod "pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e" satisfied condition "Succeeded or Failed"
    Dec 14 17:03:05.649: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 17:03:05.661
    Dec 14 17:03:05.676: INFO: Waiting for pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e to disappear
    Dec 14 17:03:05.682: INFO: Pod pod-secrets-e1ad3553-f1e4-420a-bb89-374ded6f6a5e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:05.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8823" for this suite. 12/14/22 17:03:05.69
    STEP: Destroying namespace "secret-namespace-5663" for this suite. 12/14/22 17:03:05.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:05.712
Dec 14 17:03:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 17:03:05.717
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:05.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:05.746
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 17:03:05.778
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:03:06.461
STEP: Deploying the webhook pod 12/14/22 17:03:06.475
STEP: Wait for the deployment to be ready 12/14/22 17:03:06.488
Dec 14 17:03:06.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 17:03:08.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 17:03:10.55
STEP: Verifying the service has paired with the endpoint 12/14/22 17:03:10.58
Dec 14 17:03:11.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 12/14/22 17:03:11.588
STEP: Creating a custom resource definition that should be denied by the webhook 12/14/22 17:03:11.617
Dec 14 17:03:11.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:11.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3835" for this suite. 12/14/22 17:03:11.754
STEP: Destroying namespace "webhook-3835-markers" for this suite. 12/14/22 17:03:11.787
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:05.712
    Dec 14 17:03:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 17:03:05.717
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:05.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:05.746
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 17:03:05.778
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:03:06.461
    STEP: Deploying the webhook pod 12/14/22 17:03:06.475
    STEP: Wait for the deployment to be ready 12/14/22 17:03:06.488
    Dec 14 17:03:06.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec 14 17:03:08.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 3, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 17:03:10.55
    STEP: Verifying the service has paired with the endpoint 12/14/22 17:03:10.58
    Dec 14 17:03:11.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 12/14/22 17:03:11.588
    STEP: Creating a custom resource definition that should be denied by the webhook 12/14/22 17:03:11.617
    Dec 14 17:03:11.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:11.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3835" for this suite. 12/14/22 17:03:11.754
    STEP: Destroying namespace "webhook-3835-markers" for this suite. 12/14/22 17:03:11.787
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:11.808
Dec 14 17:03:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 17:03:11.811
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:11.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:11.85
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 12/14/22 17:03:11.86
Dec 14 17:03:11.861: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 14 17:03:11.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:13.582: INFO: stderr: ""
Dec 14 17:03:13.582: INFO: stdout: "service/agnhost-replica created\n"
Dec 14 17:03:13.582: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 14 17:03:13.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:14.084: INFO: stderr: ""
Dec 14 17:03:14.084: INFO: stdout: "service/agnhost-primary created\n"
Dec 14 17:03:14.084: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 17:03:14.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:14.561: INFO: stderr: ""
Dec 14 17:03:14.561: INFO: stdout: "service/frontend created\n"
Dec 14 17:03:14.561: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 17:03:14.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:15.199: INFO: stderr: ""
Dec 14 17:03:15.199: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 17:03:15.199: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 17:03:15.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:15.729: INFO: stderr: ""
Dec 14 17:03:15.730: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 14 17:03:15.730: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 17:03:15.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
Dec 14 17:03:16.222: INFO: stderr: ""
Dec 14 17:03:16.223: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 12/14/22 17:03:16.223
Dec 14 17:03:16.223: INFO: Waiting for all frontend pods to be Running.
Dec 14 17:03:21.274: INFO: Waiting for frontend to serve content.
Dec 14 17:03:21.300: INFO: Trying to add a new entry to the guestbook.
Dec 14 17:03:26.311: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Dec 14 17:03:31.344: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 12/14/22 17:03:31.357
Dec 14 17:03:31.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:31.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:31.507: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 17:03:31.507
Dec 14 17:03:31.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:31.757: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:31.757: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 17:03:31.758
Dec 14 17:03:31.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:31.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:31.910: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 17:03:31.911
Dec 14 17:03:31.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:32.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:32.056: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 17:03:32.056
Dec 14 17:03:32.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:32.223: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:32.223: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/14/22 17:03:32.223
Dec 14 17:03:32.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
Dec 14 17:03:32.360: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:03:32.360: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:32.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3063" for this suite. 12/14/22 17:03:32.374
------------------------------
• [SLOW TEST] [20.577 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:11.808
    Dec 14 17:03:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 17:03:11.811
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:11.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:11.85
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 12/14/22 17:03:11.86
    Dec 14 17:03:11.861: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Dec 14 17:03:11.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:13.582: INFO: stderr: ""
    Dec 14 17:03:13.582: INFO: stdout: "service/agnhost-replica created\n"
    Dec 14 17:03:13.582: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Dec 14 17:03:13.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:14.084: INFO: stderr: ""
    Dec 14 17:03:14.084: INFO: stdout: "service/agnhost-primary created\n"
    Dec 14 17:03:14.084: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Dec 14 17:03:14.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:14.561: INFO: stderr: ""
    Dec 14 17:03:14.561: INFO: stdout: "service/frontend created\n"
    Dec 14 17:03:14.561: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Dec 14 17:03:14.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:15.199: INFO: stderr: ""
    Dec 14 17:03:15.199: INFO: stdout: "deployment.apps/frontend created\n"
    Dec 14 17:03:15.199: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 14 17:03:15.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:15.729: INFO: stderr: ""
    Dec 14 17:03:15.730: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Dec 14 17:03:15.730: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 14 17:03:15.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 create -f -'
    Dec 14 17:03:16.222: INFO: stderr: ""
    Dec 14 17:03:16.223: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 12/14/22 17:03:16.223
    Dec 14 17:03:16.223: INFO: Waiting for all frontend pods to be Running.
    Dec 14 17:03:21.274: INFO: Waiting for frontend to serve content.
    Dec 14 17:03:21.300: INFO: Trying to add a new entry to the guestbook.
    Dec 14 17:03:26.311: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
    Dec 14 17:03:31.344: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 12/14/22 17:03:31.357
    Dec 14 17:03:31.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:31.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:31.507: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 17:03:31.507
    Dec 14 17:03:31.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:31.757: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:31.757: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 17:03:31.758
    Dec 14 17:03:31.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:31.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:31.910: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 17:03:31.911
    Dec 14 17:03:31.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:32.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:32.056: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 17:03:32.056
    Dec 14 17:03:32.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:32.223: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:32.223: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/14/22 17:03:32.223
    Dec 14 17:03:32.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3063 delete --grace-period=0 --force -f -'
    Dec 14 17:03:32.360: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:03:32.360: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:32.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3063" for this suite. 12/14/22 17:03:32.374
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:32.386
Dec 14 17:03:32.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename events 12/14/22 17:03:32.395
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:32.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:32.431
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 12/14/22 17:03:32.436
Dec 14 17:03:32.446: INFO: created test-event-1
Dec 14 17:03:32.454: INFO: created test-event-2
Dec 14 17:03:32.459: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 12/14/22 17:03:32.459
STEP: delete collection of events 12/14/22 17:03:32.464
Dec 14 17:03:32.465: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/14/22 17:03:32.489
Dec 14 17:03:32.489: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:32.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4271" for this suite. 12/14/22 17:03:32.499
------------------------------
• [0.122 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:32.386
    Dec 14 17:03:32.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename events 12/14/22 17:03:32.395
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:32.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:32.431
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 12/14/22 17:03:32.436
    Dec 14 17:03:32.446: INFO: created test-event-1
    Dec 14 17:03:32.454: INFO: created test-event-2
    Dec 14 17:03:32.459: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 12/14/22 17:03:32.459
    STEP: delete collection of events 12/14/22 17:03:32.464
    Dec 14 17:03:32.465: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/14/22 17:03:32.489
    Dec 14 17:03:32.489: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:32.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4271" for this suite. 12/14/22 17:03:32.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:32.515
Dec 14 17:03:32.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 17:03:32.524
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:32.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:32.584
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 17:03:32.589
Dec 14 17:03:32.609: INFO: Waiting up to 5m0s for pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e" in namespace "emptydir-2453" to be "Succeeded or Failed"
Dec 14 17:03:32.620: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.109097ms
Dec 14 17:03:34.629: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019928801s
Dec 14 17:03:36.627: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018021951s
Dec 14 17:03:38.628: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018745894s
STEP: Saw pod success 12/14/22 17:03:38.628
Dec 14 17:03:38.630: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e" satisfied condition "Succeeded or Failed"
Dec 14 17:03:38.634: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e container test-container: <nil>
STEP: delete the pod 12/14/22 17:03:38.647
Dec 14 17:03:38.670: INFO: Waiting for pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e to disappear
Dec 14 17:03:38.677: INFO: Pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:38.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2453" for this suite. 12/14/22 17:03:38.688
------------------------------
• [SLOW TEST] [6.186 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:32.515
    Dec 14 17:03:32.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 17:03:32.524
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:32.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:32.584
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/14/22 17:03:32.589
    Dec 14 17:03:32.609: INFO: Waiting up to 5m0s for pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e" in namespace "emptydir-2453" to be "Succeeded or Failed"
    Dec 14 17:03:32.620: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.109097ms
    Dec 14 17:03:34.629: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019928801s
    Dec 14 17:03:36.627: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018021951s
    Dec 14 17:03:38.628: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018745894s
    STEP: Saw pod success 12/14/22 17:03:38.628
    Dec 14 17:03:38.630: INFO: Pod "pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e" satisfied condition "Succeeded or Failed"
    Dec 14 17:03:38.634: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e container test-container: <nil>
    STEP: delete the pod 12/14/22 17:03:38.647
    Dec 14 17:03:38.670: INFO: Waiting for pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e to disappear
    Dec 14 17:03:38.677: INFO: Pod pod-7d3751fa-b5b9-46dc-a781-2bb4d61c835e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:38.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2453" for this suite. 12/14/22 17:03:38.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:38.703
Dec 14 17:03:38.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename events 12/14/22 17:03:38.709
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:38.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:38.732
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 12/14/22 17:03:38.736
STEP: listing events in all namespaces 12/14/22 17:03:38.745
STEP: listing events in test namespace 12/14/22 17:03:38.751
STEP: listing events with field selection filtering on source 12/14/22 17:03:38.755
STEP: listing events with field selection filtering on reportingController 12/14/22 17:03:38.759
STEP: getting the test event 12/14/22 17:03:38.765
STEP: patching the test event 12/14/22 17:03:38.769
STEP: getting the test event 12/14/22 17:03:38.786
STEP: updating the test event 12/14/22 17:03:38.791
STEP: getting the test event 12/14/22 17:03:38.801
STEP: deleting the test event 12/14/22 17:03:38.806
STEP: listing events in all namespaces 12/14/22 17:03:38.819
STEP: listing events in test namespace 12/14/22 17:03:38.826
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:38.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2033" for this suite. 12/14/22 17:03:38.839
------------------------------
• [0.148 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:38.703
    Dec 14 17:03:38.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename events 12/14/22 17:03:38.709
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:38.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:38.732
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 12/14/22 17:03:38.736
    STEP: listing events in all namespaces 12/14/22 17:03:38.745
    STEP: listing events in test namespace 12/14/22 17:03:38.751
    STEP: listing events with field selection filtering on source 12/14/22 17:03:38.755
    STEP: listing events with field selection filtering on reportingController 12/14/22 17:03:38.759
    STEP: getting the test event 12/14/22 17:03:38.765
    STEP: patching the test event 12/14/22 17:03:38.769
    STEP: getting the test event 12/14/22 17:03:38.786
    STEP: updating the test event 12/14/22 17:03:38.791
    STEP: getting the test event 12/14/22 17:03:38.801
    STEP: deleting the test event 12/14/22 17:03:38.806
    STEP: listing events in all namespaces 12/14/22 17:03:38.819
    STEP: listing events in test namespace 12/14/22 17:03:38.826
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:38.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2033" for this suite. 12/14/22 17:03:38.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:38.881
Dec 14 17:03:38.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 17:03:38.885
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:38.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:38.909
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 12/14/22 17:03:38.913
Dec 14 17:03:38.929: INFO: Waiting up to 5m0s for pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9" in namespace "downward-api-6616" to be "running and ready"
Dec 14 17:03:38.937: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.645146ms
Dec 14 17:03:38.937: INFO: The phase of Pod annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:03:40.944: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01427849s
Dec 14 17:03:40.944: INFO: The phase of Pod annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9 is Running (Ready = true)
Dec 14 17:03:40.944: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9" satisfied condition "running and ready"
Dec 14 17:03:41.483: INFO: Successfully updated pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:45.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6616" for this suite. 12/14/22 17:03:45.535
------------------------------
• [SLOW TEST] [6.664 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:38.881
    Dec 14 17:03:38.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 17:03:38.885
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:38.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:38.909
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 12/14/22 17:03:38.913
    Dec 14 17:03:38.929: INFO: Waiting up to 5m0s for pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9" in namespace "downward-api-6616" to be "running and ready"
    Dec 14 17:03:38.937: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.645146ms
    Dec 14 17:03:38.937: INFO: The phase of Pod annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:03:40.944: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01427849s
    Dec 14 17:03:40.944: INFO: The phase of Pod annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9 is Running (Ready = true)
    Dec 14 17:03:40.944: INFO: Pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9" satisfied condition "running and ready"
    Dec 14 17:03:41.483: INFO: Successfully updated pod "annotationupdate7fef94f0-44f3-4b26-99e1-97f004ac67b9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:45.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6616" for this suite. 12/14/22 17:03:45.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:45.547
Dec 14 17:03:45.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 17:03:45.551
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:45.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:45.578
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 12/14/22 17:03:45.582
Dec 14 17:03:45.592: INFO: Waiting up to 5m0s for pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8" in namespace "projected-2310" to be "running and ready"
Dec 14 17:03:45.596: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425992ms
Dec 14 17:03:45.597: INFO: The phase of Pod labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:03:47.606: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013779288s
Dec 14 17:03:47.606: INFO: The phase of Pod labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8 is Running (Ready = true)
Dec 14 17:03:47.606: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8" satisfied condition "running and ready"
Dec 14 17:03:48.141: INFO: Successfully updated pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:52.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2310" for this suite. 12/14/22 17:03:52.209
------------------------------
• [SLOW TEST] [6.670 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:45.547
    Dec 14 17:03:45.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 17:03:45.551
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:45.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:45.578
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 12/14/22 17:03:45.582
    Dec 14 17:03:45.592: INFO: Waiting up to 5m0s for pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8" in namespace "projected-2310" to be "running and ready"
    Dec 14 17:03:45.596: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425992ms
    Dec 14 17:03:45.597: INFO: The phase of Pod labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:03:47.606: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013779288s
    Dec 14 17:03:47.606: INFO: The phase of Pod labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8 is Running (Ready = true)
    Dec 14 17:03:47.606: INFO: Pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8" satisfied condition "running and ready"
    Dec 14 17:03:48.141: INFO: Successfully updated pod "labelsupdate631d6a1b-1ec5-4461-b19e-5e1b70bdebd8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:52.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2310" for this suite. 12/14/22 17:03:52.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:52.222
Dec 14 17:03:52.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename downward-api 12/14/22 17:03:52.224
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:52.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:52.248
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 12/14/22 17:03:52.251
Dec 14 17:03:52.262: INFO: Waiting up to 5m0s for pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40" in namespace "downward-api-137" to be "Succeeded or Failed"
Dec 14 17:03:52.266: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907751ms
Dec 14 17:03:54.274: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01193073s
Dec 14 17:03:56.272: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010267141s
STEP: Saw pod success 12/14/22 17:03:56.272
Dec 14 17:03:56.272: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40" satisfied condition "Succeeded or Failed"
Dec 14 17:03:56.276: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 container dapi-container: <nil>
STEP: delete the pod 12/14/22 17:03:56.289
Dec 14 17:03:56.314: INFO: Waiting for pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 to disappear
Dec 14 17:03:56.319: INFO: Pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec 14 17:03:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-137" for this suite. 12/14/22 17:03:56.324
------------------------------
• [4.112 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:52.222
    Dec 14 17:03:52.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename downward-api 12/14/22 17:03:52.224
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:52.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:52.248
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 12/14/22 17:03:52.251
    Dec 14 17:03:52.262: INFO: Waiting up to 5m0s for pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40" in namespace "downward-api-137" to be "Succeeded or Failed"
    Dec 14 17:03:52.266: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907751ms
    Dec 14 17:03:54.274: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01193073s
    Dec 14 17:03:56.272: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010267141s
    STEP: Saw pod success 12/14/22 17:03:56.272
    Dec 14 17:03:56.272: INFO: Pod "downward-api-da6a3d42-b214-44d9-a789-d216e0091f40" satisfied condition "Succeeded or Failed"
    Dec 14 17:03:56.276: INFO: Trying to get logs from node iet9eich7uhu-3 pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 container dapi-container: <nil>
    STEP: delete the pod 12/14/22 17:03:56.289
    Dec 14 17:03:56.314: INFO: Waiting for pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 to disappear
    Dec 14 17:03:56.319: INFO: Pod downward-api-da6a3d42-b214-44d9-a789-d216e0091f40 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:03:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-137" for this suite. 12/14/22 17:03:56.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:56.335
Dec 14 17:03:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption 12/14/22 17:03:56.338
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:56.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:56.363
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:03:56.368
Dec 14 17:03:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename disruption-2 12/14/22 17:03:56.37
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:56.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:56.399
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 12/14/22 17:03:56.414
STEP: Waiting for the pdb to be processed 12/14/22 17:03:58.443
STEP: Waiting for the pdb to be processed 12/14/22 17:04:00.476
STEP: listing a collection of PDBs across all namespaces 12/14/22 17:04:00.482
STEP: listing a collection of PDBs in namespace disruption-9761 12/14/22 17:04:00.489
STEP: deleting a collection of PDBs 12/14/22 17:04:00.494
STEP: Waiting for the PDB collection to be deleted 12/14/22 17:04:00.514
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:00.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:00.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4593" for this suite. 12/14/22 17:04:00.532
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9761" for this suite. 12/14/22 17:04:00.543
------------------------------
• [4.219 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:56.335
    Dec 14 17:03:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption 12/14/22 17:03:56.338
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:56.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:56.363
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:03:56.368
    Dec 14 17:03:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename disruption-2 12/14/22 17:03:56.37
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:03:56.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:03:56.399
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 12/14/22 17:03:56.414
    STEP: Waiting for the pdb to be processed 12/14/22 17:03:58.443
    STEP: Waiting for the pdb to be processed 12/14/22 17:04:00.476
    STEP: listing a collection of PDBs across all namespaces 12/14/22 17:04:00.482
    STEP: listing a collection of PDBs in namespace disruption-9761 12/14/22 17:04:00.489
    STEP: deleting a collection of PDBs 12/14/22 17:04:00.494
    STEP: Waiting for the PDB collection to be deleted 12/14/22 17:04:00.514
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:00.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:00.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4593" for this suite. 12/14/22 17:04:00.532
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9761" for this suite. 12/14/22 17:04:00.543
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:00.554
Dec 14 17:04:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/14/22 17:04:00.558
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:00.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:00.602
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 12/14/22 17:04:00.609
STEP: Creating hostNetwork=false pod 12/14/22 17:04:00.611
Dec 14 17:04:00.630: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-727" to be "running and ready"
Dec 14 17:04:00.639: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.222404ms
Dec 14 17:04:00.639: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:04:02.645: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014448645s
Dec 14 17:04:02.645: INFO: The phase of Pod test-pod is Running (Ready = true)
Dec 14 17:04:02.645: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 12/14/22 17:04:02.65
Dec 14 17:04:02.659: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-727" to be "running and ready"
Dec 14 17:04:02.665: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649071ms
Dec 14 17:04:02.665: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:04:04.675: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016135978s
Dec 14 17:04:04.675: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Dec 14 17:04:04.675: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 12/14/22 17:04:04.681
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/14/22 17:04:04.681
Dec 14 17:04:04.682: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:04.683: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:04.683: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 17:04:04.758: INFO: Exec stderr: ""
Dec 14 17:04:04.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:04.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:04.759: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:04.759: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 17:04:04.831: INFO: Exec stderr: ""
Dec 14 17:04:04.831: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:04.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:04.832: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:04.832: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 17:04:04.920: INFO: Exec stderr: ""
Dec 14 17:04:04.920: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:04.921: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:04.921: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 17:04:04.998: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/14/22 17:04:04.998
Dec 14 17:04:04.999: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:04.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.000: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.000: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 17:04:05.166: INFO: Exec stderr: ""
Dec 14 17:04:05.167: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.168: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.168: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 17:04:05.250: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/14/22 17:04:05.251
Dec 14 17:04:05.251: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.262: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.262: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 17:04:05.375: INFO: Exec stderr: ""
Dec 14 17:04:05.375: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:05.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.376: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.376: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 17:04:05.448: INFO: Exec stderr: ""
Dec 14 17:04:05.448: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:05.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.449: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.450: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 17:04:05.545: INFO: Exec stderr: ""
Dec 14 17:04:05.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:04:05.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:04:05.546: INFO: ExecWithOptions: Clientset creation
Dec 14 17:04:05.546: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 17:04:05.633: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-727" for this suite. 12/14/22 17:04:05.647
------------------------------
• [SLOW TEST] [5.122 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:00.554
    Dec 14 17:04:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/14/22 17:04:00.558
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:00.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:00.602
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 12/14/22 17:04:00.609
    STEP: Creating hostNetwork=false pod 12/14/22 17:04:00.611
    Dec 14 17:04:00.630: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-727" to be "running and ready"
    Dec 14 17:04:00.639: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.222404ms
    Dec 14 17:04:00.639: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:04:02.645: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014448645s
    Dec 14 17:04:02.645: INFO: The phase of Pod test-pod is Running (Ready = true)
    Dec 14 17:04:02.645: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 12/14/22 17:04:02.65
    Dec 14 17:04:02.659: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-727" to be "running and ready"
    Dec 14 17:04:02.665: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649071ms
    Dec 14 17:04:02.665: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:04:04.675: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016135978s
    Dec 14 17:04:04.675: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Dec 14 17:04:04.675: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 12/14/22 17:04:04.681
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/14/22 17:04:04.681
    Dec 14 17:04:04.682: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:04.683: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:04.683: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 17:04:04.758: INFO: Exec stderr: ""
    Dec 14 17:04:04.758: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:04.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:04.759: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:04.759: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 17:04:04.831: INFO: Exec stderr: ""
    Dec 14 17:04:04.831: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:04.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:04.832: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:04.832: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 17:04:04.920: INFO: Exec stderr: ""
    Dec 14 17:04:04.920: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:04.921: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:04.921: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 17:04:04.998: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/14/22 17:04:04.998
    Dec 14 17:04:04.999: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:04.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.000: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.000: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 14 17:04:05.166: INFO: Exec stderr: ""
    Dec 14 17:04:05.167: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.168: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.168: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 14 17:04:05.250: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/14/22 17:04:05.251
    Dec 14 17:04:05.251: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.262: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.262: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 17:04:05.375: INFO: Exec stderr: ""
    Dec 14 17:04:05.375: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:05.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.376: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.376: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 14 17:04:05.448: INFO: Exec stderr: ""
    Dec 14 17:04:05.448: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:05.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.449: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.450: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 17:04:05.545: INFO: Exec stderr: ""
    Dec 14 17:04:05.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-727 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:04:05.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:04:05.546: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:04:05.546: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-727/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 14 17:04:05.633: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-727" for this suite. 12/14/22 17:04:05.647
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:05.679
Dec 14 17:04:05.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename limitrange 12/14/22 17:04:05.684
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:05.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:05.732
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 12/14/22 17:04:05.737
STEP: Setting up watch 12/14/22 17:04:05.737
STEP: Submitting a LimitRange 12/14/22 17:04:05.844
STEP: Verifying LimitRange creation was observed 12/14/22 17:04:05.85
STEP: Fetching the LimitRange to ensure it has proper values 12/14/22 17:04:05.85
Dec 14 17:04:05.865: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 17:04:05.865: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 12/14/22 17:04:05.865
STEP: Ensuring Pod has resource requirements applied from LimitRange 12/14/22 17:04:05.875
Dec 14 17:04:05.906: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 17:04:05.906: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 12/14/22 17:04:05.906
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/14/22 17:04:05.916
Dec 14 17:04:05.922: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 14 17:04:05.922: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 12/14/22 17:04:05.922
STEP: Failing to create a Pod with more than max resources 12/14/22 17:04:05.927
STEP: Updating a LimitRange 12/14/22 17:04:05.931
STEP: Verifying LimitRange updating is effective 12/14/22 17:04:05.95
STEP: Creating a Pod with less than former min resources 12/14/22 17:04:07.958
STEP: Failing to create a Pod with more than max resources 12/14/22 17:04:07.968
STEP: Deleting a LimitRange 12/14/22 17:04:07.971
STEP: Verifying the LimitRange was deleted 12/14/22 17:04:07.98
Dec 14 17:04:12.997: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 12/14/22 17:04:12.998
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:13.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6113" for this suite. 12/14/22 17:04:13.038
------------------------------
• [SLOW TEST] [7.371 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:05.679
    Dec 14 17:04:05.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename limitrange 12/14/22 17:04:05.684
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:05.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:05.732
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 12/14/22 17:04:05.737
    STEP: Setting up watch 12/14/22 17:04:05.737
    STEP: Submitting a LimitRange 12/14/22 17:04:05.844
    STEP: Verifying LimitRange creation was observed 12/14/22 17:04:05.85
    STEP: Fetching the LimitRange to ensure it has proper values 12/14/22 17:04:05.85
    Dec 14 17:04:05.865: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 14 17:04:05.865: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 12/14/22 17:04:05.865
    STEP: Ensuring Pod has resource requirements applied from LimitRange 12/14/22 17:04:05.875
    Dec 14 17:04:05.906: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 14 17:04:05.906: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 12/14/22 17:04:05.906
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/14/22 17:04:05.916
    Dec 14 17:04:05.922: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Dec 14 17:04:05.922: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 12/14/22 17:04:05.922
    STEP: Failing to create a Pod with more than max resources 12/14/22 17:04:05.927
    STEP: Updating a LimitRange 12/14/22 17:04:05.931
    STEP: Verifying LimitRange updating is effective 12/14/22 17:04:05.95
    STEP: Creating a Pod with less than former min resources 12/14/22 17:04:07.958
    STEP: Failing to create a Pod with more than max resources 12/14/22 17:04:07.968
    STEP: Deleting a LimitRange 12/14/22 17:04:07.971
    STEP: Verifying the LimitRange was deleted 12/14/22 17:04:07.98
    Dec 14 17:04:12.997: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 12/14/22 17:04:12.998
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:13.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6113" for this suite. 12/14/22 17:04:13.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:13.068
Dec 14 17:04:13.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename job 12/14/22 17:04:13.076
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:13.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:13.104
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 12/14/22 17:04:13.109
STEP: Ensuring job reaches completions 12/14/22 17:04:13.122
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2748" for this suite. 12/14/22 17:04:25.14
------------------------------
• [SLOW TEST] [12.089 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:13.068
    Dec 14 17:04:13.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename job 12/14/22 17:04:13.076
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:13.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:13.104
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 12/14/22 17:04:13.109
    STEP: Ensuring job reaches completions 12/14/22 17:04:13.122
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2748" for this suite. 12/14/22 17:04:25.14
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:25.161
Dec 14 17:04:25.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 17:04:25.163
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:25.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:25.189
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b26413a5-02a6-4d43-8507-3ecad25f10a9 12/14/22 17:04:25.192
STEP: Creating a pod to test consume secrets 12/14/22 17:04:25.201
Dec 14 17:04:25.217: INFO: Waiting up to 5m0s for pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441" in namespace "secrets-9145" to be "Succeeded or Failed"
Dec 14 17:04:25.223: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688236ms
Dec 14 17:04:27.231: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013988043s
Dec 14 17:04:29.232: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014344003s
STEP: Saw pod success 12/14/22 17:04:29.232
Dec 14 17:04:29.232: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441" satisfied condition "Succeeded or Failed"
Dec 14 17:04:29.237: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 container secret-volume-test: <nil>
STEP: delete the pod 12/14/22 17:04:29.246
Dec 14 17:04:29.267: INFO: Waiting for pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 to disappear
Dec 14 17:04:29.271: INFO: Pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:29.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9145" for this suite. 12/14/22 17:04:29.279
------------------------------
• [4.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:25.161
    Dec 14 17:04:25.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 17:04:25.163
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:25.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:25.189
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b26413a5-02a6-4d43-8507-3ecad25f10a9 12/14/22 17:04:25.192
    STEP: Creating a pod to test consume secrets 12/14/22 17:04:25.201
    Dec 14 17:04:25.217: INFO: Waiting up to 5m0s for pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441" in namespace "secrets-9145" to be "Succeeded or Failed"
    Dec 14 17:04:25.223: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688236ms
    Dec 14 17:04:27.231: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013988043s
    Dec 14 17:04:29.232: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014344003s
    STEP: Saw pod success 12/14/22 17:04:29.232
    Dec 14 17:04:29.232: INFO: Pod "pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441" satisfied condition "Succeeded or Failed"
    Dec 14 17:04:29.237: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 container secret-volume-test: <nil>
    STEP: delete the pod 12/14/22 17:04:29.246
    Dec 14 17:04:29.267: INFO: Waiting for pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 to disappear
    Dec 14 17:04:29.271: INFO: Pod pod-secrets-f4286aab-d365-4e75-aed2-f27be2b2d441 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:29.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9145" for this suite. 12/14/22 17:04:29.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:29.296
Dec 14 17:04:29.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename tables 12/14/22 17:04:29.298
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:29.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:29.331
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Dec 14 17:04:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2497" for this suite. 12/14/22 17:04:29.345
------------------------------
• [0.057 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:29.296
    Dec 14 17:04:29.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename tables 12/14/22 17:04:29.298
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:29.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:29.331
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:04:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2497" for this suite. 12/14/22 17:04:29.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:04:29.355
Dec 14 17:04:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 17:04:29.358
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:29.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:29.408
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 12/14/22 17:04:29.413
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2482;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2482;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +notcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_udp@PTR;check="$$(dig +tcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_tcp@PTR;sleep 1; done
 12/14/22 17:04:29.447
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2482;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2482;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +notcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_udp@PTR;check="$$(dig +tcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_tcp@PTR;sleep 1; done
 12/14/22 17:04:29.447
STEP: creating a pod to probe DNS 12/14/22 17:04:29.447
STEP: submitting the pod to kubernetes 12/14/22 17:04:29.447
Dec 14 17:04:29.472: INFO: Waiting up to 15m0s for pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905" in namespace "dns-2482" to be "running"
Dec 14 17:04:29.487: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905": Phase="Pending", Reason="", readiness=false. Elapsed: 14.835162ms
Dec 14 17:04:31.495: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905": Phase="Running", Reason="", readiness=true. Elapsed: 2.023417426s
Dec 14 17:04:31.496: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905" satisfied condition "running"
STEP: retrieving the pod 12/14/22 17:04:31.496
STEP: looking for the results for each expected name from probers 12/14/22 17:04:31.502
Dec 14 17:04:31.512: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.526: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.533: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.561: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.572: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.598: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.610: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.619: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.641: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.645: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:31.674: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:04:36.687: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.697: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.703: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.710: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.715: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.719: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.750: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.757: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.764: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.769: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.775: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:36.805: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:04:41.684: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.689: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.693: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.705: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.739: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.745: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.751: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.761: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:41.809: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:04:46.691: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.699: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.710: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.719: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.734: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.809: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.841: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.848: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:46.889: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:04:51.691: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.698: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.703: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.709: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.714: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.718: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.755: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.760: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.764: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.785: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.791: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:51.818: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:04:56.682: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.691: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.761: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.778: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.795: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.803: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:04:56.842: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:05:01.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.733: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.738: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.742: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.747: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.751: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
Dec 14 17:05:01.789: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

Dec 14 17:05:06.802: INFO: DNS probes using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 succeeded

STEP: deleting the pod 12/14/22 17:05:06.802
STEP: deleting the test service 12/14/22 17:05:06.83
STEP: deleting the test headless service 12/14/22 17:05:06.936
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 17:05:06.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2482" for this suite. 12/14/22 17:05:07.002
------------------------------
• [SLOW TEST] [37.680 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:04:29.355
    Dec 14 17:04:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 17:04:29.358
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:04:29.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:04:29.408
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 12/14/22 17:04:29.413
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2482;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2482;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +notcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_udp@PTR;check="$$(dig +tcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_tcp@PTR;sleep 1; done
     12/14/22 17:04:29.447
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2482;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2482;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2482.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2482.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2482.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2482.svc;check="$$(dig +notcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_udp@PTR;check="$$(dig +tcp +noall +answer +search 235.35.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.35.235_tcp@PTR;sleep 1; done
     12/14/22 17:04:29.447
    STEP: creating a pod to probe DNS 12/14/22 17:04:29.447
    STEP: submitting the pod to kubernetes 12/14/22 17:04:29.447
    Dec 14 17:04:29.472: INFO: Waiting up to 15m0s for pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905" in namespace "dns-2482" to be "running"
    Dec 14 17:04:29.487: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905": Phase="Pending", Reason="", readiness=false. Elapsed: 14.835162ms
    Dec 14 17:04:31.495: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905": Phase="Running", Reason="", readiness=true. Elapsed: 2.023417426s
    Dec 14 17:04:31.496: INFO: Pod "dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 17:04:31.496
    STEP: looking for the results for each expected name from probers 12/14/22 17:04:31.502
    Dec 14 17:04:31.512: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.519: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.526: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.533: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.561: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.572: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.598: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.610: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.619: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.641: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.645: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:31.674: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc wheezy_udp@_http._tcp.dns-test-service.dns-2482.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:04:36.687: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.697: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.703: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.710: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.715: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.719: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.750: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.757: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.764: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.769: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.775: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:36.805: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:04:41.684: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.689: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.693: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.705: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.739: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.745: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.751: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.761: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:41.809: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:04:46.691: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.699: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.710: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.719: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.734: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.809: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.829: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.841: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.848: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:46.889: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:04:51.691: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.698: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.703: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.709: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.714: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.718: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.755: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.760: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.764: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.785: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.791: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:51.818: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:04:56.682: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.691: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.761: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.778: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.795: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.803: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:04:56.842: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2482 wheezy_tcp@dns-test-service.dns-2482 wheezy_udp@dns-test-service.dns-2482.svc wheezy_tcp@dns-test-service.dns-2482.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:05:01.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.733: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.738: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.742: INFO: Unable to read jessie_udp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.747: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482 from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.751: INFO: Unable to read jessie_udp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2482.svc from pod dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905: the server could not find the requested resource (get pods dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905)
    Dec 14 17:05:01.789: INFO: Lookups using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 failed for: [wheezy_udp@dns-test-service jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2482 jessie_tcp@dns-test-service.dns-2482 jessie_udp@dns-test-service.dns-2482.svc jessie_tcp@dns-test-service.dns-2482.svc]

    Dec 14 17:05:06.802: INFO: DNS probes using dns-2482/dns-test-555d49c7-8e83-456e-bb84-1a5fc7558905 succeeded

    STEP: deleting the pod 12/14/22 17:05:06.802
    STEP: deleting the test service 12/14/22 17:05:06.83
    STEP: deleting the test headless service 12/14/22 17:05:06.936
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:05:06.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2482" for this suite. 12/14/22 17:05:07.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:05:07.041
Dec 14 17:05:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename svc-latency 12/14/22 17:05:07.048
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:07.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:07.099
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Dec 14 17:05:07.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6453 12/14/22 17:05:07.107
I1214 17:05:07.128734      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6453, replica count: 1
I1214 17:05:08.182788      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 17:05:09.183535      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 17:05:09.304: INFO: Created: latency-svc-qkh9r
Dec 14 17:05:09.320: INFO: Got endpoints: latency-svc-qkh9r [36.57535ms]
Dec 14 17:05:09.358: INFO: Created: latency-svc-tfw95
Dec 14 17:05:09.389: INFO: Got endpoints: latency-svc-tfw95 [68.017264ms]
Dec 14 17:05:09.403: INFO: Created: latency-svc-pdp9w
Dec 14 17:05:09.412: INFO: Got endpoints: latency-svc-pdp9w [90.376274ms]
Dec 14 17:05:09.417: INFO: Created: latency-svc-mm475
Dec 14 17:05:09.435: INFO: Created: latency-svc-2s9j5
Dec 14 17:05:09.436: INFO: Got endpoints: latency-svc-mm475 [114.62164ms]
Dec 14 17:05:09.458: INFO: Created: latency-svc-cn8vf
Dec 14 17:05:09.465: INFO: Got endpoints: latency-svc-2s9j5 [143.110476ms]
Dec 14 17:05:09.476: INFO: Got endpoints: latency-svc-cn8vf [153.678831ms]
Dec 14 17:05:09.628: INFO: Created: latency-svc-d4tb6
Dec 14 17:05:09.628: INFO: Created: latency-svc-xpd9s
Dec 14 17:05:09.628: INFO: Created: latency-svc-fcp85
Dec 14 17:05:09.628: INFO: Created: latency-svc-w2rrp
Dec 14 17:05:09.629: INFO: Created: latency-svc-rmbjb
Dec 14 17:05:09.637: INFO: Created: latency-svc-zvnlq
Dec 14 17:05:09.637: INFO: Created: latency-svc-fd2hq
Dec 14 17:05:09.638: INFO: Created: latency-svc-zrqsp
Dec 14 17:05:09.638: INFO: Created: latency-svc-7dpl7
Dec 14 17:05:09.638: INFO: Created: latency-svc-shv2z
Dec 14 17:05:09.672: INFO: Created: latency-svc-p8rwh
Dec 14 17:05:09.672: INFO: Created: latency-svc-m9mpp
Dec 14 17:05:09.673: INFO: Created: latency-svc-zmlcp
Dec 14 17:05:09.674: INFO: Created: latency-svc-wxgkz
Dec 14 17:05:09.674: INFO: Created: latency-svc-rz8wr
Dec 14 17:05:09.683: INFO: Got endpoints: latency-svc-d4tb6 [359.942719ms]
Dec 14 17:05:09.692: INFO: Got endpoints: latency-svc-xpd9s [367.802015ms]
Dec 14 17:05:09.693: INFO: Got endpoints: latency-svc-w2rrp [370.446015ms]
Dec 14 17:05:09.725: INFO: Got endpoints: latency-svc-rmbjb [335.662029ms]
Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-p8rwh [402.000298ms]
Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-zmlcp [401.249463ms]
Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-m9mpp [402.857808ms]
Dec 14 17:05:09.740: INFO: Created: latency-svc-pfxnf
Dec 14 17:05:09.773: INFO: Created: latency-svc-hdncg
Dec 14 17:05:09.780: INFO: Got endpoints: latency-svc-wxgkz [303.377495ms]
Dec 14 17:05:09.780: INFO: Got endpoints: latency-svc-rz8wr [458.143616ms]
Dec 14 17:05:09.790: INFO: Got endpoints: latency-svc-fcp85 [324.600272ms]
Dec 14 17:05:09.801: INFO: Got endpoints: latency-svc-zrqsp [388.705624ms]
Dec 14 17:05:09.802: INFO: Got endpoints: latency-svc-shv2z [477.830225ms]
Dec 14 17:05:09.802: INFO: Got endpoints: latency-svc-7dpl7 [477.715828ms]
Dec 14 17:05:09.824: INFO: Created: latency-svc-5rv4v
Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-pfxnf [140.684501ms]
Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-fd2hq [388.030664ms]
Dec 14 17:05:09.825: INFO: Got endpoints: latency-svc-hdncg [133.393946ms]
Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-zvnlq [501.437859ms]
Dec 14 17:05:09.845: INFO: Created: latency-svc-8k9fh
Dec 14 17:05:09.846: INFO: Got endpoints: latency-svc-5rv4v [152.369419ms]
Dec 14 17:05:09.867: INFO: Got endpoints: latency-svc-8k9fh [141.142976ms]
Dec 14 17:05:09.976: INFO: Created: latency-svc-tfch2
Dec 14 17:05:09.979: INFO: Created: latency-svc-p4lr9
Dec 14 17:05:09.998: INFO: Created: latency-svc-cdjkm
Dec 14 17:05:09.998: INFO: Created: latency-svc-nzs5g
Dec 14 17:05:09.998: INFO: Created: latency-svc-nr8bc
Dec 14 17:05:09.999: INFO: Created: latency-svc-5z7v9
Dec 14 17:05:09.999: INFO: Created: latency-svc-sfrsq
Dec 14 17:05:10.003: INFO: Created: latency-svc-nj85b
Dec 14 17:05:10.007: INFO: Created: latency-svc-j5gsp
Dec 14 17:05:10.007: INFO: Created: latency-svc-m6fq8
Dec 14 17:05:10.007: INFO: Created: latency-svc-4hg9n
Dec 14 17:05:10.008: INFO: Created: latency-svc-qd6rj
Dec 14 17:05:10.008: INFO: Created: latency-svc-jtg67
Dec 14 17:05:10.009: INFO: Created: latency-svc-jb2ck
Dec 14 17:05:10.016: INFO: Created: latency-svc-j7npc
Dec 14 17:05:10.034: INFO: Got endpoints: latency-svc-tfch2 [254.459065ms]
Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-cdjkm [241.654766ms]
Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-p4lr9 [318.298984ms]
Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-nzs5g [317.939338ms]
Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-sfrsq [177.333706ms]
Dec 14 17:05:10.075: INFO: Got endpoints: latency-svc-j5gsp [285.306486ms]
Dec 14 17:05:10.076: INFO: Created: latency-svc-87ksg
Dec 14 17:05:10.082: INFO: Got endpoints: latency-svc-nr8bc [301.983823ms]
Dec 14 17:05:10.100: INFO: Got endpoints: latency-svc-qd6rj [374.562354ms]
Dec 14 17:05:10.101: INFO: Got endpoints: latency-svc-jtg67 [276.56645ms]
Dec 14 17:05:10.119: INFO: Created: latency-svc-wqmzr
Dec 14 17:05:10.133: INFO: Got endpoints: latency-svc-j7npc [332.247148ms]
Dec 14 17:05:10.180: INFO: Created: latency-svc-mznwq
Dec 14 17:05:10.191: INFO: Got endpoints: latency-svc-nj85b [388.596388ms]
Dec 14 17:05:10.192: INFO: Got endpoints: latency-svc-4hg9n [366.259539ms]
Dec 14 17:05:10.192: INFO: Got endpoints: latency-svc-jb2ck [366.161775ms]
Dec 14 17:05:10.311: INFO: Created: latency-svc-7kj6x
Dec 14 17:05:10.313: INFO: Got endpoints: latency-svc-m6fq8 [488.527081ms]
Dec 14 17:05:10.314: INFO: Got endpoints: latency-svc-5z7v9 [468.329426ms]
Dec 14 17:05:10.315: INFO: Got endpoints: latency-svc-wqmzr [271.194433ms]
Dec 14 17:05:10.316: INFO: Got endpoints: latency-svc-87ksg [281.008844ms]
Dec 14 17:05:10.321: INFO: Got endpoints: latency-svc-mznwq [277.125819ms]
Dec 14 17:05:10.358: INFO: Got endpoints: latency-svc-7kj6x [313.782059ms]
Dec 14 17:05:10.588: INFO: Created: latency-svc-hcn8k
Dec 14 17:05:10.588: INFO: Created: latency-svc-zg7cf
Dec 14 17:05:10.590: INFO: Created: latency-svc-mwmgd
Dec 14 17:05:10.591: INFO: Created: latency-svc-x65wc
Dec 14 17:05:10.594: INFO: Created: latency-svc-67g8b
Dec 14 17:05:10.595: INFO: Created: latency-svc-l9jxq
Dec 14 17:05:10.595: INFO: Created: latency-svc-bcm55
Dec 14 17:05:10.596: INFO: Created: latency-svc-fdc9l
Dec 14 17:05:10.598: INFO: Created: latency-svc-cc9fz
Dec 14 17:05:10.598: INFO: Created: latency-svc-jmv2s
Dec 14 17:05:10.627: INFO: Created: latency-svc-zq2ms
Dec 14 17:05:10.627: INFO: Created: latency-svc-zvllj
Dec 14 17:05:10.628: INFO: Created: latency-svc-7sff5
Dec 14 17:05:10.628: INFO: Created: latency-svc-hmxd7
Dec 14 17:05:10.628: INFO: Created: latency-svc-ggnxd
Dec 14 17:05:10.675: INFO: Got endpoints: latency-svc-hcn8k [629.596211ms]
Dec 14 17:05:10.698: INFO: Got endpoints: latency-svc-x65wc [507.334744ms]
Dec 14 17:05:10.698: INFO: Got endpoints: latency-svc-zg7cf [376.929226ms]
Dec 14 17:05:10.699: INFO: Got endpoints: latency-svc-mwmgd [597.993451ms]
Dec 14 17:05:10.706: INFO: Got endpoints: latency-svc-7sff5 [391.444916ms]
Dec 14 17:05:10.706: INFO: Got endpoints: latency-svc-zq2ms [347.661996ms]
Dec 14 17:05:10.745: INFO: Got endpoints: latency-svc-ggnxd [611.407063ms]
Dec 14 17:05:10.746: INFO: Got endpoints: latency-svc-zvllj [553.867893ms]
Dec 14 17:05:10.747: INFO: Got endpoints: latency-svc-hmxd7 [664.67637ms]
Dec 14 17:05:10.756: INFO: Got endpoints: latency-svc-jmv2s [440.181962ms]
Dec 14 17:05:10.756: INFO: Got endpoints: latency-svc-cc9fz [681.039437ms]
Dec 14 17:05:10.789: INFO: Got endpoints: latency-svc-fdc9l [688.549462ms]
Dec 14 17:05:10.790: INFO: Created: latency-svc-42xnt
Dec 14 17:05:10.801: INFO: Got endpoints: latency-svc-bcm55 [487.58162ms]
Dec 14 17:05:10.806: INFO: Got endpoints: latency-svc-l9jxq [489.94005ms]
Dec 14 17:05:10.813: INFO: Got endpoints: latency-svc-67g8b [621.181307ms]
Dec 14 17:05:10.819: INFO: Created: latency-svc-65crv
Dec 14 17:05:10.819: INFO: Created: latency-svc-vcwxm
Dec 14 17:05:10.828: INFO: Created: latency-svc-j5zm9
Dec 14 17:05:10.838: INFO: Got endpoints: latency-svc-42xnt [163.506824ms]
Dec 14 17:05:10.848: INFO: Created: latency-svc-n8vtp
Dec 14 17:05:10.856: INFO: Created: latency-svc-thtkm
Dec 14 17:05:10.869: INFO: Created: latency-svc-cpwrb
Dec 14 17:05:10.874: INFO: Created: latency-svc-hr7zw
Dec 14 17:05:10.890: INFO: Got endpoints: latency-svc-vcwxm [191.090955ms]
Dec 14 17:05:10.891: INFO: Created: latency-svc-k2pvg
Dec 14 17:05:10.899: INFO: Created: latency-svc-9gpr8
Dec 14 17:05:10.907: INFO: Created: latency-svc-5mjtv
Dec 14 17:05:10.926: INFO: Created: latency-svc-zp4d6
Dec 14 17:05:10.946: INFO: Got endpoints: latency-svc-65crv [240.307756ms]
Dec 14 17:05:10.956: INFO: Created: latency-svc-zf9jx
Dec 14 17:05:10.966: INFO: Created: latency-svc-qb5zw
Dec 14 17:05:10.985: INFO: Created: latency-svc-ctl7t
Dec 14 17:05:10.994: INFO: Got endpoints: latency-svc-j5zm9 [287.607473ms]
Dec 14 17:05:11.018: INFO: Created: latency-svc-dwqr4
Dec 14 17:05:11.031: INFO: Created: latency-svc-qsk5s
Dec 14 17:05:11.041: INFO: Got endpoints: latency-svc-n8vtp [342.575366ms]
Dec 14 17:05:11.058: INFO: Created: latency-svc-f84pg
Dec 14 17:05:11.066: INFO: Created: latency-svc-bgzfl
Dec 14 17:05:11.110: INFO: Got endpoints: latency-svc-thtkm [411.077347ms]
Dec 14 17:05:11.145: INFO: Created: latency-svc-wfx6l
Dec 14 17:05:11.152: INFO: Got endpoints: latency-svc-cpwrb [407.052205ms]
Dec 14 17:05:11.193: INFO: Created: latency-svc-44j2q
Dec 14 17:05:11.197: INFO: Created: latency-svc-bdtqx
Dec 14 17:05:11.206: INFO: Got endpoints: latency-svc-hr7zw [460.090545ms]
Dec 14 17:05:11.225: INFO: Created: latency-svc-rc8pz
Dec 14 17:05:11.238: INFO: Got endpoints: latency-svc-k2pvg [491.127922ms]
Dec 14 17:05:11.254: INFO: Created: latency-svc-qxbw9
Dec 14 17:05:11.284: INFO: Got endpoints: latency-svc-9gpr8 [526.94737ms]
Dec 14 17:05:11.306: INFO: Created: latency-svc-g6njk
Dec 14 17:05:11.338: INFO: Got endpoints: latency-svc-5mjtv [582.221468ms]
Dec 14 17:05:11.355: INFO: Created: latency-svc-ksqjl
Dec 14 17:05:11.385: INFO: Got endpoints: latency-svc-zp4d6 [595.960004ms]
Dec 14 17:05:11.402: INFO: Created: latency-svc-z4ccz
Dec 14 17:05:11.438: INFO: Got endpoints: latency-svc-zf9jx [636.732513ms]
Dec 14 17:05:11.456: INFO: Created: latency-svc-9r7ns
Dec 14 17:05:11.485: INFO: Got endpoints: latency-svc-qb5zw [679.346707ms]
Dec 14 17:05:11.498: INFO: Created: latency-svc-kjc6d
Dec 14 17:05:11.534: INFO: Got endpoints: latency-svc-ctl7t [720.868975ms]
Dec 14 17:05:11.548: INFO: Created: latency-svc-95thr
Dec 14 17:05:11.583: INFO: Got endpoints: latency-svc-dwqr4 [744.72119ms]
Dec 14 17:05:11.608: INFO: Created: latency-svc-5zrvb
Dec 14 17:05:11.641: INFO: Got endpoints: latency-svc-qsk5s [750.906423ms]
Dec 14 17:05:11.658: INFO: Created: latency-svc-76p5f
Dec 14 17:05:11.688: INFO: Got endpoints: latency-svc-f84pg [741.614131ms]
Dec 14 17:05:11.709: INFO: Created: latency-svc-f8m7b
Dec 14 17:05:11.735: INFO: Got endpoints: latency-svc-bgzfl [740.367607ms]
Dec 14 17:05:11.758: INFO: Created: latency-svc-nfbhc
Dec 14 17:05:11.786: INFO: Got endpoints: latency-svc-wfx6l [745.476846ms]
Dec 14 17:05:11.805: INFO: Created: latency-svc-h8gc2
Dec 14 17:05:11.837: INFO: Got endpoints: latency-svc-bdtqx [726.935962ms]
Dec 14 17:05:11.858: INFO: Created: latency-svc-lszsl
Dec 14 17:05:11.886: INFO: Got endpoints: latency-svc-44j2q [734.051649ms]
Dec 14 17:05:11.903: INFO: Created: latency-svc-gq8rh
Dec 14 17:05:11.937: INFO: Got endpoints: latency-svc-rc8pz [730.554465ms]
Dec 14 17:05:11.950: INFO: Created: latency-svc-rlhrg
Dec 14 17:05:11.988: INFO: Got endpoints: latency-svc-qxbw9 [749.786226ms]
Dec 14 17:05:12.002: INFO: Created: latency-svc-bkkhd
Dec 14 17:05:12.043: INFO: Got endpoints: latency-svc-g6njk [759.715556ms]
Dec 14 17:05:12.064: INFO: Created: latency-svc-7rr6f
Dec 14 17:05:12.089: INFO: Got endpoints: latency-svc-ksqjl [750.605684ms]
Dec 14 17:05:12.104: INFO: Created: latency-svc-sv4n8
Dec 14 17:05:12.134: INFO: Got endpoints: latency-svc-z4ccz [748.690847ms]
Dec 14 17:05:12.151: INFO: Created: latency-svc-4nzpz
Dec 14 17:05:12.218: INFO: Got endpoints: latency-svc-9r7ns [779.613747ms]
Dec 14 17:05:12.247: INFO: Created: latency-svc-nkzz7
Dec 14 17:05:12.251: INFO: Got endpoints: latency-svc-kjc6d [766.042997ms]
Dec 14 17:05:12.272: INFO: Created: latency-svc-wzbzp
Dec 14 17:05:12.295: INFO: Got endpoints: latency-svc-95thr [760.712184ms]
Dec 14 17:05:12.326: INFO: Created: latency-svc-jqx44
Dec 14 17:05:12.342: INFO: Got endpoints: latency-svc-5zrvb [758.467773ms]
Dec 14 17:05:12.383: INFO: Created: latency-svc-7p6mt
Dec 14 17:05:12.394: INFO: Got endpoints: latency-svc-76p5f [752.746404ms]
Dec 14 17:05:12.416: INFO: Created: latency-svc-sb4j2
Dec 14 17:05:12.441: INFO: Got endpoints: latency-svc-f8m7b [753.001451ms]
Dec 14 17:05:12.472: INFO: Created: latency-svc-z5qsp
Dec 14 17:05:12.512: INFO: Got endpoints: latency-svc-nfbhc [776.824994ms]
Dec 14 17:05:12.531: INFO: Created: latency-svc-l6zsk
Dec 14 17:05:12.540: INFO: Got endpoints: latency-svc-h8gc2 [753.775166ms]
Dec 14 17:05:12.566: INFO: Created: latency-svc-tkt5v
Dec 14 17:05:12.585: INFO: Got endpoints: latency-svc-lszsl [747.784652ms]
Dec 14 17:05:12.600: INFO: Created: latency-svc-w9wx5
Dec 14 17:05:12.638: INFO: Got endpoints: latency-svc-gq8rh [752.097929ms]
Dec 14 17:05:12.661: INFO: Created: latency-svc-z9nrp
Dec 14 17:05:12.687: INFO: Got endpoints: latency-svc-rlhrg [750.288994ms]
Dec 14 17:05:12.706: INFO: Created: latency-svc-bcwfm
Dec 14 17:05:12.738: INFO: Got endpoints: latency-svc-bkkhd [749.515087ms]
Dec 14 17:05:12.752: INFO: Created: latency-svc-dcf77
Dec 14 17:05:12.794: INFO: Got endpoints: latency-svc-7rr6f [750.680384ms]
Dec 14 17:05:12.812: INFO: Created: latency-svc-kp4b6
Dec 14 17:05:12.836: INFO: Got endpoints: latency-svc-sv4n8 [747.284906ms]
Dec 14 17:05:12.854: INFO: Created: latency-svc-h4sxr
Dec 14 17:05:12.883: INFO: Got endpoints: latency-svc-4nzpz [748.64005ms]
Dec 14 17:05:12.899: INFO: Created: latency-svc-rxl8n
Dec 14 17:05:12.934: INFO: Got endpoints: latency-svc-nkzz7 [716.126837ms]
Dec 14 17:05:12.952: INFO: Created: latency-svc-phg6j
Dec 14 17:05:12.994: INFO: Got endpoints: latency-svc-wzbzp [742.368323ms]
Dec 14 17:05:13.010: INFO: Created: latency-svc-r7pxs
Dec 14 17:05:13.035: INFO: Got endpoints: latency-svc-jqx44 [740.233626ms]
Dec 14 17:05:13.050: INFO: Created: latency-svc-5jss9
Dec 14 17:05:13.083: INFO: Got endpoints: latency-svc-7p6mt [741.273181ms]
Dec 14 17:05:13.095: INFO: Created: latency-svc-mxprl
Dec 14 17:05:13.134: INFO: Got endpoints: latency-svc-sb4j2 [740.316916ms]
Dec 14 17:05:13.151: INFO: Created: latency-svc-9sx7h
Dec 14 17:05:13.185: INFO: Got endpoints: latency-svc-z5qsp [744.216815ms]
Dec 14 17:05:13.199: INFO: Created: latency-svc-2gk4h
Dec 14 17:05:13.243: INFO: Got endpoints: latency-svc-l6zsk [731.527234ms]
Dec 14 17:05:13.257: INFO: Created: latency-svc-kvrjc
Dec 14 17:05:13.286: INFO: Got endpoints: latency-svc-tkt5v [745.317301ms]
Dec 14 17:05:13.310: INFO: Created: latency-svc-f8vpn
Dec 14 17:05:13.344: INFO: Got endpoints: latency-svc-w9wx5 [758.546062ms]
Dec 14 17:05:13.365: INFO: Created: latency-svc-rbzsn
Dec 14 17:05:13.384: INFO: Got endpoints: latency-svc-z9nrp [745.936072ms]
Dec 14 17:05:13.404: INFO: Created: latency-svc-975cs
Dec 14 17:05:13.434: INFO: Got endpoints: latency-svc-bcwfm [747.353296ms]
Dec 14 17:05:13.453: INFO: Created: latency-svc-6smzh
Dec 14 17:05:13.486: INFO: Got endpoints: latency-svc-dcf77 [748.083095ms]
Dec 14 17:05:13.502: INFO: Created: latency-svc-kj6sv
Dec 14 17:05:13.536: INFO: Got endpoints: latency-svc-kp4b6 [741.582851ms]
Dec 14 17:05:13.550: INFO: Created: latency-svc-2ds7k
Dec 14 17:05:13.584: INFO: Got endpoints: latency-svc-h4sxr [747.804973ms]
Dec 14 17:05:13.599: INFO: Created: latency-svc-xgnhq
Dec 14 17:05:13.636: INFO: Got endpoints: latency-svc-rxl8n [753.292853ms]
Dec 14 17:05:13.657: INFO: Created: latency-svc-k7xbh
Dec 14 17:05:13.684: INFO: Got endpoints: latency-svc-phg6j [749.874728ms]
Dec 14 17:05:13.701: INFO: Created: latency-svc-mc9kj
Dec 14 17:05:13.733: INFO: Got endpoints: latency-svc-r7pxs [739.209428ms]
Dec 14 17:05:13.752: INFO: Created: latency-svc-hfjzj
Dec 14 17:05:13.783: INFO: Got endpoints: latency-svc-5jss9 [748.25354ms]
Dec 14 17:05:13.804: INFO: Created: latency-svc-bwctt
Dec 14 17:05:13.834: INFO: Got endpoints: latency-svc-mxprl [751.336334ms]
Dec 14 17:05:13.856: INFO: Created: latency-svc-xl6qr
Dec 14 17:05:13.886: INFO: Got endpoints: latency-svc-9sx7h [751.900985ms]
Dec 14 17:05:13.909: INFO: Created: latency-svc-ghbpn
Dec 14 17:05:13.935: INFO: Got endpoints: latency-svc-2gk4h [749.515185ms]
Dec 14 17:05:13.953: INFO: Created: latency-svc-f9vht
Dec 14 17:05:13.992: INFO: Got endpoints: latency-svc-kvrjc [749.024641ms]
Dec 14 17:05:14.007: INFO: Created: latency-svc-xvzph
Dec 14 17:05:14.039: INFO: Got endpoints: latency-svc-f8vpn [752.844229ms]
Dec 14 17:05:14.052: INFO: Created: latency-svc-mgfnx
Dec 14 17:05:14.087: INFO: Got endpoints: latency-svc-rbzsn [743.622064ms]
Dec 14 17:05:14.104: INFO: Created: latency-svc-md86c
Dec 14 17:05:14.138: INFO: Got endpoints: latency-svc-975cs [753.478716ms]
Dec 14 17:05:14.150: INFO: Created: latency-svc-d7zqw
Dec 14 17:05:14.188: INFO: Got endpoints: latency-svc-6smzh [753.779464ms]
Dec 14 17:05:14.202: INFO: Created: latency-svc-pqmnt
Dec 14 17:05:14.238: INFO: Got endpoints: latency-svc-kj6sv [751.790352ms]
Dec 14 17:05:14.255: INFO: Created: latency-svc-nfqh2
Dec 14 17:05:14.290: INFO: Got endpoints: latency-svc-2ds7k [754.204882ms]
Dec 14 17:05:14.307: INFO: Created: latency-svc-qrlcb
Dec 14 17:05:14.344: INFO: Got endpoints: latency-svc-xgnhq [760.090759ms]
Dec 14 17:05:14.365: INFO: Created: latency-svc-458tt
Dec 14 17:05:14.387: INFO: Got endpoints: latency-svc-k7xbh [750.335754ms]
Dec 14 17:05:14.420: INFO: Created: latency-svc-5xw4w
Dec 14 17:05:14.435: INFO: Got endpoints: latency-svc-mc9kj [751.611473ms]
Dec 14 17:05:14.469: INFO: Created: latency-svc-q8fsg
Dec 14 17:05:14.487: INFO: Got endpoints: latency-svc-hfjzj [753.413733ms]
Dec 14 17:05:14.521: INFO: Created: latency-svc-4msjd
Dec 14 17:05:14.551: INFO: Got endpoints: latency-svc-bwctt [767.823928ms]
Dec 14 17:05:14.570: INFO: Created: latency-svc-sxwqz
Dec 14 17:05:14.583: INFO: Got endpoints: latency-svc-xl6qr [748.720235ms]
Dec 14 17:05:14.612: INFO: Created: latency-svc-b5x96
Dec 14 17:05:14.637: INFO: Got endpoints: latency-svc-ghbpn [750.518637ms]
Dec 14 17:05:14.654: INFO: Created: latency-svc-zbgtm
Dec 14 17:05:14.702: INFO: Got endpoints: latency-svc-f9vht [766.76636ms]
Dec 14 17:05:14.731: INFO: Created: latency-svc-clkkw
Dec 14 17:05:14.739: INFO: Got endpoints: latency-svc-xvzph [746.806949ms]
Dec 14 17:05:14.757: INFO: Created: latency-svc-l7ccp
Dec 14 17:05:14.793: INFO: Got endpoints: latency-svc-mgfnx [754.03191ms]
Dec 14 17:05:14.807: INFO: Created: latency-svc-5hfkz
Dec 14 17:05:14.835: INFO: Got endpoints: latency-svc-md86c [747.873182ms]
Dec 14 17:05:14.849: INFO: Created: latency-svc-wr6f6
Dec 14 17:05:14.886: INFO: Got endpoints: latency-svc-d7zqw [747.515982ms]
Dec 14 17:05:14.903: INFO: Created: latency-svc-drzgh
Dec 14 17:05:14.946: INFO: Got endpoints: latency-svc-pqmnt [757.940694ms]
Dec 14 17:05:14.990: INFO: Got endpoints: latency-svc-nfqh2 [751.714612ms]
Dec 14 17:05:14.991: INFO: Created: latency-svc-q8zn4
Dec 14 17:05:15.017: INFO: Created: latency-svc-6s2t2
Dec 14 17:05:15.033: INFO: Got endpoints: latency-svc-qrlcb [742.736128ms]
Dec 14 17:05:15.053: INFO: Created: latency-svc-z7xkn
Dec 14 17:05:15.086: INFO: Got endpoints: latency-svc-458tt [741.465157ms]
Dec 14 17:05:15.102: INFO: Created: latency-svc-2j478
Dec 14 17:05:15.136: INFO: Got endpoints: latency-svc-5xw4w [748.772562ms]
Dec 14 17:05:15.151: INFO: Created: latency-svc-zs9fw
Dec 14 17:05:15.185: INFO: Got endpoints: latency-svc-q8fsg [749.36056ms]
Dec 14 17:05:15.201: INFO: Created: latency-svc-2t6r9
Dec 14 17:05:15.235: INFO: Got endpoints: latency-svc-4msjd [748.598074ms]
Dec 14 17:05:15.257: INFO: Created: latency-svc-f54qj
Dec 14 17:05:15.287: INFO: Got endpoints: latency-svc-sxwqz [735.69187ms]
Dec 14 17:05:15.304: INFO: Created: latency-svc-qrqrh
Dec 14 17:05:15.343: INFO: Got endpoints: latency-svc-b5x96 [759.188212ms]
Dec 14 17:05:15.359: INFO: Created: latency-svc-8tn4h
Dec 14 17:05:15.385: INFO: Got endpoints: latency-svc-zbgtm [748.47153ms]
Dec 14 17:05:15.407: INFO: Created: latency-svc-l7wjj
Dec 14 17:05:15.436: INFO: Got endpoints: latency-svc-clkkw [734.382569ms]
Dec 14 17:05:15.465: INFO: Created: latency-svc-mdm6t
Dec 14 17:05:15.490: INFO: Got endpoints: latency-svc-l7ccp [750.91114ms]
Dec 14 17:05:15.507: INFO: Created: latency-svc-vhtxd
Dec 14 17:05:15.534: INFO: Got endpoints: latency-svc-5hfkz [741.45204ms]
Dec 14 17:05:15.556: INFO: Created: latency-svc-62lx4
Dec 14 17:05:15.596: INFO: Got endpoints: latency-svc-wr6f6 [760.494071ms]
Dec 14 17:05:15.619: INFO: Created: latency-svc-8wvx9
Dec 14 17:05:15.640: INFO: Got endpoints: latency-svc-drzgh [753.698109ms]
Dec 14 17:05:15.657: INFO: Created: latency-svc-4jpwj
Dec 14 17:05:15.682: INFO: Got endpoints: latency-svc-q8zn4 [732.057588ms]
Dec 14 17:05:15.712: INFO: Created: latency-svc-6mcz8
Dec 14 17:05:15.734: INFO: Got endpoints: latency-svc-6s2t2 [744.335389ms]
Dec 14 17:05:15.755: INFO: Created: latency-svc-g77w8
Dec 14 17:05:15.791: INFO: Got endpoints: latency-svc-z7xkn [758.287575ms]
Dec 14 17:05:15.806: INFO: Created: latency-svc-785n2
Dec 14 17:05:15.834: INFO: Got endpoints: latency-svc-2j478 [748.214952ms]
Dec 14 17:05:15.861: INFO: Created: latency-svc-xnbxk
Dec 14 17:05:15.887: INFO: Got endpoints: latency-svc-zs9fw [750.970176ms]
Dec 14 17:05:15.918: INFO: Created: latency-svc-v7vpq
Dec 14 17:05:15.935: INFO: Got endpoints: latency-svc-2t6r9 [749.805413ms]
Dec 14 17:05:15.961: INFO: Created: latency-svc-nq9sb
Dec 14 17:05:15.986: INFO: Got endpoints: latency-svc-f54qj [750.257727ms]
Dec 14 17:05:16.000: INFO: Created: latency-svc-4tgj7
Dec 14 17:05:16.033: INFO: Got endpoints: latency-svc-qrqrh [746.09459ms]
Dec 14 17:05:16.070: INFO: Created: latency-svc-cfnqm
Dec 14 17:05:16.086: INFO: Got endpoints: latency-svc-8tn4h [743.326336ms]
Dec 14 17:05:16.099: INFO: Created: latency-svc-4jh7h
Dec 14 17:05:16.137: INFO: Got endpoints: latency-svc-l7wjj [751.318054ms]
Dec 14 17:05:16.153: INFO: Created: latency-svc-s5t8z
Dec 14 17:05:16.187: INFO: Got endpoints: latency-svc-mdm6t [750.913285ms]
Dec 14 17:05:16.204: INFO: Created: latency-svc-zpx7p
Dec 14 17:05:16.239: INFO: Got endpoints: latency-svc-vhtxd [748.620609ms]
Dec 14 17:05:16.262: INFO: Created: latency-svc-9sb59
Dec 14 17:05:16.286: INFO: Got endpoints: latency-svc-62lx4 [751.297315ms]
Dec 14 17:05:16.302: INFO: Created: latency-svc-t65db
Dec 14 17:05:16.333: INFO: Got endpoints: latency-svc-8wvx9 [737.094562ms]
Dec 14 17:05:16.353: INFO: Created: latency-svc-7znhz
Dec 14 17:05:16.386: INFO: Got endpoints: latency-svc-4jpwj [746.781964ms]
Dec 14 17:05:16.403: INFO: Created: latency-svc-w9qtd
Dec 14 17:05:16.442: INFO: Got endpoints: latency-svc-6mcz8 [760.186029ms]
Dec 14 17:05:16.458: INFO: Created: latency-svc-d9bw9
Dec 14 17:05:16.489: INFO: Got endpoints: latency-svc-g77w8 [754.172625ms]
Dec 14 17:05:16.505: INFO: Created: latency-svc-qzcw9
Dec 14 17:05:16.534: INFO: Got endpoints: latency-svc-785n2 [742.583074ms]
Dec 14 17:05:16.559: INFO: Created: latency-svc-jsj2k
Dec 14 17:05:16.586: INFO: Got endpoints: latency-svc-xnbxk [751.498417ms]
Dec 14 17:05:16.609: INFO: Created: latency-svc-4b8k8
Dec 14 17:05:16.643: INFO: Got endpoints: latency-svc-v7vpq [755.739032ms]
Dec 14 17:05:16.658: INFO: Created: latency-svc-z2qnv
Dec 14 17:05:16.690: INFO: Got endpoints: latency-svc-nq9sb [754.740603ms]
Dec 14 17:05:16.709: INFO: Created: latency-svc-l6hnv
Dec 14 17:05:16.734: INFO: Got endpoints: latency-svc-4tgj7 [748.31746ms]
Dec 14 17:05:16.751: INFO: Created: latency-svc-4qxmm
Dec 14 17:05:16.783: INFO: Got endpoints: latency-svc-cfnqm [749.606603ms]
Dec 14 17:05:16.799: INFO: Created: latency-svc-trvsw
Dec 14 17:05:16.833: INFO: Got endpoints: latency-svc-4jh7h [746.989519ms]
Dec 14 17:05:16.851: INFO: Created: latency-svc-x7xnf
Dec 14 17:05:16.882: INFO: Got endpoints: latency-svc-s5t8z [745.609485ms]
Dec 14 17:05:16.920: INFO: Created: latency-svc-jf2b5
Dec 14 17:05:16.936: INFO: Got endpoints: latency-svc-zpx7p [748.158693ms]
Dec 14 17:05:16.950: INFO: Created: latency-svc-b72tr
Dec 14 17:05:16.984: INFO: Got endpoints: latency-svc-9sb59 [744.744991ms]
Dec 14 17:05:16.999: INFO: Created: latency-svc-dgptx
Dec 14 17:05:17.033: INFO: Got endpoints: latency-svc-t65db [747.33893ms]
Dec 14 17:05:17.048: INFO: Created: latency-svc-2ghqd
Dec 14 17:05:17.083: INFO: Got endpoints: latency-svc-7znhz [750.269295ms]
Dec 14 17:05:17.100: INFO: Created: latency-svc-pdf57
Dec 14 17:05:17.136: INFO: Got endpoints: latency-svc-w9qtd [749.509866ms]
Dec 14 17:05:17.154: INFO: Created: latency-svc-2nzx6
Dec 14 17:05:17.197: INFO: Got endpoints: latency-svc-d9bw9 [754.908478ms]
Dec 14 17:05:17.246: INFO: Got endpoints: latency-svc-qzcw9 [756.744185ms]
Dec 14 17:05:17.286: INFO: Got endpoints: latency-svc-jsj2k [751.921324ms]
Dec 14 17:05:17.336: INFO: Got endpoints: latency-svc-4b8k8 [749.91013ms]
Dec 14 17:05:17.382: INFO: Got endpoints: latency-svc-z2qnv [739.374981ms]
Dec 14 17:05:17.445: INFO: Got endpoints: latency-svc-l6hnv [754.752758ms]
Dec 14 17:05:17.485: INFO: Got endpoints: latency-svc-4qxmm [750.333171ms]
Dec 14 17:05:17.535: INFO: Got endpoints: latency-svc-trvsw [751.362055ms]
Dec 14 17:05:17.584: INFO: Got endpoints: latency-svc-x7xnf [750.625189ms]
Dec 14 17:05:17.647: INFO: Got endpoints: latency-svc-jf2b5 [764.818314ms]
Dec 14 17:05:17.684: INFO: Got endpoints: latency-svc-b72tr [748.51486ms]
Dec 14 17:05:17.734: INFO: Got endpoints: latency-svc-dgptx [749.625335ms]
Dec 14 17:05:17.791: INFO: Got endpoints: latency-svc-2ghqd [757.071825ms]
Dec 14 17:05:17.835: INFO: Got endpoints: latency-svc-pdf57 [751.323383ms]
Dec 14 17:05:17.885: INFO: Got endpoints: latency-svc-2nzx6 [749.017611ms]
Dec 14 17:05:17.886: INFO: Latencies: [68.017264ms 90.376274ms 114.62164ms 133.393946ms 140.684501ms 141.142976ms 143.110476ms 152.369419ms 153.678831ms 163.506824ms 177.333706ms 191.090955ms 240.307756ms 241.654766ms 254.459065ms 271.194433ms 276.56645ms 277.125819ms 281.008844ms 285.306486ms 287.607473ms 301.983823ms 303.377495ms 313.782059ms 317.939338ms 318.298984ms 324.600272ms 332.247148ms 335.662029ms 342.575366ms 347.661996ms 359.942719ms 366.161775ms 366.259539ms 367.802015ms 370.446015ms 374.562354ms 376.929226ms 388.030664ms 388.596388ms 388.705624ms 391.444916ms 401.249463ms 402.000298ms 402.857808ms 407.052205ms 411.077347ms 440.181962ms 458.143616ms 460.090545ms 468.329426ms 477.715828ms 477.830225ms 487.58162ms 488.527081ms 489.94005ms 491.127922ms 501.437859ms 507.334744ms 526.94737ms 553.867893ms 582.221468ms 595.960004ms 597.993451ms 611.407063ms 621.181307ms 629.596211ms 636.732513ms 664.67637ms 679.346707ms 681.039437ms 688.549462ms 716.126837ms 720.868975ms 726.935962ms 730.554465ms 731.527234ms 732.057588ms 734.051649ms 734.382569ms 735.69187ms 737.094562ms 739.209428ms 739.374981ms 740.233626ms 740.316916ms 740.367607ms 741.273181ms 741.45204ms 741.465157ms 741.582851ms 741.614131ms 742.368323ms 742.583074ms 742.736128ms 743.326336ms 743.622064ms 744.216815ms 744.335389ms 744.72119ms 744.744991ms 745.317301ms 745.476846ms 745.609485ms 745.936072ms 746.09459ms 746.781964ms 746.806949ms 746.989519ms 747.284906ms 747.33893ms 747.353296ms 747.515982ms 747.784652ms 747.804973ms 747.873182ms 748.083095ms 748.158693ms 748.214952ms 748.25354ms 748.31746ms 748.47153ms 748.51486ms 748.598074ms 748.620609ms 748.64005ms 748.690847ms 748.720235ms 748.772562ms 749.017611ms 749.024641ms 749.36056ms 749.509866ms 749.515087ms 749.515185ms 749.606603ms 749.625335ms 749.786226ms 749.805413ms 749.874728ms 749.91013ms 750.257727ms 750.269295ms 750.288994ms 750.333171ms 750.335754ms 750.518637ms 750.605684ms 750.625189ms 750.680384ms 750.906423ms 750.91114ms 750.913285ms 750.970176ms 751.297315ms 751.318054ms 751.323383ms 751.336334ms 751.362055ms 751.498417ms 751.611473ms 751.714612ms 751.790352ms 751.900985ms 751.921324ms 752.097929ms 752.746404ms 752.844229ms 753.001451ms 753.292853ms 753.413733ms 753.478716ms 753.698109ms 753.775166ms 753.779464ms 754.03191ms 754.172625ms 754.204882ms 754.740603ms 754.752758ms 754.908478ms 755.739032ms 756.744185ms 757.071825ms 757.940694ms 758.287575ms 758.467773ms 758.546062ms 759.188212ms 759.715556ms 760.090759ms 760.186029ms 760.494071ms 760.712184ms 764.818314ms 766.042997ms 766.76636ms 767.823928ms 776.824994ms 779.613747ms]
Dec 14 17:05:17.886: INFO: 50 %ile: 744.744991ms
Dec 14 17:05:17.886: INFO: 90 %ile: 754.908478ms
Dec 14 17:05:17.886: INFO: 99 %ile: 776.824994ms
Dec 14 17:05:17.886: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Dec 14 17:05:17.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-6453" for this suite. 12/14/22 17:05:17.896
------------------------------
• [SLOW TEST] [10.862 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:05:07.041
    Dec 14 17:05:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename svc-latency 12/14/22 17:05:07.048
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:07.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:07.099
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Dec 14 17:05:07.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6453 12/14/22 17:05:07.107
    I1214 17:05:07.128734      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6453, replica count: 1
    I1214 17:05:08.182788      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1214 17:05:09.183535      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 14 17:05:09.304: INFO: Created: latency-svc-qkh9r
    Dec 14 17:05:09.320: INFO: Got endpoints: latency-svc-qkh9r [36.57535ms]
    Dec 14 17:05:09.358: INFO: Created: latency-svc-tfw95
    Dec 14 17:05:09.389: INFO: Got endpoints: latency-svc-tfw95 [68.017264ms]
    Dec 14 17:05:09.403: INFO: Created: latency-svc-pdp9w
    Dec 14 17:05:09.412: INFO: Got endpoints: latency-svc-pdp9w [90.376274ms]
    Dec 14 17:05:09.417: INFO: Created: latency-svc-mm475
    Dec 14 17:05:09.435: INFO: Created: latency-svc-2s9j5
    Dec 14 17:05:09.436: INFO: Got endpoints: latency-svc-mm475 [114.62164ms]
    Dec 14 17:05:09.458: INFO: Created: latency-svc-cn8vf
    Dec 14 17:05:09.465: INFO: Got endpoints: latency-svc-2s9j5 [143.110476ms]
    Dec 14 17:05:09.476: INFO: Got endpoints: latency-svc-cn8vf [153.678831ms]
    Dec 14 17:05:09.628: INFO: Created: latency-svc-d4tb6
    Dec 14 17:05:09.628: INFO: Created: latency-svc-xpd9s
    Dec 14 17:05:09.628: INFO: Created: latency-svc-fcp85
    Dec 14 17:05:09.628: INFO: Created: latency-svc-w2rrp
    Dec 14 17:05:09.629: INFO: Created: latency-svc-rmbjb
    Dec 14 17:05:09.637: INFO: Created: latency-svc-zvnlq
    Dec 14 17:05:09.637: INFO: Created: latency-svc-fd2hq
    Dec 14 17:05:09.638: INFO: Created: latency-svc-zrqsp
    Dec 14 17:05:09.638: INFO: Created: latency-svc-7dpl7
    Dec 14 17:05:09.638: INFO: Created: latency-svc-shv2z
    Dec 14 17:05:09.672: INFO: Created: latency-svc-p8rwh
    Dec 14 17:05:09.672: INFO: Created: latency-svc-m9mpp
    Dec 14 17:05:09.673: INFO: Created: latency-svc-zmlcp
    Dec 14 17:05:09.674: INFO: Created: latency-svc-wxgkz
    Dec 14 17:05:09.674: INFO: Created: latency-svc-rz8wr
    Dec 14 17:05:09.683: INFO: Got endpoints: latency-svc-d4tb6 [359.942719ms]
    Dec 14 17:05:09.692: INFO: Got endpoints: latency-svc-xpd9s [367.802015ms]
    Dec 14 17:05:09.693: INFO: Got endpoints: latency-svc-w2rrp [370.446015ms]
    Dec 14 17:05:09.725: INFO: Got endpoints: latency-svc-rmbjb [335.662029ms]
    Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-p8rwh [402.000298ms]
    Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-zmlcp [401.249463ms]
    Dec 14 17:05:09.726: INFO: Got endpoints: latency-svc-m9mpp [402.857808ms]
    Dec 14 17:05:09.740: INFO: Created: latency-svc-pfxnf
    Dec 14 17:05:09.773: INFO: Created: latency-svc-hdncg
    Dec 14 17:05:09.780: INFO: Got endpoints: latency-svc-wxgkz [303.377495ms]
    Dec 14 17:05:09.780: INFO: Got endpoints: latency-svc-rz8wr [458.143616ms]
    Dec 14 17:05:09.790: INFO: Got endpoints: latency-svc-fcp85 [324.600272ms]
    Dec 14 17:05:09.801: INFO: Got endpoints: latency-svc-zrqsp [388.705624ms]
    Dec 14 17:05:09.802: INFO: Got endpoints: latency-svc-shv2z [477.830225ms]
    Dec 14 17:05:09.802: INFO: Got endpoints: latency-svc-7dpl7 [477.715828ms]
    Dec 14 17:05:09.824: INFO: Created: latency-svc-5rv4v
    Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-pfxnf [140.684501ms]
    Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-fd2hq [388.030664ms]
    Dec 14 17:05:09.825: INFO: Got endpoints: latency-svc-hdncg [133.393946ms]
    Dec 14 17:05:09.824: INFO: Got endpoints: latency-svc-zvnlq [501.437859ms]
    Dec 14 17:05:09.845: INFO: Created: latency-svc-8k9fh
    Dec 14 17:05:09.846: INFO: Got endpoints: latency-svc-5rv4v [152.369419ms]
    Dec 14 17:05:09.867: INFO: Got endpoints: latency-svc-8k9fh [141.142976ms]
    Dec 14 17:05:09.976: INFO: Created: latency-svc-tfch2
    Dec 14 17:05:09.979: INFO: Created: latency-svc-p4lr9
    Dec 14 17:05:09.998: INFO: Created: latency-svc-cdjkm
    Dec 14 17:05:09.998: INFO: Created: latency-svc-nzs5g
    Dec 14 17:05:09.998: INFO: Created: latency-svc-nr8bc
    Dec 14 17:05:09.999: INFO: Created: latency-svc-5z7v9
    Dec 14 17:05:09.999: INFO: Created: latency-svc-sfrsq
    Dec 14 17:05:10.003: INFO: Created: latency-svc-nj85b
    Dec 14 17:05:10.007: INFO: Created: latency-svc-j5gsp
    Dec 14 17:05:10.007: INFO: Created: latency-svc-m6fq8
    Dec 14 17:05:10.007: INFO: Created: latency-svc-4hg9n
    Dec 14 17:05:10.008: INFO: Created: latency-svc-qd6rj
    Dec 14 17:05:10.008: INFO: Created: latency-svc-jtg67
    Dec 14 17:05:10.009: INFO: Created: latency-svc-jb2ck
    Dec 14 17:05:10.016: INFO: Created: latency-svc-j7npc
    Dec 14 17:05:10.034: INFO: Got endpoints: latency-svc-tfch2 [254.459065ms]
    Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-cdjkm [241.654766ms]
    Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-p4lr9 [318.298984ms]
    Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-nzs5g [317.939338ms]
    Dec 14 17:05:10.044: INFO: Got endpoints: latency-svc-sfrsq [177.333706ms]
    Dec 14 17:05:10.075: INFO: Got endpoints: latency-svc-j5gsp [285.306486ms]
    Dec 14 17:05:10.076: INFO: Created: latency-svc-87ksg
    Dec 14 17:05:10.082: INFO: Got endpoints: latency-svc-nr8bc [301.983823ms]
    Dec 14 17:05:10.100: INFO: Got endpoints: latency-svc-qd6rj [374.562354ms]
    Dec 14 17:05:10.101: INFO: Got endpoints: latency-svc-jtg67 [276.56645ms]
    Dec 14 17:05:10.119: INFO: Created: latency-svc-wqmzr
    Dec 14 17:05:10.133: INFO: Got endpoints: latency-svc-j7npc [332.247148ms]
    Dec 14 17:05:10.180: INFO: Created: latency-svc-mznwq
    Dec 14 17:05:10.191: INFO: Got endpoints: latency-svc-nj85b [388.596388ms]
    Dec 14 17:05:10.192: INFO: Got endpoints: latency-svc-4hg9n [366.259539ms]
    Dec 14 17:05:10.192: INFO: Got endpoints: latency-svc-jb2ck [366.161775ms]
    Dec 14 17:05:10.311: INFO: Created: latency-svc-7kj6x
    Dec 14 17:05:10.313: INFO: Got endpoints: latency-svc-m6fq8 [488.527081ms]
    Dec 14 17:05:10.314: INFO: Got endpoints: latency-svc-5z7v9 [468.329426ms]
    Dec 14 17:05:10.315: INFO: Got endpoints: latency-svc-wqmzr [271.194433ms]
    Dec 14 17:05:10.316: INFO: Got endpoints: latency-svc-87ksg [281.008844ms]
    Dec 14 17:05:10.321: INFO: Got endpoints: latency-svc-mznwq [277.125819ms]
    Dec 14 17:05:10.358: INFO: Got endpoints: latency-svc-7kj6x [313.782059ms]
    Dec 14 17:05:10.588: INFO: Created: latency-svc-hcn8k
    Dec 14 17:05:10.588: INFO: Created: latency-svc-zg7cf
    Dec 14 17:05:10.590: INFO: Created: latency-svc-mwmgd
    Dec 14 17:05:10.591: INFO: Created: latency-svc-x65wc
    Dec 14 17:05:10.594: INFO: Created: latency-svc-67g8b
    Dec 14 17:05:10.595: INFO: Created: latency-svc-l9jxq
    Dec 14 17:05:10.595: INFO: Created: latency-svc-bcm55
    Dec 14 17:05:10.596: INFO: Created: latency-svc-fdc9l
    Dec 14 17:05:10.598: INFO: Created: latency-svc-cc9fz
    Dec 14 17:05:10.598: INFO: Created: latency-svc-jmv2s
    Dec 14 17:05:10.627: INFO: Created: latency-svc-zq2ms
    Dec 14 17:05:10.627: INFO: Created: latency-svc-zvllj
    Dec 14 17:05:10.628: INFO: Created: latency-svc-7sff5
    Dec 14 17:05:10.628: INFO: Created: latency-svc-hmxd7
    Dec 14 17:05:10.628: INFO: Created: latency-svc-ggnxd
    Dec 14 17:05:10.675: INFO: Got endpoints: latency-svc-hcn8k [629.596211ms]
    Dec 14 17:05:10.698: INFO: Got endpoints: latency-svc-x65wc [507.334744ms]
    Dec 14 17:05:10.698: INFO: Got endpoints: latency-svc-zg7cf [376.929226ms]
    Dec 14 17:05:10.699: INFO: Got endpoints: latency-svc-mwmgd [597.993451ms]
    Dec 14 17:05:10.706: INFO: Got endpoints: latency-svc-7sff5 [391.444916ms]
    Dec 14 17:05:10.706: INFO: Got endpoints: latency-svc-zq2ms [347.661996ms]
    Dec 14 17:05:10.745: INFO: Got endpoints: latency-svc-ggnxd [611.407063ms]
    Dec 14 17:05:10.746: INFO: Got endpoints: latency-svc-zvllj [553.867893ms]
    Dec 14 17:05:10.747: INFO: Got endpoints: latency-svc-hmxd7 [664.67637ms]
    Dec 14 17:05:10.756: INFO: Got endpoints: latency-svc-jmv2s [440.181962ms]
    Dec 14 17:05:10.756: INFO: Got endpoints: latency-svc-cc9fz [681.039437ms]
    Dec 14 17:05:10.789: INFO: Got endpoints: latency-svc-fdc9l [688.549462ms]
    Dec 14 17:05:10.790: INFO: Created: latency-svc-42xnt
    Dec 14 17:05:10.801: INFO: Got endpoints: latency-svc-bcm55 [487.58162ms]
    Dec 14 17:05:10.806: INFO: Got endpoints: latency-svc-l9jxq [489.94005ms]
    Dec 14 17:05:10.813: INFO: Got endpoints: latency-svc-67g8b [621.181307ms]
    Dec 14 17:05:10.819: INFO: Created: latency-svc-65crv
    Dec 14 17:05:10.819: INFO: Created: latency-svc-vcwxm
    Dec 14 17:05:10.828: INFO: Created: latency-svc-j5zm9
    Dec 14 17:05:10.838: INFO: Got endpoints: latency-svc-42xnt [163.506824ms]
    Dec 14 17:05:10.848: INFO: Created: latency-svc-n8vtp
    Dec 14 17:05:10.856: INFO: Created: latency-svc-thtkm
    Dec 14 17:05:10.869: INFO: Created: latency-svc-cpwrb
    Dec 14 17:05:10.874: INFO: Created: latency-svc-hr7zw
    Dec 14 17:05:10.890: INFO: Got endpoints: latency-svc-vcwxm [191.090955ms]
    Dec 14 17:05:10.891: INFO: Created: latency-svc-k2pvg
    Dec 14 17:05:10.899: INFO: Created: latency-svc-9gpr8
    Dec 14 17:05:10.907: INFO: Created: latency-svc-5mjtv
    Dec 14 17:05:10.926: INFO: Created: latency-svc-zp4d6
    Dec 14 17:05:10.946: INFO: Got endpoints: latency-svc-65crv [240.307756ms]
    Dec 14 17:05:10.956: INFO: Created: latency-svc-zf9jx
    Dec 14 17:05:10.966: INFO: Created: latency-svc-qb5zw
    Dec 14 17:05:10.985: INFO: Created: latency-svc-ctl7t
    Dec 14 17:05:10.994: INFO: Got endpoints: latency-svc-j5zm9 [287.607473ms]
    Dec 14 17:05:11.018: INFO: Created: latency-svc-dwqr4
    Dec 14 17:05:11.031: INFO: Created: latency-svc-qsk5s
    Dec 14 17:05:11.041: INFO: Got endpoints: latency-svc-n8vtp [342.575366ms]
    Dec 14 17:05:11.058: INFO: Created: latency-svc-f84pg
    Dec 14 17:05:11.066: INFO: Created: latency-svc-bgzfl
    Dec 14 17:05:11.110: INFO: Got endpoints: latency-svc-thtkm [411.077347ms]
    Dec 14 17:05:11.145: INFO: Created: latency-svc-wfx6l
    Dec 14 17:05:11.152: INFO: Got endpoints: latency-svc-cpwrb [407.052205ms]
    Dec 14 17:05:11.193: INFO: Created: latency-svc-44j2q
    Dec 14 17:05:11.197: INFO: Created: latency-svc-bdtqx
    Dec 14 17:05:11.206: INFO: Got endpoints: latency-svc-hr7zw [460.090545ms]
    Dec 14 17:05:11.225: INFO: Created: latency-svc-rc8pz
    Dec 14 17:05:11.238: INFO: Got endpoints: latency-svc-k2pvg [491.127922ms]
    Dec 14 17:05:11.254: INFO: Created: latency-svc-qxbw9
    Dec 14 17:05:11.284: INFO: Got endpoints: latency-svc-9gpr8 [526.94737ms]
    Dec 14 17:05:11.306: INFO: Created: latency-svc-g6njk
    Dec 14 17:05:11.338: INFO: Got endpoints: latency-svc-5mjtv [582.221468ms]
    Dec 14 17:05:11.355: INFO: Created: latency-svc-ksqjl
    Dec 14 17:05:11.385: INFO: Got endpoints: latency-svc-zp4d6 [595.960004ms]
    Dec 14 17:05:11.402: INFO: Created: latency-svc-z4ccz
    Dec 14 17:05:11.438: INFO: Got endpoints: latency-svc-zf9jx [636.732513ms]
    Dec 14 17:05:11.456: INFO: Created: latency-svc-9r7ns
    Dec 14 17:05:11.485: INFO: Got endpoints: latency-svc-qb5zw [679.346707ms]
    Dec 14 17:05:11.498: INFO: Created: latency-svc-kjc6d
    Dec 14 17:05:11.534: INFO: Got endpoints: latency-svc-ctl7t [720.868975ms]
    Dec 14 17:05:11.548: INFO: Created: latency-svc-95thr
    Dec 14 17:05:11.583: INFO: Got endpoints: latency-svc-dwqr4 [744.72119ms]
    Dec 14 17:05:11.608: INFO: Created: latency-svc-5zrvb
    Dec 14 17:05:11.641: INFO: Got endpoints: latency-svc-qsk5s [750.906423ms]
    Dec 14 17:05:11.658: INFO: Created: latency-svc-76p5f
    Dec 14 17:05:11.688: INFO: Got endpoints: latency-svc-f84pg [741.614131ms]
    Dec 14 17:05:11.709: INFO: Created: latency-svc-f8m7b
    Dec 14 17:05:11.735: INFO: Got endpoints: latency-svc-bgzfl [740.367607ms]
    Dec 14 17:05:11.758: INFO: Created: latency-svc-nfbhc
    Dec 14 17:05:11.786: INFO: Got endpoints: latency-svc-wfx6l [745.476846ms]
    Dec 14 17:05:11.805: INFO: Created: latency-svc-h8gc2
    Dec 14 17:05:11.837: INFO: Got endpoints: latency-svc-bdtqx [726.935962ms]
    Dec 14 17:05:11.858: INFO: Created: latency-svc-lszsl
    Dec 14 17:05:11.886: INFO: Got endpoints: latency-svc-44j2q [734.051649ms]
    Dec 14 17:05:11.903: INFO: Created: latency-svc-gq8rh
    Dec 14 17:05:11.937: INFO: Got endpoints: latency-svc-rc8pz [730.554465ms]
    Dec 14 17:05:11.950: INFO: Created: latency-svc-rlhrg
    Dec 14 17:05:11.988: INFO: Got endpoints: latency-svc-qxbw9 [749.786226ms]
    Dec 14 17:05:12.002: INFO: Created: latency-svc-bkkhd
    Dec 14 17:05:12.043: INFO: Got endpoints: latency-svc-g6njk [759.715556ms]
    Dec 14 17:05:12.064: INFO: Created: latency-svc-7rr6f
    Dec 14 17:05:12.089: INFO: Got endpoints: latency-svc-ksqjl [750.605684ms]
    Dec 14 17:05:12.104: INFO: Created: latency-svc-sv4n8
    Dec 14 17:05:12.134: INFO: Got endpoints: latency-svc-z4ccz [748.690847ms]
    Dec 14 17:05:12.151: INFO: Created: latency-svc-4nzpz
    Dec 14 17:05:12.218: INFO: Got endpoints: latency-svc-9r7ns [779.613747ms]
    Dec 14 17:05:12.247: INFO: Created: latency-svc-nkzz7
    Dec 14 17:05:12.251: INFO: Got endpoints: latency-svc-kjc6d [766.042997ms]
    Dec 14 17:05:12.272: INFO: Created: latency-svc-wzbzp
    Dec 14 17:05:12.295: INFO: Got endpoints: latency-svc-95thr [760.712184ms]
    Dec 14 17:05:12.326: INFO: Created: latency-svc-jqx44
    Dec 14 17:05:12.342: INFO: Got endpoints: latency-svc-5zrvb [758.467773ms]
    Dec 14 17:05:12.383: INFO: Created: latency-svc-7p6mt
    Dec 14 17:05:12.394: INFO: Got endpoints: latency-svc-76p5f [752.746404ms]
    Dec 14 17:05:12.416: INFO: Created: latency-svc-sb4j2
    Dec 14 17:05:12.441: INFO: Got endpoints: latency-svc-f8m7b [753.001451ms]
    Dec 14 17:05:12.472: INFO: Created: latency-svc-z5qsp
    Dec 14 17:05:12.512: INFO: Got endpoints: latency-svc-nfbhc [776.824994ms]
    Dec 14 17:05:12.531: INFO: Created: latency-svc-l6zsk
    Dec 14 17:05:12.540: INFO: Got endpoints: latency-svc-h8gc2 [753.775166ms]
    Dec 14 17:05:12.566: INFO: Created: latency-svc-tkt5v
    Dec 14 17:05:12.585: INFO: Got endpoints: latency-svc-lszsl [747.784652ms]
    Dec 14 17:05:12.600: INFO: Created: latency-svc-w9wx5
    Dec 14 17:05:12.638: INFO: Got endpoints: latency-svc-gq8rh [752.097929ms]
    Dec 14 17:05:12.661: INFO: Created: latency-svc-z9nrp
    Dec 14 17:05:12.687: INFO: Got endpoints: latency-svc-rlhrg [750.288994ms]
    Dec 14 17:05:12.706: INFO: Created: latency-svc-bcwfm
    Dec 14 17:05:12.738: INFO: Got endpoints: latency-svc-bkkhd [749.515087ms]
    Dec 14 17:05:12.752: INFO: Created: latency-svc-dcf77
    Dec 14 17:05:12.794: INFO: Got endpoints: latency-svc-7rr6f [750.680384ms]
    Dec 14 17:05:12.812: INFO: Created: latency-svc-kp4b6
    Dec 14 17:05:12.836: INFO: Got endpoints: latency-svc-sv4n8 [747.284906ms]
    Dec 14 17:05:12.854: INFO: Created: latency-svc-h4sxr
    Dec 14 17:05:12.883: INFO: Got endpoints: latency-svc-4nzpz [748.64005ms]
    Dec 14 17:05:12.899: INFO: Created: latency-svc-rxl8n
    Dec 14 17:05:12.934: INFO: Got endpoints: latency-svc-nkzz7 [716.126837ms]
    Dec 14 17:05:12.952: INFO: Created: latency-svc-phg6j
    Dec 14 17:05:12.994: INFO: Got endpoints: latency-svc-wzbzp [742.368323ms]
    Dec 14 17:05:13.010: INFO: Created: latency-svc-r7pxs
    Dec 14 17:05:13.035: INFO: Got endpoints: latency-svc-jqx44 [740.233626ms]
    Dec 14 17:05:13.050: INFO: Created: latency-svc-5jss9
    Dec 14 17:05:13.083: INFO: Got endpoints: latency-svc-7p6mt [741.273181ms]
    Dec 14 17:05:13.095: INFO: Created: latency-svc-mxprl
    Dec 14 17:05:13.134: INFO: Got endpoints: latency-svc-sb4j2 [740.316916ms]
    Dec 14 17:05:13.151: INFO: Created: latency-svc-9sx7h
    Dec 14 17:05:13.185: INFO: Got endpoints: latency-svc-z5qsp [744.216815ms]
    Dec 14 17:05:13.199: INFO: Created: latency-svc-2gk4h
    Dec 14 17:05:13.243: INFO: Got endpoints: latency-svc-l6zsk [731.527234ms]
    Dec 14 17:05:13.257: INFO: Created: latency-svc-kvrjc
    Dec 14 17:05:13.286: INFO: Got endpoints: latency-svc-tkt5v [745.317301ms]
    Dec 14 17:05:13.310: INFO: Created: latency-svc-f8vpn
    Dec 14 17:05:13.344: INFO: Got endpoints: latency-svc-w9wx5 [758.546062ms]
    Dec 14 17:05:13.365: INFO: Created: latency-svc-rbzsn
    Dec 14 17:05:13.384: INFO: Got endpoints: latency-svc-z9nrp [745.936072ms]
    Dec 14 17:05:13.404: INFO: Created: latency-svc-975cs
    Dec 14 17:05:13.434: INFO: Got endpoints: latency-svc-bcwfm [747.353296ms]
    Dec 14 17:05:13.453: INFO: Created: latency-svc-6smzh
    Dec 14 17:05:13.486: INFO: Got endpoints: latency-svc-dcf77 [748.083095ms]
    Dec 14 17:05:13.502: INFO: Created: latency-svc-kj6sv
    Dec 14 17:05:13.536: INFO: Got endpoints: latency-svc-kp4b6 [741.582851ms]
    Dec 14 17:05:13.550: INFO: Created: latency-svc-2ds7k
    Dec 14 17:05:13.584: INFO: Got endpoints: latency-svc-h4sxr [747.804973ms]
    Dec 14 17:05:13.599: INFO: Created: latency-svc-xgnhq
    Dec 14 17:05:13.636: INFO: Got endpoints: latency-svc-rxl8n [753.292853ms]
    Dec 14 17:05:13.657: INFO: Created: latency-svc-k7xbh
    Dec 14 17:05:13.684: INFO: Got endpoints: latency-svc-phg6j [749.874728ms]
    Dec 14 17:05:13.701: INFO: Created: latency-svc-mc9kj
    Dec 14 17:05:13.733: INFO: Got endpoints: latency-svc-r7pxs [739.209428ms]
    Dec 14 17:05:13.752: INFO: Created: latency-svc-hfjzj
    Dec 14 17:05:13.783: INFO: Got endpoints: latency-svc-5jss9 [748.25354ms]
    Dec 14 17:05:13.804: INFO: Created: latency-svc-bwctt
    Dec 14 17:05:13.834: INFO: Got endpoints: latency-svc-mxprl [751.336334ms]
    Dec 14 17:05:13.856: INFO: Created: latency-svc-xl6qr
    Dec 14 17:05:13.886: INFO: Got endpoints: latency-svc-9sx7h [751.900985ms]
    Dec 14 17:05:13.909: INFO: Created: latency-svc-ghbpn
    Dec 14 17:05:13.935: INFO: Got endpoints: latency-svc-2gk4h [749.515185ms]
    Dec 14 17:05:13.953: INFO: Created: latency-svc-f9vht
    Dec 14 17:05:13.992: INFO: Got endpoints: latency-svc-kvrjc [749.024641ms]
    Dec 14 17:05:14.007: INFO: Created: latency-svc-xvzph
    Dec 14 17:05:14.039: INFO: Got endpoints: latency-svc-f8vpn [752.844229ms]
    Dec 14 17:05:14.052: INFO: Created: latency-svc-mgfnx
    Dec 14 17:05:14.087: INFO: Got endpoints: latency-svc-rbzsn [743.622064ms]
    Dec 14 17:05:14.104: INFO: Created: latency-svc-md86c
    Dec 14 17:05:14.138: INFO: Got endpoints: latency-svc-975cs [753.478716ms]
    Dec 14 17:05:14.150: INFO: Created: latency-svc-d7zqw
    Dec 14 17:05:14.188: INFO: Got endpoints: latency-svc-6smzh [753.779464ms]
    Dec 14 17:05:14.202: INFO: Created: latency-svc-pqmnt
    Dec 14 17:05:14.238: INFO: Got endpoints: latency-svc-kj6sv [751.790352ms]
    Dec 14 17:05:14.255: INFO: Created: latency-svc-nfqh2
    Dec 14 17:05:14.290: INFO: Got endpoints: latency-svc-2ds7k [754.204882ms]
    Dec 14 17:05:14.307: INFO: Created: latency-svc-qrlcb
    Dec 14 17:05:14.344: INFO: Got endpoints: latency-svc-xgnhq [760.090759ms]
    Dec 14 17:05:14.365: INFO: Created: latency-svc-458tt
    Dec 14 17:05:14.387: INFO: Got endpoints: latency-svc-k7xbh [750.335754ms]
    Dec 14 17:05:14.420: INFO: Created: latency-svc-5xw4w
    Dec 14 17:05:14.435: INFO: Got endpoints: latency-svc-mc9kj [751.611473ms]
    Dec 14 17:05:14.469: INFO: Created: latency-svc-q8fsg
    Dec 14 17:05:14.487: INFO: Got endpoints: latency-svc-hfjzj [753.413733ms]
    Dec 14 17:05:14.521: INFO: Created: latency-svc-4msjd
    Dec 14 17:05:14.551: INFO: Got endpoints: latency-svc-bwctt [767.823928ms]
    Dec 14 17:05:14.570: INFO: Created: latency-svc-sxwqz
    Dec 14 17:05:14.583: INFO: Got endpoints: latency-svc-xl6qr [748.720235ms]
    Dec 14 17:05:14.612: INFO: Created: latency-svc-b5x96
    Dec 14 17:05:14.637: INFO: Got endpoints: latency-svc-ghbpn [750.518637ms]
    Dec 14 17:05:14.654: INFO: Created: latency-svc-zbgtm
    Dec 14 17:05:14.702: INFO: Got endpoints: latency-svc-f9vht [766.76636ms]
    Dec 14 17:05:14.731: INFO: Created: latency-svc-clkkw
    Dec 14 17:05:14.739: INFO: Got endpoints: latency-svc-xvzph [746.806949ms]
    Dec 14 17:05:14.757: INFO: Created: latency-svc-l7ccp
    Dec 14 17:05:14.793: INFO: Got endpoints: latency-svc-mgfnx [754.03191ms]
    Dec 14 17:05:14.807: INFO: Created: latency-svc-5hfkz
    Dec 14 17:05:14.835: INFO: Got endpoints: latency-svc-md86c [747.873182ms]
    Dec 14 17:05:14.849: INFO: Created: latency-svc-wr6f6
    Dec 14 17:05:14.886: INFO: Got endpoints: latency-svc-d7zqw [747.515982ms]
    Dec 14 17:05:14.903: INFO: Created: latency-svc-drzgh
    Dec 14 17:05:14.946: INFO: Got endpoints: latency-svc-pqmnt [757.940694ms]
    Dec 14 17:05:14.990: INFO: Got endpoints: latency-svc-nfqh2 [751.714612ms]
    Dec 14 17:05:14.991: INFO: Created: latency-svc-q8zn4
    Dec 14 17:05:15.017: INFO: Created: latency-svc-6s2t2
    Dec 14 17:05:15.033: INFO: Got endpoints: latency-svc-qrlcb [742.736128ms]
    Dec 14 17:05:15.053: INFO: Created: latency-svc-z7xkn
    Dec 14 17:05:15.086: INFO: Got endpoints: latency-svc-458tt [741.465157ms]
    Dec 14 17:05:15.102: INFO: Created: latency-svc-2j478
    Dec 14 17:05:15.136: INFO: Got endpoints: latency-svc-5xw4w [748.772562ms]
    Dec 14 17:05:15.151: INFO: Created: latency-svc-zs9fw
    Dec 14 17:05:15.185: INFO: Got endpoints: latency-svc-q8fsg [749.36056ms]
    Dec 14 17:05:15.201: INFO: Created: latency-svc-2t6r9
    Dec 14 17:05:15.235: INFO: Got endpoints: latency-svc-4msjd [748.598074ms]
    Dec 14 17:05:15.257: INFO: Created: latency-svc-f54qj
    Dec 14 17:05:15.287: INFO: Got endpoints: latency-svc-sxwqz [735.69187ms]
    Dec 14 17:05:15.304: INFO: Created: latency-svc-qrqrh
    Dec 14 17:05:15.343: INFO: Got endpoints: latency-svc-b5x96 [759.188212ms]
    Dec 14 17:05:15.359: INFO: Created: latency-svc-8tn4h
    Dec 14 17:05:15.385: INFO: Got endpoints: latency-svc-zbgtm [748.47153ms]
    Dec 14 17:05:15.407: INFO: Created: latency-svc-l7wjj
    Dec 14 17:05:15.436: INFO: Got endpoints: latency-svc-clkkw [734.382569ms]
    Dec 14 17:05:15.465: INFO: Created: latency-svc-mdm6t
    Dec 14 17:05:15.490: INFO: Got endpoints: latency-svc-l7ccp [750.91114ms]
    Dec 14 17:05:15.507: INFO: Created: latency-svc-vhtxd
    Dec 14 17:05:15.534: INFO: Got endpoints: latency-svc-5hfkz [741.45204ms]
    Dec 14 17:05:15.556: INFO: Created: latency-svc-62lx4
    Dec 14 17:05:15.596: INFO: Got endpoints: latency-svc-wr6f6 [760.494071ms]
    Dec 14 17:05:15.619: INFO: Created: latency-svc-8wvx9
    Dec 14 17:05:15.640: INFO: Got endpoints: latency-svc-drzgh [753.698109ms]
    Dec 14 17:05:15.657: INFO: Created: latency-svc-4jpwj
    Dec 14 17:05:15.682: INFO: Got endpoints: latency-svc-q8zn4 [732.057588ms]
    Dec 14 17:05:15.712: INFO: Created: latency-svc-6mcz8
    Dec 14 17:05:15.734: INFO: Got endpoints: latency-svc-6s2t2 [744.335389ms]
    Dec 14 17:05:15.755: INFO: Created: latency-svc-g77w8
    Dec 14 17:05:15.791: INFO: Got endpoints: latency-svc-z7xkn [758.287575ms]
    Dec 14 17:05:15.806: INFO: Created: latency-svc-785n2
    Dec 14 17:05:15.834: INFO: Got endpoints: latency-svc-2j478 [748.214952ms]
    Dec 14 17:05:15.861: INFO: Created: latency-svc-xnbxk
    Dec 14 17:05:15.887: INFO: Got endpoints: latency-svc-zs9fw [750.970176ms]
    Dec 14 17:05:15.918: INFO: Created: latency-svc-v7vpq
    Dec 14 17:05:15.935: INFO: Got endpoints: latency-svc-2t6r9 [749.805413ms]
    Dec 14 17:05:15.961: INFO: Created: latency-svc-nq9sb
    Dec 14 17:05:15.986: INFO: Got endpoints: latency-svc-f54qj [750.257727ms]
    Dec 14 17:05:16.000: INFO: Created: latency-svc-4tgj7
    Dec 14 17:05:16.033: INFO: Got endpoints: latency-svc-qrqrh [746.09459ms]
    Dec 14 17:05:16.070: INFO: Created: latency-svc-cfnqm
    Dec 14 17:05:16.086: INFO: Got endpoints: latency-svc-8tn4h [743.326336ms]
    Dec 14 17:05:16.099: INFO: Created: latency-svc-4jh7h
    Dec 14 17:05:16.137: INFO: Got endpoints: latency-svc-l7wjj [751.318054ms]
    Dec 14 17:05:16.153: INFO: Created: latency-svc-s5t8z
    Dec 14 17:05:16.187: INFO: Got endpoints: latency-svc-mdm6t [750.913285ms]
    Dec 14 17:05:16.204: INFO: Created: latency-svc-zpx7p
    Dec 14 17:05:16.239: INFO: Got endpoints: latency-svc-vhtxd [748.620609ms]
    Dec 14 17:05:16.262: INFO: Created: latency-svc-9sb59
    Dec 14 17:05:16.286: INFO: Got endpoints: latency-svc-62lx4 [751.297315ms]
    Dec 14 17:05:16.302: INFO: Created: latency-svc-t65db
    Dec 14 17:05:16.333: INFO: Got endpoints: latency-svc-8wvx9 [737.094562ms]
    Dec 14 17:05:16.353: INFO: Created: latency-svc-7znhz
    Dec 14 17:05:16.386: INFO: Got endpoints: latency-svc-4jpwj [746.781964ms]
    Dec 14 17:05:16.403: INFO: Created: latency-svc-w9qtd
    Dec 14 17:05:16.442: INFO: Got endpoints: latency-svc-6mcz8 [760.186029ms]
    Dec 14 17:05:16.458: INFO: Created: latency-svc-d9bw9
    Dec 14 17:05:16.489: INFO: Got endpoints: latency-svc-g77w8 [754.172625ms]
    Dec 14 17:05:16.505: INFO: Created: latency-svc-qzcw9
    Dec 14 17:05:16.534: INFO: Got endpoints: latency-svc-785n2 [742.583074ms]
    Dec 14 17:05:16.559: INFO: Created: latency-svc-jsj2k
    Dec 14 17:05:16.586: INFO: Got endpoints: latency-svc-xnbxk [751.498417ms]
    Dec 14 17:05:16.609: INFO: Created: latency-svc-4b8k8
    Dec 14 17:05:16.643: INFO: Got endpoints: latency-svc-v7vpq [755.739032ms]
    Dec 14 17:05:16.658: INFO: Created: latency-svc-z2qnv
    Dec 14 17:05:16.690: INFO: Got endpoints: latency-svc-nq9sb [754.740603ms]
    Dec 14 17:05:16.709: INFO: Created: latency-svc-l6hnv
    Dec 14 17:05:16.734: INFO: Got endpoints: latency-svc-4tgj7 [748.31746ms]
    Dec 14 17:05:16.751: INFO: Created: latency-svc-4qxmm
    Dec 14 17:05:16.783: INFO: Got endpoints: latency-svc-cfnqm [749.606603ms]
    Dec 14 17:05:16.799: INFO: Created: latency-svc-trvsw
    Dec 14 17:05:16.833: INFO: Got endpoints: latency-svc-4jh7h [746.989519ms]
    Dec 14 17:05:16.851: INFO: Created: latency-svc-x7xnf
    Dec 14 17:05:16.882: INFO: Got endpoints: latency-svc-s5t8z [745.609485ms]
    Dec 14 17:05:16.920: INFO: Created: latency-svc-jf2b5
    Dec 14 17:05:16.936: INFO: Got endpoints: latency-svc-zpx7p [748.158693ms]
    Dec 14 17:05:16.950: INFO: Created: latency-svc-b72tr
    Dec 14 17:05:16.984: INFO: Got endpoints: latency-svc-9sb59 [744.744991ms]
    Dec 14 17:05:16.999: INFO: Created: latency-svc-dgptx
    Dec 14 17:05:17.033: INFO: Got endpoints: latency-svc-t65db [747.33893ms]
    Dec 14 17:05:17.048: INFO: Created: latency-svc-2ghqd
    Dec 14 17:05:17.083: INFO: Got endpoints: latency-svc-7znhz [750.269295ms]
    Dec 14 17:05:17.100: INFO: Created: latency-svc-pdf57
    Dec 14 17:05:17.136: INFO: Got endpoints: latency-svc-w9qtd [749.509866ms]
    Dec 14 17:05:17.154: INFO: Created: latency-svc-2nzx6
    Dec 14 17:05:17.197: INFO: Got endpoints: latency-svc-d9bw9 [754.908478ms]
    Dec 14 17:05:17.246: INFO: Got endpoints: latency-svc-qzcw9 [756.744185ms]
    Dec 14 17:05:17.286: INFO: Got endpoints: latency-svc-jsj2k [751.921324ms]
    Dec 14 17:05:17.336: INFO: Got endpoints: latency-svc-4b8k8 [749.91013ms]
    Dec 14 17:05:17.382: INFO: Got endpoints: latency-svc-z2qnv [739.374981ms]
    Dec 14 17:05:17.445: INFO: Got endpoints: latency-svc-l6hnv [754.752758ms]
    Dec 14 17:05:17.485: INFO: Got endpoints: latency-svc-4qxmm [750.333171ms]
    Dec 14 17:05:17.535: INFO: Got endpoints: latency-svc-trvsw [751.362055ms]
    Dec 14 17:05:17.584: INFO: Got endpoints: latency-svc-x7xnf [750.625189ms]
    Dec 14 17:05:17.647: INFO: Got endpoints: latency-svc-jf2b5 [764.818314ms]
    Dec 14 17:05:17.684: INFO: Got endpoints: latency-svc-b72tr [748.51486ms]
    Dec 14 17:05:17.734: INFO: Got endpoints: latency-svc-dgptx [749.625335ms]
    Dec 14 17:05:17.791: INFO: Got endpoints: latency-svc-2ghqd [757.071825ms]
    Dec 14 17:05:17.835: INFO: Got endpoints: latency-svc-pdf57 [751.323383ms]
    Dec 14 17:05:17.885: INFO: Got endpoints: latency-svc-2nzx6 [749.017611ms]
    Dec 14 17:05:17.886: INFO: Latencies: [68.017264ms 90.376274ms 114.62164ms 133.393946ms 140.684501ms 141.142976ms 143.110476ms 152.369419ms 153.678831ms 163.506824ms 177.333706ms 191.090955ms 240.307756ms 241.654766ms 254.459065ms 271.194433ms 276.56645ms 277.125819ms 281.008844ms 285.306486ms 287.607473ms 301.983823ms 303.377495ms 313.782059ms 317.939338ms 318.298984ms 324.600272ms 332.247148ms 335.662029ms 342.575366ms 347.661996ms 359.942719ms 366.161775ms 366.259539ms 367.802015ms 370.446015ms 374.562354ms 376.929226ms 388.030664ms 388.596388ms 388.705624ms 391.444916ms 401.249463ms 402.000298ms 402.857808ms 407.052205ms 411.077347ms 440.181962ms 458.143616ms 460.090545ms 468.329426ms 477.715828ms 477.830225ms 487.58162ms 488.527081ms 489.94005ms 491.127922ms 501.437859ms 507.334744ms 526.94737ms 553.867893ms 582.221468ms 595.960004ms 597.993451ms 611.407063ms 621.181307ms 629.596211ms 636.732513ms 664.67637ms 679.346707ms 681.039437ms 688.549462ms 716.126837ms 720.868975ms 726.935962ms 730.554465ms 731.527234ms 732.057588ms 734.051649ms 734.382569ms 735.69187ms 737.094562ms 739.209428ms 739.374981ms 740.233626ms 740.316916ms 740.367607ms 741.273181ms 741.45204ms 741.465157ms 741.582851ms 741.614131ms 742.368323ms 742.583074ms 742.736128ms 743.326336ms 743.622064ms 744.216815ms 744.335389ms 744.72119ms 744.744991ms 745.317301ms 745.476846ms 745.609485ms 745.936072ms 746.09459ms 746.781964ms 746.806949ms 746.989519ms 747.284906ms 747.33893ms 747.353296ms 747.515982ms 747.784652ms 747.804973ms 747.873182ms 748.083095ms 748.158693ms 748.214952ms 748.25354ms 748.31746ms 748.47153ms 748.51486ms 748.598074ms 748.620609ms 748.64005ms 748.690847ms 748.720235ms 748.772562ms 749.017611ms 749.024641ms 749.36056ms 749.509866ms 749.515087ms 749.515185ms 749.606603ms 749.625335ms 749.786226ms 749.805413ms 749.874728ms 749.91013ms 750.257727ms 750.269295ms 750.288994ms 750.333171ms 750.335754ms 750.518637ms 750.605684ms 750.625189ms 750.680384ms 750.906423ms 750.91114ms 750.913285ms 750.970176ms 751.297315ms 751.318054ms 751.323383ms 751.336334ms 751.362055ms 751.498417ms 751.611473ms 751.714612ms 751.790352ms 751.900985ms 751.921324ms 752.097929ms 752.746404ms 752.844229ms 753.001451ms 753.292853ms 753.413733ms 753.478716ms 753.698109ms 753.775166ms 753.779464ms 754.03191ms 754.172625ms 754.204882ms 754.740603ms 754.752758ms 754.908478ms 755.739032ms 756.744185ms 757.071825ms 757.940694ms 758.287575ms 758.467773ms 758.546062ms 759.188212ms 759.715556ms 760.090759ms 760.186029ms 760.494071ms 760.712184ms 764.818314ms 766.042997ms 766.76636ms 767.823928ms 776.824994ms 779.613747ms]
    Dec 14 17:05:17.886: INFO: 50 %ile: 744.744991ms
    Dec 14 17:05:17.886: INFO: 90 %ile: 754.908478ms
    Dec 14 17:05:17.886: INFO: 99 %ile: 776.824994ms
    Dec 14 17:05:17.886: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:05:17.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-6453" for this suite. 12/14/22 17:05:17.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:05:17.91
Dec 14 17:05:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 17:05:17.913
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:17.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:17.947
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Dec 14 17:05:17.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:05:24.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9469" for this suite. 12/14/22 17:05:24.443
------------------------------
• [SLOW TEST] [6.549 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:05:17.91
    Dec 14 17:05:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename custom-resource-definition 12/14/22 17:05:17.913
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:17.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:17.947
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Dec 14 17:05:17.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:05:24.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9469" for this suite. 12/14/22 17:05:24.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:05:24.463
Dec 14 17:05:24.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-pred 12/14/22 17:05:24.466
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:24.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:24.511
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 14 17:05:24.516: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 17:05:24.534: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 17:05:24.544: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
Dec 14 17:05:24.562: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:05:24.562: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container coredns ready: true, restart count 0
Dec 14 17:05:24.562: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 17:05:24.562: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 17:05:24.562: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 17:05:24.562: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:05:24.562: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 17:05:24.562: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:05:24.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:05:24.562: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 17:05:24.562: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
Dec 14 17:05:24.579: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:05:24.579: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container coredns ready: true, restart count 0
Dec 14 17:05:24.579: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 17:05:24.579: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 17:05:24.579: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 17:05:24.579: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:05:24.579: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 17:05:24.579: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:05:24.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:05:24.579: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 17:05:24.579: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
Dec 14 17:05:24.609: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:05:24.609: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:05:24.609: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 17:05:24.609: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container e2e ready: true, restart count 0
Dec 14 17:05:24.609: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:05:24.609: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:05:24.609: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 17:05:24.609: INFO: svc-latency-rc-4x9w7 from svc-latency-6453 started at 2022-12-14 17:05:07 +0000 UTC (1 container statuses recorded)
Dec 14 17:05:24.609: INFO: 	Container svc-latency-rc ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 17:05:24.609
Dec 14 17:05:24.628: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6515" to be "running"
Dec 14 17:05:24.636: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382393ms
Dec 14 17:05:26.643: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015125445s
Dec 14 17:05:28.648: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.019865718s
Dec 14 17:05:28.648: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 17:05:28.656
STEP: Trying to apply a random label on the found node. 12/14/22 17:05:28.69
STEP: verifying the node has the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 42 12/14/22 17:05:28.71
STEP: Trying to relaunch the pod, now with labels. 12/14/22 17:05:28.728
Dec 14 17:05:28.738: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6515" to be "not pending"
Dec 14 17:05:28.755: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034394ms
Dec 14 17:05:30.765: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026290498s
Dec 14 17:05:32.759: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02067966s
Dec 14 17:05:34.763: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024836988s
Dec 14 17:05:36.760: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 8.021873276s
Dec 14 17:05:36.761: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 off the node iet9eich7uhu-3 12/14/22 17:05:36.765
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 12/14/22 17:05:36.792
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:05:36.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6515" for this suite. 12/14/22 17:05:36.806
------------------------------
• [SLOW TEST] [12.356 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:05:24.463
    Dec 14 17:05:24.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-pred 12/14/22 17:05:24.466
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:24.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:24.511
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 14 17:05:24.516: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 17:05:24.534: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 17:05:24.544: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
    Dec 14 17:05:24.562: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:05:24.562: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 17:05:24.562: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 17:05:24.562: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 17:05:24.562: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Dec 14 17:05:24.562: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:05:24.562: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container kube-scheduler ready: true, restart count 2
    Dec 14 17:05:24.562: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:05:24.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:05:24.562: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 17:05:24.562: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
    Dec 14 17:05:24.579: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:05:24.579: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 17:05:24.579: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 17:05:24.579: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 17:05:24.579: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 14 17:05:24.579: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:05:24.579: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 14 17:05:24.579: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:05:24.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:05:24.579: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 17:05:24.579: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
    Dec 14 17:05:24.609: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container e2e ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 17:05:24.609: INFO: svc-latency-rc-4x9w7 from svc-latency-6453 started at 2022-12-14 17:05:07 +0000 UTC (1 container statuses recorded)
    Dec 14 17:05:24.609: INFO: 	Container svc-latency-rc ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/14/22 17:05:24.609
    Dec 14 17:05:24.628: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6515" to be "running"
    Dec 14 17:05:24.636: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382393ms
    Dec 14 17:05:26.643: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015125445s
    Dec 14 17:05:28.648: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.019865718s
    Dec 14 17:05:28.648: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/14/22 17:05:28.656
    STEP: Trying to apply a random label on the found node. 12/14/22 17:05:28.69
    STEP: verifying the node has the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 42 12/14/22 17:05:28.71
    STEP: Trying to relaunch the pod, now with labels. 12/14/22 17:05:28.728
    Dec 14 17:05:28.738: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6515" to be "not pending"
    Dec 14 17:05:28.755: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034394ms
    Dec 14 17:05:30.765: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026290498s
    Dec 14 17:05:32.759: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02067966s
    Dec 14 17:05:34.763: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024836988s
    Dec 14 17:05:36.760: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 8.021873276s
    Dec 14 17:05:36.761: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 off the node iet9eich7uhu-3 12/14/22 17:05:36.765
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-7b96836e-afad-4185-b173-b30f022164e2 12/14/22 17:05:36.792
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:05:36.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6515" for this suite. 12/14/22 17:05:36.806
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:05:36.821
Dec 14 17:05:36.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename emptydir 12/14/22 17:05:36.823
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:36.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:36.855
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 17:05:36.86
Dec 14 17:05:36.871: INFO: Waiting up to 5m0s for pod "pod-230210ca-4710-434e-bff2-50326c16141d" in namespace "emptydir-7931" to be "Succeeded or Failed"
Dec 14 17:05:36.877: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138254ms
Dec 14 17:05:38.885: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Running", Reason="", readiness=false. Elapsed: 2.013385373s
Dec 14 17:05:40.885: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013349098s
Dec 14 17:05:42.882: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010381967s
STEP: Saw pod success 12/14/22 17:05:42.882
Dec 14 17:05:42.882: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d" satisfied condition "Succeeded or Failed"
Dec 14 17:05:42.888: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-230210ca-4710-434e-bff2-50326c16141d container test-container: <nil>
STEP: delete the pod 12/14/22 17:05:42.901
Dec 14 17:05:42.919: INFO: Waiting for pod pod-230210ca-4710-434e-bff2-50326c16141d to disappear
Dec 14 17:05:42.923: INFO: Pod pod-230210ca-4710-434e-bff2-50326c16141d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec 14 17:05:42.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7931" for this suite. 12/14/22 17:05:42.931
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:05:36.821
    Dec 14 17:05:36.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename emptydir 12/14/22 17:05:36.823
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:36.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:36.855
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/14/22 17:05:36.86
    Dec 14 17:05:36.871: INFO: Waiting up to 5m0s for pod "pod-230210ca-4710-434e-bff2-50326c16141d" in namespace "emptydir-7931" to be "Succeeded or Failed"
    Dec 14 17:05:36.877: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138254ms
    Dec 14 17:05:38.885: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Running", Reason="", readiness=false. Elapsed: 2.013385373s
    Dec 14 17:05:40.885: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013349098s
    Dec 14 17:05:42.882: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010381967s
    STEP: Saw pod success 12/14/22 17:05:42.882
    Dec 14 17:05:42.882: INFO: Pod "pod-230210ca-4710-434e-bff2-50326c16141d" satisfied condition "Succeeded or Failed"
    Dec 14 17:05:42.888: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-230210ca-4710-434e-bff2-50326c16141d container test-container: <nil>
    STEP: delete the pod 12/14/22 17:05:42.901
    Dec 14 17:05:42.919: INFO: Waiting for pod pod-230210ca-4710-434e-bff2-50326c16141d to disappear
    Dec 14 17:05:42.923: INFO: Pod pod-230210ca-4710-434e-bff2-50326c16141d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:05:42.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7931" for this suite. 12/14/22 17:05:42.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:05:42.95
Dec 14 17:05:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename dns 12/14/22 17:05:42.953
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:42.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:42.984
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 12/14/22 17:05:42.987
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_tcp@PTR;sleep 1; done
 12/14/22 17:05:43.039
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_tcp@PTR;sleep 1; done
 12/14/22 17:05:43.039
STEP: creating a pod to probe DNS 12/14/22 17:05:43.039
STEP: submitting the pod to kubernetes 12/14/22 17:05:43.04
Dec 14 17:05:43.103: INFO: Waiting up to 15m0s for pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037" in namespace "dns-2121" to be "running"
Dec 14 17:05:43.111: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268765ms
Dec 14 17:05:45.118: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015160213s
Dec 14 17:05:47.122: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Running", Reason="", readiness=true. Elapsed: 4.019399766s
Dec 14 17:05:47.122: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037" satisfied condition "running"
STEP: retrieving the pod 12/14/22 17:05:47.122
STEP: looking for the results for each expected name from probers 12/14/22 17:05:47.128
Dec 14 17:05:47.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:47.148: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:47.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:47.197: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:47.202: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:47.248: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:05:52.259: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:52.265: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:52.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:52.318: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:52.324: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:52.355: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:05:57.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:57.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:57.305: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:57.310: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:05:57.340: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:06:02.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:02.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:02.314: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:02.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:02.361: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:06:07.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:07.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:07.302: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:07.308: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:07.340: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:06:12.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:12.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:12.297: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:12.302: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
Dec 14 17:06:12.330: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

Dec 14 17:06:17.325: INFO: DNS probes using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 succeeded

STEP: deleting the pod 12/14/22 17:06:17.325
STEP: deleting the test service 12/14/22 17:06:17.354
STEP: deleting the test headless service 12/14/22 17:06:17.42
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec 14 17:06:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2121" for this suite. 12/14/22 17:06:17.46
------------------------------
• [SLOW TEST] [34.527 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:05:42.95
    Dec 14 17:05:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename dns 12/14/22 17:05:42.953
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:05:42.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:05:42.984
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 12/14/22 17:05:42.987
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_tcp@PTR;sleep 1; done
     12/14/22 17:05:43.039
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.13.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.13.117_tcp@PTR;sleep 1; done
     12/14/22 17:05:43.039
    STEP: creating a pod to probe DNS 12/14/22 17:05:43.039
    STEP: submitting the pod to kubernetes 12/14/22 17:05:43.04
    Dec 14 17:05:43.103: INFO: Waiting up to 15m0s for pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037" in namespace "dns-2121" to be "running"
    Dec 14 17:05:43.111: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268765ms
    Dec 14 17:05:45.118: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015160213s
    Dec 14 17:05:47.122: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037": Phase="Running", Reason="", readiness=true. Elapsed: 4.019399766s
    Dec 14 17:05:47.122: INFO: Pod "dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037" satisfied condition "running"
    STEP: retrieving the pod 12/14/22 17:05:47.122
    STEP: looking for the results for each expected name from probers 12/14/22 17:05:47.128
    Dec 14 17:05:47.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:47.148: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:47.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:47.197: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:47.202: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:47.248: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:05:52.259: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:52.265: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:52.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:52.318: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:52.324: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:52.355: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:05:57.255: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:57.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:57.305: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:57.310: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:05:57.340: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:06:02.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:02.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:02.314: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:02.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:02.361: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:06:07.257: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:07.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:07.302: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:07.308: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:07.340: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:06:12.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:12.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:12.297: INFO: Unable to read jessie_udp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:12.302: INFO: Unable to read jessie_tcp@dns-test-service.dns-2121.svc.cluster.local from pod dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037: the server could not find the requested resource (get pods dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037)
    Dec 14 17:06:12.330: INFO: Lookups using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 failed for: [wheezy_udp@dns-test-service.dns-2121.svc.cluster.local wheezy_tcp@dns-test-service.dns-2121.svc.cluster.local jessie_udp@dns-test-service.dns-2121.svc.cluster.local jessie_tcp@dns-test-service.dns-2121.svc.cluster.local]

    Dec 14 17:06:17.325: INFO: DNS probes using dns-2121/dns-test-c30275eb-d2b4-4016-8f1c-d4e4ea97c037 succeeded

    STEP: deleting the pod 12/14/22 17:06:17.325
    STEP: deleting the test service 12/14/22 17:06:17.354
    STEP: deleting the test headless service 12/14/22 17:06:17.42
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:06:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2121" for this suite. 12/14/22 17:06:17.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:06:17.479
Dec 14 17:06:17.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 17:06:17.482
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:06:17.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:06:17.521
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 12/14/22 17:06:17.567
STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 17:06:17.589
Dec 14 17:06:17.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:06:17.619: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 17:06:18.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:06:18.634: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 17:06:19.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:06:19.640: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
Dec 14 17:06:20.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:06:20.634: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/14/22 17:06:20.638
Dec 14 17:06:20.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:06:20.675: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 17:06:21.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:06:21.699: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 17:06:22.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:06:22.695: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
Dec 14 17:06:23.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:06:23.688: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 17:06:23.693
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8247, will wait for the garbage collector to delete the pods 12/14/22 17:06:23.693
Dec 14 17:06:23.758: INFO: Deleting DaemonSet.extensions daemon-set took: 9.800129ms
Dec 14 17:06:23.859: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.46903ms
Dec 14 17:06:26.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:06:26.063: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 17:06:26.069: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33407"},"items":null}

Dec 14 17:06:26.073: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33408"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:06:26.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8247" for this suite. 12/14/22 17:06:26.105
------------------------------
• [SLOW TEST] [8.635 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:06:17.479
    Dec 14 17:06:17.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 17:06:17.482
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:06:17.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:06:17.521
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 12/14/22 17:06:17.567
    STEP: Check that daemon pods launch on every node of the cluster. 12/14/22 17:06:17.589
    Dec 14 17:06:17.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:06:17.619: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 17:06:18.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:06:18.634: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 17:06:19.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 17:06:19.640: INFO: Node iet9eich7uhu-3 is running 0 daemon pod, expected 1
    Dec 14 17:06:20.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 17:06:20.634: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/14/22 17:06:20.638
    Dec 14 17:06:20.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 17:06:20.675: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 17:06:21.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 17:06:21.699: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 17:06:22.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec 14 17:06:22.695: INFO: Node iet9eich7uhu-1 is running 0 daemon pod, expected 1
    Dec 14 17:06:23.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 14 17:06:23.688: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 17:06:23.693
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8247, will wait for the garbage collector to delete the pods 12/14/22 17:06:23.693
    Dec 14 17:06:23.758: INFO: Deleting DaemonSet.extensions daemon-set took: 9.800129ms
    Dec 14 17:06:23.859: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.46903ms
    Dec 14 17:06:26.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:06:26.063: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 17:06:26.069: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33407"},"items":null}

    Dec 14 17:06:26.073: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33408"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:06:26.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8247" for this suite. 12/14/22 17:06:26.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:06:26.118
Dec 14 17:06:26.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename statefulset 12/14/22 17:06:26.121
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:06:26.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:06:26.15
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-64 12/14/22 17:06:26.156
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 12/14/22 17:06:26.167
Dec 14 17:06:26.193: INFO: Found 0 stateful pods, waiting for 3
Dec 14 17:06:36.205: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:06:36.206: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:06:36.206: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:06:36.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 17:06:36.509: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 17:06:36.509: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 17:06:36.509: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/14/22 17:06:46.544
Dec 14 17:06:46.572: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/14/22 17:06:46.572
STEP: Updating Pods in reverse ordinal order 12/14/22 17:06:56.597
Dec 14 17:06:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 17:06:56.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 17:06:56.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 17:06:56.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 12/14/22 17:07:06.863
Dec 14 17:07:06.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 17:07:07.180: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 17:07:07.180: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 17:07:07.180: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 17:07:17.246: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 12/14/22 17:07:27.266
Dec 14 17:07:27.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 17:07:27.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 17:07:27.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 17:07:27.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec 14 17:07:37.564: INFO: Deleting all statefulset in ns statefulset-64
Dec 14 17:07:37.568: INFO: Scaling statefulset ss2 to 0
Dec 14 17:07:47.599: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 17:07:47.605: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec 14 17:07:47.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-64" for this suite. 12/14/22 17:07:47.662
------------------------------
• [SLOW TEST] [81.555 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:06:26.118
    Dec 14 17:06:26.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename statefulset 12/14/22 17:06:26.121
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:06:26.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:06:26.15
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-64 12/14/22 17:06:26.156
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 12/14/22 17:06:26.167
    Dec 14 17:06:26.193: INFO: Found 0 stateful pods, waiting for 3
    Dec 14 17:06:36.205: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 17:06:36.206: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 17:06:36.206: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Dec 14 17:06:36.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 17:06:36.509: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 17:06:36.509: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 17:06:36.509: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/14/22 17:06:46.544
    Dec 14 17:06:46.572: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/14/22 17:06:46.572
    STEP: Updating Pods in reverse ordinal order 12/14/22 17:06:56.597
    Dec 14 17:06:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 17:06:56.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 17:06:56.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 17:06:56.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 12/14/22 17:07:06.863
    Dec 14 17:07:06.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 14 17:07:07.180: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 14 17:07:07.180: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 14 17:07:07.180: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 14 17:07:17.246: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 12/14/22 17:07:27.266
    Dec 14 17:07:27.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=statefulset-64 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 14 17:07:27.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 14 17:07:27.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 14 17:07:27.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec 14 17:07:37.564: INFO: Deleting all statefulset in ns statefulset-64
    Dec 14 17:07:37.568: INFO: Scaling statefulset ss2 to 0
    Dec 14 17:07:47.599: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 14 17:07:47.605: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:07:47.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-64" for this suite. 12/14/22 17:07:47.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:07:47.677
Dec 14 17:07:47.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename replication-controller 12/14/22 17:07:47.681
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:47.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:47.709
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-2tw59" 12/14/22 17:07:47.713
Dec 14 17:07:47.721: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
Dec 14 17:07:48.727: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
Dec 14 17:07:48.734: INFO: Found 1 replicas for "e2e-rc-2tw59" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-2tw59" 12/14/22 17:07:48.734
STEP: Updating a scale subresource 12/14/22 17:07:48.74
STEP: Verifying replicas where modified for replication controller "e2e-rc-2tw59" 12/14/22 17:07:48.749
Dec 14 17:07:48.749: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
Dec 14 17:07:49.767: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
Dec 14 17:07:49.781: INFO: Found 2 replicas for "e2e-rc-2tw59" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec 14 17:07:49.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7013" for this suite. 12/14/22 17:07:49.789
------------------------------
• [2.122 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:07:47.677
    Dec 14 17:07:47.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename replication-controller 12/14/22 17:07:47.681
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:47.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:47.709
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-2tw59" 12/14/22 17:07:47.713
    Dec 14 17:07:47.721: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
    Dec 14 17:07:48.727: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
    Dec 14 17:07:48.734: INFO: Found 1 replicas for "e2e-rc-2tw59" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-2tw59" 12/14/22 17:07:48.734
    STEP: Updating a scale subresource 12/14/22 17:07:48.74
    STEP: Verifying replicas where modified for replication controller "e2e-rc-2tw59" 12/14/22 17:07:48.749
    Dec 14 17:07:48.749: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
    Dec 14 17:07:49.767: INFO: Get Replication Controller "e2e-rc-2tw59" to confirm replicas
    Dec 14 17:07:49.781: INFO: Found 2 replicas for "e2e-rc-2tw59" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:07:49.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7013" for this suite. 12/14/22 17:07:49.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:07:49.81
Dec 14 17:07:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename projected 12/14/22 17:07:49.813
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:49.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:49.84
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-c2f1e6de-9d44-4aeb-af8f-5f658088dd38 12/14/22 17:07:49.845
STEP: Creating a pod to test consume configMaps 12/14/22 17:07:49.868
Dec 14 17:07:49.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7" in namespace "projected-6895" to be "Succeeded or Failed"
Dec 14 17:07:49.890: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.444945ms
Dec 14 17:07:51.895: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010046462s
Dec 14 17:07:53.896: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062861s
STEP: Saw pod success 12/14/22 17:07:53.896
Dec 14 17:07:53.896: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7" satisfied condition "Succeeded or Failed"
Dec 14 17:07:53.902: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 container agnhost-container: <nil>
STEP: delete the pod 12/14/22 17:07:54.156
Dec 14 17:07:54.179: INFO: Waiting for pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 to disappear
Dec 14 17:07:54.183: INFO: Pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec 14 17:07:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6895" for this suite. 12/14/22 17:07:54.189
------------------------------
• [4.388 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:07:49.81
    Dec 14 17:07:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename projected 12/14/22 17:07:49.813
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:49.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:49.84
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-c2f1e6de-9d44-4aeb-af8f-5f658088dd38 12/14/22 17:07:49.845
    STEP: Creating a pod to test consume configMaps 12/14/22 17:07:49.868
    Dec 14 17:07:49.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7" in namespace "projected-6895" to be "Succeeded or Failed"
    Dec 14 17:07:49.890: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.444945ms
    Dec 14 17:07:51.895: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010046462s
    Dec 14 17:07:53.896: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062861s
    STEP: Saw pod success 12/14/22 17:07:53.896
    Dec 14 17:07:53.896: INFO: Pod "pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7" satisfied condition "Succeeded or Failed"
    Dec 14 17:07:53.902: INFO: Trying to get logs from node iet9eich7uhu-3 pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 container agnhost-container: <nil>
    STEP: delete the pod 12/14/22 17:07:54.156
    Dec 14 17:07:54.179: INFO: Waiting for pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 to disappear
    Dec 14 17:07:54.183: INFO: Pod pod-projected-configmaps-0c08b0e3-4074-4b7f-93c2-a21c1a6293d7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:07:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6895" for this suite. 12/14/22 17:07:54.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:07:54.202
Dec 14 17:07:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:54.204
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:54.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:54.228
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 12/14/22 17:07:54.232
Dec 14 17:07:54.233: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3577 proxy --unix-socket=/tmp/kubectl-proxy-unix1185732503/test'
STEP: retrieving proxy /api/ output 12/14/22 17:07:54.384
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 17:07:54.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3577" for this suite. 12/14/22 17:07:54.394
------------------------------
• [0.206 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:07:54.202
    Dec 14 17:07:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:54.204
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:54.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:54.228
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 12/14/22 17:07:54.232
    Dec 14 17:07:54.233: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-3577 proxy --unix-socket=/tmp/kubectl-proxy-unix1185732503/test'
    STEP: retrieving proxy /api/ output 12/14/22 17:07:54.384
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:07:54.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3577" for this suite. 12/14/22 17:07:54.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:07:54.41
Dec 14 17:07:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:54.412
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:54.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:54.439
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 12/14/22 17:07:54.442
Dec 14 17:07:54.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 create -f -'
Dec 14 17:07:55.248: INFO: stderr: ""
Dec 14 17:07:55.248: INFO: stdout: "pod/pause created\n"
Dec 14 17:07:55.248: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 17:07:55.248: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6849" to be "running and ready"
Dec 14 17:07:55.264: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.497135ms
Dec 14 17:07:55.264: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'iet9eich7uhu-3' to be 'Running' but was 'Pending'
Dec 14 17:07:57.271: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.023474999s
Dec 14 17:07:57.271: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 17:07:57.271: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 12/14/22 17:07:57.271
Dec 14 17:07:57.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 label pods pause testing-label=testing-label-value'
Dec 14 17:07:57.412: INFO: stderr: ""
Dec 14 17:07:57.412: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 12/14/22 17:07:57.412
Dec 14 17:07:57.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pod pause -L testing-label'
Dec 14 17:07:57.529: INFO: stderr: ""
Dec 14 17:07:57.529: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 12/14/22 17:07:57.529
Dec 14 17:07:57.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 label pods pause testing-label-'
Dec 14 17:07:57.672: INFO: stderr: ""
Dec 14 17:07:57.672: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 12/14/22 17:07:57.672
Dec 14 17:07:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pod pause -L testing-label'
Dec 14 17:07:57.789: INFO: stderr: ""
Dec 14 17:07:57.789: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 12/14/22 17:07:57.789
Dec 14 17:07:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 delete --grace-period=0 --force -f -'
Dec 14 17:07:57.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:07:57.920: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 17:07:57.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get rc,svc -l name=pause --no-headers'
Dec 14 17:07:58.058: INFO: stderr: "No resources found in kubectl-6849 namespace.\n"
Dec 14 17:07:58.058: INFO: stdout: ""
Dec 14 17:07:58.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 17:07:58.192: INFO: stderr: ""
Dec 14 17:07:58.193: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 17:07:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6849" for this suite. 12/14/22 17:07:58.198
------------------------------
• [3.795 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:07:54.41
    Dec 14 17:07:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:54.412
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:54.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:54.439
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 12/14/22 17:07:54.442
    Dec 14 17:07:54.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 create -f -'
    Dec 14 17:07:55.248: INFO: stderr: ""
    Dec 14 17:07:55.248: INFO: stdout: "pod/pause created\n"
    Dec 14 17:07:55.248: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Dec 14 17:07:55.248: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6849" to be "running and ready"
    Dec 14 17:07:55.264: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.497135ms
    Dec 14 17:07:55.264: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'iet9eich7uhu-3' to be 'Running' but was 'Pending'
    Dec 14 17:07:57.271: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.023474999s
    Dec 14 17:07:57.271: INFO: Pod "pause" satisfied condition "running and ready"
    Dec 14 17:07:57.271: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 12/14/22 17:07:57.271
    Dec 14 17:07:57.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 label pods pause testing-label=testing-label-value'
    Dec 14 17:07:57.412: INFO: stderr: ""
    Dec 14 17:07:57.412: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 12/14/22 17:07:57.412
    Dec 14 17:07:57.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pod pause -L testing-label'
    Dec 14 17:07:57.529: INFO: stderr: ""
    Dec 14 17:07:57.529: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 12/14/22 17:07:57.529
    Dec 14 17:07:57.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 label pods pause testing-label-'
    Dec 14 17:07:57.672: INFO: stderr: ""
    Dec 14 17:07:57.672: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 12/14/22 17:07:57.672
    Dec 14 17:07:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pod pause -L testing-label'
    Dec 14 17:07:57.789: INFO: stderr: ""
    Dec 14 17:07:57.789: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 12/14/22 17:07:57.789
    Dec 14 17:07:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 delete --grace-period=0 --force -f -'
    Dec 14 17:07:57.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 14 17:07:57.920: INFO: stdout: "pod \"pause\" force deleted\n"
    Dec 14 17:07:57.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get rc,svc -l name=pause --no-headers'
    Dec 14 17:07:58.058: INFO: stderr: "No resources found in kubectl-6849 namespace.\n"
    Dec 14 17:07:58.058: INFO: stdout: ""
    Dec 14 17:07:58.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-6849 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 14 17:07:58.192: INFO: stderr: ""
    Dec 14 17:07:58.193: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:07:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6849" for this suite. 12/14/22 17:07:58.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:07:58.206
Dec 14 17:07:58.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:58.208
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:58.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:58.231
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 17:07:58.236
Dec 14 17:07:58.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 17:07:58.383: INFO: stderr: ""
Dec 14 17:07:58.383: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 12/14/22 17:07:58.383
Dec 14 17:07:58.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Dec 14 17:07:59.889: INFO: stderr: ""
Dec 14 17:07:59.889: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 17:07:59.889
Dec 14 17:07:59.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 delete pods e2e-test-httpd-pod'
Dec 14 17:08:01.681: INFO: stderr: ""
Dec 14 17:08:01.681: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec 14 17:08:01.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8524" for this suite. 12/14/22 17:08:01.691
------------------------------
• [3.495 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:07:58.206
    Dec 14 17:07:58.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename kubectl 12/14/22 17:07:58.208
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:07:58.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:07:58.231
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 17:07:58.236
    Dec 14 17:07:58.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 14 17:07:58.383: INFO: stderr: ""
    Dec 14 17:07:58.383: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 12/14/22 17:07:58.383
    Dec 14 17:07:58.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Dec 14 17:07:59.889: INFO: stderr: ""
    Dec 14 17:07:59.889: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/14/22 17:07:59.889
    Dec 14 17:07:59.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1642710381 --namespace=kubectl-8524 delete pods e2e-test-httpd-pod'
    Dec 14 17:08:01.681: INFO: stderr: ""
    Dec 14 17:08:01.681: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:08:01.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8524" for this suite. 12/14/22 17:08:01.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:08:01.702
Dec 14 17:08:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename discovery 12/14/22 17:08:01.708
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:01.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:01.736
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 12/14/22 17:08:01.744
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Dec 14 17:08:02.757: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 14 17:08:02.759: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 14 17:08:02.759: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec 14 17:08:02.759: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 14 17:08:02.759: INFO: Checking APIGroup: apps
Dec 14 17:08:02.761: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 14 17:08:02.761: INFO: Versions found [{apps/v1 v1}]
Dec 14 17:08:02.761: INFO: apps/v1 matches apps/v1
Dec 14 17:08:02.761: INFO: Checking APIGroup: events.k8s.io
Dec 14 17:08:02.762: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 14 17:08:02.762: INFO: Versions found [{events.k8s.io/v1 v1}]
Dec 14 17:08:02.762: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 14 17:08:02.762: INFO: Checking APIGroup: authentication.k8s.io
Dec 14 17:08:02.764: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 14 17:08:02.764: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec 14 17:08:02.764: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 14 17:08:02.764: INFO: Checking APIGroup: authorization.k8s.io
Dec 14 17:08:02.765: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 14 17:08:02.765: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec 14 17:08:02.765: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 14 17:08:02.765: INFO: Checking APIGroup: autoscaling
Dec 14 17:08:02.767: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec 14 17:08:02.767: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Dec 14 17:08:02.767: INFO: autoscaling/v2 matches autoscaling/v2
Dec 14 17:08:02.767: INFO: Checking APIGroup: batch
Dec 14 17:08:02.768: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 14 17:08:02.769: INFO: Versions found [{batch/v1 v1}]
Dec 14 17:08:02.769: INFO: batch/v1 matches batch/v1
Dec 14 17:08:02.769: INFO: Checking APIGroup: certificates.k8s.io
Dec 14 17:08:02.770: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 14 17:08:02.770: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec 14 17:08:02.770: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 14 17:08:02.770: INFO: Checking APIGroup: networking.k8s.io
Dec 14 17:08:02.771: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 14 17:08:02.772: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec 14 17:08:02.772: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 14 17:08:02.772: INFO: Checking APIGroup: policy
Dec 14 17:08:02.773: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 14 17:08:02.773: INFO: Versions found [{policy/v1 v1}]
Dec 14 17:08:02.773: INFO: policy/v1 matches policy/v1
Dec 14 17:08:02.773: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 14 17:08:02.776: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 14 17:08:02.776: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec 14 17:08:02.776: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 14 17:08:02.776: INFO: Checking APIGroup: storage.k8s.io
Dec 14 17:08:02.779: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 14 17:08:02.779: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 14 17:08:02.779: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 14 17:08:02.779: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 14 17:08:02.781: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 14 17:08:02.781: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec 14 17:08:02.781: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 14 17:08:02.781: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 14 17:08:02.783: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 14 17:08:02.783: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec 14 17:08:02.783: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 14 17:08:02.783: INFO: Checking APIGroup: scheduling.k8s.io
Dec 14 17:08:02.784: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 14 17:08:02.785: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec 14 17:08:02.785: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 14 17:08:02.785: INFO: Checking APIGroup: coordination.k8s.io
Dec 14 17:08:02.786: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 14 17:08:02.786: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec 14 17:08:02.786: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 14 17:08:02.786: INFO: Checking APIGroup: node.k8s.io
Dec 14 17:08:02.787: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 14 17:08:02.787: INFO: Versions found [{node.k8s.io/v1 v1}]
Dec 14 17:08:02.787: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 14 17:08:02.787: INFO: Checking APIGroup: discovery.k8s.io
Dec 14 17:08:02.789: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 14 17:08:02.789: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Dec 14 17:08:02.789: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 14 17:08:02.789: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 14 17:08:02.790: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Dec 14 17:08:02.790: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Dec 14 17:08:02.790: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Dec 14 17:08:02.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-807" for this suite. 12/14/22 17:08:02.796
------------------------------
• [1.107 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:08:01.702
    Dec 14 17:08:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename discovery 12/14/22 17:08:01.708
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:01.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:01.736
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 12/14/22 17:08:01.744
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Dec 14 17:08:02.757: INFO: Checking APIGroup: apiregistration.k8s.io
    Dec 14 17:08:02.759: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Dec 14 17:08:02.759: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Dec 14 17:08:02.759: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Dec 14 17:08:02.759: INFO: Checking APIGroup: apps
    Dec 14 17:08:02.761: INFO: PreferredVersion.GroupVersion: apps/v1
    Dec 14 17:08:02.761: INFO: Versions found [{apps/v1 v1}]
    Dec 14 17:08:02.761: INFO: apps/v1 matches apps/v1
    Dec 14 17:08:02.761: INFO: Checking APIGroup: events.k8s.io
    Dec 14 17:08:02.762: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Dec 14 17:08:02.762: INFO: Versions found [{events.k8s.io/v1 v1}]
    Dec 14 17:08:02.762: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Dec 14 17:08:02.762: INFO: Checking APIGroup: authentication.k8s.io
    Dec 14 17:08:02.764: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Dec 14 17:08:02.764: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Dec 14 17:08:02.764: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Dec 14 17:08:02.764: INFO: Checking APIGroup: authorization.k8s.io
    Dec 14 17:08:02.765: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Dec 14 17:08:02.765: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Dec 14 17:08:02.765: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Dec 14 17:08:02.765: INFO: Checking APIGroup: autoscaling
    Dec 14 17:08:02.767: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Dec 14 17:08:02.767: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Dec 14 17:08:02.767: INFO: autoscaling/v2 matches autoscaling/v2
    Dec 14 17:08:02.767: INFO: Checking APIGroup: batch
    Dec 14 17:08:02.768: INFO: PreferredVersion.GroupVersion: batch/v1
    Dec 14 17:08:02.769: INFO: Versions found [{batch/v1 v1}]
    Dec 14 17:08:02.769: INFO: batch/v1 matches batch/v1
    Dec 14 17:08:02.769: INFO: Checking APIGroup: certificates.k8s.io
    Dec 14 17:08:02.770: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Dec 14 17:08:02.770: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Dec 14 17:08:02.770: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Dec 14 17:08:02.770: INFO: Checking APIGroup: networking.k8s.io
    Dec 14 17:08:02.771: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Dec 14 17:08:02.772: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Dec 14 17:08:02.772: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Dec 14 17:08:02.772: INFO: Checking APIGroup: policy
    Dec 14 17:08:02.773: INFO: PreferredVersion.GroupVersion: policy/v1
    Dec 14 17:08:02.773: INFO: Versions found [{policy/v1 v1}]
    Dec 14 17:08:02.773: INFO: policy/v1 matches policy/v1
    Dec 14 17:08:02.773: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Dec 14 17:08:02.776: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Dec 14 17:08:02.776: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Dec 14 17:08:02.776: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Dec 14 17:08:02.776: INFO: Checking APIGroup: storage.k8s.io
    Dec 14 17:08:02.779: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Dec 14 17:08:02.779: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Dec 14 17:08:02.779: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Dec 14 17:08:02.779: INFO: Checking APIGroup: admissionregistration.k8s.io
    Dec 14 17:08:02.781: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Dec 14 17:08:02.781: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Dec 14 17:08:02.781: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Dec 14 17:08:02.781: INFO: Checking APIGroup: apiextensions.k8s.io
    Dec 14 17:08:02.783: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Dec 14 17:08:02.783: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Dec 14 17:08:02.783: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Dec 14 17:08:02.783: INFO: Checking APIGroup: scheduling.k8s.io
    Dec 14 17:08:02.784: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Dec 14 17:08:02.785: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Dec 14 17:08:02.785: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Dec 14 17:08:02.785: INFO: Checking APIGroup: coordination.k8s.io
    Dec 14 17:08:02.786: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Dec 14 17:08:02.786: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Dec 14 17:08:02.786: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Dec 14 17:08:02.786: INFO: Checking APIGroup: node.k8s.io
    Dec 14 17:08:02.787: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Dec 14 17:08:02.787: INFO: Versions found [{node.k8s.io/v1 v1}]
    Dec 14 17:08:02.787: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Dec 14 17:08:02.787: INFO: Checking APIGroup: discovery.k8s.io
    Dec 14 17:08:02.789: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Dec 14 17:08:02.789: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Dec 14 17:08:02.789: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Dec 14 17:08:02.789: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Dec 14 17:08:02.790: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Dec 14 17:08:02.790: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Dec 14 17:08:02.790: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:08:02.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-807" for this suite. 12/14/22 17:08:02.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:08:02.814
Dec 14 17:08:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename secrets 12/14/22 17:08:02.816
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:02.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:02.844
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-d0b9cf55-a975-48d0-b34a-c844dc425644 12/14/22 17:08:02.854
STEP: Creating secret with name s-test-opt-upd-b25251e4-e4d2-40f7-8ff6-d1d1e5d84d86 12/14/22 17:08:02.863
STEP: Creating the pod 12/14/22 17:08:02.869
Dec 14 17:08:02.880: INFO: Waiting up to 5m0s for pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78" in namespace "secrets-2822" to be "running and ready"
Dec 14 17:08:02.885: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696184ms
Dec 14 17:08:02.885: INFO: The phase of Pod pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:08:04.892: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78": Phase="Running", Reason="", readiness=true. Elapsed: 2.011404478s
Dec 14 17:08:04.892: INFO: The phase of Pod pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78 is Running (Ready = true)
Dec 14 17:08:04.892: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d0b9cf55-a975-48d0-b34a-c844dc425644 12/14/22 17:08:04.928
STEP: Updating secret s-test-opt-upd-b25251e4-e4d2-40f7-8ff6-d1d1e5d84d86 12/14/22 17:08:04.939
STEP: Creating secret with name s-test-opt-create-c48fce0c-eb18-4ea3-be17-a44f5cea8d18 12/14/22 17:08:04.947
STEP: waiting to observe update in volume 12/14/22 17:08:04.952
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec 14 17:08:06.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2822" for this suite. 12/14/22 17:08:07.013
------------------------------
• [4.210 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:08:02.814
    Dec 14 17:08:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename secrets 12/14/22 17:08:02.816
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:02.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:02.844
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-d0b9cf55-a975-48d0-b34a-c844dc425644 12/14/22 17:08:02.854
    STEP: Creating secret with name s-test-opt-upd-b25251e4-e4d2-40f7-8ff6-d1d1e5d84d86 12/14/22 17:08:02.863
    STEP: Creating the pod 12/14/22 17:08:02.869
    Dec 14 17:08:02.880: INFO: Waiting up to 5m0s for pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78" in namespace "secrets-2822" to be "running and ready"
    Dec 14 17:08:02.885: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696184ms
    Dec 14 17:08:02.885: INFO: The phase of Pod pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:08:04.892: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78": Phase="Running", Reason="", readiness=true. Elapsed: 2.011404478s
    Dec 14 17:08:04.892: INFO: The phase of Pod pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78 is Running (Ready = true)
    Dec 14 17:08:04.892: INFO: Pod "pod-secrets-28d66374-095b-4aee-ab7f-1d528fa9de78" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d0b9cf55-a975-48d0-b34a-c844dc425644 12/14/22 17:08:04.928
    STEP: Updating secret s-test-opt-upd-b25251e4-e4d2-40f7-8ff6-d1d1e5d84d86 12/14/22 17:08:04.939
    STEP: Creating secret with name s-test-opt-create-c48fce0c-eb18-4ea3-be17-a44f5cea8d18 12/14/22 17:08:04.947
    STEP: waiting to observe update in volume 12/14/22 17:08:04.952
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:08:06.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2822" for this suite. 12/14/22 17:08:07.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:08:07.025
Dec 14 17:08:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pods 12/14/22 17:08:07.027
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:07.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:07.063
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 12/14/22 17:08:07.07
STEP: submitting the pod to kubernetes 12/14/22 17:08:07.071
Dec 14 17:08:07.089: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" in namespace "pods-2243" to be "running and ready"
Dec 14 17:08:07.098: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034292ms
Dec 14 17:08:07.098: INFO: The phase of Pod pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:08:09.104: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 2.015366549s
Dec 14 17:08:09.104: INFO: The phase of Pod pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 is Running (Ready = true)
Dec 14 17:08:09.104: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/14/22 17:08:09.11
STEP: updating the pod 12/14/22 17:08:09.117
Dec 14 17:08:09.646: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01"
Dec 14 17:08:09.647: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" in namespace "pods-2243" to be "terminated with reason DeadlineExceeded"
Dec 14 17:08:09.658: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 11.89463ms
Dec 14 17:08:11.666: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 2.019043781s
Dec 14 17:08:13.667: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=false. Elapsed: 4.020708698s
Dec 14 17:08:15.668: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021651426s
Dec 14 17:08:15.668: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec 14 17:08:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2243" for this suite. 12/14/22 17:08:15.675
------------------------------
• [SLOW TEST] [8.658 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:08:07.025
    Dec 14 17:08:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pods 12/14/22 17:08:07.027
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:07.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:07.063
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 12/14/22 17:08:07.07
    STEP: submitting the pod to kubernetes 12/14/22 17:08:07.071
    Dec 14 17:08:07.089: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" in namespace "pods-2243" to be "running and ready"
    Dec 14 17:08:07.098: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.034292ms
    Dec 14 17:08:07.098: INFO: The phase of Pod pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:08:09.104: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 2.015366549s
    Dec 14 17:08:09.104: INFO: The phase of Pod pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 is Running (Ready = true)
    Dec 14 17:08:09.104: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/14/22 17:08:09.11
    STEP: updating the pod 12/14/22 17:08:09.117
    Dec 14 17:08:09.646: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01"
    Dec 14 17:08:09.647: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" in namespace "pods-2243" to be "terminated with reason DeadlineExceeded"
    Dec 14 17:08:09.658: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 11.89463ms
    Dec 14 17:08:11.666: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=true. Elapsed: 2.019043781s
    Dec 14 17:08:13.667: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Running", Reason="", readiness=false. Elapsed: 4.020708698s
    Dec 14 17:08:15.668: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021651426s
    Dec 14 17:08:15.668: INFO: Pod "pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:08:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2243" for this suite. 12/14/22 17:08:15.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:08:15.686
Dec 14 17:08:15.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-pred 12/14/22 17:08:15.689
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:15.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:15.711
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec 14 17:08:15.714: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 17:08:15.727: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 17:08:15.731: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
Dec 14 17:08:15.743: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.743: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:08:15.743: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.744: INFO: 	Container coredns ready: true, restart count 0
Dec 14 17:08:15.744: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.744: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 17:08:15.744: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.744: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 17:08:15.744: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.744: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 17:08:15.744: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.744: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:08:15.744: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.745: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 17:08:15.745: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:08:15.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:08:15.745: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 17:08:15.745: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
Dec 14 17:08:15.755: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:08:15.755: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container coredns ready: true, restart count 0
Dec 14 17:08:15.755: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 17:08:15.755: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 17:08:15.755: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 17:08:15.755: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:08:15.755: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 17:08:15.755: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:08:15.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:08:15.755: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 17:08:15.755: INFO: 
Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
Dec 14 17:08:15.765: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 17:08:15.765: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 17:08:15.765: INFO: pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 from pods-2243 started at 2022-12-14 17:08:07 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container pause ready: false, restart count 0
Dec 14 17:08:15.765: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 17:08:15.765: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container e2e ready: true, restart count 0
Dec 14 17:08:15.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:08:15.765: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
Dec 14 17:08:15.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 17:08:15.765: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/14/22 17:08:15.765
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1730b7ee4b8e2db6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/14/22 17:08:15.812
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:08:16.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-296" for this suite. 12/14/22 17:08:16.827
------------------------------
• [1.152 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:08:15.686
    Dec 14 17:08:15.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-pred 12/14/22 17:08:15.689
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:15.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:15.711
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec 14 17:08:15.714: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 14 17:08:15.727: INFO: Waiting for terminating namespaces to be deleted...
    Dec 14 17:08:15.731: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-1 before test
    Dec 14 17:08:15.743: INFO: kube-flannel-ds-plh4q from kube-flannel started at 2022-12-14 15:35:26 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.743: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:08:15.743: INFO: coredns-787d4945fb-dthzx from kube-system started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.744: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 17:08:15.744: INFO: kube-addon-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.744: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 17:08:15.744: INFO: kube-apiserver-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:26:32 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.744: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 17:08:15.744: INFO: kube-controller-manager-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.744: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Dec 14 17:08:15.744: INFO: kube-proxy-6lpvc from kube-system started at 2022-12-14 15:35:18 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.744: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:08:15.744: INFO: kube-scheduler-iet9eich7uhu-1 from kube-system started at 2022-12-14 15:34:49 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.745: INFO: 	Container kube-scheduler ready: true, restart count 2
    Dec 14 17:08:15.745: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-758mh from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:08:15.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:08:15.745: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 17:08:15.745: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-2 before test
    Dec 14 17:08:15.755: INFO: kube-flannel-ds-flfrk from kube-flannel started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:08:15.755: INFO: coredns-787d4945fb-c79wd from kube-system started at 2022-12-14 15:35:12 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container coredns ready: true, restart count 0
    Dec 14 17:08:15.755: INFO: kube-addon-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-addon-manager ready: true, restart count 1
    Dec 14 17:08:15.755: INFO: kube-apiserver-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:27:07 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 14 17:08:15.755: INFO: kube-controller-manager-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 14 17:08:15.755: INFO: kube-proxy-xtp9v from kube-system started at 2022-12-14 15:35:15 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:08:15.755: INFO: kube-scheduler-iet9eich7uhu-2 from kube-system started at 2022-12-14 15:33:33 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 14 17:08:15.755: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-m2f7k from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:08:15.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:08:15.755: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 14 17:08:15.755: INFO: 
    Logging pods the apiserver thinks is on node iet9eich7uhu-3 before test
    Dec 14 17:08:15.765: INFO: kube-flannel-ds-vfpcb from kube-flannel started at 2022-12-14 16:06:39 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container kube-flannel ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: kube-proxy-b4ld4 from kube-system started at 2022-12-14 15:35:13 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: pod-update-activedeadlineseconds-a63fe2da-8ddc-4c8e-9713-9b697692ee01 from pods-2243 started at 2022-12-14 17:08:07 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container pause ready: false, restart count 0
    Dec 14 17:08:15.765: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:36:02 +0000 UTC (1 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: sonobuoy-e2e-job-d217a1a041484246 from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container e2e ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: sonobuoy-systemd-logs-daemon-set-0785ca08c2e746da-877cx from sonobuoy started at 2022-12-14 15:36:13 +0000 UTC (2 container statuses recorded)
    Dec 14 17:08:15.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 14 17:08:15.765: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/14/22 17:08:15.765
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1730b7ee4b8e2db6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/14/22 17:08:15.812
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:08:16.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-296" for this suite. 12/14/22 17:08:16.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:08:16.843
Dec 14 17:08:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename taint-multiple-pods 12/14/22 17:08:16.845
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:16.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:16.873
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Dec 14 17:08:16.877: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 17:09:16.903: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Dec 14 17:09:16.909: INFO: Starting informer...
STEP: Starting pods... 12/14/22 17:09:16.909
Dec 14 17:09:17.144: INFO: Pod1 is running on iet9eich7uhu-3. Tainting Node
Dec 14 17:09:17.364: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2762" to be "running"
Dec 14 17:09:17.372: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064962ms
Dec 14 17:09:19.380: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015548392s
Dec 14 17:09:19.381: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec 14 17:09:19.381: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2762" to be "running"
Dec 14 17:09:19.387: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.72034ms
Dec 14 17:09:19.387: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec 14 17:09:19.387: INFO: Pod2 is running on iet9eich7uhu-3. Tainting Node
STEP: Trying to apply a taint on the Node 12/14/22 17:09:19.387
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 17:09:19.415
STEP: Waiting for Pod1 and Pod2 to be deleted 12/14/22 17:09:19.423
Dec 14 17:09:25.434: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 14 17:09:45.520: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 17:09:45.542
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:09:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-2762" for this suite. 12/14/22 17:09:45.559
------------------------------
• [SLOW TEST] [88.729 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:08:16.843
    Dec 14 17:08:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename taint-multiple-pods 12/14/22 17:08:16.845
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:08:16.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:08:16.873
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Dec 14 17:08:16.877: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 17:09:16.903: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Dec 14 17:09:16.909: INFO: Starting informer...
    STEP: Starting pods... 12/14/22 17:09:16.909
    Dec 14 17:09:17.144: INFO: Pod1 is running on iet9eich7uhu-3. Tainting Node
    Dec 14 17:09:17.364: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2762" to be "running"
    Dec 14 17:09:17.372: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064962ms
    Dec 14 17:09:19.380: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015548392s
    Dec 14 17:09:19.381: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec 14 17:09:19.381: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2762" to be "running"
    Dec 14 17:09:19.387: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.72034ms
    Dec 14 17:09:19.387: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec 14 17:09:19.387: INFO: Pod2 is running on iet9eich7uhu-3. Tainting Node
    STEP: Trying to apply a taint on the Node 12/14/22 17:09:19.387
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 17:09:19.415
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/14/22 17:09:19.423
    Dec 14 17:09:25.434: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec 14 17:09:45.520: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/14/22 17:09:45.542
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:09:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-2762" for this suite. 12/14/22 17:09:45.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:09:45.58
Dec 14 17:09:45.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 17:09:45.583
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:09:45.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:09:45.67
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb in namespace container-probe-2668 12/14/22 17:09:45.674
Dec 14 17:09:45.685: INFO: Waiting up to 5m0s for pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb" in namespace "container-probe-2668" to be "not pending"
Dec 14 17:09:45.691: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170779ms
Dec 14 17:09:47.701: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01582841s
Dec 14 17:09:47.702: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb" satisfied condition "not pending"
Dec 14 17:09:47.702: INFO: Started pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb in namespace container-probe-2668
STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 17:09:47.702
Dec 14 17:09:47.707: INFO: Initial restart count of pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb is 0
STEP: deleting the pod 12/14/22 17:13:48.64
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 17:13:48.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2668" for this suite. 12/14/22 17:13:48.692
------------------------------
• [SLOW TEST] [243.126 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:09:45.58
    Dec 14 17:09:45.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 17:09:45.583
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:09:45.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:09:45.67
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb in namespace container-probe-2668 12/14/22 17:09:45.674
    Dec 14 17:09:45.685: INFO: Waiting up to 5m0s for pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb" in namespace "container-probe-2668" to be "not pending"
    Dec 14 17:09:45.691: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170779ms
    Dec 14 17:09:47.701: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01582841s
    Dec 14 17:09:47.702: INFO: Pod "liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb" satisfied condition "not pending"
    Dec 14 17:09:47.702: INFO: Started pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb in namespace container-probe-2668
    STEP: checking the pod's current state and verifying that restartCount is present 12/14/22 17:09:47.702
    Dec 14 17:09:47.707: INFO: Initial restart count of pod liveness-79dc5135-7059-4d05-bab4-3c3cfd28fefb is 0
    STEP: deleting the pod 12/14/22 17:13:48.64
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:13:48.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2668" for this suite. 12/14/22 17:13:48.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:13:48.708
Dec 14 17:13:48.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename sched-preemption 12/14/22 17:13:48.725
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:13:48.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:13:48.775
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec 14 17:13:48.809: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 17:14:48.850: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 12/14/22 17:14:48.855
Dec 14 17:14:48.890: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 17:14:48.899: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 17:14:48.932: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 17:14:48.940: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 14 17:14:48.967: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 14 17:14:48.973: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/14/22 17:14:48.973
Dec 14 17:14:48.973: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:14:48.981: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.674174ms
Dec 14 17:14:50.990: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016516577s
Dec 14 17:14:53.008: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034487458s
Dec 14 17:14:54.989: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015824711s
Dec 14 17:14:56.987: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013783519s
Dec 14 17:14:58.991: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.017191556s
Dec 14 17:14:58.991: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 14 17:14:58.991: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:14:59.003: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.969835ms
Dec 14 17:14:59.003: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 17:14:59.003: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:14:59.008: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277041ms
Dec 14 17:15:01.021: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017490931s
Dec 14 17:15:01.021: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 17:15:01.021: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:15:01.026: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.458819ms
Dec 14 17:15:01.026: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 17:15:01.026: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:15:01.036: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.164814ms
Dec 14 17:15:01.036: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 14 17:15:01.036: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
Dec 14 17:15:01.041: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.595211ms
Dec 14 17:15:01.041: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/14/22 17:15:01.041
Dec 14 17:15:01.061: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec 14 17:15:01.066: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.82658ms
Dec 14 17:15:03.077: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015584898s
Dec 14 17:15:05.083: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021599893s
Dec 14 17:15:07.075: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013504484s
Dec 14 17:15:07.075: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:15:07.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-882" for this suite. 12/14/22 17:15:07.22
------------------------------
• [SLOW TEST] [78.520 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:13:48.708
    Dec 14 17:13:48.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename sched-preemption 12/14/22 17:13:48.725
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:13:48.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:13:48.775
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec 14 17:13:48.809: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 14 17:14:48.850: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 12/14/22 17:14:48.855
    Dec 14 17:14:48.890: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 14 17:14:48.899: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 14 17:14:48.932: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 14 17:14:48.940: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 14 17:14:48.967: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 14 17:14:48.973: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/14/22 17:14:48.973
    Dec 14 17:14:48.973: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:14:48.981: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.674174ms
    Dec 14 17:14:50.990: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016516577s
    Dec 14 17:14:53.008: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034487458s
    Dec 14 17:14:54.989: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015824711s
    Dec 14 17:14:56.987: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013783519s
    Dec 14 17:14:58.991: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.017191556s
    Dec 14 17:14:58.991: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 14 17:14:58.991: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:14:59.003: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.969835ms
    Dec 14 17:14:59.003: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 17:14:59.003: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:14:59.008: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277041ms
    Dec 14 17:15:01.021: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017490931s
    Dec 14 17:15:01.021: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 17:15:01.021: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:15:01.026: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.458819ms
    Dec 14 17:15:01.026: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 17:15:01.026: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:15:01.036: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.164814ms
    Dec 14 17:15:01.036: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 14 17:15:01.036: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-882" to be "running"
    Dec 14 17:15:01.041: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.595211ms
    Dec 14 17:15:01.041: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/14/22 17:15:01.041
    Dec 14 17:15:01.061: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec 14 17:15:01.066: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.82658ms
    Dec 14 17:15:03.077: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015584898s
    Dec 14 17:15:05.083: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021599893s
    Dec 14 17:15:07.075: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013504484s
    Dec 14 17:15:07.075: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:15:07.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-882" for this suite. 12/14/22 17:15:07.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:15:07.235
Dec 14 17:15:07.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename container-probe 12/14/22 17:15:07.238
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:07.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:07.264
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Dec 14 17:15:07.279: INFO: Waiting up to 5m0s for pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d" in namespace "container-probe-8778" to be "running and ready"
Dec 14 17:15:07.285: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046576ms
Dec 14 17:15:07.285: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:15:09.300: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 2.02093206s
Dec 14 17:15:09.300: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:11.295: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 4.016852164s
Dec 14 17:15:11.296: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:13.364: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 6.085371287s
Dec 14 17:15:13.364: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:15.291: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 8.012239014s
Dec 14 17:15:15.291: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:17.305: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 10.026803662s
Dec 14 17:15:17.305: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:19.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 12.013457218s
Dec 14 17:15:19.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:21.305: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 14.025961618s
Dec 14 17:15:21.305: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:23.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 16.01583451s
Dec 14 17:15:23.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:25.290: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 18.011832635s
Dec 14 17:15:25.291: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:27.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 20.015102346s
Dec 14 17:15:27.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:29.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 22.015806122s
Dec 14 17:15:29.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:31.295: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 24.01644407s
Dec 14 17:15:31.295: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:33.309: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 26.030627752s
Dec 14 17:15:33.309: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:35.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 28.013665525s
Dec 14 17:15:35.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:37.296: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 30.017197599s
Dec 14 17:15:37.296: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
Dec 14 17:15:39.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=true. Elapsed: 32.013782844s
Dec 14 17:15:39.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = true)
Dec 14 17:15:39.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d" satisfied condition "running and ready"
Dec 14 17:15:39.297: INFO: Container started at 2022-12-14 17:15:08 +0000 UTC, pod became ready at 2022-12-14 17:15:37 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec 14 17:15:39.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8778" for this suite. 12/14/22 17:15:39.306
------------------------------
• [SLOW TEST] [32.081 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:15:07.235
    Dec 14 17:15:07.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename container-probe 12/14/22 17:15:07.238
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:07.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:07.264
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Dec 14 17:15:07.279: INFO: Waiting up to 5m0s for pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d" in namespace "container-probe-8778" to be "running and ready"
    Dec 14 17:15:07.285: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046576ms
    Dec 14 17:15:07.285: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:15:09.300: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 2.02093206s
    Dec 14 17:15:09.300: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:11.295: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 4.016852164s
    Dec 14 17:15:11.296: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:13.364: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 6.085371287s
    Dec 14 17:15:13.364: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:15.291: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 8.012239014s
    Dec 14 17:15:15.291: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:17.305: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 10.026803662s
    Dec 14 17:15:17.305: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:19.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 12.013457218s
    Dec 14 17:15:19.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:21.305: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 14.025961618s
    Dec 14 17:15:21.305: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:23.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 16.01583451s
    Dec 14 17:15:23.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:25.290: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 18.011832635s
    Dec 14 17:15:25.291: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:27.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 20.015102346s
    Dec 14 17:15:27.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:29.294: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 22.015806122s
    Dec 14 17:15:29.294: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:31.295: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 24.01644407s
    Dec 14 17:15:31.295: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:33.309: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 26.030627752s
    Dec 14 17:15:33.309: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:35.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 28.013665525s
    Dec 14 17:15:35.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:37.296: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=false. Elapsed: 30.017197599s
    Dec 14 17:15:37.296: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = false)
    Dec 14 17:15:39.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d": Phase="Running", Reason="", readiness=true. Elapsed: 32.013782844s
    Dec 14 17:15:39.292: INFO: The phase of Pod test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d is Running (Ready = true)
    Dec 14 17:15:39.292: INFO: Pod "test-webserver-4beb0aac-44c7-4ec0-b7a9-8760d003ec5d" satisfied condition "running and ready"
    Dec 14 17:15:39.297: INFO: Container started at 2022-12-14 17:15:08 +0000 UTC, pod became ready at 2022-12-14 17:15:37 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:15:39.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8778" for this suite. 12/14/22 17:15:39.306
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:15:39.316
Dec 14 17:15:39.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename security-context-test 12/14/22 17:15:39.318
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:39.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:39.349
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Dec 14 17:15:39.363: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419" in namespace "security-context-test-9975" to be "Succeeded or Failed"
Dec 14 17:15:39.371: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Pending", Reason="", readiness=false. Elapsed: 7.582002ms
Dec 14 17:15:41.379: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015859124s
Dec 14 17:15:43.380: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016714791s
Dec 14 17:15:43.380: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419" satisfied condition "Succeeded or Failed"
Dec 14 17:15:43.431: INFO: Got logs for pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec 14 17:15:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9975" for this suite. 12/14/22 17:15:43.439
------------------------------
• [4.134 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:15:39.316
    Dec 14 17:15:39.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename security-context-test 12/14/22 17:15:39.318
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:39.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:39.349
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Dec 14 17:15:39.363: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419" in namespace "security-context-test-9975" to be "Succeeded or Failed"
    Dec 14 17:15:39.371: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Pending", Reason="", readiness=false. Elapsed: 7.582002ms
    Dec 14 17:15:41.379: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015859124s
    Dec 14 17:15:43.380: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016714791s
    Dec 14 17:15:43.380: INFO: Pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419" satisfied condition "Succeeded or Failed"
    Dec 14 17:15:43.431: INFO: Got logs for pod "busybox-privileged-false-e748aecc-2d6f-4a85-98fa-8cc7c0287419": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:15:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9975" for this suite. 12/14/22 17:15:43.439
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:15:43.453
Dec 14 17:15:43.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename daemonsets 12/14/22 17:15:43.455
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:43.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:43.478
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Dec 14 17:15:43.509: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/14/22 17:15:43.517
Dec 14 17:15:43.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:43.524: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/14/22 17:15:43.524
Dec 14 17:15:43.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:43.562: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 17:15:44.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:44.569: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 17:15:45.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 17:15:45.570: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/14/22 17:15:45.584
Dec 14 17:15:45.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 17:15:45.618: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec 14 17:15:46.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:46.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/14/22 17:15:46.629
Dec 14 17:15:46.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:46.650: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 17:15:47.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:47.656: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 17:15:48.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:48.656: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
Dec 14 17:15:49.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 17:15:49.657: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/14/22 17:15:49.664
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1390, will wait for the garbage collector to delete the pods 12/14/22 17:15:49.665
Dec 14 17:15:49.729: INFO: Deleting DaemonSet.extensions daemon-set took: 9.839925ms
Dec 14 17:15:49.830: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.093264ms
Dec 14 17:15:52.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:52.537: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 17:15:52.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35398"},"items":null}

Dec 14 17:15:52.545: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35398"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:15:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1390" for this suite. 12/14/22 17:15:52.593
------------------------------
• [SLOW TEST] [9.152 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:15:43.453
    Dec 14 17:15:43.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename daemonsets 12/14/22 17:15:43.455
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:43.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:43.478
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Dec 14 17:15:43.509: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/14/22 17:15:43.517
    Dec 14 17:15:43.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:43.524: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/14/22 17:15:43.524
    Dec 14 17:15:43.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:43.562: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 17:15:44.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:44.569: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 17:15:45.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 17:15:45.570: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/14/22 17:15:45.584
    Dec 14 17:15:45.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 17:15:45.618: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Dec 14 17:15:46.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:46.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/14/22 17:15:46.629
    Dec 14 17:15:46.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:46.650: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 17:15:47.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:47.656: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 17:15:48.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:48.656: INFO: Node iet9eich7uhu-2 is running 0 daemon pod, expected 1
    Dec 14 17:15:49.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 14 17:15:49.657: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/14/22 17:15:49.664
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1390, will wait for the garbage collector to delete the pods 12/14/22 17:15:49.665
    Dec 14 17:15:49.729: INFO: Deleting DaemonSet.extensions daemon-set took: 9.839925ms
    Dec 14 17:15:49.830: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.093264ms
    Dec 14 17:15:52.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 14 17:15:52.537: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 14 17:15:52.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35398"},"items":null}

    Dec 14 17:15:52.545: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35398"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:15:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1390" for this suite. 12/14/22 17:15:52.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:15:52.61
Dec 14 17:15:52.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename webhook 12/14/22 17:15:52.614
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:52.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:52.647
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/14/22 17:15:52.67
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:15:53.483
STEP: Deploying the webhook pod 12/14/22 17:15:53.5
STEP: Wait for the deployment to be ready 12/14/22 17:15:53.515
Dec 14 17:15:53.524: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 17:15:55.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/14/22 17:15:57.554
STEP: Verifying the service has paired with the endpoint 12/14/22 17:15:57.58
Dec 14 17:15:58.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/14/22 17:15:58.59
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:15:58.59
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/14/22 17:15:58.615
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/14/22 17:15:59.634
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:15:59.635
STEP: Having no error when timeout is longer than webhook latency 12/14/22 17:16:00.697
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:16:00.697
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/14/22 17:16:05.74
STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:16:05.74
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec 14 17:16:10.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6662" for this suite. 12/14/22 17:16:10.902
STEP: Destroying namespace "webhook-6662-markers" for this suite. 12/14/22 17:16:10.926
------------------------------
• [SLOW TEST] [18.337 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:15:52.61
    Dec 14 17:15:52.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename webhook 12/14/22 17:15:52.614
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:15:52.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:15:52.647
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/14/22 17:15:52.67
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/14/22 17:15:53.483
    STEP: Deploying the webhook pod 12/14/22 17:15:53.5
    STEP: Wait for the deployment to be ready 12/14/22 17:15:53.515
    Dec 14 17:15:53.524: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Dec 14 17:15:55.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 15, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/14/22 17:15:57.554
    STEP: Verifying the service has paired with the endpoint 12/14/22 17:15:57.58
    Dec 14 17:15:58.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/14/22 17:15:58.59
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:15:58.59
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/14/22 17:15:58.615
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/14/22 17:15:59.634
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:15:59.635
    STEP: Having no error when timeout is longer than webhook latency 12/14/22 17:16:00.697
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:16:00.697
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/14/22 17:16:05.74
    STEP: Registering slow webhook via the AdmissionRegistration API 12/14/22 17:16:05.74
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:16:10.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6662" for this suite. 12/14/22 17:16:10.902
    STEP: Destroying namespace "webhook-6662-markers" for this suite. 12/14/22 17:16:10.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/14/22 17:16:10.99
Dec 14 17:16:10.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
STEP: Building a namespace api object, basename pod-network-test 12/14/22 17:16:10.995
STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:16:11.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:16:11.069
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4979 12/14/22 17:16:11.073
STEP: creating a selector 12/14/22 17:16:11.073
STEP: Creating the service pods in kubernetes 12/14/22 17:16:11.073
Dec 14 17:16:11.073: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 17:16:11.200: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4979" to be "running and ready"
Dec 14 17:16:11.219: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.10892ms
Dec 14 17:16:11.219: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:16:13.226: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025932921s
Dec 14 17:16:13.226: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 17:16:15.229: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028544245s
Dec 14 17:16:15.229: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 17:16:17.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032623827s
Dec 14 17:16:17.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 17:16:19.228: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027553577s
Dec 14 17:16:19.228: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 17:16:21.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037651143s
Dec 14 17:16:21.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 14 17:16:23.228: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.027866736s
Dec 14 17:16:23.228: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 14 17:16:23.228: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 14 17:16:23.234: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4979" to be "running and ready"
Dec 14 17:16:23.239: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.064238ms
Dec 14 17:16:23.239: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 14 17:16:23.239: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 14 17:16:23.243: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4979" to be "running and ready"
Dec 14 17:16:23.247: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 3.837808ms
Dec 14 17:16:23.247: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Dec 14 17:16:25.255: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011689233s
Dec 14 17:16:25.255: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Dec 14 17:16:27.257: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.013990846s
Dec 14 17:16:27.258: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Dec 14 17:16:29.255: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.011424693s
Dec 14 17:16:29.255: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Dec 14 17:16:31.258: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.014888479s
Dec 14 17:16:31.259: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Dec 14 17:16:33.257: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013821784s
Dec 14 17:16:33.257: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 14 17:16:33.258: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 12/14/22 17:16:33.263
Dec 14 17:16:33.276: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4979" to be "running"
Dec 14 17:16:33.283: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.073326ms
Dec 14 17:16:35.292: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016118487s
Dec 14 17:16:35.293: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 14 17:16:35.301: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 17:16:35.301: INFO: Breadth first check of 10.233.64.167 on host 192.168.121.21...
Dec 14 17:16:35.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.64.167&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:16:35.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:16:35.310: INFO: ExecWithOptions: Clientset creation
Dec 14 17:16:35.310: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.167%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 17:16:35.436: INFO: Waiting for responses: map[]
Dec 14 17:16:35.436: INFO: reached 10.233.64.167 after 0/1 tries
Dec 14 17:16:35.436: INFO: Breadth first check of 10.233.66.165 on host 192.168.121.16...
Dec 14 17:16:35.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.66.165&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:16:35.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:16:35.443: INFO: ExecWithOptions: Clientset creation
Dec 14 17:16:35.444: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.165%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 17:16:35.540: INFO: Waiting for responses: map[]
Dec 14 17:16:35.540: INFO: reached 10.233.66.165 after 0/1 tries
Dec 14 17:16:35.540: INFO: Breadth first check of 10.233.67.195 on host 192.168.121.56...
Dec 14 17:16:35.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.67.195&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:16:35.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
Dec 14 17:16:35.550: INFO: ExecWithOptions: Clientset creation
Dec 14 17:16:35.550: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.67.195%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 17:16:35.659: INFO: Waiting for responses: map[]
Dec 14 17:16:35.659: INFO: reached 10.233.67.195 after 0/1 tries
Dec 14 17:16:35.659: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec 14 17:16:35.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4979" for this suite. 12/14/22 17:16:35.668
------------------------------
• [SLOW TEST] [24.685 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/14/22 17:16:10.99
    Dec 14 17:16:10.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    STEP: Building a namespace api object, basename pod-network-test 12/14/22 17:16:10.995
    STEP: Waiting for a default service account to be provisioned in namespace 12/14/22 17:16:11.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/14/22 17:16:11.069
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4979 12/14/22 17:16:11.073
    STEP: creating a selector 12/14/22 17:16:11.073
    STEP: Creating the service pods in kubernetes 12/14/22 17:16:11.073
    Dec 14 17:16:11.073: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 14 17:16:11.200: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4979" to be "running and ready"
    Dec 14 17:16:11.219: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.10892ms
    Dec 14 17:16:11.219: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 14 17:16:13.226: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025932921s
    Dec 14 17:16:13.226: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 17:16:15.229: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028544245s
    Dec 14 17:16:15.229: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 17:16:17.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032623827s
    Dec 14 17:16:17.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 17:16:19.228: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027553577s
    Dec 14 17:16:19.228: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 17:16:21.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037651143s
    Dec 14 17:16:21.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 14 17:16:23.228: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.027866736s
    Dec 14 17:16:23.228: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 14 17:16:23.228: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 14 17:16:23.234: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4979" to be "running and ready"
    Dec 14 17:16:23.239: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.064238ms
    Dec 14 17:16:23.239: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 14 17:16:23.239: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 14 17:16:23.243: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4979" to be "running and ready"
    Dec 14 17:16:23.247: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 3.837808ms
    Dec 14 17:16:23.247: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Dec 14 17:16:25.255: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011689233s
    Dec 14 17:16:25.255: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Dec 14 17:16:27.257: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.013990846s
    Dec 14 17:16:27.258: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Dec 14 17:16:29.255: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.011424693s
    Dec 14 17:16:29.255: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Dec 14 17:16:31.258: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.014888479s
    Dec 14 17:16:31.259: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Dec 14 17:16:33.257: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013821784s
    Dec 14 17:16:33.257: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 14 17:16:33.258: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 12/14/22 17:16:33.263
    Dec 14 17:16:33.276: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4979" to be "running"
    Dec 14 17:16:33.283: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.073326ms
    Dec 14 17:16:35.292: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016118487s
    Dec 14 17:16:35.293: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 14 17:16:35.301: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Dec 14 17:16:35.301: INFO: Breadth first check of 10.233.64.167 on host 192.168.121.21...
    Dec 14 17:16:35.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.64.167&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:16:35.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:16:35.310: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:16:35.310: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.167%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 17:16:35.436: INFO: Waiting for responses: map[]
    Dec 14 17:16:35.436: INFO: reached 10.233.64.167 after 0/1 tries
    Dec 14 17:16:35.436: INFO: Breadth first check of 10.233.66.165 on host 192.168.121.16...
    Dec 14 17:16:35.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.66.165&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:16:35.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:16:35.443: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:16:35.444: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.165%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 17:16:35.540: INFO: Waiting for responses: map[]
    Dec 14 17:16:35.540: INFO: reached 10.233.66.165 after 0/1 tries
    Dec 14 17:16:35.540: INFO: Breadth first check of 10.233.67.195 on host 192.168.121.56...
    Dec 14 17:16:35.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.67.196:9080/dial?request=hostname&protocol=udp&host=10.233.67.195&port=8081&tries=1'] Namespace:pod-network-test-4979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 14 17:16:35.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1642710381
    Dec 14 17:16:35.550: INFO: ExecWithOptions: Clientset creation
    Dec 14 17:16:35.550: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.67.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.67.195%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 14 17:16:35.659: INFO: Waiting for responses: map[]
    Dec 14 17:16:35.659: INFO: reached 10.233.67.195 after 0/1 tries
    Dec 14 17:16:35.659: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec 14 17:16:35.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4979" for this suite. 12/14/22 17:16:35.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Dec 14 17:16:35.689: INFO: Running AfterSuite actions on node 1
Dec 14 17:16:35.689: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Dec 14 17:16:35.689: INFO: Running AfterSuite actions on node 1
    Dec 14 17:16:35.689: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.148 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6013.416 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h40m14.332216863s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

